[[["automatic detection of generated text is easiest when humans are fooled", "daphne ippolito | daniel duckworth | chris callison-burch | douglas eck", "recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. the capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies\u2014top-_k_, nucleus sampling, and untruncated random sampling\u2014and show that improvements in decoding methods have primarily optimized for fooling humans. this comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. we also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems."], "generation"], [["parallel data augmentation for formality style transfer", "yi zhang | tao ge | xu sun", "the main barrier to progress in the task of formality style transfer is the inadequacy of training data. in this paper, we study how to augment parallel data and propose novel and simple data augmentation methods for this task to obtain useful sentence pairs with easily accessible models and systems. experiments demonstrate that our augmented parallel data largely helps improve formality style transfer when it is used to pre-train the model, leading to the state-of-the-art results in the gyafc benchmark dataset."], "sentiment analysis, stylistic analysis, and argument mining"], [["low-resource deep entity resolution with transfer and active learning", "jungo kasai | kun qian | sairam gurajada | yunyao li | lucian popa", "entity resolution (er) is the task of identifying different representations of the same real-world entities across databases. it is a key step for knowledge base creation and text mining. recent adaptation of deep learning methods for er mitigates the need for dataset-specific feature engineering by constructing distributed representations of entity records. while these methods achieve state-of-the-art performance over benchmark data, they require large amounts of labeled data, which are typically unavailable in realistic er applications. in this paper, we develop a deep learning-based method that targets low-resource settings for er through a novel combination of transfer learning and active learning. we design an architecture that allows us to learn a transferable model from a high-resource setting to a low-resource one. to further adapt to the target dataset, we incorporate active learning that carefully selects a few informative examples to fine-tune the transferred model. empirical evaluation demonstrates that our method achieves comparable, if not better, performance compared to state-of-the-art learning-based methods while using an order of magnitude fewer labels."], "information extraction, retrieval and text mining"], [["beyond accuracy: behavioral testing of nlp models with checklist", "marco tulio ribeiro | tongshuang wu | carlos guestrin | sameer singh", "although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of nlp models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. inspired by principles of behavioral testing in software engineering, we introduce checklist, a task-agnostic methodology for testing nlp models. checklist includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. we illustrate the utility of checklist with tests for three tasks, identifying critical failures in both commercial and state-of-art models. in a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. in another user study, nlp practitioners with checklist created twice as many tests, and found almost three times as many bugs as users without it."], "resources and evaluation"], [["optimizing deeper transformers on small datasets", "peng xu | dhruv kumar | wei yang | wenjie zi | keyi tang | chenyang huang | jackie chi kit cheung | simon j.d. prince | yanshuai cao", "it is a common belief that training deep transformers from scratch requires large datasets. consequently, for small datasets, people usually use shallow and simple additional layers on top of pre-trained models during fine-tuning. this work shows that this does not always need to be the case: with proper initialization and optimization, the benefits of very deep transformers can carry over to challenging tasks with small datasets, including text-to-sql semantic parsing and logical reading comprehension. in particular, we successfully train 48 layers of transformers, comprising 24 fine-tuned layers from pre-trained roberta and 24 relation-aware layers trained from scratch. with fewer training steps and no task-specific pre-training, we obtain the state of the art performance on the challenging cross-domain text-to-sql parsing benchmark spider. we achieve this by deriving a novel data dependent transformer fixed-update initialization scheme (dt-fixup), inspired by the prior t-fixup work. further error analysis shows that increasing depth can help improve generalization on small datasets for hard cases that require reasoning and structural understanding."], "machine learning for nlp"], [["implicit representations of meaning in neural language models", "belinda z. li | maxwell nye | jacob andreas", "does the effectiveness of neural language models derive entirely from accurate modeling of surface word co-occurrence statistics, or do these models represent and reason about the world they describe? in bart and t5 transformer language models, we identify contextual word representations that function as *models of entities and situations* as they evolve throughout a discourse. these neural representations have functional similarities to linguistic models of dynamic semantics: they support a linear readout of each entity\u2019s current properties and relations, and can be manipulated with predictable effects on language generation. our results indicate that prediction in pretrained neural language models is supported, at least in part, by dynamic representations of meaning and implicit simulation of entity state, and that this behavior can be learned with only text as training data."], "interpretability and analysis of models for nlp"], [["adansp: uncertainty-driven adaptive decoding in neural semantic parsing", "xiang zhang | shizhu he | kang liu | jun zhao", "neural semantic parsers utilize the encoder-decoder framework to learn an end-to-end model for semantic parsing that transduces a natural language sentence to the formal semantic representation. to keep the model aware of the underlying grammar in target sequences, many constrained decoders were devised in a multi-stage paradigm, which decode to the sketches or abstract syntax trees first, and then decode to target semantic tokens. we instead to propose an adaptive decoding method to avoid such intermediate representations. the decoder is guided by model uncertainty and automatically uses deeper computations when necessary. thus it can predict tokens adaptively. our model outperforms the state-of-the-art neural models and does not need any expertise like predefined grammar or sketches in the meantime."], "question answering"], [["towards fine-grained text sentiment transfer", "fuli luo | peng li | pengcheng yang | jie zhou | yutong tan | baobao chang | zhifang sui | xu sun", "in this paper, we focus on the task of fine-grained text sentiment transfer (fgst). this task aims to revise an input sequence to satisfy a given sentiment intensity, while preserving the original semantic content. different from the conventional sentiment transfer task that only reverses the sentiment polarity (positive/negative) of text, the ftst task requires more nuanced and fine-grained control of sentiment. to remedy this, we propose a novel seq2sentiseq model. specifically, the numeric sentiment intensity value is incorporated into the decoder via a gaussian kernel layer to finely control the sentiment intensity of the output. moreover, to tackle the problem of lacking parallel data, we propose a cycle reinforcement learning algorithm to guide the model training. in this framework, the elaborately designed rewards can balance both sentiment transformation and content preservation, while not requiring any ground truth output. experimental results show that our approach can outperform existing methods by a large margin in both automatic evaluation and human evaluation."], "generation"], [["controllable paraphrase generation with a syntactic exemplar", "mingda chen | qingming tang | sam wiseman | kevin gimpel", "prior work on controllable text generation usually assumes that the controlled attribute can take on one of a small set of values known a priori. in this work, we propose a novel task, where the syntax of a generated sentence is controlled rather by a sentential exemplar. to evaluate quantitatively with standard metrics, we create a novel dataset with human annotations. we also develop a variational model with a neural module specifically designed for capturing syntactic knowledge and several multitask training objectives to promote disentangled representation learning. empirically, the proposed model is observed to achieve improvements over baselines and learn to capture desirable characteristics."], "generation"], [["inflecting when there\u2019s no majority: limitations of encoder-decoder neural networks as cognitive models for german plurals", "kate mccurdy | sharon goldwater | adam lopez", "can artificial neural networks learn to represent inflectional morphology and generalize to new words as human speakers do? kirov and cotterell (2018) argue that the answer is yes: modern encoder-decoder (ed) architectures learn human-like behavior when inflecting english verbs, such as extending the regular past tense form /-(e)d/ to novel words. however, their work does not address the criticism raised by marcus et al. (1995): that neural models may learn to extend not the regular, but the most frequent class \u2014 and thus fail on tasks like german number inflection, where infrequent suffixes like /-s/ can still be productively generalized. to investigate this question, we first collect a new dataset from german speakers (production and ratings of plural forms for novel nouns) that is designed to avoid sources of information unavailable to the ed model. the speaker data show high variability, and two suffixes evince \u2018regular\u2019 behavior, appearing more often with phonologically atypical inputs. encoder-decoder models do generalize the most frequently produced plural class, but do not show human-like variability or \u2018regular\u2019 extension of these other plural markers. we conclude that modern neural models may still struggle with minority-class generalization."], "linguistic theories, cognitive modeling and psycholinguistics"], [["cross-language sentence selection via data augmentation and rationale training", "yanda chen | chris kedzie | suraj nair | petra galuscakova | rui zhang | douglas oard | kathleen mckeown", "this paper proposes an approach to cross-language sentence selection in a low-resource setting. it uses data augmentation and negative sampling techniques on noisy parallel sentence data to directly learn a cross-lingual embedding-based query relevance model. results show that this approach performs as well as or better than multiple state-of-the-art machine translation + monolingual retrieval systems trained on the same parallel data. moreover, when a rationale training secondary objective is applied to encourage the model to match word alignment hints from a phrase-based statistical machine translation model, consistent improvements are seen across three language pairs (english-somali, english-swahili and english-tagalog) over a variety of state-of-the-art baselines."], "information extraction, retrieval and text mining"], [["roles and utilization of attention heads in transformer-based neural language models", "jae-young jo | sung-hyon myaeng", "sentence encoders based on the transformer architecture have shown promising results on various natural language tasks. the main impetus lies in the pre-trained neural language models that capture long-range dependencies among words, owing to multi-head attention that is unique in the architecture. however, little is known for how linguistic properties are processed, represented, and utilized for downstream tasks among hundreds of attention heads inside the pre-trained transformer-based model. for the initial goal of examining the roles of attention heads in handling a set of linguistic features, we conducted a set of experiments with ten probing tasks and three downstream tasks on four pre-trained transformer families (gpt, gpt2, bert, and electra). meaningful insights are shown through the lens of heat map visualization and utilized to propose a relatively simple sentence representation method that takes advantage of most influential attention heads, resulting in additional performance improvements on the downstream tasks."], "interpretability and analysis of models for nlp"], [["towards transparent and explainable attention models", "akash kumar mohankumar | preksha nema | sharan narasimhan | mitesh m. khapra | balaji vasan srinivasan | balaraman ravindran", "recent studies on interpretability of attention distributions have led to notions of faithful and plausible explanations for a model\u2019s predictions. attention distributions can be considered a faithful explanation if a higher attention weight implies a greater impact on the model\u2019s prediction. they can be considered a plausible explanation if they provide a human-understandable justification for the model\u2019s predictions. in this work, we first explain why current attention mechanisms in lstm based encoders can neither provide a faithful nor a plausible explanation of the model\u2019s predictions. we observe that in lstm based encoders the hidden representations at different time-steps are very similar to each other (high conicity) and attention weights in these situations do not carry much meaning because even a random permutation of the attention weights does not affect the model\u2019s predictions. based on experiments on a wide variety of tasks and datasets, we observe attention distributions often attribute the model\u2019s predictions to unimportant words such as punctuation and fail to offer a plausible explanation for the predictions. to make attention mechanisms more faithful and plausible, we propose a modified lstm cell with a diversity-driven training objective that ensures that the hidden representations learned at different time steps are diverse. we show that the resulting attention distributions offer more transparency as they (i) provide a more precise importance ranking of the hidden states (ii) are better indicative of words important for the model\u2019s predictions (iii) correlate better with gradient-based attribution methods. human evaluations indicate that the attention distributions learned by our model offer a plausible explanation of the model\u2019s predictions. our code has been made publicly available at https://github.com/akashkm99/interpretable-attention"], "interpretability and analysis of models for nlp"], [["inferential machine comprehension: answering questions by recursively deducing the evidence chain from text", "jianxing yu | zhengjun zha | jian yin", "this paper focuses on the topic of inferential machine comprehension, which aims to fully understand the meanings of given text to answer generic questions, especially the ones needed reasoning skills. in particular, we first encode the given document, question and options in a context aware way. we then propose a new network to solve the inference problem by decomposing it into a series of attention-based reasoning steps. the result of the previous step acts as the context of next step. to make each step can be directly inferred from the text, we design an operational cell with prior structure. by recursively linking the cells, the inferred results are synthesized together to form the evidence chain for reasoning, where the reasoning direction can be guided by imposing structural constraints to regulate interactions on the cells. moreover, a termination mechanism is introduced to dynamically determine the uncertain reasoning depth, and the network is trained by reinforcement learning. experimental results on 3 popular data sets, including mctest, race and multirc, demonstrate the effectiveness of our approach."], "question answering"], [["implicit discourse relation identification for open-domain dialogues", "mingyu derek ma | kevin bowden | jiaqi wu | wen cui | marilyn walker", "discourse relation identification has been an active area of research for many years, and the challenge of identifying implicit relations remains largely an unsolved task, especially in the context of an open-domain dialogue system. previous work primarily relies on a corpora of formal text which is inherently non-dialogic, i.e., news and journals. this data however is not suitable to handle the nuances of informal dialogue nor is it capable of navigating the plethora of valid topics present in open-domain dialogue. in this paper, we designed a novel discourse relation identification pipeline specifically tuned for open-domain dialogue systems. we firstly propose a method to automatically extract the implicit discourse relation argument pairs and labels from a dataset of dialogic turns, resulting in a novel corpus of discourse relation pairs; the first of its kind to attempt to identify the discourse relations connecting the dialogic turns in open-domain discourse. moreover, we have taken the first steps to leverage the dialogue features unique to our task to further improve the identification of such relations by performing feature ablation and incorporating dialogue features to enhance the state-of-the-art model."], "discourse and pragmatics"], [["a wind of change: detecting and evaluating lexical semantic change across times and domains", "dominik schlechtweg | anna h\u00e4tty | marco del tredici | sabine schulte im walde", "we perform an interdisciplinary large-scale evaluation for detecting lexical semantic divergences in a diachronic and in a synchronic task: semantic sense changes across time, and semantic sense changes across domains. our work addresses the superficialness and lack of comparison in assessing models of diachronic lexical change, by bringing together and extending benchmark models on a common state-of-the-art evaluation task. in addition, we demonstrate that the same evaluation task and modelling approaches can successfully be utilised for the synchronic detection of domain-specific sense divergences in the field of term extraction."], "resources and evaluation"], [["better oov translation with bilingual terminology mining", "matthias huck | viktor hangya | alexander fraser", "unseen words, also called out-of-vocabulary words (oovs), are difficult for machine translation. in neural machine translation, byte-pair encoding can be used to represent oovs, but they are still often incorrectly translated. we improve the translation of oovs in nmt using easy-to-obtain monolingual data. we look for oovs in the text to be translated and translate them using simple-to-construct bilingual word embeddings (bwes). in our mt experiments we take the 5-best candidates, which is motivated by intrinsic mining experiments. using all five of the proposed target language words as queries we mine target-language sentences. we then back-translate, forcing the back-translation of each of the five proposed target-language oov-translation-candidates to be the original source-language oov. we show that by using this synthetic data to fine-tune our system the translation of oovs can be dramatically improved. in our experiments we use a system trained on europarl and mine sentences containing medical terms from monolingual data."], "machine translation and multilinguality"], [["open-domain why-question answering with adversarial learning to encode answer texts", "jong-hoon oh | kazuma kadowaki | julien kloetzer | ryu iida | kentaro torisawa", "in this paper, we propose a method for why-question answering (why-qa) that uses an adversarial learning framework. existing why-qa methods retrieve \u201canswer passages\u201d that usually consist of several sentences. these multi-sentence passages contain not only the reason sought by a why-question and its connection to the why-question, but also redundant and/or unrelated parts. we use our proposed \u201cadversarial networks for generating compact-answer representation\u201d (agr) to generate from a passage a vector representation of the non-redundant reason sought by a why-question and exploit the representation for judging whether the passage actually answers the why-question. through a series of experiments using japanese why-qa datasets, we show that these representations improve the performance of our why-qa neural model as well as that of a bert-based why-qa model. we show that they also improve a state-of-the-art distantly supervised open-domain qa (ds-qa) method on publicly available english datasets, even though the target task is not a why-qa."], "question answering"], [["towards debiasing sentence representations", "paul pu liang | irene mengze li | emily zheng | yao chong lim | ruslan salakhutdinov | louis-philippe morency", "as natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. while some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as elmo and bert. in this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, sent-debias, to reduce these biases. we show that sent-debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. we hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer nlp."], "ethics in nlp"], [["make up your mind! adversarial generation of inconsistent natural language explanations", "oana-maria camburu | brendan shillingford | pasquale minervini | thomas lukasiewicz | phil blunsom", "to increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions. in this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as \u201dbecause there is a dog in the image.\u201d and \u201dbecause there is no dog in the [same] image.\u201d, exposing flaws in either the decision-making process of the model or in the generation of the explanations. we introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations. moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks. finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions. our framework shows that this model is capable of generating a significant number of inconsistent explanations."], "interpretability and analysis of models for nlp"], [["automatic poetry generation from prosaic text", "tim van de cruys", "in the last few years, a number of successful approaches have emerged that are able to adequately model various aspects of natural language. in particular, language models based on neural networks have improved the state of the art with regard to predictive language modeling, while topic models are successful at capturing clear-cut, semantic dimensions. in this paper, we will explore how these approaches can be adapted and combined to model the linguistic and literary aspects needed for poetry generation. the system is exclusively trained on standard, non-poetic text, and its output is constrained in order to confer a poetic character to the generated verse. the framework is applied to the generation of poems in both english and french, and is equally evaluated for both languages. even though it only uses standard, non-poetic text as input, the system yields state of the art results for poetry generation."], "generation"], [["coreference resolution with entity equalization", "ben kantor | amir globerson", "a key challenge in coreference resolution is to capture properties of entity clusters, and use those in the resolution process. here we provide a simple and effective approach for achieving this, via an \u201centity equalization\u201d mechanism. the equalization approach represents each mention in a cluster via an approximation of the sum of all mentions in the cluster. we show how this can be done in a fully differentiable end-to-end manner, thus enabling high-order inferences in the resolution process. our approach, which also employs bert embeddings, results in new state-of-the-art results on the conll-2012 coreference resolution task, improving average f1 by 3.6%."], "discourse and pragmatics"], [["modelling context and syntactical features for aspect-based sentiment analysis", "minh hieu phan | philip o. ogunbona", "the aspect-based sentiment analysis (absa) consists of two conceptual tasks, namely an aspect extraction and an aspect sentiment classification. rather than considering the tasks separately, we build an end-to-end absa solution. previous works in absa tasks did not fully leverage the importance of syntactical information. hence, the aspect extraction model often failed to detect the boundaries of multi-word aspect terms. on the other hand, the aspect sentiment classifier was unable to account for the syntactical correlation between aspect terms and the context words. this paper explores the grammatical aspect of the sentence and employs the self-attention mechanism for syntactical learning. we combine part-of-speech embeddings, dependency-based embeddings and contextualized embeddings (e.g. bert, roberta) to enhance the performance of the aspect extractor. we also propose the syntactic relative distance to de-emphasize the adverse effects of unrelated words, having weak syntactic connection with the aspect terms. this increases the accuracy of the aspect sentiment classifier. our solutions outperform the state-of-the-art models on semeval-2014 dataset in both two subtasks."], "sentiment analysis, stylistic analysis, and argument mining"], [["contextualized sparse representations for real-time open-domain question answering", "jinhyuk lee | minjoon seo | hannaneh hajishirzi | jaewoo kang", "open-domain question answering can be formulated as a phrase retrieval problem, in which we can expect huge scalability and speed benefit but often suffer from low accuracy due to the limitation of existing phrase representation models. in this paper, we aim to improve the quality of each phrase embedding by augmenting it with a contextualized sparse representation (sparc). unlike previous sparse vectors that are term-frequency-based (e.g., tf-idf) or directly learned (only few thousand dimensions), we leverage rectified self-attention to indirectly learn sparse vectors in n-gram vocabulary space. by augmenting the previous phrase retrieval model (seo et al., 2019) with sparc, we show 4%+ improvement in curatedtrec and squad-open. our curatedtrec score is even better than the best known retrieve & read model with at least 45x faster inference speed."], "question answering"], [["logical natural language generation from open-domain tables", "wenhu chen | jianshu chen | yu su | zhiyu chen | william yang wang", "neural natural language generation (nlg) models have recently shown remarkable progress in fluency and coherence. however, existing studies on neural nlg are primarily focused on surface-level realizations with limited emphasis on logical inference, an important aspect of human thinking and language. in this paper, we suggest a new nlg task where a model is tasked with generating natural language statements that can be logically entailed by the facts in an open-domain semi-structured table. to facilitate the study of the proposed logical nlg problem, we use the existing tabfact dataset~(citation) featured with a wide range of logical/symbolic inferences as our testbed, and propose new automatic metrics to evaluate the fidelity of generation models w.r.t. logical inference. the new task poses challenges to the existing monotonic generation frameworks due to the mismatch between sequence order and logical order. in our experiments, we comprehensively survey different generation architectures (lstm, transformer, pre-trained lm) trained with different algorithms (rl, adversarial training, coarse-to-fine) on the dataset and made following observations: 1) pre-trained lm can significantly boost both the fluency and logical fidelity metrics, 2) rl and adversarial training are trading fluency for fidelity, 3) coarse-to-fine generation can help partially alleviate the fidelity issue while maintaining high language fluency. the code and data are available at https://github.com/wenhuchen/logicnlg."], "generation"], [["knowledge graph-augmented abstractive summarization with semantic-driven cloze reward", "luyang huang | lingfei wu | lu wang", "sequence-to-sequence models for abstractive summarization have been studied extensively, yet the generated summaries commonly suffer from fabricated content, and are often found to be near-extractive. we argue that, to address these issues, the summarizer should acquire semantic interpretation over input, e.g., via structured representation, to allow the generation of more informative summaries. in this paper, we present asgard, a novel framework for abstractive summarization with graph-augmentation and semantic-driven reward. we propose the use of dual encoders\u2014a sequential document encoder and a graph-structured encoder\u2014to maintain the global context and local characteristics of entities, complementing each other. we further design a reward based on a multiple choice cloze test to drive the model to better capture entity interactions. results show that our models produce significantly higher rouge scores than a variant without knowledge graph as input on both new york times and cnn/daily mail datasets. we also obtain better or comparable performance compared to systems that are fine-tuned from large pretrained language models. human judges further rate our model outputs as more informative and containing fewer unfaithful errors."], "summarization"], [["learning transferable feature representations using neural networks", "himanshu sharad bhatt | shourya roy | arun rajkumar | sriranjani ramakrishnan", "learning representations such that the source and target distributions appear as similar as possible has benefited transfer learning tasks across several applications. generally it requires labeled data from the source and only unlabeled data from the target to learn such representations. while these representations act like a bridge to transfer knowledge learned in the source to the target; they may lead to negative transfer when the source specific characteristics detract their ability to represent the target data. we present a novel neural network architecture to simultaneously learn a two-part representation which is based on the principle of segregating source specific representation from the common representation. the first part captures the source specific characteristics while the second part captures the truly common representation. our architecture optimizes an objective function which acts adversarial for the source specific part if it contributes towards the cross-domain learning. we empirically show that two parts of the representation, in different arrangements, outperforms existing learning algorithms on the source learning as well as cross-domain tasks on multiple datasets."], "machine learning for nlp"], [["meaning to form: measuring systematicity as information", "tiago pimentel | arya d. mccarthy | damian blasi | brian roark | ryan cotterell", "a longstanding debate in semiotics centers on the relationship between linguistic signs and their corresponding semantics: is there an arbitrary relationship between a word form and its meaning, or does some systematic phenomenon pervade? for instance, does the character bigram \u2018gl\u2019 have any systematic relationship to the meaning of words like \u2018glisten\u2019, \u2018gleam\u2019 and \u2018glow\u2019? in this work, we offer a holistic quantification of the systematicity of the sign using mutual information and recurrent neural networks. we employ these in a data-driven and massively multilingual approach to the question, examining 106 languages. we find a statistically significant reduction in entropy when modeling a word form conditioned on its semantic representation. encouragingly, we also recover well-attested english examples of systematic affixes. we conclude with the meta-point: our approximate effect size (measured in bits) is quite small\u2014despite some amount of systematicity between form and meaning, an arbitrary relationship and its resulting benefits dominate human language."], "machine translation and multilinguality"], [["gcdt: a global context enhanced deep transition architecture for sequence labeling", "yijin liu | fandong meng | jinchao zhang | jinan xu | yufeng chen | jie zhou", "current state-of-the-art systems for sequence labeling are typically based on the family of recurrent neural networks (rnns). however, the shallow connections between consecutive hidden states of rnns and insufficient modeling of global information restrict the potential performance of those models. in this paper, we try to address these issues, and thus propose a global context enhanced deep transition architecture for sequence labeling named gcdt. we deepen the state transition path at each position in a sentence, and further assign every token with a global representation learned from the entire sentence. experiments on two standard sequence labeling tasks show that, given only training data and the ubiquitous word embeddings (glove), our gcdt achieves 91.96 f1 on the conll03 ner task and 95.43 f1 on the conll2000 chunking task, which outperforms the best reported results under the same settings. furthermore, by leveraging bert as an additional resource, we establish new state-of-the-art results with 93.47 f1 on ner and 97.30 f1 on chunking."], "tagging, chunking, syntax and parsing"], [["recursive template-based frame generation for task oriented dialog", "rashmi gangadharaiah | balakrishnan narayanaswamy", "the natural language understanding (nlu) component in task oriented dialog systems processes a user\u2019s request and converts it into structured information that can be consumed by downstream components such as the dialog state tracker (dst). this information is typically represented as a semantic frame that captures the intent and slot-labels provided by the user. we first show that such a shallow representation is insufficient for complex dialog scenarios, because it does not capture the recursive nature inherent in many domains. we propose a recursive, hierarchical frame-based representation and show how to learn it from data. we formulate the frame generation task as a template-based tree decoding task, where the decoder recursively generates a template and then fills slot values into the template. we extend local tree-based loss functions with terms that provide global supervision and show how to optimize them end-to-end. we achieve a small improvement on the widely used atis dataset and a much larger improvement on a more complex dataset we describe here."], "dialogue and interactive systems"], [["embedding imputation with grounded language information", "ziyi yang | chenguang zhu | vin sachidananda | eric darve", "due to the ubiquitous use of embeddings as input representations for a wide range of natural language tasks, imputation of embeddings for rare and unseen words is a critical problem in language processing. embedding imputation involves learning representations for rare or unseen words during the training of an embedding model, often in a post-hoc manner. in this paper, we propose an approach for embedding imputation which uses grounded information in the form of a knowledge graph. this is in contrast to existing approaches which typically make use of vector space properties or subword information. we propose an online method to construct a graph from grounded information and design an algorithm to map from the resulting graphical structure to the space of the pre-trained embeddings. finally, we evaluate our approach on a range of rare and unseen word tasks across various domains and show that our model can learn better representations. for example, on the card-660 task our method improves pearson\u2019s and spearman\u2019s correlation coefficients upon the state-of-the-art by 11% and 17.8% respectively using glove embeddings."], "semantics"], [["enhancing machine translation with dependency-aware self-attention", "emanuele bugliarello | naoaki okazaki", "most neural machine translation models only rely on pairs of parallel sentences, assuming syntactic information is automatically learned by an attention mechanism. in this work, we investigate different approaches to incorporate syntactic knowledge in the transformer model and also propose a novel, parameter-free, dependency-aware self-attention mechanism that improves its translation quality, especially for long sentences and in low-resource scenarios. we show the efficacy of each approach on wmt english-german and english-turkish, and wat english-japanese translation tasks."], "machine translation and multilinguality"], [["towards multimodal sarcasm detection (an _obviously_ perfect paper)", "santiago castro | devamanyu hazarika | ver\u00f3nica p\u00e9rez-rosas | roger zimmermann | rada mihalcea | soujanya poria", "sarcasm is often expressed through several verbal and non-verbal cues, e.g., a change of tone, overemphasis in a word, a drawn-out syllable, or a straight looking face. most of the recent work in sarcasm detection has been carried out on textual data. in this paper, we argue that incorporating multimodal cues can improve the automatic classification of sarcasm. as a first step towards enabling the development of multimodal approaches for sarcasm detection, we propose a new sarcasm dataset, multimodal sarcasm detection dataset (mustard), compiled from popular tv shows. mustard consists of audiovisual utterances annotated with sarcasm labels. each utterance is accompanied by its context of historical utterances in the dialogue, which provides additional information on the scenario where the utterance occurs. our initial results show that the use of multimodal information can reduce the relative error rate of sarcasm detection by up to 12.9% in f-score when compared to the use of individual modalities. the full dataset is publicly available for use at https://github.com/soujanyaporia/mustard."], "sentiment analysis, stylistic analysis, and argument mining"], [["few-shot representation learning for out-of-vocabulary words", "ziniu hu | ting chen | kai-wei chang | yizhou sun", "existing approaches for learning word embedding often assume there are sufficient occurrences for each word in the corpus, such that the representation of words can be accurately estimated from their contexts. however, in real-world scenarios, out-of-vocabulary (a.k.a. oov) words that do not appear in training corpus emerge frequently. how to learn accurate representations of these words to augment a pre-trained embedding by only a few observations is a challenging research problem. in this paper, we formulate the learning of oov embedding as a few-shot regression problem by fitting a representation function to predict an oracle embedding vector (defined as embedding trained with abundant observations) based on limited contexts. specifically, we propose a novel hierarchical attention network-based embedding framework to serve as the neural regression function, in which the context information of a word is encoded and aggregated from k observations. furthermore, we propose to use model-agnostic meta-learning (maml) for adapting the learned model to the new corpus fast and robustly. experiments show that the proposed approach significantly outperforms existing methods in constructing an accurate embedding for oov words and improves downstream tasks when the embedding is utilized."], "machine learning for nlp"], [["understanding attention for text classification", "xiaobing sun | wei lu", "attention has been proven successful in many natural language processing (nlp) tasks. recently, many researchers started to investigate the interpretability of attention on nlp tasks. many existing approaches focused on examining whether the local attention weights could reflect the importance of input representations. in this work, we present a study on understanding the internal mechanism of attention by looking into the gradient update process, checking its behavior when approaching a local minimum during training. we propose to analyze for each word token the following two quantities: its polarity score and its attention score, where the latter is a global assessment on the token\u2019s significance. we discuss conditions under which the attention mechanism may become more (or less) interpretable, and show how the interplay between the two quantities can contribute towards model performance."], "interpretability and analysis of models for nlp"], [["predicting human activities from user-generated content", "steven wilson | rada mihalcea", "the activities we do are linked to our interests, personality, political preferences, and decisions we make about the future. in this paper, we explore the task of predicting human activities from user-generated content. we collect a dataset containing instances of social media users writing about a range of everyday activities. we then use a state-of-the-art sentence embedding framework tailored to recognize the semantics of human activities and perform an automatic clustering of these activities. we train a neural network model to make predictions about which clusters contain activities that were performed by a given user based on the text of their previous posts and self-description. additionally, we explore the degree to which incorporating inferred user traits into our model helps with this prediction task."], "computational social science, social media and cultural analytics"], [["token-level dynamic self-attention network for multi-passage reading comprehension", "yimeng zhuang | huadong wang", "multi-passage reading comprehension requires the ability to combine cross-passage information and reason over multiple passages to infer the answer. in this paper, we introduce the dynamic self-attention network (dynsan) for multi-passage reading comprehension task, which processes cross-passage information at token-level and meanwhile avoids substantial computational costs. the core module of the dynamic self-attention is a proposed gated token selection mechanism, which dynamically selects important tokens from a sequence. these chosen tokens will attend to each other via a self-attention mechanism to model long-range dependencies. besides, convolutional layers are combined with the dynamic self-attention to enhance the model\u2019s capacity of extracting local semantic. the experimental results show that the proposed dynsan achieves new state-of-the-art performance on the searchqa, quasar-t and wikihop datasets. further ablation study also validates the effectiveness of our model components."], "question answering"], [["gcan: graph-aware co-attention networks for explainable fake news detection on social media", "yi-ju lu | cheng-te li", "this paper solves the fake news detection problem under a more realistic scenario on social media. given the source short-text tweet and the corresponding sequence of retweet users without text comments, we aim at predicting whether the source tweet is fake or not, and generating explanation by highlighting the evidences on suspicious retweeters and the words they concern. we develop a novel neural network-based model, graph-aware co-attention networks (gcan), to achieve the goal. extensive experiments conducted on real tweet datasets exhibit that gcan can significantly outperform state-of-the-art methods by 16% in accuracy on average. in addition, the case studies also show that gcan can produce reasonable explanations."], "computational social science, social media and cultural analytics"], [["semi-supervised semantic dependency parsing using crf autoencoders", "zixia jia | youmi ma | jiong cai | kewei tu", "semantic dependency parsing, which aims to find rich bi-lexical relationships, allows words to have multiple dependency heads, resulting in graph-structured representations. we propose an approach to semi-supervised learning of semantic dependency parsers based on the crf autoencoder framework. our encoder is a discriminative neural semantic dependency parser that predicts the latent parse graph of the input sentence. our decoder is a generative neural model that reconstructs the input sentence conditioned on the latent parse graph. our model is arc-factored and therefore parsing and learning are both tractable. experiments show our model achieves significant and consistent improvement over the supervised baseline."], "semantics"], [["joint slot filling and intent detection via capsule neural networks", "chenwei zhang | yaliang li | nan du | wei fan | philip yu", "being able to recognize words as slots and detect the intent of an utterance has been a keen issue in natural language understanding. the existing works either treat slot filling and intent detection separately in a pipeline manner, or adopt joint models which sequentially label slots while summarizing the utterance-level intent without explicitly preserving the hierarchical relationship among words, slots, and intents. to exploit the semantic hierarchy for effective modeling, we propose a capsule-based neural network model which accomplishes slot filling and intent detection via a dynamic routing-by-agreement schema. a re-routing schema is proposed to further synergize the slot filling performance using the inferred intent representation. experiments on two real-world datasets show the effectiveness of our model when compared with other alternative model architectures, as well as existing natural language understanding services."], "information extraction, retrieval and text mining"], [["on the spontaneous emergence of discrete and compositional signals", "nur geffen lan | emmanuel chemla | shane steinert-threlkeld", "we propose a general framework to study language emergence through signaling games with neural agents. using a continuous latent space, we are able to (i) train using backpropagation, (ii) show that discrete messages nonetheless naturally emerge. we explore whether categorical perception effects follow and show that the messages are not compositional."], "interpretability and analysis of models for nlp"], [["gender in danger? evaluating speech translation technology on the must-she corpus", "luisa bentivogli | beatrice savoldi | matteo negri | mattia a. di gangi | roldano cattoni | marco turchi", "translating from languages without productive grammatical gender like english into gender-marked languages is a well-known difficulty for machines. this difficulty is also due to the fact that the training data on which models are built typically reflect the asymmetries of natural languages, gender bias included. exclusively fed with textual data, machine translation is intrinsically constrained by the fact that the input sentence does not always contain clues about the gender identity of the referred human entities. but what happens with speech translation, where the input is an audio signal? can audio provide additional information to reduce gender bias? we present the first thorough investigation of gender bias in speech translation, contributing with: i) the release of a benchmark useful for future studies, and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (english-italian/french)."], "machine translation and multilinguality"], [["dynamically fused graph network for multi-hop reasoning", "lin qiu | yunxuan xiao | yanru qu | hao zhou | lei li | weinan zhang | yong yu", "text-based question answering (tbqa) has been studied extensively in recent years. most existing approaches focus on finding the answer to a question within a single paragraph. however, many difficult questions require multiple supporting evidence from scattered text among two or more documents. in this paper, we propose dynamically fused graph network (dfgn), a novel method to answer those questions requiring multiple scattered evidence and reasoning over them. inspired by human\u2019s step-by-step reasoning behavior, dfgn includes a dynamic fusion layer that starts from the entities mentioned in the given query, explores along the entity graph dynamically built from the text, and gradually finds relevant supporting entities from the given documents. we evaluate dfgn on hotpotqa, a public tbqa dataset requiring multi-hop reasoning. dfgn achieves competitive results on the public board. furthermore, our analysis shows dfgn produces interpretable reasoning chains."], "question answering"], [["generating summaries with topic templates and structured convolutional decoders", "laura perez-beltrachini | yang liu | mirella lapata", "existing neural generation approaches create multi-sentence text as a single sequence. in this paper we propose a structured convolutional decoder that is guided by the content structure of target summaries. we compare our model with existing sequential decoders on three data sets representing different domains. automatic and human evaluation demonstrate that our summaries have better content coverage."], "summarization"], [["towards emotional support dialog systems", "siyang liu | chujie zheng | orianna demasi | sahand sabour | yu li | zhou yu | yong jiang | minlie huang", "emotional support is a crucial ability for many conversation scenarios, including social interactions, mental health support, and customer service chats. following reasonable procedures and using various support skills can help to effectively provide support. however, due to the lack of a well-designed task and corpora of effective emotional support conversations, research on building emotional support into dialog systems remains lacking. in this paper, we define the emotional support conversation (esc) task and propose an esc framework, which is grounded on the helping skills theory. we construct an emotion support conversation dataset (esconv) with rich annotation (especially support strategy) in a help-seeker and supporter mode. to ensure a corpus of high-quality conversations that provide examples of effective emotional support, we take extensive effort to design training tutorials for supporters and several mechanisms for quality control during data collection. finally, we evaluate state-of-the-art dialog models with respect to the ability to provide emotional support. our results show the importance of support strategies in providing effective emotional support and the utility of esconv in training more emotional support systems."], "dialogue and interactive systems"], [["beyond bleu:training neural machine translation with semantic similarity", "john wieting | taylor berg-kirkpatrick | kevin gimpel | graham neubig", "while most neural machine translation (nmt)systems are still trained using maximum likelihood estimation, recent work has demonstrated that optimizing systems to directly improve evaluation metrics such as bleu can significantly improve final translation accuracy. however, training with bleu has some limitations: it doesn\u2019t assign partial credit, it has a limited range of output values, and it can penalize semantically correct hypotheses if they differ lexically from the reference. in this paper, we introduce an alternative reward function for optimizing nmt systems that is based on recent work in semantic similarity. we evaluate on four disparate languages trans-lated to english, and find that training with our proposed metric results in better translations as evaluated by bleu, semantic similarity, and human evaluation, and also that the optimization procedure converges faster. analysis suggests that this is because the proposed metric is more conducive to optimization, assigning partial credit and providing more diversity in scores than bleu"], "machine translation and multilinguality"], [["the right tool for the job: matching model and instance complexities", "roy schwartz | gabriel stanovsky | swabha swayamdipta | jesse dodge | noah a. smith", "as nlp models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs. to better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) \u201cexit\u201d from neural network calculations for simple instances, and late (and accurate) exit for hard instances. to achieve this, we add classifiers to different layers of bert and use their calibrated confidence scores to make early exit decisions. we test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks. our method presents a favorable speed/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy. our method also requires almost no additional training resources (in either time or parameters) compared to the baseline bert model. finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed/accuracy tradeoff using a single trained model, by setting a single variable at inference time. we publicly release our code."], "machine learning for nlp"], [["chartdialogs: plotting from natural language instructions", "yutong shao | ndapa nakashole", "this paper presents the problem of conversational plotting agents that carry out plotting actions from natural language instructions. to facilitate the development of such agents, we introduce chartdialogs, a new multi-turn dialog dataset, covering a popular plotting library, matplotlib. the dataset contains over 15,000 dialog turns from 3,200 dialogs covering the majority of matplotlib plot types. extensive experiments show the best-performing method achieving 61% plotting accuracy, demonstrating that the dataset presents a non-trivial challenge for future research on this task."], "resources and evaluation"], [["parallel corpus filtering via pre-trained language models", "boliang zhang | ajay nagesh | kevin knight", "web-crawled data provides a good source of parallel corpora for training machine translation models. it is automatically obtained, but extremely noisy, and recent work shows that neural machine translation systems are more sensitive to noise than traditional statistical machine translation methods. in this paper, we propose a novel approach to filter out noisy sentence pairs from web-crawled corpora via pre-trained language models. we measure sentence parallelism by leveraging the multilingual capability of bert and use the generative pre-training (gpt) language model as a domain filter to balance data domains. we evaluate the proposed method on the wmt 2018 parallel corpus filtering shared task, and on our own web-crawled japanese-chinese parallel corpus. our method significantly outperforms baselines and achieves a new state-of-the-art. in an unsupervised setting, our method achieves comparable performance to the top-1 supervised method. we also evaluate on a web-crawled japanese-chinese parallel corpus that we make publicly available."], "machine translation and multilinguality"], [["iterative edit-based unsupervised sentence simplification", "dhruv kumar | lili mou | lukasz golab | olga vechtomova", "we present a novel iterative, edit-based approach to unsupervised sentence simplification. our model is guided by a scoring function involving fluency, simplicity, and meaning preservation. then, we iteratively perform word and phrase-level edits on the complex sentence. compared with previous approaches, our model does not require a parallel training set, but is more controllable and interpretable. experiments on newsela and wikilarge datasets show that our approach is nearly as effective as state-of-the-art supervised approaches."], "generation"], [["sources of transfer in multilingual named entity recognition", "david mueller | nicholas andrews | mark dredze", "named-entities are inherently multilingual, and annotations in any given language may be limited. this motivates us to consider polyglot named-entity recognition (ner), where one model is trained using annotated data drawn from more than one language. however, a straightforward implementation of this simple idea does not always work in practice: naive training of ner models using annotated data drawn from multiple languages consistently underperforms models trained on monolingual data alone, despite having access to more training data. the starting point of this paper is a simple solution to this problem, in which polyglot models are fine-tuned on monolingual data to consistently and significantly outperform their monolingual counterparts. to explain this phenomena, we explore the sources of multilingual transfer in polyglot ner models and examine the weight structure of polyglot models compared to their monolingual counterparts. we find that polyglot models efficiently share many parameters across languages and that fine-tuning may utilize a large number of those parameters."], "information extraction, retrieval and text mining"], [["unsupervised information extraction: regularizing discriminative approaches with relation distribution losses", "\u00e9tienne simon | vincent guigue | benjamin piwowarski", "unsupervised relation extraction aims at extracting relations between entities in text. previous unsupervised approaches are either generative or discriminative. in a supervised setting, discriminative approaches, such as deep neural network classifiers, have demonstrated substantial improvement. however, these models are hard to train without supervision, and the currently proposed solutions are unstable. to overcome this limitation, we introduce a skewness loss which encourages the classifier to predict a relation with confidence given a sentence, and a distribution distance loss enforcing that all relations are predicted in average. these losses improve the performance of discriminative based models, and enable us to train deep neural networks satisfactorily, surpassing current state of the art on three different datasets."], "information extraction, retrieval and text mining"], [["careful selection of knowledge to solve open book question answering", "pratyay banerjee | kuntal kumar pal | arindam mitra | chitta baral", "open book question answering is a type of natural language based qa (nlqa) where questions are expected to be answered with respect to a given set of open book facts, and common knowledge about a topic. recently a challenge involving such qa, openbookqa, has been proposed. unlike most other nlqa that focus on linguistic understanding, openbookqa requires deeper reasoning involving linguistic understanding as well as reasoning with common knowledge. in this paper we address qa with respect to the openbookqa dataset and combine state of the art language models with abductive information retrieval (ir), information gain based re-ranking, passage selection and weighted scoring to achieve 72.0% accuracy, an 11.6% improvement over the current state of the art."], "question answering"], [["smart to-do: automatic generation of to-do items from emails", "sudipto mukherjee | subhabrata mukherjee | marcello hasegawa | ahmed hassan awadallah | ryen white", "intelligent features in email service applications aim to increase productivity by helping people organize their folders, compose their emails and respond to pending tasks. in this work, we explore a new application, smart-to-do, that helps users with task management over emails. we introduce a new task and dataset for automatically generating to-do items from emails where the sender has promised to perform an action. we design a two-stage process leveraging recent advances in neural text generation and sequence-to-sequence learning, obtaining bleu and rouge scores of 0.23 and 0.63 for this task. to the best of our knowledge, this is the first work to address the problem of composing to-do items from emails."], "nlp applications"], [["emotion-cause pair extraction: a new task to emotion analysis in texts", "rui xia | zixiang ding", "emotion cause extraction (ece), the task aimed at extracting the potential causes behind certain emotions in text, has gained much attention in recent years due to its wide applications. however, it suffers from two shortcomings: 1) the emotion must be annotated before cause extraction in ece, which greatly limits its applications in real-world scenarios; 2) the way to first annotate emotion and then extract the cause ignores the fact that they are mutually indicative. in this work, we propose a new task: emotion-cause pair extraction (ecpe), which aims to extract the potential pairs of emotions and corresponding causes in a document. we propose a 2-step approach to address this new ecpe task, which first performs individual emotion extraction and cause extraction via multi-task learning, and then conduct emotion-cause pairing and filtering. the experimental results on a benchmark emotion cause corpus prove the feasibility of the ecpe task as well as the effectiveness of our approach."], "sentiment analysis, stylistic analysis, and argument mining"], [["abstractive text summarization based on deep learning and semantic content generalization", "panagiotis kouris | georgios alexandridis | andreas stafylopatis", "this work proposes a novel framework for enhancing abstractive text summarization based on the combination of deep learning techniques along with semantic data transformations. initially, a theoretical model for semantic-based text generalization is introduced and used in conjunction with a deep encoder-decoder architecture in order to produce a summary in generalized form. subsequently, a methodology is proposed which transforms the aforementioned generalized summary into human-readable form, retaining at the same time important informational aspects of the original text and addressing the problem of out-of-vocabulary or rare words. the overall approach is evaluated on two popular datasets with encouraging results."], "summarization"], [["dialogue coherence assessment without explicit dialogue act labels", "mohsen mesgar | sebastian b\u00fccker | iryna gurevych", "recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels. it indicates two drawbacks, (a) semantics of utterances are limited to entity mentions, and (b) the performance of coherence models strongly relies on the quality of the input dialogue act labels. we address these issues by introducing a novel approach to dialogue coherence assessment. we use dialogue act prediction as an auxiliary task in a multi-task learning scenario to obtain informative utterance representations for coherence assessment. our approach alleviates the need for explicit dialogue act labels during evaluation. the results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the dailydialogue corpus, and performs on par with them on the switchboard corpus for ranking dialogues concerning their coherence. we release our source code."], "discourse and pragmatics"], [["text-based ideal points", "keyon vafa | suresh naidu | david blei", "ideal point models analyze lawmakers\u2019 votes to quantify their political positions, or ideal points. but votes are not the only way to express a political position. lawmakers also give speeches, release press statements, and post tweets. in this paper, we introduce the text-based ideal point model (tbip), an unsupervised probabilistic topic model that analyzes texts to quantify the political positions of its authors. we demonstrate the tbip with two types of politicized text data: u.s. senate speeches and senator tweets. though the model does not analyze their votes or political affiliations, the tbip separates lawmakers by party, learns interpretable politicized topics, and infers ideal points close to the classical vote-based ideal points. one benefit of analyzing texts, as opposed to votes, is that the tbip can estimate ideal points of anyone who authors political texts, including non-voting actors. to this end, we use it to study tweets from the 2020 democratic presidential candidates. using only the texts of their tweets, it identifies them along an interpretable progressive-to-moderate spectrum."], "computational social science, social media and cultural analytics"], [["enhancing unsupervised generative dependency parser with contextual information", "wenjuan han | yong jiang | kewei tu", "most of the unsupervised dependency parsers are based on probabilistic generative models that learn the joint distribution of the given sentence and its parse. probabilistic generative models usually explicit decompose the desired dependency tree into factorized grammar rules, which lack the global features of the entire sentence. in this paper, we propose a novel probabilistic model called discriminative neural dependency model with valence (d-ndmv) that generates a sentence and its parse from a continuous latent representation, which encodes global contextual information of the generated sentence. we propose two approaches to model the latent representation: the first deterministically summarizes the representation from the sentence and the second probabilistically models the representation conditioned on the sentence. our approach can be regarded as a new type of autoencoder model to unsupervised dependency parsing that combines the benefits of both generative and discriminative techniques. in particular, our approach breaks the context-free independence assumption in previous generative approaches and therefore becomes more expressive. our extensive experimental results on seventeen datasets from various sources show that our approach achieves competitive accuracy compared with both generative and discriminative state-of-the-art unsupervised dependency parsers."], "tagging, chunking, syntax and parsing"], [["corpus-based check-up for thesaurus", "natalia loukachevitch", "in this paper we discuss the usefulness of applying a checking procedure to existing thesauri. the procedure is based on the analysis of discrepancies of corpus-based and thesaurus-based word similarities. we applied the procedure to more than 30 thousand words of the russian wordnet and found some serious errors in word sense description, including inaccurate relationships and missing senses of ambiguous words."], "resources and evaluation"], [["multi-task learning for coherence modeling", "youmna farag | helen yannakoudakis", "we address the task of assessing discourse coherence, an aspect of text quality that is essential for many nlp tasks, such as summarization and language assessment. we propose a hierarchical neural network trained in a multi-task fashion that learns to predict a document-level coherence score (at the network\u2019s top layers) along with word-level grammatical roles (at the bottom layers), taking advantage of inductive transfer between the two tasks. we assess the extent to which our framework generalizes to different domains and prediction tasks, and demonstrate its effectiveness not only on standard binary evaluation coherence tasks, but also on real-world tasks involving the prediction of varying degrees of coherence, achieving a new state of the art."], "discourse and pragmatics"], [["the cascade transformer: an application for efficient answer sentence selection", "luca soldaini | alessandro moschitti", "large transformer-based language models have been shown to be very effective in many classification tasks. however, their computational complexity prevents their use in applications requiring the classification of a large set of candidates. while previous works have investigated approaches to reduce model size, relatively little attention has been paid to techniques to improve batch throughput during inference. in this paper, we introduce the cascade transformer, a simple yet effective technique to adapt transformer-based models into a cascade of rankers. each ranker is used to prune a subset of candidates in a batch, thus dramatically increasing throughput at inference time. partial encodings from the transformer model are shared among rerankers, providing further speed-up. when compared to a state-of-the-art transformer model, our approach reduces computation by 37% with almost no impact on accuracy, as measured on two english question answering datasets."], "question answering"], [["cross-lingual training for automatic question generation", "vishwajeet kumar | nitish joshi | arijit mukherjee | ganesh ramakrishnan | preethi jyothi", "automatic question generation (qg) is a challenging problem in natural language understanding. qg systems are typically built assuming access to a large number of training instances where each instance is a question and its corresponding answer. for a new language, such training instances are hard to obtain making the qg problem even more challenging. using this as our motivation, we study the reuse of an available large qg dataset in a secondary language (e.g. english) to learn a qg model for a primary language (e.g. hindi) of interest. for the primary language, we assume access to a large amount of monolingual text but only a small qg dataset. we propose a cross-lingual qg model which uses the following training regime: (i) unsupervised pretraining of language models in both primary and secondary languages and (ii) joint supervised training for qg in both languages. we demonstrate the efficacy of our proposed approach using two different primary languages, hindi and chinese. our proposed framework clearly outperforms a number of baseline models, including a fully-supervised transformer-based model trained on the qg datasets in the primary language. we also create and release a new question answering dataset for hindi consisting of 6555 sentences."], "generation"], [["neural data-to-text generation via jointly learning the segmentation and correspondence", "xiaoyu shen | ernie chang | hui su | cheng niu | dietrich klakow", "the neural attention model has achieved great success in data-to-text generation tasks. though usually excelling at producing fluent text, it suffers from the problem of information missing, repetition and \u201challucination\u201d. due to the black-box nature of the neural attention architecture, avoiding these problems in a systematic way is non-trivial. to address this concern, we propose to explicitly segment target text into fragment units and align them with their data correspondences. the segmentation and correspondence are jointly learned as latent variables without any human annotations. we further impose a soft statistical constraint to regularize the segmental granularity. the resulting architecture maintains the same expressive power as neural attention models, while being able to generate fully interpretable outputs with several times less computational cost. on both e2e and webnlg benchmarks, we show the proposed model consistently outperforms its neural attention counterparts."], "generation"], [["balancing objectives in counseling conversations: advancing forwards or looking backwards", "justine zhang | cristian danescu-niculescu-mizil", "throughout a conversation, participants make choices that can orient the flow of the interaction. such choices are particularly salient in the consequential domain of crisis counseling, where a difficulty for counselors is balancing between two key objectives: advancing the conversation towards a resolution, and empathetically addressing the crisis situation. in this work, we develop an unsupervised methodology to quantify how counselors manage this balance. our main intuition is that if an utterance can only receive a narrow range of appropriate replies, then its likely aim is to advance the conversation forwards, towards a target within that range. likewise, an utterance that can only appropriately follow a narrow range of possible utterances is likely aimed backwards at addressing a specific situation within that range. by applying this intuition, we can map each utterance to a continuous orientation axis that captures the degree to which it is intended to direct the flow of the conversation forwards or backwards. this unsupervised method allows us to characterize counselor behaviors in a large dataset of crisis counseling conversations, where we show that known counseling strategies intuitively align with this axis. we also illustrate how our measure can be indicative of a conversation\u2019s progress, as well as its effectiveness."], "computational social science, social media and cultural analytics"], [["docred: a large-scale document-level relation extraction dataset", "yuan yao | deming ye | peng li | xu han | yankai lin | zhenghao liu | zhiyuan liu | lixin huang | jie zhou | maosong sun", "multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (re) methods that typically focus on extracting intra-sentence relations for single entity pairs. in order to accelerate the research on document-level re, we introduce docred, a new dataset constructed from wikipedia and wikidata with three features: (1) docred annotates both named entities and relations, and is the largest human-annotated dataset for document-level re from plain text; (2) docred requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables docred to be adopted for both supervised and weakly supervised scenarios. in order to verify the challenges of document-level re, we implement recent state-of-the-art methods for re and conduct a thorough evaluation of these methods on docred. empirical results show that docred is challenging for existing re methods, which indicates that document-level re remains an open problem and requires further efforts. based on the detailed analysis on the experiments, we discuss multiple promising directions for future research. we make docred and the code for our baselines publicly available at https://github.com/thunlp/docred."], "resources and evaluation"], [["a prism module for semantic disentanglement in name entity recognition", "kun liu | shen li | daqi zheng | zhengdong lu | sheng gao | si li", "natural language processing has been perplexed for many years by the problem that multiple semantics are mixed inside a word, even with the help of context. to solve this problem, we propose a prism module to disentangle the semantic aspects of words and reduce noise at the input layer of a model. in the prism module, some words are selectively replaced with task-related semantic aspects, then these denoised word representations can be fed into downstream tasks to make them easier. besides, we also introduce a structure to train this module jointly with the downstream model without additional data. this module can be easily integrated into the downstream model and significantly improve the performance of baselines on named entity recognition (ner) task. the ablation analysis demonstrates the rationality of the method. as a side effect, the proposed method also provides a way to visualize the contribution of each word."], "tagging, chunking, syntax and parsing"], [["eigensent: spectral sentence embeddings using higher-order dynamic mode decomposition", "subhradeep kayal | george tsatsaronis", "distributed representation of words, or word embeddings, have motivated methods for calculating semantic representations of word sequences such as phrases, sentences and paragraphs. most of the existing methods to do so either use algorithms to learn such representations, or improve on calculating weighted averages of the word vectors. in this work, we experiment with spectral methods of signal representation and summarization as mechanisms for constructing such word-sequence embeddings in an unsupervised fashion. in particular, we explore an algorithm rooted in fluid-dynamics, known as higher-order dynamic mode decomposition, which is designed to capture the eigenfrequencies, and hence the fundamental transition dynamics, of periodic and quasi-periodic systems. it is empirically observed that this approach, which we call eigensent, can summarize transitions in a sequence of words and generate an embedding that can represent well the sequence itself. to the best of the authors\u2019 knowledge, this is the first application of a spectral decomposition and signal summarization technique on text, to create sentence embeddings. we test the efficacy of this algorithm in creating sentence embeddings on three public datasets, where it performs appreciably well. moreover it is also shown that, due to the positive combination of their complementary properties, concatenating the embeddings generated by eigensent with simple word vector averaging achieves state-of-the-art results."], "semantics"], [["modeling code-switch languages using bilingual parallel corpus", "grandee lee | haizhou li", "language modeling is the technique to estimate the probability of a sequence of words. a bilingual language model is expected to model the sequential dependency for words across languages, which is difficult due to the inherent lack of suitable training data as well as diverse syntactic structure across languages. we propose a bilingual attention language model (balm) that simultaneously performs language modeling objective with a quasi-translation objective to model both the monolingual as well as the cross-lingual sequential dependency. the attention mechanism learns the bilingual context from a parallel corpus. balm achieves state-of-the-art performance on the seame code-switch database by reducing the perplexity of 20.5% over the best-reported result. we also apply balm in bilingual lexicon induction, and language normalization tasks to validate the idea."], "nlp applications"], [["attentive pooling with learnable norms for text representation", "chuhan wu | fangzhao wu | tao qi | xiaohui cui | yongfeng huang", "pooling is an important technique for learning text representations in many neural nlp models. in conventional pooling methods such as average, max and attentive pooling, text representations are weighted summations of the l1 or l\u221e norm of input features. however, their pooling norms are always fixed and may not be optimal for learning accurate text representations in different tasks. in addition, in many popular pooling methods such as max and attentive pooling some features may be over-emphasized, while other useful ones are not fully exploited. in this paper, we propose an attentive pooling with learnable norms (apln) approach for text representation. different from existing pooling methods that use a fixed pooling norm, we propose to learn the norm in an end-to-end manner to automatically find the optimal ones for text representation in different tasks. in addition, we propose two methods to ensure the numerical stability of the model training. the first one is scale limiting, which re-scales the input to ensure non-negativity and alleviate the risk of exponential explosion. the second one is re-formulation, which decomposes the exponent operation to avoid computing the real-valued powers of the input and further accelerate the pooling operation. experimental results on four benchmark datasets show that our approach can effectively improve the performance of attentive pooling."], "machine learning for nlp"], [["span selection pre-training for question answering", "michael glass | alfio gliozzo | rishav chakravarti | anthony ferritto | lin pan | g p shrivatsa bhargav | dinesh garg | avi sil", "bert (bidirectional encoder representations from transformers) and related pre-trained transformers have provided large gains across many language understanding tasks, achieving a new state-of-the-art (sota). bert is pretrained on two auxiliary tasks: masked language model and next sentence prediction. in this paper we introduce a new pre-training task inspired by reading comprehension to better align the pre-training from memorization to understanding. span selection pretraining (sspt) poses cloze-like training instances, but rather than draw the answer from the model\u2019s parameters, it is selected from a relevant passage. we find significant and consistent improvements over both bert-base and bert-large on multiple machine reading comprehension (mrc) datasets. specifically, our proposed model has strong empirical evidence as it obtains sota results on natural questions, a new benchmark mrc dataset, outperforming bert-large by 3 f1 points on short answer prediction. we also show significant impact in hotpotqa, improving answer prediction f1 by 4 points and supporting fact prediction f1 by 1 point and outperforming the previous best system. moreover, we show that our pre-training approach is particularly effective when training data is limited, improving the learning curve by a large amount."], "machine learning for nlp"], [["neural architectures for nested ner through linearization", "jana strakov\u00e1 | milan straka | jan hajic", "we propose two neural network architectures for nested named entity recognition (ner), a setting in which named entities may overlap and also be labeled with more than one label. we encode the nested labels using a linearized scheme. in our first proposed approach, the nested labels are modeled as multilabels corresponding to the cartesian product of the nested labels in a standard lstm-crf architecture. in the second one, the nested ner is viewed as a sequence-to-sequence problem, in which the input sequence consists of the tokens and output sequence of the labels, using hard attention on the word whose label is being predicted. the proposed methods outperform the nested ner state of the art on four corpora: ace-2004, ace-2005, genia and czech cnec. we also enrich our architectures with the recently published contextual embeddings: elmo, bert and flair, reaching further improvements for the four nested entity corpora. in addition, we report flat ner state-of-the-art results for conll-2002 dutch and spanish and for conll-2003 english."], "tagging, chunking, syntax and parsing"], [["unsupervised paraphrasing by simulated annealing", "xianggen liu | lili mou | fandong meng | hao zhou | jie zhou | sen song", "we propose upsa, a novel approach that accomplishes unsupervised paraphrasing by simulated annealing. we model paraphrase generation as an optimization problem and propose a sophisticated objective function, involving semantic similarity, expression diversity, and language fluency of paraphrases. upsa searches the sentence space towards this objective by performing a sequence of local editing. we evaluate our approach on various datasets, namely, quora, wikianswers, mscoco, and twitter. extensive results show that upsa achieves the state-of-the-art performance compared with previous unsupervised methods in terms of both automatic and human evaluations. further, our approach outperforms most existing domain-adapted supervised models, showing the generalizability of upsa."], "generation"], [["fact-based text editing", "hayate iso | chao qiao | hang li", "we propose a novel text editing task, referred to as fact-based text editing, in which the goal is to revise a given document to better describe the facts in a knowledge base (e.g., several triples). the task is important in practice because reflecting the truth is a common requirement in text editing. first, we propose a method for automatically generating a dataset for research on fact-based text editing, where each instance consists of a draft text, a revised text, and several facts represented in triples. we apply the method into two public table-to-text datasets, obtaining two new datasets consisting of 233k and 37k instances, respectively. next, we propose a new neural network architecture for fact-based text editing, called facteditor, which edits a draft text by referring to given facts using a buffer, a stream, and a memory. a straightforward approach to address the problem would be to employ an encoder-decoder model. our experimental results on the two datasets show that facteditor outperforms the encoder-decoder approach in terms of fidelity and fluency. the results also show that facteditor conducts inference faster than the encoder-decoder approach."], "generation"], [["stock embeddings acquired from news articles and price history, and an application to portfolio optimization", "xin du | kumiko tanaka-ishii", "previous works that integrated news articles to better process stock prices used a variety of neural networks to predict price movements. the textual and price information were both encoded in the neural network, and it is therefore difficult to apply this approach in situations other than the original framework of the notoriously hard problem of price prediction. in contrast, this paper presents a method to encode the influence of news articles through a vector representation of stocks called a stock embedding. the stock embedding is acquired with a deep learning framework using both news articles and price history. because the embedding takes the operational form of a vector, it is applicable to other financial problems besides price prediction. as one example application, we show the results of portfolio optimization using reuters & bloomberg headlines, producing a capital gain 2.8 times larger than that obtained with a baseline method using only stock price data. this suggests that the proposed stock embedding can leverage textual financial semantics to solve financial prediction problems."], "computational social science, social media and cultural analytics"], [["grounding conversations with improvised dialogues", "hyundong cho | jonathan may", "effective dialogue involves grounding, the process of establishing mutual knowledge that is essential for communication between people. modern dialogue systems are not explicitly trained to build common ground, and therefore overlook this important aspect of communication. improvisational theater (improv) intrinsically contains a high proportion of dialogue focused on building common ground, and makes use of the yes-and principle, a strong grounding speech act, to establish coherence and an actionable objective reality. we collect a corpus of more than 26,000 yes-and turns, transcribing them from improv dialogues and extracting them from larger, but more sparsely populated movie script dialogue corpora, via a bootstrapped classifier. we fine-tune chit-chat dialogue systems with our corpus to encourage more grounded, relevant conversation and confirm these findings with human evaluations."], "dialogue and interactive systems"], [["dual supervised learning for natural language understanding and generation", "shang-yu su | chao-wei huang | yun-nung chen", "natural language understanding (nlu) and natural language generation (nlg) are both critical research topics in the nlp and dialogue fields. natural language understanding is to extract the core semantic meaning from the given utterances, while natural language generation is opposite, of which the goal is to construct corresponding sentences based on the given semantics. however, such dual relationship has not been investigated in literature. this paper proposes a novel learning framework for natural language understanding and generation on top of dual supervised learning, providing a way to exploit the duality. the preliminary experiments show that the proposed approach boosts the performance for both tasks, demonstrating the effectiveness of the dual relationship."], "dialogue and interactive systems"], [["unsupervised pivot translation for distant languages", "yichong leng | xu tan | tao qin | xiang-yang li | tie-yan liu", "unsupervised neural machine translation (nmt) has attracted a lot of attention recently. while state-of-the-art methods for unsupervised translation usually perform well between similar languages (e.g., english-german translation), they perform poorly between distant languages, because unsupervised alignment does not work well for distant languages. in this work, we introduce unsupervised pivot translation for distant languages, which translates a language to a distant language through multiple hops, and the unsupervised translation on each hop is relatively easier than the original direct translation. we propose a learning to route (ltr) method to choose the translation path between the source and target languages. ltr is trained on language pairs whose best translation path is available and is applied on the unseen language pairs for path selection. experiments on 20 languages and 294 distant language pairs demonstrate the advantages of the unsupervised pivot translation for distant languages, as well as the effectiveness of the proposed ltr for path selection. specifically, in the best case, ltr achieves an improvement of 5.58 bleu points over the conventional direct unsupervised method."], "machine translation and multilinguality"], [["extractive summarization as text matching", "ming zhong | pengfei liu | yiran chen | danqing wang | xipeng qiu | xuanjing huang", "this paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. instead of following the commonly used framework of extracting sentences individually and modeling the relationship between sentences, we formulate the extractive summarization task as a semantic text matching problem, in which a source document and candidate summaries will be (extracted from the original text) matched in a semantic space. notably, this paradigm shift to semantic matching framework is well-grounded in our comprehensive analysis of the inherent gap between sentence-level and summary-level extractors based on the property of the dataset. besides, even instantiating the framework with a simple form of a matching model, we have driven the state-of-the-art extractive result on cnn/dailymail to a new level (44.41 in rouge-1). experiments on the other five datasets also show the effectiveness of the matching framework. we believe the power of this matching-based summarization framework has not been fully exploited. to encourage more instantiations in the future, we have released our codes, processed dataset, as well as generated summaries in https://github.com/maszhongming/matchsum."], "summarization"], [["obtaining faithful interpretations from compositional neural networks", "sanjay subramanian | ben bogin | nitish gupta | tomer wolfson | sameer singh | jonathan berant | matt gardner", "neural module networks (nmns) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. however, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model\u2019s reasoning; that is, that all modules perform their intended behaviour. in this work, we propose and conduct a systematic evaluation of the intermediate outputs of nmns on nlvr2 and drop, two datasets which require composing multiple reasoning steps. we find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. to remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy."], "interpretability and analysis of models for nlp"], [["neural-dinf: a neural network based framework for measuring document influence", "jie tan | changlin yang | ying li | siliang tang | chen huang | yueting zhuang", "measuring the scholarly impact of a document without citations is an important and challenging problem. existing approaches such as document influence model (dim) are based on dynamic topic models, which only consider the word frequency change. in this paper, we use both frequency changes and word semantic shifts to measure document influence by developing a neural network framework. our model has three steps. firstly, we train the word embeddings for different time periods. subsequently, we propose an unsupervised method to align vectors for different time periods. finally, we compute the influence value of documents. our experimental results show that our model outperforms dim."], "nlp applications"], [["skep: sentiment knowledge enhanced pre-training for sentiment analysis", "hao tian | can gao | xinyan xiao | hao liu | bolei he | hua wu | haifeng wang | feng wu", "recently, sentiment analysis has seen remarkable advance with the help of pre-training approaches. however, sentiment knowledge, such as sentiment words and aspect-sentiment pairs, is ignored in the process of pre-training, despite the fact that they are widely used in traditional sentiment analysis approaches. in this paper, we introduce sentiment knowledge enhanced pre-training (skep) in order to learn a unified sentiment representation for multiple sentiment analysis tasks. with the help of automatically-mined knowledge, skep conducts sentiment masking and constructs three sentiment knowledge prediction objectives, so as to embed sentiment information at the word, polarity and aspect level into pre-trained sentiment representation. in particular, the prediction of aspect-sentiment pairs is converted into multi-label classification, aiming to capture the dependency between words in a pair. experiments on three kinds of sentiment tasks show that skep significantly outperforms strong pre-training baseline, and achieves new state-of-the-art results on most of the test datasets. we release our code at https://github.com/baidu/senta."], "sentiment analysis, stylistic analysis, and argument mining"], [["learning robust models for e-commerce product search", "thanh nguyen | nikhil rao | karthik subbian", "showing items that do not match search query intent degrades customer experience in e-commerce. these mismatches result from counterfactual biases of the ranking algorithms toward noisy behavioral signals such as clicks and purchases in the search logs. mitigating the problem requires a large labeled dataset, which is expensive and time-consuming to obtain. in this paper, we develop a deep, end-to-end model that learns to effectively classify mismatches and to generate hard mismatched examples to improve the classifier. we train the model end-to-end by introducing a latent variable into the cross-entropy loss that alternates between using the real and generated samples. this not only makes the classifier more robust but also boosts the overall ranking performance. our model achieves a relative gain compared to baselines by over 26% in f-score, and over 17% in area under pr curve. on live search traffic, our model gains significant improvement in multiple countries."], "information extraction, retrieval and text mining"], [["end-to-end neural word alignment outperforms giza++", "thomas zenkel | joern wuebker | john denero", "word alignment was once a core unsupervised learning task in natural language processing because of its essential role in training statistical machine translation (mt) models. although unnecessary for training neural mt models, word alignment still plays an important role in interactive applications of neural machine translation, such as annotation transfer and lexicon injection. while statistical mt methods have been replaced by neural approaches with superior performance, the twenty-year-old giza++ toolkit remains a key component of state-of-the-art word alignment systems. prior work on neural word alignment has only been able to outperform giza++ by using its output during training. we present the first end-to-end neural word alignment method that consistently outperforms giza++ on three data sets. our approach repurposes a transformer model trained for supervised translation to also serve as an unsupervised word alignment model in a manner that is tightly integrated and does not affect translation quality."], "machine translation and multilinguality"], [["mlqa: evaluating cross-lingual extractive question answering", "patrick lewis | barlas oguz | ruty rinott | sebastian riedel | holger schwenk", "question answering (qa) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. such annotated datasets are difficult and costly to collect, and rarely exist in languages other than english, making building qa systems that work well in other languages challenging. in order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. we present mlqa, a multi-way aligned extractive qa evaluation benchmark intended to spur research in this area. mlqa contains qa instances in 7 languages, english, arabic, german, spanish, hindi, vietnamese and simplified chinese. mlqa has over 12k instances in english and 5k in each other language, with each instance parallel between 4 languages on average. we evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on mlqa. in all cases, transfer results are shown to be significantly behind training-language performance."], "question answering"], [["expressing visual relationships via language", "hao tan | franck dernoncourt | zhe lin | trung bui | mohit bansal", "describing images with text is a fundamental problem in vision-language research. current studies in this domain mostly focus on single image captioning. however, in various real applications (e.g., image editing, difference interpretation, and retrieval), generating relational captions for two images, can also be very useful. this important problem has not been explored mostly due to lack of datasets and effective models. to push forward the research in this direction, we first introduce a new language-guided image editing dataset that contains a large number of real image pairs with corresponding editing instructions. we then propose a new relational speaker model based on an encoder-decoder architecture with static relational attention and sequential multi-head attention. we also extend the model with dynamic relational attention, which calculates visual alignment while decoding. our models are evaluated on our newly collected and two public datasets consisting of image pairs annotated with relationship sentences. experimental results, based on both automatic and human evaluation, demonstrate that our model outperforms all baselines and existing methods on all the datasets."], "language grounding to vision, robotics and beyond"], [["comparison of diverse decoding methods from conditional language models", "daphne ippolito | reno kriz | jo\u00e3o sedoc | maria kustikova | chris callison-burch", "while conditional language models have greatly improved in their ability to output high quality natural language, many nlp applications benefit from being able to generate a diverse set of candidate sequences. diverse decoding strategies aim to, within a given-sized candidate list, cover as much of the space of high-quality outputs as possible, leading to improvements for tasks that rerank and combine candidate outputs. standard decoding methods, such as beam search, optimize for generating high likelihood sequences rather than diverse ones, though recent work has focused on increasing diversity in these methods. in this work, we perform an extensive survey of decoding-time strategies for generating diverse outputs from a conditional language model. in addition, we present a novel method where we over-sample candidates, then use clustering to remove similar sequences, thus achieving high diversity without sacrificing quality."], "dialogue and interactive systems"], [["simultaneous translation with flexible policy via restricted imitation learning", "baigong zheng | renjie zheng | mingbo ma | liang huang", "simultaneous translation is widely useful but remains one of the most difficult tasks in nlp. previous work either uses fixed-latency policies, or train a complicated two-staged model using reinforcement learning. we propose a much simpler single model that adds a \u201cdelay\u201d token to the target vocabulary, and design a restricted dynamic oracle to greatly simplify training. experiments on chinese <-> english simultaneous translation show that our work leads to flexible policies that achieve better bleu scores and lower latencies compared to both fixed and rl-learned policies."], "machine translation and multilinguality"], [["unified dual-view cognitive model for interpretable claim verification", "lianwei wu | yuan rao | yuqian lan | ling sun | zhaoyin qi", "recent studies constructing direct interactions between the claim and each single user response (a comment or a relevant article) to capture evidence have shown remarkable success in interpretable claim verification. owing to different single responses convey different cognition of individual users (i.e., audiences), the captured evidence belongs to the perspective of individual cognition. however, individuals\u2019 cognition of social things is not always able to truly reflect the objective. there may be one-sided or biased semantics in their opinions on a claim. the captured evidence correspondingly contains some unobjective and biased evidence fragments, deteriorating task performance. in this paper, we propose a dual-view model based on the views of collective and individual cognition (cicd) for interpretable claim verification. from the view of the collective cognition, we not only capture the word-level semantics based on individual users, but also focus on sentence-level semantics (i.e., the overall responses) among all users and adjust the proportion between them to generate global evidence. from the view of individual cognition, we select the top-k articles with high degree of difference and interact with the claim to explore the local key evidence fragments. to weaken the bias of individual cognition-view evidence, we devise inconsistent loss to suppress the divergence between global and local evidence for strengthening the consistent shared evidence between the both. experiments on three benchmark datasets confirm that cicd achieves state-of-the-art performance."], "computational social science, social media and cultural analytics"], [["from zero to hero: human-in-the-loop entity linking in low resource domains", "jan-christoph klie | richard eckart de castilho | iryna gurevych", "entity linking (el) is concerned with disambiguating entity mentions in a text against knowledge bases (kb). it is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge. the use of el in such domains requires handling noisy texts, low resource settings and domain-specific kbs. existing approaches are mostly inappropriate for this, as they depend on training data. however, in the above scenario, there exists hardly annotated data, and it needs to be created from scratch. we therefore present a novel domain-agnostic human-in-the-loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users. we evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy. in a user study, the annotation speed improves by 35% compared to annotating without interactive support; users report that they strongly prefer our system. an open-source and ready-to-use implementation based on the text annotation platform inception (https://inception-project.github.io) is made available."], "nlp applications"], [["tweetqa: a social media focused question answering dataset", "wenhan xiong | jiawei wu | hong wang | vivek kulkarni | mo yu | shiyu chang | xiaoxiao guo | william yang wang", "with social media becoming increasingly popular on which lots of news and real-time events are reported, developing automated question answering systems is critical to the effective-ness of many applications that rely on real-time knowledge. while previous datasets have concentrated on question answering (qa) for formal text like news and wikipedia, we present the first large-scale dataset for qa over social media data. to ensure that the tweets we collected are useful, we only gather tweets used by journalists to write news articles. we then ask human annotators to write questions and answers upon these tweets. unlike otherqa datasets like squad in which the answers are extractive, we allow the answers to be abstractive. we show that two recently proposed neural models that perform well on formal texts are limited in their performance when applied to our dataset. in addition, even the fine-tuned bert model is still lagging behind human performance with a large margin. our results thus point to the need of improved qa systems targeting social media text."], "computational social science, social media and cultural analytics"], [["protaugment: intent detection meta-learning through unsupervised diverse paraphrasing", "thomas dopierre | christophe gravier | wilfried logerais", "recent research considers few-shot intent detection as a meta-learning problem: the model is learning to learn from a consecutive set of small tasks named episodes. in this work, we propose protaugment, a meta-learning algorithm for short texts classification (the intent detection task). protaugment is a novel extension of prototypical networks, that limits overfitting on the bias introduced by the few-shots classification objective at each episode. it relies on diverse paraphrasing: a conditional language model is first fine-tuned for paraphrasing, and diversity is later introduced at the decoding stage at each meta-learning episode. the diverse paraphrasing is unsupervised as it is applied to unlabelled data, and then fueled to the prototypical network training objective as a consistency loss. protaugment is the state-of-the-art method for intent detection meta-learning, at no extra labeling efforts and without the need to fine-tune a conditional language model on a given application domain."], "dialogue and interactive systems"], [["robust representation learning of biomedical names", "minh c. phan | aixin sun | yi tay", "biomedical concepts are often mentioned in medical documents under different name variations (synonyms). this mismatch between surface forms is problematic, resulting in difficulties pertaining to learning effective representations. consequently, this has tremendous implications such as rendering downstream applications inefficacious and/or potentially unreliable. this paper proposes a new framework for learning robust representations of biomedical names and terms. the idea behind our approach is to consider and encode contextual meaning, conceptual meaning, and the similarity between synonyms during the representation learning process. via extensive experiments, we show that our proposed method outperforms other baselines on a battery of retrieval, similarity and relatedness benchmarks. moreover, our proposed method is also able to compute meaningful representations for unseen names, resulting in high practical utility in real-world applications."], "semantics"], [["unsupervised dual paraphrasing for two-stage semantic parsing", "ruisheng cao | su zhu | chenyu yang | chen liu | rao ma | yanbin zhao | lu chen | kai yu", "one daunting problem for semantic parsing is the scarcity of annotation. aiming to reduce nontrivial human labor, we propose a two-stage semantic parsing framework, where the first stage utilizes an unsupervised paraphrase model to convert an unlabeled natural language utterance into the canonical utterance. the downstream naive semantic parser accepts the intermediate output and returns the target logical form. furthermore, the entire training process is split into two phases: pre-training and cycle learning. three tailored self-supervised tasks are introduced throughout training to activate the unsupervised paraphrase model. experimental results on benchmarks overnight and geogranno demonstrate that our framework is effective and compatible with supervised training."], "semantics"], [["isarcasm: a dataset of intended sarcasm", "silviu oprea | walid magdy", "we consider the distinction between intended and perceived sarcasm in the context of textual sarcasm detection. the former occurs when an utterance is sarcastic from the perspective of its author, while the latter occurs when the utterance is interpreted as sarcastic by the audience. we show the limitations of previous labelling methods in capturing intended sarcasm and introduce the isarcasm dataset of tweets labeled for sarcasm directly by their authors. examining the state-of-the-art sarcasm detection models on our dataset showed low performance compared to previously studied datasets, which indicates that these datasets might be biased or obvious and sarcasm could be a phenomenon under-studied computationally thus far. by providing the isarcasm dataset, we aim to encourage future nlp research to develop methods for detecting sarcasm in text as intended by the authors of the text, not as labeled under assumptions that we demonstrate to be sub-optimal."], "resources and evaluation"], [["adversarial domain adaptation using artificial titles for abstractive title generation", "francine chen | yan-ying chen", "a common issue in training a deep learning, abstractive summarization model is lack of a large set of training summaries. this paper examines techniques for adapting from a labeled source domain to an unlabeled target domain in the context of an encoder-decoder model for text generation. in addition to adversarial domain adaptation (ada), we introduce the use of artificial titles and sequential training to capture the grammatical style of the unlabeled target domain. evaluation on adapting to/from news articles and stack exchange posts indicates that the use of these techniques can boost performance for both unsupervised adaptation as well as fine-tuning with limited target data."], "summarization"], [["data-to-text generation with entity modeling", "ratish puduppully | li dong | mirella lapata", "recent approaches to data-to-text generation have shown great promise thanks to the use of large-scale datasets and the application of neural network architectures which are trained end-to-end. these models rely on representation learning to select content appropriately, structure it coherently, and verbalize it grammatically, treating entities as nothing more than vocabulary tokens. in this work we propose an entity-centric neural architecture for data-to-text generation. our model creates entity-specific representations which are dynamically updated. text is generated conditioned on the data input and entity memory representations using hierarchical attention at each time step. we present experiments on the rotowire benchmark and a (five times larger) new dataset on the baseball domain which we create. our results show that the proposed model outperforms competitive baselines in automatic and human evaluation."], "generation"], [["adaptive compression of word embeddings", "yeachan kim | kang-min kim | sangkeun lee", "distributed representations of words have been an indispensable component for natural language processing (nlp) tasks. however, the large memory footprint of word embeddings makes it challenging to deploy nlp models to memory-constrained devices (e.g., self-driving cars, mobile devices). in this paper, we propose a novel method to adaptively compress word embeddings. we fundamentally follow a code-book approach that represents words as discrete codes such as (8, 5, 2, 4). however, unlike prior works that assign the same length of codes to all words, we adaptively assign different lengths of codes to each word by learning downstream tasks. the proposed method works in two steps. first, each word directly learns to select its code length in an end-to-end manner by applying the gumbel-softmax tricks. after selecting the code length, each word learns discrete codes through a neural network with a binary constraint. to showcase the general applicability of the proposed method, we evaluate the performance on four different downstream tasks. comprehensive evaluation results clearly show that our method is effective and makes the highly compressed word embeddings without hurting the task accuracy. moreover, we show that our model assigns word to each code-book by considering the significance of tasks."], "semantics"], [["pre-training is (almost) all you need: an application to commonsense reasoning", "alexandre tamborrino | nicola pellican\u00f2 | baptiste pannier | pascal voitot | louise naudin", "fine-tuning of pre-trained transformer models has become the standard approach for solving common nlp tasks. most of the existing approaches rely on a randomly initialized classifier on top of such networks. we argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task. in this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase. we study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the copa, swag, hellaswag and commonsenseqa datasets. by exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g. 80% test accuracy on copa) that are comparable to supervised approaches. moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g x10 standard deviation reduction on copa test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances."], "machine learning for nlp"], [["figurative usage detection of symptom words to improve personal health mention detection", "adith iyer | aditya joshi | sarvnaz karimi | ross sparks | cecile paris", "personal health mention detection deals with predicting whether or not a given sentence is a report of a health condition. past work mentions errors in this prediction when symptom words, i.e., names of symptoms of interest, are used in a figurative sense. therefore, we combine a state-of-the-art figurative usage detection with cnn-based personal health mention detection. to do so, we present two methods: a pipeline-based approach and a feature augmentation-based approach. the introduction of figurative usage detection results in an average improvement of 2.21% f-score of personal health mention detection, in the case of the feature augmentation-based approach. this paper demonstrates the promise of using figurative usage detection to improve personal health mention detection."], "nlp applications"], [["nile : natural language inference with faithful natural language explanations", "sawan kumar | partha talukdar", "the recent growth in the popularity and success of deep learning models on nlp classification tasks has accompanied the need for generating some form of natural language explanation of the predicted labels. such generated natural language (nl) explanations are expected to be faithful, i.e., they should correlate well with the model\u2019s internal decision making. in this work, we focus on the task of natural language inference (nli) and address the following question: can we build nli systems which produce labels with high accuracy, while also generating faithful explanations of its decisions? we propose natural-language inference over label-specific explanations (nile), a novel nli method which utilizes auto-generated label-specific nl explanations to produce labels along with its faithful explanation. we demonstrate nile\u2019s effectiveness over previously reported methods through automated and human evaluation of the produced labels and explanations. our evaluation of nile also supports the claim that accurate systems capable of providing testable explanations of their decisions can be designed. we discuss the faithfulness of nile\u2019s explanations in terms of sensitivity of the decisions to the corresponding explanations. we argue that explicit evaluation of faithfulness, in addition to label and explanation accuracy, is an important step in evaluating model\u2019s explanations. further, we demonstrate that task-specific probes are necessary to establish such sensitivity."], "semantics"], [["exploiting personal characteristics of debaters for predicting persuasiveness", "khalid al khatib | michael v\u00f6lske | shahbaz syed | nikolay kolyada | benno stein", "predicting the persuasiveness of arguments has applications as diverse as writing assistance, essay scoring, and advertising. while clearly relevant to the task, the personal characteristics of an argument\u2019s source and audience have not yet been fully exploited toward automated persuasiveness prediction. in this paper, we model debaters\u2019 prior beliefs, interests, and personality traits based on their previous activity, without dependence on explicit user profiles or questionnaires. using a dataset of over 60,000 argumentative discussions, comprising more than three million individual posts collected from the subreddit r/changemyview, we demonstrate that our modeling of debater\u2019s characteristics enhances the prediction of argument persuasiveness as well as of debaters\u2019 resistance to persuasion."], "sentiment analysis, stylistic analysis, and argument mining"], [["meta-reinforced multi-domain state generator for dialogue systems", "yi huang | junlan feng | min hu | xiaoting wu | xiaoyu du | shuo ma", "a dialogue state tracker (dst) is a core component of a modular task-oriented dialogue system. tremendous progress has been made in recent years. however, the major challenges remain. the state-of-the-art accuracy for dst is below 50% for a multi-domain dialogue task. a learnable dst for any new domain requires a large amount of labeled in-domain data and training from scratch. in this paper, we propose a meta-reinforced multi-domain state generator (meret). our first contribution is to improve the dst accuracy. we enhance a neural model based dst generator with a reward manager, which is built on policy gradient reinforcement learning (rl) to fine-tune the generator. with this change, we are able to improve the joint accuracy of dst from 48.79% to 50.91% on the multiwoz corpus. second, we explore to train a dst meta-learning model with a few domains as source domains and a new domain as target domain. we apply the model-agnostic meta-learning algorithm (maml) to dst and the obtained meta-learning model is used for new domain adaptation. our experimental results show this solution is able to outperform the traditional training approach with extremely less training data in target domain."], "dialogue and interactive systems"], [["fine-grained fact verification with kernel graph attention network", "zhenghao liu | chenyan xiong | maosong sun | zhiyuan liu", "fact verification requires fine-grained natural language inference capability that finds subtle clues to identify the syntactical and semantically correct but not well-supported claims. this paper presents kernel graph attention network (kgat), which conducts more fine-grained fact verification with kernel-based attentions. given a claim and a set of potential evidence sentences that form an evidence graph, kgat introduces node kernels, which better measure the importance of the evidence node, and edge kernels, which conduct fine-grained evidence propagation in the graph, into graph attention networks for more accurate fact verification. kgat achieves a 70.38% fever score and significantly outperforms existing fact verification models on fever, a large-scale benchmark for fact verification. our analyses illustrate that, compared to dot-product attentions, the kernel-based attention concentrates more on relevant evidence sentences and meaningful clues in the evidence graph, which is the main source of kgat\u2019s effectiveness. all source codes of this work are available at https://github.com/thunlp/kernelgat."], "semantics"], [["improving visual question answering by referring to generated paragraph captions", "hyounghun kim | mohit bansal", "paragraph-style image captions describe diverse aspects of an image as opposed to the more common single-sentence captions that only provide an abstract description of the image. these paragraph captions can hence contain substantial information of the image for tasks such as visual question answering. moreover, this textual information is complementary with visual information present in the image because it can discuss both more abstract concepts and more explicit, intermediate symbolic information about objects, events, and scenes that can directly be matched with the textual question and copied into the textual answer (i.e., via easier modality match). hence, we propose a combined visual and textual question answering (vtqa) model which takes as input a paragraph caption as well as the corresponding image, and answers the given question based on both inputs. in our model, the inputs are fused to extract related information by cross-attention (early fusion), then fused again in the form of consensus (late fusion), and finally expected answers are given an extra score to enhance the chance of selection (later fusion). empirical results show that paragraph captions, even when automatically generated (via an rl-based encoder-decoder model), help correctly answer more visual questions. overall, our joint model, when trained on the visual genome dataset, significantly improves the vqa performance over a strong baseline model."], "question answering"], [["towards comprehensive description generation from factual attribute-value tables", "tianyu liu | fuli luo | pengcheng yang | wei wu | baobao chang | zhifang sui", "the comprehensive descriptions for factual attribute-value tables, which should be accurate, informative and loyal, can be very helpful for end users to understand the structured data in this form. however previous neural generators might suffer from key attributes missing, less informative and groundless information problems, which impede the generation of high-quality comprehensive descriptions for tables. to relieve these problems, we first propose force attention (fa) method to encourage the generator to pay more attention to the uncovered attributes to avoid potential key attributes missing. furthermore, we propose reinforcement learning for information richness to generate more informative as well as more loyal descriptions for tables. in our experiments, we utilize the widely used wikibio dataset as a benchmark. besides, we create wb-filter based on wikibio to test our model in the simulated user-oriented scenarios, in which the generated descriptions should accord with particular user interests. experimental results show that our model outperforms the state-of-the-art baselines on both automatic and human evaluation."], "generation"], [["fine-tuning pre-trained transformer language models to distantly supervised relation extraction", "christoph alt | marc h\u00fcbner | leonhard hennig", "distantly supervised relation extraction is widely used to extract relational facts from text, but suffers from noisy labels. current relation extraction methods try to alleviate the noise by multi-instance learning and by providing supporting linguistic and contextual information to more efficiently guide the relation classification. while achieving state-of-the-art results, we observed these models to be biased towards recognizing a limited set of relations with high precision, while ignoring those in the long tail. to address this gap, we utilize a pre-trained language model, the openai generative pre-trained transformer (gpt) (radford et al., 2018). the gpt and similar models have been shown to capture semantic and syntactic features, and also a notable amount of \u201ccommon-sense\u201d knowledge, which we hypothesize are important features for recognizing a more diverse set of relations. by extending the gpt to the distantly supervised setting, and fine-tuning it on the nyt10 dataset, we show that it predicts a larger set of distinct relation types with high confidence. manual and automated evaluation of our model shows that it achieves a state-of-the-art auc score of 0.422 on the nyt10 dataset, and performs especially well at higher recall levels."], "machine translation and multilinguality"], [["show, describe and conclude: on exploiting the structure information of chest x-ray reports", "baoyu jing | zeya wang | eric xing", "chest x-ray (cxr) images are commonly used for clinical screening and diagnosis. automatically writing reports for these images can considerably lighten the workload of radiologists for summarizing descriptive findings and conclusive impressions. the complex structures between and within sections of the reports pose a great challenge to the automatic report generation. specifically, the section impression is a diagnostic summarization over the section findings; and the appearance of normality dominates each section over that of abnormality. existing studies rarely explore and consider this fundamental structure information. in this work, we propose a novel framework which exploits the structure information between and within report sections for generating cxr imaging reports. first, we propose a two-stage strategy that explicitly models the relationship between findings and impression. second, we design a novel co-operative multi-agent system that implicitly captures the imbalanced distribution between abnormality and normality. experiments on two cxr report datasets show that our method achieves state-of-the-art performance in terms of various evaluation metrics. our results expose that the proposed approach is able to generate high-quality medical reports through integrating the structure information."], "language grounding to vision, robotics and beyond"], [["explain yourself! leveraging language models for commonsense reasoning", "nazneen fatema rajani | bryan mccann | caiming xiong | richard socher", "deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. we collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called common sense explanations (cos-e). we use cos-e to train language models to automatically generate explanations that can be used during training and inference in a novel commonsense auto-generated explanation (cage) framework. cage improves the state-of-the-art by 10% on the challenging commonsenseqa task. we further study commonsense reasoning in dnns using both human and auto-generated explanations including transfer to out-of-domain tasks. empirical results indicate that we can effectively leverage language models for commonsense reasoning."], "question answering"], [["out of the echo chamber: detecting countering debate speeches", "matan orbach | yonatan bilu | assaf toledo | dan lahav | michal jacovi | ranit aharonov | noam slonim", "an educated and informed consumption of media content has become a challenge in modern times. with the shift from traditional news outlets to social media and similar venues, a major concern is that readers are becoming encapsulated in \u201cecho chambers\u201d and may fall prey to fake news and disinformation, lacking easy access to dissenting views. we suggest a novel task aiming to alleviate some of these concerns \u2013 that of detecting articles that most effectively counter the arguments \u2013 and not just the stance \u2013 made in a given text. we study this problem in the context of debate speeches. given such a speech, we aim to identify, from among a set of speeches on the same topic and with an opposing stance, the ones that directly counter it. we provide a large dataset of 3,685 such speeches (in english), annotated for this relation, which hopefully would be of general interest to the nlp community. we explore several algorithms addressing this task, and while some are successful, all fall short of expert human performance, suggesting room for further research. all data collected during this work is freely available for research."], "sentiment analysis, stylistic analysis, and argument mining"], [["phonetic and visual priors for decipherment of informal romanization", "maria ryskina | matthew r. gormley | taylor berg-kirkpatrick", "informal romanization is an idiosyncratic process used by humans in informal digital communication to encode non-latin script languages into latin character sets found on common keyboards. character substitution choices differ between users but have been shown to be governed by the same main principles observed across a variety of languages\u2014namely, character pairs are often associated through phonetic or visual similarity. we propose a noisy-channel wfst cascade model for deciphering the original non-latin script from observed romanized text in an unsupervised fashion. we train our model directly on romanized data from two languages: egyptian arabic and russian. we demonstrate that adding inductive bias through phonetic and visual priors on character mappings substantially improves the model\u2019s performance on both languages, yielding results much closer to the supervised skyline. finally, we introduce a new dataset of romanized russian, collected from a russian social network website and partially annotated for our experiments."], "phonology, morphology and word segmentation"], [["a probabilistic generative model for typographical analysis of early modern printing", "kartik goyal | chris dyer | christopher warren | maxwell g\u2019sell | taylor berg-kirkpatrick", "we propose a deep and interpretable probabilistic generative model to analyze glyph shapes in printed early modern documents. we focus on clustering extracted glyph images into underlying templates in the presence of multiple confounding sources of variance. our approach introduces a neural editor model that first generates well-understood printing phenomena like spatial perturbations from template parameters via interpertable latent variables, and then modifies the result by generating a non-interpretable latent vector responsible for inking variations, jitter, noise from the archiving process, and other unforeseen phenomena associated with early modern printing. critically, by introducing an inference network whose input is restricted to the visual residual between the observation and the interpretably-modified template, we are able to control and isolate what the vector-valued latent variable captures. we show that our approach outperforms rigid interpretable clustering baselines (c.f. ocular) and overly-flexible deep generative models (vae) alike on the task of completely unsupervised discovery of typefaces in mixed-fonts documents."], "machine learning for nlp"], [["do you have the right scissors? tailoring pre-trained language models via monte-carlo methods", "ning miao | yuxuan song | hao zhou | lei li", "it has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data. in practice, we observe that fine-tuning a pre-trained model on a small dataset may lead to over- and/or under-estimate problem. in this paper, we propose mc-tailor, a novel method to alleviate the above issue in text generation tasks by truncating and transferring the probability mass from over-estimated regions to under-estimated ones. experiments on a variety of text generation datasets show that mc-tailor consistently and significantly outperforms the fine-tuning approach."], "machine learning for nlp"], [["zero-shot semantic parsing for instructions", "ofer givoli | roi reichart", "we consider a zero-shot semantic parsing task: parsing instructions into compositional logical forms, in domains that were not seen during training. we present a new dataset with 1,390 examples from 7 application domains (e.g. a calendar or a file manager), each example consisting of a triplet: (a) the application\u2019s initial state, (b) an instruction, to be carried out in the context of that state, and (c) the state of the application after carrying out the instruction. we introduce a new training algorithm that aims to train a semantic parser on examples from a set of source domains, so that it can effectively parse instructions from an unknown target domain. we integrate our algorithm into the floating parser of pasupat and liang (2015), and further augment the parser with features and a logical form candidate filtering logic, to support zero-shot adaptation. our experiments with various zero-shot adaptation setups demonstrate substantial performance gains over a non-adapted parser."], "semantics"], [["complex question decomposition for semantic parsing", "haoyu zhang | jingjing cai | jianjun xu | ji wang", "in this work, we focus on complex question semantic parsing and propose a novel hierarchical semantic parsing (hsp) method, which utilizes the decompositionality of complex questions for semantic parsing. our model is designed within a three-stage parsing architecture based on the idea of decomposition-integration. in the first stage, we propose a question decomposer which decomposes a complex question into a sequence of sub-questions. in the second stage, we design an information extractor to derive the type and predicate information of these questions. in the last stage, we integrate the generated information from previous stages and generate a logical form for the complex question. we conduct experiments on complexwebquestions which is a large scale complex question semantic parsing dataset, results show that our model achieves significant improvement compared to state-of-the-art methods."], "semantics"], [["gl-gin: fast and accurate non-autoregressive model for joint multiple intent detection and slot filling", "libo qin | fuxuan wei | tianbao xie | xiao xu | wanxiang che | ting liu", "multi-intent slu can handle multiple intents in an utterance, which has attracted increasing attention. however, the state-of-the-art joint models heavily rely on autoregressive approaches, resulting in two issues: slow inference speed and information leakage. in this paper, we explore a non-autoregressive model for joint multiple intent detection and slot filling, achieving more fast and accurate. specifically, we propose a global-locally graph interaction network (gl-gin) where a local slot-aware graph interaction layer is proposed to model slot dependency for alleviating uncoordinated slots problem while a global intent-slot graph interaction layer is introduced to model the interaction between multiple intents and all slots in the utterance. experimental results on two public datasets show that our framework achieves state-of-the-art performance while being 11.5 times faster."], "dialogue and interactive systems"], [["empirical linguistic study of sentence embeddings", "katarzyna krasnowska-kiera\u015b | alina wr\u00f3blewska", "the purpose of the research is to answer the question whether linguistic information is retained in vector representations of sentences. we introduce a method of analysing the content of sentence embeddings based on universal probing tasks, along with the classification datasets for two contrasting languages. we perform a series of probing and downstream experiments with different types of sentence embeddings, followed by a thorough analysis of the experimental results. aside from dependency parser-based embeddings, linguistic information is retained best in the recently proposed laser sentence embeddings."], "resources and evaluation"], [["r\u02c63: reverse, retrieve, and rank for sarcasm generation with commonsense knowledge", "tuhin chakrabarty | debanjan ghosh | smaranda muresan | nanyun peng", "we propose an unsupervised approach for sarcasm generation based on a non-sarcastic input sentence. our method employs a retrieve-and-edit framework to instantiate two major characteristics of sarcasm: reversal of valence and semantic incongruity with the context, which could include shared commonsense or world knowledge between the speaker and the listener. while prior works on sarcasm generation predominantly focus on context incongruity, we show that combining valence reversal and semantic incongruity based on the commonsense knowledge generates sarcasm of higher quality. human evaluation shows that our system generates sarcasm better than humans 34% of the time, and better than a reinforced hybrid baseline 90% of the time."], "generation"], [["ruddit: norms of offensiveness for english reddit comments", "rishav hada | sohi sudhir | pushkar mishra | helen yannakoudakis | saif m. mohammad | ekaterina shutova", "on social media platforms, hateful and offensive language negatively impact the mental well-being of users and the participation of people from diverse backgrounds. automatic methods to detect offensive language have largely relied on datasets with categorical labels. however, comments can vary in their degree of offensiveness. we create the first dataset of english language reddit comments that has fine-grained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive). the dataset was annotated using best\u2013worst scaling, a form of comparative annotation that has been shown to alleviate known biases of using rating scales. we show that the method produces highly reliable offensiveness scores. finally, we evaluate the ability of widely-used neural models to predict offensiveness scores on this new dataset."], "resources and evaluation"], [["instance-based learning of span representations: a case study through named entity recognition", "hiroki ouchi | jun suzuki | sosuke kobayashi | sho yokoi | tatsuki kuribayashi | ryuto konno | kentaro inui", "interpretable rationales for model predictions play a critical role in practical applications. in this study, we develop models possessing interpretable inference process for structured prediction. specifically, we present a method of instance-based learning that learns similarities between spans. at inference time, each span is assigned a class label based on its similar spans in the training set, where it is easy to understand how much each training instance contributes to the predictions. through empirical analysis on named entity recognition, we demonstrate that our method enables to build models that have high interpretability without sacrificing performance."], "information extraction, retrieval and text mining"], [["integrated directional gradients: feature interaction attribution for neural nlp models", "sandipan sikdar | parantapa bhattacharya | kieran heese", "in this paper, we introduce integrated directional gradients (idg), a method for attributing importance scores to groups of features, indicating their relevance to the output of a neural network model for a given input. the success of deep neural networks has been attributed to their ability to capture higher level feature interactions. hence, in the last few years capturing the importance of these feature interactions has received increased prominence in ml interpretability literature. in this paper, we formally define the feature group attribution problem and outline a set of axioms that any intuitive feature group attribution method should satisfy. earlier, cooperative game theory inspired axiomatic methods only borrowed axioms from solution concepts (such as shapley value) for individual feature attributions and introduced their own extensions to model interactions. in contrast, our formulation is inspired by axioms satisfied by characteristic functions as well as solution concepts in cooperative game theory literature. we believe that characteristic functions are much better suited to model importance of groups compared to just solution concepts. we demonstrate that our proposed method, idg, satisfies all the axioms. using idg we analyze two state-of-the-art text classifiers on three benchmark datasets for sentiment analysis. our experiments show that idg is able to effectively capture semantic interactions in linguistic models via negations and conjunctions."], "interpretability and analysis of models for nlp"], [["active imitation learning with noisy guidance", "kiant\u00e9 brantley | amr sharaf | hal daum\u00e9 iii", "imitation learning algorithms provide state-of-the-art results on many structured prediction tasks by learning near-optimal search policies. such algorithms assume training-time access to an expert that can provide the optimal action at any queried state; unfortunately, the number of such queries is often prohibitive, frequently rendering these approaches impractical. to combat this query complexity, we consider an active learning setting in which the learning algorithm has additional access to a much cheaper noisy heuristic that provides noisy guidance. our algorithm, leaqi, learns a difference classifier that predicts when the expert is likely to disagree with the heuristic, and queries the expert only when necessary. we apply leaqi to three sequence labelling tasks, demonstrating significantly fewer queries to the expert and comparable (or better) accuracies over a passive approach."], "machine learning for nlp"], [["incremental learning from scratch for task-oriented dialogue systems", "weikang wang | jiajun zhang | qian li | mei-yuh hwang | chengqing zong | zhifei li", "clarifying user needs is essential for existing task-oriented dialogue systems. however, in real-world applications, developers can never guarantee that all possible user demands are taken into account in the design phase. consequently, existing systems will break down when encountering unconsidered user needs. to address this problem, we propose a novel incremental learning framework to design task-oriented dialogue systems, or for short incremental dialogue system (ids), without pre-defining the exhaustive list of user needs. specifically, we introduce an uncertainty estimation module to evaluate the confidence of giving correct responses. if there is high confidence, ids will provide responses to users. otherwise, humans will be involved in the dialogue process, and ids can learn from human intervention through an online learning module. to evaluate our method, we propose a new dataset which simulates unanticipated user needs in the deployment stage. experiments show that ids is robust to unconsidered user actions, and can update itself online by smartly selecting only the most effective training data, and hence attains better performance with less annotation cost."], "dialogue and interactive systems"], [["compositional questions do not necessitate multi-hop reasoning", "sewon min | eric wallace | sameer singh | matt gardner | hannaneh hajishirzi | luke zettlemoyer", "multi-hop reading comprehension (rc) questions are challenging because they require reading and reasoning over multiple paragraphs. we argue that it can be difficult to construct large multi-hop rc datasets. for example, even highly compositional questions can be answered with a single hop if they target specific entity types, or the facts needed to answer them are redundant. our analysis is centered on hotpotqa, where we show that single-hop reasoning can solve much more of the dataset than previously thought. we introduce a single-hop bert-based rc model that achieves 67 f1\u2014comparable to state-of-the-art multi-hop models. we also design an evaluation setting where humans are not shown all of the necessary paragraphs for the intended multi-hop reasoning but can still answer over 80% of questions. together with detailed error analysis, these results suggest there should be an increasing focus on the role of evidence in multi-hop reasoning and possibly even a shift towards information retrieval style evaluations with large and diverse evidence collections."], "question answering"], [["analysis of automatic annotation suggestions for hard discourse-level tasks in expert domains", "claudia schulz | christian m. meyer | jan kiesewetter | michael sailer | elisabeth bauer | martin r. fischer | frank fischer | iryna gurevych", "many complex discourse-level tasks can aid domain experts in their work but require costly expert annotations for data creation. to speed up and ease annotations, we investigate the viability of automatically generated annotation suggestions for such tasks. as an example, we choose a task that is particularly hard for both humans and machines: the segmentation and classification of epistemic activities in diagnostic reasoning texts. we create and publish a new dataset covering two domains and carefully analyse the suggested annotations. we find that suggestions have positive effects on annotation speed and performance, while not introducing noteworthy biases. envisioning suggestion models that improve with newly annotated texts, we contrast methods for continuous model adjustment and suggest the most effective setup for suggestions in future expert tasks."], "resources and evaluation"], [["what you say and how you say it matters: predicting stock volatility using verbal and vocal cues", "yu qin | yi yang", "predicting financial risk is an essential task in financial market. prior research has shown that textual information in a firm\u2019s financial statement can be used to predict its stock\u2019s risk level. nowadays, firm ceos communicate information not only verbally through press releases and financial reports, but also nonverbally through investor meetings and earnings conference calls. there are anecdotal evidences that ceo\u2019s vocal features, such as emotions and voice tones, can reveal the firm\u2019s performance. however, how vocal features can be used to predict risk levels, and to what extent, is still unknown. to fill the gap, we obtain earnings call audio recordings and textual transcripts for s&p 500 companies in recent years. we propose a multimodal deep regression model (mdrm) that jointly model ceo\u2019s verbal (from text) and vocal (from audio) information in a conference call. empirical results show that our model that jointly considers verbal and vocal features achieves significant and substantial prediction error reduction. we also discuss several interesting findings and the implications to financial markets. the processed earnings conference calls data (text and audio) are released for readers who are interested in reproducing the results or designing trading strategy."], "nlp applications"], [["revisiting higher-order dependency parsers", "erick fonseca | andr\u00e9 f. t. martins", "neural encoders have allowed dependency parsers to shift from higher-order structured models to simpler first-order ones, making decoding faster and still achieving better accuracy than non-neural parsers. this has led to a belief that neural encoders can implicitly encode structural constraints, such as siblings and grandparents in a tree. we tested this hypothesis and found that neural parsers may benefit from higher-order features, even when employing a powerful pre-trained encoder, such as bert. while the gains of higher-order features are small in the presence of a powerful encoder, they are consistent for long-range dependencies and long sentences. in particular, higher-order models are more accurate on full sentence parses and on the exact match of modifier lists, indicating that they deal better with larger, more complex structures."], "tagging, chunking, syntax and parsing"], [["harnessing the linguistic signal to predict scalar inferences", "sebastian schuster | yuxing chen | judith degen", "pragmatic inferences often subtly depend on the presence or absence of linguistic features. for example, the presence of a partitive construction (of the) increases the strength of a so-called scalar inference: listeners perceive the inference that chris did not eat all of the cookies to be stronger after hearing \u201cchris ate some of the cookies\u201d than after hearing the same utterance without a partitive, \u201cchris ate some cookies\u201d. in this work, we explore to what extent neural network sentence encoders can learn to predict the strength of scalar inferences. we first show that an lstm-based sentence encoder trained on an english dataset of human inference strength ratings is able to predict ratings with high accuracy (r = 0.78). we then probe the model\u2019s behavior using manually constructed minimal sentence pairs and corpus data. we first that the model inferred previously established associations between linguistic features and inference strength, suggesting that the model learns to use linguistic features to predict pragmatic inferences."], "discourse and pragmatics"], [["cross-modal commentator: automatic machine commenting based on cross-modal information", "pengcheng yang | zhihan zhang | fuli luo | lei li | chengyang huang | xu sun", "automatic commenting of online articles can provide additional opinions and facts to the reader, which improves user experience and engagement on social media platforms. previous work focuses on automatic commenting based solely on textual content. however, in real-scenarios, online articles usually contain multiple modal contents. for instance, graphic news contains plenty of images in addition to text. contents other than text are also vital because they are not only more attractive to the reader but also may provide critical information. to remedy this, we propose a new task: cross-model automatic commenting (cmac), which aims to make comments by integrating multiple modal contents. we construct a large-scale dataset for this task and explore several representative methods. going a step further, an effective co-attention model is presented to capture the dependency between textual and visual information. evaluation results show that our proposed model can achieve better performance than competitive baselines."], "generation"], [["bilingual mutual information based adaptive training for neural machine translation", "yangyifan xu | yijin liu | fandong meng | jiajun zhang | jinan xu | jie zhou", "recently, token-level adaptive training has achieved promising improvement in machine translation, where the cross-entropy loss function is adjusted by assigning different training weights to different tokens, in order to alleviate the token imbalance problem. however, previous approaches only use static word frequency information in the target language without considering the source language, which is insufficient for bilingual tasks like machine translation. in this paper, we propose a novel bilingual mutual information (bmi) based adaptive objective, which measures the learning difficulty for each target token from the perspective of bilingualism, and assigns an adaptive weight accordingly to improve token-level adaptive training. this method assigns larger training weights to tokens with higher bmi, so that easy tokens are updated with coarse granularity while difficult tokens are updated with fine granularity. experimental results on wmt14 english-to-german and wmt19 chinese-to-english demonstrate the superiority of our approach compared with the transformer baseline and previous token-level adaptive training approaches. further analyses confirm that our method can improve the lexical diversity."], "machine translation and multilinguality"], [["on the compositionality prediction of noun phrases using poincar\u00e9 embeddings", "abhik jana | dima puzyrev | alexander panchenko | pawan goyal | chris biemann | animesh mukherjee", "the compositionality degree of multiword expressions indicates to what extent the meaning of a phrase can be derived from the meaning of its constituents and their grammatical relations. prediction of (non)-compositionality is a task that has been frequently addressed with distributional semantic models. we introduce a novel technique to blend hierarchical information with distributional information for predicting compositionality. in particular, we use hypernymy information of the multiword and its constituents encoded in the form of the recently introduced poincar\u00e9 embeddings in addition to the distributional information to detect compositionality for noun phrases. using a weighted average of the distributional similarity and a poincar\u00e9 similarity function, we obtain consistent and substantial, statistically significant improvement across three gold standard datasets over state-of-the-art models based on distributional information only. unlike traditional approaches that solely use an unsupervised setting, we have also framed the problem as a supervised task, obtaining comparable improvements. further, we publicly release our poincar\u00e9 embeddings, which are trained on the output of handcrafted lexical-syntactic patterns on a large corpus."], "semantics"], [["deep unknown intent detection with margin loss", "ting-en lin | hua xu", "identifying the unknown (novel) user intents that have never appeared in the training set is a challenging task in the dialogue system. in this paper, we present a two-stage method for detecting unknown intents. we use bidirectional long short-term memory (bilstm) network with the margin loss as the feature extractor. with margin loss, we can learn discriminative deep features by forcing the network to maximize inter-class variance and to minimize intra-class variance. then, we feed the feature vectors to the density-based novelty detection algorithm, local outlier factor (lof), to detect unknown intents. experiments on two benchmark datasets show that our method can yield consistent improvements compared with the baseline methods."], "dialogue and interactive systems"], [["gpt-too: a language-model-first approach for amr-to-text generation", "manuel mager | ram\u00f3n fernandez astudillo | tahira naseem | md arafat sultan | young-suk lee | radu florian | salim roukos", "abstract meaning representations (amrs) are broad-coverage sentence-level semantic graphs. existing approaches to generating text from amr have focused on training sequence-to-sequence or graph-to-sequence models on amr annotated data only. in this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring. despite the simplicity of the approach, our experimental results show these models outperform all previous techniques on the english ldc2017t10 dataset, including the recent use of transformer architectures. in addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach."], "generation"], [["a novel cascade binary tagging framework for relational triple extraction", "zhepei wei | jianlin su | yue wang | yuan tian | yi chang", "extracting relational triples from unstructured text is crucial for large-scale knowledge graph construction. however, few existing works excel in solving the overlapping triple problem where multiple relational triples in the same sentence share the same entities. in this work, we introduce a fresh perspective to revisit the relational triple extraction task and propose a novel cascade binary tagging framework (casrel) derived from a principled problem formulation. instead of treating relations as discrete labels as in previous works, our new framework models relations as functions that map subjects to objects in a sentence, which naturally handles the overlapping problem. experiments show that the casrel framework already outperforms state-of-the-art methods even when its encoder module uses a randomly initialized bert encoder, showing the power of the new tagging framework. it enjoys further performance boost when employing a pre-trained bert encoder, outperforming the strongest baseline by 17.5 and 30.2 absolute gain in f1-score on two public datasets nyt and webnlg, respectively. in-depth analysis on different scenarios of overlapping triples shows that the method delivers consistent performance gain across all these scenarios. the source code and data are released online."], "information extraction, retrieval and text mining"], [["doqa - accessing domain-specific faqs via conversational qa", "jon ander campos | arantxa otegi | aitor soroa | jan deriu | mark cieliebak | eneko agirre", "the goal of this work is to build conversational question answering (qa) interfaces for the large body of domain-specific information available in faq sites. we present doqa, a dataset with 2,437 dialogues and 10,917 qa pairs. the dialogues are collected from three stack exchange sites using the wizard of oz method with crowdsourcing. compared to previous work, doqa comprises well-defined information needs, leading to more coherent and natural conversations with less factoid questions and is multi-domain. in addition, we introduce a more realistic information retrieval (ir) scenario where the system needs to find the answer in any of the faq documents. the results of an existing, strong, system show that, thanks to transfer learning from a wikipedia qa dataset and fine tuning on a single faq domain, it is possible to build high quality conversational qa systems for faqs without in-domain training data. the good results carry over into the more challenging ir scenario. in both cases, there is still ample room for improvement, as indicated by the higher human upperbound."], "question answering"], [["jointly masked sequence-to-sequence model for non-autoregressive neural machine translation", "junliang guo | linli xu | enhong chen", "the masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. however, few works have adopted this technique in the sequence-to-sequence models. in this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(nat). specifically, we first empirically study the functionalities of the encoder and the decoder in nat models, and find that the encoder takes a more important role than the decoder regarding the translation quality. therefore, we propose to train the encoder more rigorously by masking the encoder input while training. as for the decoder, we propose to train it based on the consecutive masking of the decoder input with an n-gram loss function to alleviate the problem of translating duplicate words. the two types of masks are applied to the model jointly at the training stage. we conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 bleu scores on wmt14 english-german/german-english tasks with 5+ times speed up compared with an autoregressive model."], "machine translation and multilinguality"], [["politeness transfer: a tag and generate approach", "aman madaan | amrith setlur | tanmay parekh | barnabas poczos | graham neubig | yiming yang | ruslan salakhutdinov | alan w black | shrimai prabhumoye", "this paper introduces a new task of politeness transfer which involves converting non-polite sentences to polite sentences while preserving the meaning. we also provide a dataset of more than 1.39 instances automatically labeled for politeness to encourage benchmark evaluations on this new task. we design a tag and generate pipeline that identifies stylistic attributes and subsequently generates a sentence in the target style while preserving most of the source content. for politeness as well as five other transfer tasks, our model outperforms the state-of-the-art methods on automatic metrics for content preservation, with a comparable or better performance on style transfer accuracy. additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer accuracy across all the six style transfer tasks. the data and code is located at https://github.com/tag-and-generate."], "generation"], [["document-level event role filler extraction using multi-granularity contextualized encoding", "xinya du | claire cardie", "few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. this is problematic when the information needed to recognize an event argument is spread across multiple sentences. we argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. we first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models\u2019 performance. to dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. we evaluate our models on the muc-4 event extraction dataset, and show that our best system performs substantially better than prior work. we also report findings on the relationship between context length and neural model performance on the task."], "information extraction, retrieval and text mining"], [["tagged back-translation revisited: why does it really work?", "benjamin marie | raphael rubino | atsushi fujita", "in this paper, we show that neural machine translation (nmt) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts. such nmt systems better translate human-produced translations, i.e., translationese, but may largely worsen the translation quality of original texts. our analysis reveals that adding a simple tag to back-translations prevents this quality degradation and improves on average the overall translation quality by helping the nmt system to distinguish back-translated data from original parallel data during training. we also show that, in contrast to high-resource configurations, nmt systems trained in low-resource settings are much less vulnerable to overfit back-translations. we conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown."], "machine translation and multilinguality"], [["improving disentangled text representation learning with information-theoretic guidance", "pengyu cheng | martin renqiang min | dinghan shen | christopher malon | yizhe zhang | yitong li | lawrence carin", "learning disentangled representations of natural language is essential for many nlp tasks, e.g., conditional text generation, style transfer, personalized dialogue systems, etc. similar problems have been studied extensively for other forms of data, such as images and videos. however, the discrete nature of natural language makes the disentangling of textual representations more challenging (e.g., the manipulation over the data space cannot be easily achieved). inspired by information theory, we propose a novel method that effectively manifests disentangled representations of text, without any supervision on semantics. a new mutual information upper bound is derived and leveraged to measure dependence between style and content. by minimizing this upper bound, the proposed method induces style and content embeddings into two independent low-dimensional spaces. experiments on both conditional text generation and text-style transfer demonstrate the high quality of our disentangled representation in terms of content and style preservation."], "machine learning for nlp"], [["dual slot selector via local reliability verification for dialogue state tracking", "jinyu guo | kai shuang | jijie li | zihan wang", "the goal of dialogue state tracking (dst) is to predict the current dialogue state given all previous dialogue contexts. existing approaches generally predict the dialogue state at every turn from scratch. however, the overwhelming majority of the slots in each turn should simply inherit the slot values from the previous turn. therefore, the mechanism of treating slots equally in each turn not only is inefficient but also may lead to additional errors because of the redundant slot value generation. to address this problem, we devise the two-stage dss-dst which consists of the dual slot selector based on the current turn dialogue, and the slot value generator based on the dialogue history. the dual slot selector determines each slot whether to update slot value or to inherit the slot value from the previous turn from two aspects: (1) if there is a strong relationship between it and the current turn dialogue utterances; (2) if a slot value with high reliability can be obtained for it through the current turn dialogue. the slots selected to be updated are permitted to enter the slot value generator to update values by a hybrid method, while the other slots directly inherit the values from the previous turn. empirical results show that our method achieves 56.93%, 60.73%, and 58.04% joint accuracy on multiwoz 2.0, multiwoz 2.1, and multiwoz 2.2 datasets respectively and achieves a new state-of-the-art performance with significant improvements."], "dialogue and interactive systems"], [["are you convinced? choosing the more convincing evidence with a siamese network", "martin gleize | eyal shnarch | leshem choshen | lena dankin | guy moshkowich | ranit aharonov | noam slonim", "with the advancement in argument detection, we suggest to pay more attention to the challenging task of identifying the more convincing arguments. machines capable of responding and interacting with humans in helpful ways have become ubiquitous. we now expect them to discuss with us the more delicate questions in our world, and they should do so armed with effective arguments. but what makes an argument more persuasive? what will convince you? in this paper, we present a new data set, ibm-eviconv, of pairs of evidence labeled for convincingness, designed to be more challenging than existing alternatives. we also propose a siamese neural network architecture shown to outperform several baselines on both a prior convincingness data set and our own. finally, we provide insights into our experimental results and the various kinds of argumentative value our method is capable of detecting."], "sentiment analysis, stylistic analysis, and argument mining"], [["the knowref coreference corpus: removing gender and number cues for difficult pronominal anaphora resolution", "ali emami | paul trichelair | adam trischler | kaheer suleman | hannes schulz | jackie chi kit cheung", "we introduce a new benchmark for coreference resolution and nli, knowref, that targets common-sense understanding and world knowledge. previous coreference resolution tasks can largely be solved by exploiting the number and gender of the antecedents, or have been handcrafted and do not reflect the diversity of naturally occurring text. we present a corpus of over 8,000 annotated text passages with ambiguous pronominal anaphora. these instances are both challenging and realistic. we show that various coreference systems, whether rule-based, feature-rich, or neural, perform significantly worse on the task than humans, who display high inter-annotator agreement. to explain this performance gap, we show empirically that state-of-the art models often fail to capture context, instead relying on the gender or number of candidate antecedents to make a decision. we then use problem-specific insights to propose a data-augmentation trick called antecedent switching to alleviate this tendency in models. finally, we show that antecedent switching yields promising results on other tasks as well: we use it to achieve state-of-the-art results on the gap coreference task."], "linguistic theories, cognitive modeling and psycholinguistics"], [["low-dimensional hyperbolic knowledge graph embeddings", "ines chami | adva wolf | da-cheng juan | frederic sala | sujith ravi | christopher r\u00e9", "knowledge graph (kg) embeddings learn low- dimensional representations of entities and relations to predict missing facts. kgs often exhibit hierarchical and logical patterns which must be preserved in the embedding space. for hierarchical data, hyperbolic embedding methods have shown promise for high-fidelity and parsimonious representations. however, existing hyperbolic embedding methods do not account for the rich logical patterns in kgs. in this work, we introduce a class of hyperbolic kg embedding models that simultaneously capture hierarchical and logical patterns. our approach combines hyperbolic reflections and rotations with attention to model complex relational patterns. experimental results on standard kg benchmarks show that our method improves over previous euclidean- and hyperbolic-based efforts by up to 6.1% in mean reciprocal rank (mrr) in low dimensions. furthermore, we observe that different geometric transformations capture different types of relations while attention- based transformations generalize to multiple relations. in high dimensions, our approach yields new state-of-the-art mrrs of 49.6% on wn18rr and 57.7% on yago3-10."], "machine learning for nlp"], [["a large-scale multi-document summarization dataset from the wikipedia current events portal", "demian gholipour ghalandari | chris hokamp | nghia the pham | john glover | georgiana ifrim", "multi-document summarization (mds) aims to compress the content in large document collections into short summaries and has important applications in story clustering for newsfeeds, presentation of search results, and timeline generation. however, there is a lack of datasets that realistically address such use cases at a scale large enough for training supervised models for this task. this work presents a new dataset for mds that is large both in the total number of document clusters and in the size of individual clusters. we build this dataset by leveraging the wikipedia current events portal (wcep), which provides concise and neutral human-written summaries of news events, with links to external source articles. we also automatically extend these source articles by looking for related articles in the common crawl archive. we provide a quantitative analysis of the dataset and empirical results for several state-of-the-art mds techniques."], "summarization"], [["ptb graph parsing with tree approximation", "yoshihide kato | shigeki matsubara", "the penn treebank (ptb) represents syntactic structures as graphs due to nonlocal dependencies. this paper proposes a method that approximates ptb graph-structured representations by trees. by our approximation method, we can reduce nonlocal dependency identification and constituency parsing into single tree-based parsing. an experimental result demonstrates that our approximation method with an off-the-shelf tree-based constituency parser significantly outperforms the previous methods in nonlocal dependency identification."], "tagging, chunking, syntax and parsing"], [["lexically constrained neural machine translation with levenshtein transformer", "raymond hendy susanto | shamil chollampatt | liling tan", "this paper proposes a simple and effective algorithm for incorporating lexical constraints in neural machine translation. previous work either required re-training existing models with the lexical constraints or incorporating them during beam search decoding with significantly higher computational overheads. leveraging the flexibility and speed of a recently proposed levenshtein transformer model (gu et al., 2019), our method injects terminology constraints at inference time without any impact on decoding speed. our method does not require any modification to the training procedure and can be easily applied at runtime with custom dictionaries. experiments on english-german wmt datasets show that our approach improves an unconstrained baseline and previous approaches."], "machine translation and multilinguality"], [["cross-modal coherence modeling for caption generation", "malihe alikhani | piyush sharma | shengjie li | radu soricut | matthew stone", "we use coherence relations inspired by computational models of discourse to study the information needs and goals of image captioning. using an annotation protocol specifically devised for capturing image\u2013caption coherence relations, we annotate 10,000 instances from publicly-available image\u2013caption pairs. we introduce a new task for learning inferences in imagery and text, coherence relation prediction, and show that these coherence annotations can be exploited to learn relation classifiers as an intermediary step, and also train coherence-aware, controllable image captioning models. the results show a dramatic improvement in the consistency and quality of the generated captions with respect to information needs specified via coherence relations."], "language grounding to vision, robotics and beyond"], [["predicting declension class from form and meaning", "adina williams | tiago pimentel | hagen blix | arya d. mccarthy | eleanor chodroff | ryan cotterell", "the noun lexica of many natural languages are divided into several declension classes with characteristic morphological properties. class membership is far from deterministic, but the phonological form of a noun and/or its meaning can often provide imperfect clues. here, we investigate the strength of those clues. more specifically, we operationalize this by measuring how much information, in bits, we can glean about declension class from knowing the form and/or meaning of nouns. we know that form and meaning are often also indicative of grammatical gender\u2014which, as we quantitatively verify, can itself share information with declension class\u2014so we also control for gender. we find for two indo-european languages (czech and german) that form and meaning respectively share significant amounts of information with class (and contribute additional information above and beyond gender). the three-way interaction between class, form, and meaning (given gender) is also significant. our study is important for two reasons: first, we introduce a new method that provides additional quantitative support for a classic linguistic finding that form and meaning are relevant for the classification of nouns into declensions. secondly, we show not only that individual declensions classes vary in the strength of their clues within a language, but also that these variations themselves vary across languages."], "phonology, morphology and word segmentation"], [["neural crf model for sentence alignment in text simplification", "chao jiang | mounica maddela | wuwei lan | yang zhong | wei xu", "the success of a text simplification system heavily depends on the quality and quantity of complex-simple sentence pairs in the training corpus, which are extracted by aligning sentences between parallel articles. to evaluate and improve sentence alignment quality, we create two manually annotated sentence-aligned datasets from two commonly used text simplification corpora, newsela and wikipedia. we propose a novel neural crf alignment model which not only leverages the sequential nature of sentences in parallel documents but also utilizes a neural sentence pair model to capture semantic similarity. experiments demonstrate that our proposed approach outperforms all the previous work on monolingual sentence alignment task by more than 5 points in f1. we apply our crf aligner to construct two new text simplification datasets, newsela-auto and wiki-auto, which are much larger and of better quality compared to the existing datasets. a transformer-based seq2seq model trained on our datasets establishes a new state-of-the-art for text simplification in both automatic and human evaluation."], "generation"], [["detecting propaganda techniques in memes", "dimitar dimitrov | bishr bin ali | shaden shaar | firoj alam | fabrizio silvestri | hamed firooz | preslav nakov | giovanni da san martino", "propaganda can be defined as a form of communication that aims to influence the opinions or the actions of people towards a specific goal; this is achieved by means of well-defined rhetorical and psychological devices. propaganda, in the form we know it today, can be dated back to the beginning of the 17th century. however, it is with the advent of the internet and the social media that propaganda has started to spread on a much larger scale than before, thus becoming major societal and political issue. nowadays, a large fraction of propaganda in social media is multimodal, mixing textual with visual content. with this in mind, here we propose a new multi-label multimodal task: detecting the type of propaganda techniques used in memes. we further create and release a new corpus of 950 memes, carefully annotated with 22 propaganda techniques, which can appear in the text, in the image, or in both. our analysis of the corpus shows that understanding both modalities together is essential for detecting these techniques. this is further confirmed in our experiments with several state-of-the-art multimodal models."], "nlp applications"], [["entity-relation extraction as multi-turn question answering", "xiaoya li | fan yin | zijun sun | xiayu li | arianna yuan | duo chai | mingxin zhou | jiwei li", "in this paper, we propose a new paradigm for the task of entity-relation extraction. we cast the task as a multi-turn question answering problem, i.e., the extraction of entities and elations is transformed to the task of identifying answer spans from the context. this multi-turn qa formalization comes with several key advantages: firstly, the question query encodes important information for the entity/relation class we want to identify; secondly, qa provides a natural way of jointly modeling entity and relation; and thirdly, it allows us to exploit the well developed machine reading comprehension (mrc) models. experiments on the ace and the conll04 corpora demonstrate that the proposed paradigm significantly outperforms previous best models. we are able to obtain the state-of-the-art results on all of the ace04, ace05 and conll04 datasets, increasing the sota results on the three datasets to 49.6 (+1.2), 60.3 (+0.7) and 69.2 (+1.4), respectively. additionally, we construct and will release a newly developed dataset resume, which requires multi-step reasoning to construct entity dependencies, as opposed to the single-step dependency extraction in the triplet exaction in previous datasets. the proposed multi-turn qa model also achieves the best performance on the resume dataset."], "information extraction, retrieval and text mining"], [["efficient constituency parsing by pointing", "thanh-tung nguyen | xuan-phi nguyen | shafiq joty | xiaoli li", "we propose a novel constituency parsing model that casts the parsing problem into a series of pointing tasks. specifically, our model estimates the likelihood of a span being a legitimate tree constituent via the pointing score corresponding to the boundary words of the span. our parsing model supports efficient top-down decoding and our learning objective is able to enforce structural consistency without resorting to the expensive cky inference. the experiments on the standard english penn treebank parsing task show that our method achieves 92.78 f1 without using pre-trained models, which is higher than all the existing methods with similar time complexity. using pre-trained bert, our model achieves 95.48 f1, which is competitive with the state-of-the-art while being faster. our approach also establishes new state-of-the-art in basque and swedish in the spmrl shared tasks on multilingual constituency parsing."], "tagging, chunking, syntax and parsing"], [["diverse and informative dialogue generation with context-specific commonsense knowledge awareness", "sixing wu | ying li | dawei zhang | yang zhou | zhonghai wu", "generative dialogue systems tend to produce generic responses, which often leads to boring conversations. for alleviating this issue, recent studies proposed to retrieve and introduce knowledge facts from knowledge graphs. while this paradigm works to a certain extent, it usually retrieves knowledge facts only based on the entity word itself, without considering the specific dialogue context. thus, the introduction of the context-irrelevant knowledge facts can impact the quality of generations. to this end, this paper proposes a novel commonsense knowledge-aware dialogue generation model, conkadi. we design a felicitous fact mechanism to help the model focus on the knowledge facts that are highly relevant to the context; furthermore, two techniques, context-knowledge fusion and flexible mode fusion are proposed to facilitate the integration of the knowledge in the conkadi. we collect and build a large-scale chinese dataset aligned with the commonsense knowledge for dialogue generation. extensive evaluations over both an open-released english dataset and our chinese dataset demonstrate that our approach conkadi outperforms the state-of-the-art approach ccm, in most experiments."], "dialogue and interactive systems"], [["do neural language models show preferences for syntactic formalisms?", "artur kulmizev | vinit ravishankar | mostafa abdou | joakim nivre", "recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces. however, such studies often suffer from limited scope by focusing on a single language and a single linguistic formalism. in this study, we aim to investigate the extent to which the semblance of syntactic structure captured by language models adheres to a surface-syntactic or deep syntactic style of analysis, and whether the patterns are consistent across different languages. we apply a probe for extracting directed dependency trees to bert and elmo models trained on 13 different languages, probing for two different syntactic annotation styles: universal dependencies (ud), prioritizing deep syntactic relations, and surface-syntactic universal dependencies (sud), focusing on surface structure. we find that both models exhibit a preference for ud over sud \u2014 with interesting variations across languages and layers \u2014 and that the strength of this preference is correlated with differences in tree shape."], "tagging, chunking, syntax and parsing"], [["vault: variable unified long text representation for machine reading comprehension", "haoyang wen | anthony ferritto | heng ji | radu florian | avi sil", "existing models on machine reading comprehension (mrc) require complex model architecture for effectively modeling long texts with paragraph representation and classification, thereby making inference computationally inefficient for production use. in this work, we propose vault: a light-weight and parallel-efficient paragraph representation for mrc based on contextualized representation from long document input, trained using a new gaussian distribution-based objective that pays close attention to the partially correct instances that are close to the ground-truth. we validate our vault architecture showing experimental results on two benchmark mrc datasets that require long context modeling; one wikipedia-based (natural questions (nq)) and the other on technotes (techqa). vault can achieve comparable performance on nq with a state-of-the-art (sota) complex document modeling approach while being 16 times faster, demonstrating the efficiency of our proposed model. we also demonstrate that our model can also be effectively adapted to a completely different domain \u2013 techqa \u2013 with large improvement over a model fine-tuned on a previously published large plm."], "question answering"], [["improved sentiment detection via label transfer from monolingual to synthetic code-switched text", "bidisha samanta | niloy ganguly | soumen chakrabarti", "multilingual writers and speakers often alternate between two languages in a single discourse. this practice is called \u201ccode-switching\u201d. existing sentiment detection methods are usually trained on sentiment-labeled monolingual text. manually labeled code-switched text, especially involving minority languages, is extremely rare. consequently, the best monolingual methods perform relatively poorly on code-switched text. we present an effective technique for synthesizing labeled code-switched text from labeled monolingual text, which is relatively readily available. the idea is to replace carefully selected subtrees of constituency parses of sentences in the resource-rich language with suitable token spans selected from automatic translations to the resource-poor language. by augmenting the scarce labeled code-switched text with plentiful synthetic labeled code-switched text, we achieve significant improvements in sentiment labeling accuracy (1.5%, 5.11% 7.20%) for three different language pairs (english-hindi, english-spanish and english-bengali). the improvement is even significant in hatespeech detection whereby we achieve a 4% improvement using only synthetic code-switched data (6% with data augmentation)."], "sentiment analysis, stylistic analysis, and argument mining"], [["interpreting pretrained contextualized representations via reductions to static embeddings", "rishi bommasani | kelly davis | claire cardie", "contextualized representations (e.g. elmo, bert) have become the default pretrained representations for downstream nlp applications. in some settings, this transition has rendered their static embedding predecessors (e.g. word2vec, glove) obsolete. as a side-effect, we observe that older interpretability methods for static embeddings \u2014 while more diverse and mature than those available for their dynamic counterparts \u2014 are underutilized in studying newer contextualized representations. consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data. concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings."], "interpretability and analysis of models for nlp"], [["neuinfer: knowledge inference on n-ary facts", "saiping guan | xiaolong jin | jiafeng guo | yuanzhuo wang | xueqi cheng", "knowledge inference on knowledge graph has attracted extensive attention, which aims to find out connotative valid facts in knowledge graph and is very helpful for improving the performance of many downstream applications. however, researchers have mainly poured attention to knowledge inference on binary facts. the studies on n-ary facts are relatively scarcer, although they are also ubiquitous in the real world. therefore, this paper addresses knowledge inference on n-ary facts. we represent each n-ary fact as a primary triple coupled with a set of its auxiliary descriptive attribute-value pair(s). we further propose a neural network model, neuinfer, for knowledge inference on n-ary facts. besides handling the common task to infer an unknown element in a whole fact, neuinfer can cope with a new type of task, flexible knowledge inference. it aims to infer an unknown element in a partial fact consisting of the primary triple coupled with any number of its auxiliary description(s). experimental results demonstrate the remarkable superiority of neuinfer."], "semantics"], [["predicting performance for natural language processing tasks", "mengzhou xia | antonios anastasopoulos | ruochen xu | yiming yang | graham neubig", "given the complexity of combinations of tasks, languages, and domains in natural language processing (nlp) research, it is computationally prohibitive to exhaustively test newly proposed models on each possible experimental setting. in this work, we attempt to explore the possibility of gaining plausible judgments of how well an nlp model can perform under an experimental setting, without actually training or testing the model. to do so, we build regression models to predict the evaluation score of an nlp experiment given the experimental settings as input. experimenting on~9 different nlp tasks, we find that our predictors can produce meaningful predictions over unseen languages and different modeling architectures, outperforming reasonable baselines as well as human experts. %we represent experimental settings using an array of features. going further, we outline how our predictor can be used to find a small subset of representative experiments that should be run in order to obtain plausible predictions for all other experimental settings."], "nlp applications"], [["mobilebert: a compact task-agnostic bert for resource-limited devices", "zhiqing sun | hongkun yu | xiaodan song | renjie liu | yiming yang | denny zhou", "natural language processing (nlp) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. however, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices. in this paper, we propose mobilebert for compressing and accelerating the popular bert model. like the original bert, mobilebert is task-agnostic, that is, it can be generically applied to various downstream nlp tasks via simple fine-tuning. basically, mobilebert is a thin version of bert_large, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. to train mobilebert, we first train a specially designed teacher model, an inverted-bottleneck incorporated bert_large model. then, we conduct knowledge transfer from this teacher to mobilebert. empirical studies show that mobilebert is 4.3x smaller and 5.5x faster than bert_base while achieving competitive results on well-known benchmarks. on the natural language inference tasks of glue, mobilebert achieves a glue score of 77.7 (0.6 lower than bert_base), and 62 ms latency on a pixel 4 phone. on the squad v1.1/v2.0 question answering task, mobilebert achieves a dev f1 score of 90.0/79.2 (1.5/2.1 higher than bert_base)."], "machine learning for nlp"], [["label-specific dual graph neural network for multi-label text classification", "qianwen ma | chunyuan yuan | wei zhou | songlin hu", "multi-label text classification is one of the fundamental tasks in natural language processing. previous studies have difficulties to distinguish similar labels well because they learn the same document representations for different labels, that is they do not explicitly extract label-specific semantic components from documents. moreover, they do not fully explore the high-order interactions among these semantic components, which is very helpful to predict tail labels. in this paper, we propose a novel label-specific dual graph neural network (ldgn), which incorporates category information to learn label-specific components from documents, and employs dual graph convolution network (gcn) to model complete and adaptive interactions among these components based on the statistical label co-occurrence and dynamic reconstruction graph in a joint way. experimental results on three benchmark datasets demonstrate that ldgn significantly outperforms the state-of-the-art models, and also achieves better performance with respect to tail labels."], "information extraction, retrieval and text mining"], [["parsing into variable-in-situ logico-semantic graphs", "yufei chen | weiwei sun", "we propose variable-in-situ logico-semantic graphs to bridge the gap between semantic graph and logical form parsing. the new type of graph-based meaning representation allows us to include analysis for scope-related phenomena, such as quantification, negation and modality, in a way that is consistent with the state-of-the-art underspecification approach. moreover, the well-formedness of such a graph is clear, since model-theoretic interpretation is available. we demonstrate the effectiveness of this new perspective by developing a new state-of-the-art semantic parser for english resource semantics. at the core of this parser is a novel neural graph rewriting system which combines the strengths of hyperedge replacement grammar, a knowledge-intensive model, and graph neural networks, a data-intensive model. our parser achieves an accuracy of 92.39% in terms of elementary dependency match, which is a 2.88 point improvement over the best data-driven model in the literature. the output of our parser is highly coherent: at least 91% graphs are valid, in that they allow at least one sound scope-resolved logical form."], "semantics"], [["a batch normalized inference network keeps the kl vanishing away", "qile zhu | wei bi | xiaojiang liu | xiyao ma | xiaolin li | dapeng wu", "variational autoencoder (vae) is widely used as a generative model to approximate a model\u2019s posterior on latent variables by combining the amortized variational inference and deep neural networks. however, when paired with strong autoregressive decoders, vae often converges to a degenerated local optimum known as \u201cposterior collapse\u201d. previous approaches consider the kullback\u2013leibler divergence (kl) individual for each datapoint. we propose to let the kl follow a distribution across the whole dataset, and analyze that it is sufficient to prevent posterior collapse by keeping the expectation of the kl\u2019s distribution positive. then we propose batch normalized-vae (bn-vae), a simple but effective approach to set a lower bound of the expectation by regularizing the distribution of the approximate posterior\u2019s parameters. without introducing any new model component or modifying the objective, our approach can avoid the posterior collapse effectively and efficiently. we further show that the proposed bn-vae can be extended to conditional vae (cvae). empirically, our approach surpasses strong autoregressive baselines on language modeling, text classification and dialogue generation, and rivals more complex approaches while keeping almost the same training time as vae."], "machine learning for nlp"], [["automatic domain adaptation outperforms manual domain adaptation for predicting financial outcomes", "marina sedinkina | nikolas breitkopf | hinrich sch\u00fctze", "in this paper, we automatically create sentiment dictionaries for predicting financial outcomes. we compare three approaches: (i) manual adaptation of the domain-general dictionary h4n, (ii) automatic adaptation of h4n and (iii) a combination consisting of first manual, then automatic adaptation. in our experiments, we demonstrate that the automatically adapted sentiment dictionary outperforms the previous state of the art in predicting the financial outcomes excess return and volatility. in particular, automatic adaptation performs better than manual adaptation. in our analysis, we find that annotation based on an expert\u2019s a priori belief about a word\u2019s meaning can be incorrect \u2013 annotation should be performed based on the word\u2019s contexts in the target domain instead."], "nlp applications"], [["learning attention-based embeddings for relation prediction in knowledge graphs", "deepak nathani | jatin chauhan | charu sharma | manohar kaul", "the recent proliferation of knowledge graphs (kgs) coupled with incomplete or partial information, in the form of missing relations (links) between entities, has fueled a lot of research on knowledge base completion (also known as relation prediction). several recent works suggest that convolutional neural network (cnn) based models generate richer and more expressive feature embeddings and hence also perform well on relation prediction. however, we observe that these kg embeddings treat triples independently and thus fail to cover the complex and hidden information that is inherently implicit in the local neighborhood surrounding a triple. to this effect, our paper proposes a novel attention-based feature embedding that captures both entity and relation features in any given entity\u2019s neighborhood. additionally, we also encapsulate relation clusters and multi-hop relations in our model. our empirical study offers insights into the efficacy of our attention-based model and we show marked performance gains in comparison to state-of-the-art methods on all datasets."], "semantics"], [["zeroshotceres: zero-shot relation extraction from semi-structured webpages", "colin lockard | prashant shiralkar | xin luna dong | hannaneh hajishirzi", "in many documents, such as semi-structured webpages, textual semantics are augmented with additional information conveyed using visual elements including layout, font size, and color. prior work on information extraction from semi-structured websites has required learning an extraction model specific to a given template via either manually labeled or distantly supervised data from that template. in this work, we propose a solution for \u201czero-shot\u201d open-domain relation extraction from webpages with a previously unseen template, including from websites with little overlap with existing sources of knowledge for distant supervision and websites in entirely new subject verticals. our model uses a graph neural network-based approach to build a rich representation of text fields on a webpage and the relationships between them, enabling generalization to new templates. experiments show this approach provides a 31% f1 gain over a baseline for zero-shot extraction in a new subject vertical."], "information extraction, retrieval and text mining"], [["a top-down neural architecture towards text-level parsing of discourse rhetorical structure", "longyin zhang | yuqing xing | fang kong | peifeng li | guodong zhou", "due to its great importance in deep natural language understanding and various down-stream applications, text-level parsing of discourse rhetorical structure (drs) has been drawing more and more attention in recent years. however, all the previous studies on text-level discourse parsing adopt bottom-up approaches, which much limit the drs determination on local information and fail to well benefit from global information of the overall discourse. in this paper, we justify from both computational and perceptive points-of-view that the top-down architecture is more suitable for text-level drs parsing. on the basis, we propose a top-down neural architecture toward text-level drs parsing. in particular, we cast discourse parsing as a recursive split point ranking task, where a split point is classified to different levels according to its rank and the elementary discourse units (edus) associated with it are arranged accordingly. in this way, we can determine the complete drs as a hierarchical tree structure via an encoder-decoder with an internal stack. experimentation on both the english rst-dt corpus and the chinese cdtb corpus shows the great effectiveness of our proposed top-down approach towards text-level drs parsing."], "discourse and pragmatics"], [["semi-supervised domain adaptation for dependency parsing", "zhenghua li | xue peng | min zhang | rui wang | luo si", "during the past decades, due to the lack of sufficient labeled data, most studies on cross-domain parsing focus on unsupervised domain adaptation, assuming there is no target-domain training data. however, unsupervised approaches make limited progress so far due to the intrinsic difficulty of both domain adaptation and parsing. this paper tackles the semi-supervised domain adaptation problem for chinese dependency parsing, based on two newly-annotated large-scale domain-aware datasets. we propose a simple domain embedding approach to merge the source- and target-domain training data, which is shown to be more effective than both direct corpus concatenation and multi-task learning. in order to utilize unlabeled target-domain data, we employ the recent contextualized word representations and show that a simple fine-tuning procedure can further boost cross-domain parsing accuracy by large margin."], "tagging, chunking, syntax and parsing"], [["relation embedding with dihedral group in knowledge graph", "canran xu | ruijiang li", "link prediction is critical for the application of incomplete knowledge graph (kg) in the downstream tasks. as a family of effective approaches for link predictions, embedding methods try to learn low-rank representations for both entities and relations such that the bilinear form defined therein is a well-behaved scoring function. despite of their successful performances, existing bilinear forms overlook the modeling of relation compositions, resulting in lacks of interpretability for reasoning on kg. to fulfill this gap, we propose a new model called dihedral, named after dihedral symmetry group. this new model learns knowledge graph embeddings that can capture relation compositions by nature. furthermore, our approach models the relation embeddings parametrized by discrete values, thereby decrease the solution space drastically. our experiments show that dihedral is able to capture all desired properties such as (skew-) symmetry, inversion and (non-) abelian composition, and outperforms existing bilinear form based approach and is comparable to or better than deep learning models such as conve."], "information extraction, retrieval and text mining"], [["mart: memory-augmented recurrent transformer for coherent video paragraph captioning", "jie lei | liwei wang | yelong shen | dong yu | tamara berg | mohit bansal", "generating multi-sentence descriptions for videos is one of the most challenging captioning tasks due to its high requirements for not only visual relevance but also discourse-based coherence across the sentences in the paragraph. towards this goal, we propose a new approach called memory-augmented recurrent transformer (mart), which uses a memory module to augment the transformer architecture. the memory module generates a highly summarized memory state from the video segments and the sentence history so as to help better prediction of the next sentence (w.r.t. coreference and repetition aspects), thus encouraging coherent paragraph generation. extensive experiments, human evaluations, and qualitative analyses on two popular datasets activitynet captions and youcookii show that mart generates more coherent and less repetitive paragraph captions than baseline methods, while maintaining relevance to the input video events."], "language grounding to vision, robotics and beyond"], [["understanding advertisements with bert", "kanika kalra | bhargav kurma | silpa vadakkeeveetil sreelatha | manasi patwardhan | shirish karande", "we consider a task based on cvpr 2018 challenge dataset on advertisement (ad) understanding. the task involves detecting the viewer\u2019s interpretation of an ad image captured as text. recent results have shown that the embedded scene-text in the image holds a vital cue for this task. motivated by this, we fine-tune the base bert model for a sentence-pair classification task. despite utilizing the scene-text as the only source of visual information, we could achieve a hit-or-miss accuracy of 84.95% on the challenge test data. to enable bert to process other visual information, we append image captions to the scene-text. this achieves an accuracy of 89.69%, which is an improvement of 4.7%. this is the best reported result for this task."], "nlp applications"], [["towards language agnostic universal representations", "armen aghajanyan | xia song | saurabh tiwary", "when a bilingual student learns to solve word problems in math, we expect the student to be able to solve these problem in both languages the student is fluent in, even if the math lessons were only taught in one language. however, current representations in machine learning are language dependent. in this work, we present a method to decouple the language from the problem by learning language agnostic representations and therefore allowing training a model in one language and applying to a different one in a zero shot fashion. we learn these representations by taking inspiration from linguistics, specifically the universal grammar hypothesis and learn universal latent representations that are language agnostic. we demonstrate the capabilities of these representations by showing that models trained on a single language using language agnostic representations achieve very similar accuracies in other languages."], "machine learning for nlp"], [["generating informative conversational response using recurrent knowledge-interaction and knowledge-copy", "xiexiong lin | weiyu jian | jianshan he | taifeng wang | wei chu", "knowledge-driven conversation approaches have achieved remarkable research attention recently. however, generating an informative response with multiple relevant knowledge without losing fluency and coherence is still one of the main challenges. to address this issue, this paper proposes a method that uses recurrent knowledge interaction among response decoding steps to incorporate appropriate knowledge. furthermore, we introduce a knowledge copy mechanism using a knowledge-aware pointer network to copy words from external knowledge according to knowledge attention distribution. our joint neural conversation model which integrates recurrent knowledge-interaction and knowledge copy (kic) performs well on generating informative responses. experiments demonstrate that our model with fewer parameters yields significant improvements over competitive baselines on two datasets wizard-of-wikipedia(average bleu +87%; abs.: 0.034) and duconv(average bleu +20%; abs.: 0.047)) with different knowledge formats (textual & structured) and different languages (english & chinese)."], "dialogue and interactive systems"], [["end-to-end sequential metaphor identification inspired by linguistic theories", "rui mao | chenghua lin | frank guerin", "end-to-end training with deep neural networks (dnn) is a currently popular method for metaphor identification. however, standard sequence tagging models do not explicitly take advantage of linguistic theories of metaphor identification. we experiment with two dnn models which are inspired by two human metaphor identification procedures. by testing on three public datasets, we find that our models achieve state-of-the-art performance in end-to-end metaphor identification."], "linguistic theories, cognitive modeling and psycholinguistics"], [["kaggledbqa: realistic evaluation of text-to-sql parsers", "chia-hsuan lee | oleksandr polozov | matthew richardson", "the goal of database question answering is to enable natural language querying of real-life relational databases in diverse application domains. recently, large-scale datasets such as spider and wikisql facilitated novel modeling techniques for text-to-sql parsing, improving zero-shot generalization to unseen databases. in this work, we examine the challenges that still prevent these techniques from practical deployment. first, we present kaggledbqa, a new cross-domain evaluation dataset of real web databases, with domain-specific data types, original formatting, and unrestricted questions. second, we re-examine the choice of evaluation tasks for text-to-sql parsers as applied in real-life settings. finally, we augment our in-domain evaluation task with database documentation, a naturally occurring source of implicit domain knowledge. we show that kaggledbqa presents a challenge to state-of-the-art zero-shot parsers but a more realistic evaluation setting and creative use of associated database documentation boosts their accuracy by over 13.2%, doubling their performance."], "resources and evaluation"], [["tacred revisited: a thorough evaluation of the tacred relation extraction task", "christoph alt | aleksandra gabryszak | leonhard hennig", "tacred is one of the largest, most widely used crowdsourced datasets in relation extraction (re). but, even with recent advances in unsupervised pre-training and knowledge enhanced neural re, models still show a high error rate. in this paper, we investigate the questions: have we reached a performance ceiling or is there still room for improvement? and how do crowd annotations, dataset, and models contribute to this error rate? to answer these questions, we first validate the most challenging 5k examples in the development and test sets using trained annotators. we find that label errors account for 8% absolute f1 test error, and that more than 50% of the examples need to be relabeled. on the relabeled test set the average f1 score of a large baseline model set improves from 62.1 to 70.1. after validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art re models. we show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked."], "information extraction, retrieval and text mining"], [["a complete shift-reduce chinese discourse parser with robust dynamic oracle", "shyh-shiun hung | hen-hsen huang | hsin-hsi chen", "this work proposes a standalone, complete chinese discourse parser for practical applications. we approach chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. we revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. experimental results show that our chinese discourse parser achieves the state-of-the-art performance."], "discourse and pragmatics"], [["dynamic memory induction networks for few-shot text classification", "ruiying geng | binhua li | yongbin li | jian sun | xiaodan zhu", "this paper proposes dynamic memory induction networks (dmin) for few-short text classification. the model develops a dynamic routing mechanism over static memory, enabling it to better adapt to unseen classes, a critical capability for few-short classification. the model also expands the induction process with supervised learning weights and query information to enhance the generalization ability of meta-learning. the proposed model brings forward the state-of-the-art performance significantly by 2~4% improvement on the minircv1 and odic datasets. detailed analysis is further performed to show how the proposed network achieves the new performance."], "information extraction, retrieval and text mining"], [["drts parsing with structure-aware encoding and decoding", "qiankun fu | yue zhang | jiangming liu | meishan zhang", "discourse representation tree structure (drts) parsing is a novel semantic parsing task which has been concerned most recently. state-of-the-art performance can be achieved by a neural sequence-to-sequence model, treating the tree construction as an incremental sequence generation problem. structural information such as input syntax and the intermediate skeleton of the partial output has been ignored in the model, which could be potentially useful for the drts parsing. in this work, we propose a structural-aware model at both the encoder and decoder phase to integrate the structural information, where graph attention network (gat) is exploited for effectively modeling. experimental results on a benchmark dataset show that our proposed model is effective and can obtain the best performance in the literature."], "discourse and pragmatics"], [["enhancing pre-trained language representations with rich knowledge for machine reading comprehension", "an yang | quan wang | jing liu | kai liu | yajuan lyu | hua wu | qiaoqiao she | sujian li", "machine reading comprehension (mrc) is a crucial and challenging task in nlp. recently, pre-trained language models (lms), especially bert, have achieved remarkable success, presenting new state-of-the-art results in mrc. in this work, we investigate the potential of leveraging external knowledge bases (kbs) to further improve bert for mrc. we introduce kt-net, which employs an attention mechanism to adaptively select desired knowledge from kbs, and then fuses selected knowledge with bert to enable context- and knowledge-aware predictions. we believe this would combine the merits of both deep lms and curated kbs towards better mrc. experimental results indicate that kt-net offers significant and consistent improvements over bert, outperforming competitive baselines on record and squad1.1 benchmarks. notably, it ranks the 1st place on the record leaderboard, and is also the best single model on the squad1.1 leaderboard at the time of submission (march 4th, 2019)."], "question answering"], [["spanmlt: a span-based multi-task learning framework for pair-wise aspect and opinion terms extraction", "he zhao | longtao huang | rong zhang | quan lu | hui xue", "aspect terms extraction and opinion terms extraction are two key problems of fine-grained aspect based sentiment analysis (absa). the aspect-opinion pairs can provide a global profile about a product or service for consumers and opinion mining systems. however, traditional methods can not directly output aspect-opinion pairs without given aspect terms or opinion terms. although some recent co-extraction methods have been proposed to extract both terms jointly, they fail to extract them as pairs. to this end, this paper proposes an end-to-end method to solve the task of pair-wise aspect and opinion terms extraction (paote). furthermore, this paper treats the problem from a perspective of joint term and relation extraction rather than under the sequence tagging formulation performed in most prior works. we propose a multi-task learning framework based on shared spans, where the terms are extracted under the supervision of span boundaries. meanwhile, the pair-wise relations are jointly identified using the span representations. extensive experiments show that our model consistently outperforms state-of-the-art methods."], "sentiment analysis, stylistic analysis, and argument mining"], [["representation learning for information extraction from form-like documents", "bodhisattwa prasad majumder | navneet potti | sandeep tata | james bradley wendt | qi zhao | marc najork", "we propose a novel approach using representation learning for tackling the problem of extracting structured information from form-like document images. we propose an extraction system that uses knowledge of the types of the target fields to generate extraction candidates and a neural network architecture that learns a dense representation of each candidate based on neighboring words in the document. these learned representations are not only useful in solving the extraction task for unseen document templates from two different domains but are also interpretable, as we show using loss cases."], "information extraction, retrieval and text mining"], [["do neural models learn systematicity of monotonicity inference in natural language?", "hitomi yanaka | koji mineshima | daisuke bekki | kentaro inui", "despite the success of language models using neural networks, it remains unclear to what extent neural models have the generalization ability to perform inferences. in this paper, we introduce a method for evaluating whether neural models can learn systematicity of monotonicity inference in natural language, namely, the regularity for performing arbitrary inferences with generalization on composition. we consider four aspects of monotonicity inferences and test whether the models can systematically interpret lexical and logical phenomena on different training/test splits. a series of experiments show that three neural models systematically draw inferences on unseen combinations of lexical and logical phenomena when the syntactic structures of the sentences are similar between the training and test sets. however, the performance of the models significantly decreases when the structures are slightly changed in the test set while retaining all vocabularies and constituents already appearing in the training set. this indicates that the generalization ability of neural models is limited to cases where the syntactic structures are nearly the same as those in the training set."], "semantics"], [["enhancing air quality prediction with social media and natural language processing", "jyun-yu jiang | xue sun | wei wang | sean young", "accompanied by modern industrial developments, air pollution has already become a major concern for human health. hence, air quality measures, such as the concentration of pm2.5, have attracted increasing attention. even some studies apply historical measurements into air quality forecast, the changes of air quality conditions are still hard to monitor. in this paper, we propose to exploit social media and natural language processing techniques to enhance air quality prediction. social media users are treated as social sensors with their findings and locations. after filtering noisy tweets using word selection and topic modeling, a deep learning model based on convolutional neural networks and over-tweet-pooling is proposed to enhance air quality prediction. we conduct experiments on 7-month real-world twitter datasets in the five most heavily polluted states in the usa. the results show that our approach significantly improves air quality prediction over the baseline that does not use social media by 6.9% to 17.7% in macro-f1 scores."], "computational social science, social media and cultural analytics"], [["multi-task semantic dependency parsing with policy gradient for learning easy-first strategies", "shuhei kurita | anders s\u00f8gaard", "in semantic dependency parsing (sdp), semantic relations form directed acyclic graphs, rather than trees. we propose a new iterative predicate selection (ips) algorithm for sdp. our ips algorithm combines the graph-based and transition-based parsing approaches in order to handle multiple semantic head words. we train the ips model using a combination of multi-task learning and task-specific policy gradient training. trained this way, ips achieves a new state of the art on the semeval 2015 task 18 datasets. furthermore, we observe that policy gradient training learns an easy-first strategy."], "tagging, chunking, syntax and parsing"], [["multiscale collaborative deep models for neural machine translation", "xiangpeng wei | heng yu | yue hu | yue zhang | rongxiang weng | weihua luo", "recent evidence reveals that neural machine translation (nmt) models with deeper neural networks can be more effective but are difficult to train. in this paper, we present a multiscale collaborative (msc) framework to ease the training of nmt models that are substantially deeper than those used previously. we explicitly boost the gradient back-propagation from top to bottom levels by introducing a block-scale collaboration mechanism into deep nmt models. then, instead of forcing the whole encoder stack directly learns a desired representation, we let each encoder block learns a fine-grained representation and enhance it by encoding spatial dependencies using a context-scale collaboration. we provide empirical evidence showing that the msc nets are easy to optimize and can obtain improvements of translation quality from considerably increased depth. on iwslt translation tasks with three translation directions, our extremely deep models (with 72-layer encoders) surpass strong baselines by +2.2~+3.1 bleu points. in addition, our deep msc achieves a bleu score of 30.56 on wmt14 english-to-german task that significantly outperforms state-of-the-art deep nmt models. we have included the source code in supplementary materials."], "machine translation and multilinguality"], [["zero-shot word sense disambiguation using sense definition embeddings", "sawan kumar | sharmistha jat | karan saxena | partha talukdar", "word sense disambiguation (wsd) is a long-standing but open problem in natural language processing (nlp). wsd corpora are typically small in size, owing to an expensive annotation process. current supervised wsd methods treat senses as discrete labels and also resort to predicting the most-frequent-sense (mfs) for words unseen during training. this leads to poor performance on rare and unseen senses. to overcome this challenge, we propose extended wsd incorporating sense embeddings (ewise), a supervised model to perform wsd by predicting over a continuous sense embedding space as opposed to a discrete label space. this allows ewise to generalize over both seen and unseen senses, thus achieving generalized zero-shot learning. to obtain target sense embeddings, ewise utilizes sense definitions. ewise learns a novel sentence encoder for sense definitions by using wordnet relations and also conve, a recently proposed knowledge graph embedding method. we also compare ewise against other sentence encoders pretrained on large corpora to generate definition embeddings. ewise achieves new state-of-the-art wsd performance."], "semantics"], [["hibert: document level pre-training of hierarchical bidirectional transformers for document summarization", "xingxing zhang | furu wei | ming zhou", "neural extractive summarization models usually employ a hierarchical encoder for document encoding and they are trained using sentence-level labels, which are created heuristically using rule-based methods. training the hierarchical encoder with these inaccurate labels is challenging. inspired by the recent work on pre-training transformer sentence encoders (devlin et al., 2018), we propose hibert (as shorthand for hierachical bidirectional encoder representations from transformers) for document encoding and a method to pre-train it using unlabeled data. we apply the pre-trained hibert to our summarization model and it outperforms its randomly initialized counterpart by 1.25 rouge on the cnn/dailymail dataset and by 2.0 rouge on a version of new york times dataset. we also achieve the state-of-the-art performance on these two datasets."], "summarization"], [["meld: a multimodal multi-party dataset for emotion recognition in conversations", "soujanya poria | devamanyu hazarika | navonil majumder | gautam naik | erik cambria | rada mihalcea", "emotion recognition in conversations is a challenging task that has recently gained popularity due to its potential applications. until now, however, a large-scale multimodal multi-party emotional conversational database containing more than two speakers per dialogue was missing. thus, we propose the multimodal emotionlines dataset (meld), an extension and enhancement of emotionlines. meld contains about 13,000 utterances from 1,433 dialogues from the tv-series friends. each utterance is annotated with emotion and sentiment labels, and encompasses audio, visual and textual modalities. we propose several strong multimodal baselines and show the importance of contextual and multimodal information for emotion recognition in conversations. the full dataset is available for use at http://affective-meld.github.io."], "sentiment analysis, stylistic analysis, and argument mining"], [["unsupervised alignment-based iterative evidence retrieval for multi-hop question answering", "vikas yadav | steven bethard | mihai surdeanu", "evidence retrieval is a critical stage of question answering (qa), necessary not only to improve performance, but also to explain the decisions of the qa method. we introduce a simple, fast, and unsupervised iterative evidence retrieval method, which relies on three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only glove embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, which (c) stops when the terms in the given question and candidate answers are covered by the retrieved justifications. despite its simplicity, our approach outperforms all the previous methods (including supervised methods) on the evidence selection task on two datasets: multirc and qasc. when these evidence sentences are fed into a roberta answer classification component, we achieve state-of-the-art qa performance on these two datasets."], "question answering"], [["understanding the language of political agreement and disagreement in legislative texts", "maryam davoodi | eric waltenburg | dan goldwasser", "while national politics often receive the spotlight, the overwhelming majority of legislation proposed, discussed, and enacted is done at the state level. despite this fact, there is little awareness of the dynamics that lead to adopting these policies. in this paper, we take the first step towards a better understanding of these processes and the underlying dynamics that shape them, using data-driven methods. we build a new large-scale dataset, from multiple data sources, connecting state bills and legislator information, geographical information about their districts, and donations and donors\u2019 information. we suggest a novel task, predicting the legislative body\u2019s vote breakdown for a given bill, according to different criteria of interest, such as gender, rural-urban and ideological splits. finally, we suggest a shared relational embedding model, representing the interactions between the text of the bill and the legislative context in which it is presented. our experiments show that providing this context helps improve the prediction over strong text-based models."], "computational social science, social media and cultural analytics"], [["quantifying attention flow in transformers", "samira abnar | willem zuidema", "in the transformer model, \u201cself-attention\u201d combines information from attended embeddings into the representation of the focal embedding in the next layer. thus, across layers of the transformer, information originating from different tokens gets increasingly mixed. this makes attention weights unreliable as explanations probes. in this paper, we consider the problem of quantifying this flow of information through self-attention. we propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. we show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients."], "interpretability and analysis of models for nlp"], [["automatic generation of high quality ccgbanks for parser domain adaptation", "masashi yoshikawa | hiroshi noji | koji mineshima | daisuke bekki", "we propose a new domain adaptation method for combinatory categorial grammar (ccg) parsing, based on the idea of automatic generation of ccg corpora exploiting cheaper resources of dependency trees. our solution is conceptually simple, and not relying on a specific parser architecture, making it applicable to the current best-performing parsers. we conduct extensive parsing experiments with detailed discussion; on top of existing benchmark datasets on (1) biomedical texts and (2) question sentences, we create experimental datasets of (3) speech conversation and (4) math problems. when applied to the proposed method, an off-the-shelf ccg parser shows significant performance gains, improving from 90.7% to 96.6% on speech conversation, and from 88.5% to 96.8% on math problems."], "tagging, chunking, syntax and parsing"], [["effective inter-clause modeling for end-to-end emotion-cause pair extraction", "penghui wei | jiahao zhao | wenji mao", "emotion-cause pair extraction aims to extract all emotion clauses coupled with their cause clauses from a given document. previous work employs two-step approaches, in which the first step extracts emotion clauses and cause clauses separately, and the second step trains a classifier to filter out negative pairs. however, such pipeline-style system for emotion-cause pair extraction is suboptimal because it suffers from error propagation and the two steps may not adapt to each other well. in this paper, we tackle emotion-cause pair extraction from a ranking perspective, i.e., ranking clause pair candidates in a document, and propose a one-step neural approach which emphasizes inter-clause modeling to perform end-to-end extraction. it models the interrelations between the clauses in a document to learn clause representations with graph attention, and enhances clause pair representations with kernel-based relative position embedding for effective ranking. experimental results show that our approach significantly outperforms the current two-step systems, especially in the condition of extracting multiple pairs in one document."], "sentiment analysis, stylistic analysis, and argument mining"], [["multimodal abstractive summarization for how2 videos", "shruti palaskar | jind\u0159ich libovick\u00fd | spandana gella | florian metze", "in this paper, we study abstractive summarization for open-domain videos. unlike the traditional text news summarization, the goal is less to \u201ccompress\u201d text information but rather to provide a fluent textual summary of information that has been collected and fused from different source modalities, in our case video and audio transcripts (or text). we show how a multi-source sequence-to-sequence model with hierarchical attention can integrate information from different modalities into a coherent output, compare various models trained with different modalities and present pilot experiments on the how2 corpus of instructional videos. we also propose a new evaluation metric (content f1) for abstractive summarization task that measures semantic adequacy rather than fluency of the summaries, which is covered by metrics like rouge and bleu."], "language grounding to vision, robotics and beyond"], [["using lstms to assess the obligatoriness of phonological distinctive features for phonotactic learning", "nicole mirea | klinton bicknell", "to ascertain the importance of phonetic information in the form of phonological distinctive features for the purpose of segment-level phonotactic acquisition, we compare the performance of two recurrent neural network models of phonotactic learning: one that has access to distinctive features at the start of the learning process, and one that does not. though the predictions of both models are significantly correlated with human judgments of non-words, the feature-naive model significantly outperforms the feature-aware one in terms of probability assigned to a held-out test set of english words, suggesting that distinctive features are not obligatory for learning phonotactic patterns at the segment level."], "phonology, morphology and word segmentation"], [["scirex: a challenge dataset for document-level information extraction", "sarthak jain | madeleine van zuylen | hannaneh hajishirzi | iz beltagy", "extracting information from full documents is an important problem in many domains, but most previous work focus on identifying relationships within a sentence or a paragraph. it is challenging to create a large-scale information extraction (ie) dataset at the document level since it requires an understanding of the whole document to annotate entities and their document-level relationships that usually span beyond sentences or even sections. in this paper, we introduce scirex, a document level ie dataset that encompasses multiple ie tasks, including salient entity identification and document level n-ary relation identification from scientific articles. we annotate our dataset by integrating automatic and human annotations, leveraging existing scientific knowledge resources. we develop a neural model as a strong baseline that extends previous state-of-the-art ie models to document-level ie. analyzing the model performance shows a significant gap between human performance and current baselines, inviting the community to use our dataset as a challenge to develop document-level ie models. our data and code are publicly available at https://github.com/allenai/scirex ."], "information extraction, retrieval and text mining"], [["stacl: simultaneous translation with implicit anticipation and controllable latency using prefix-to-prefix framework", "mingbo ma | liang huang | hao xiong | renjie zheng | kaibo liu | baigong zheng | chuanqiang zhang | zhongjun he | hairong liu | xing li | hua wu | haifeng wang", "simultaneous translation, which translates sentences before they are finished, is use- ful in many scenarios but is notoriously dif- ficult due to word-order differences. while the conventional seq-to-seq framework is only suitable for full-sentence translation, we pro- pose a novel prefix-to-prefix framework for si- multaneous translation that implicitly learns to anticipate in a single translation model. within this framework, we present a very sim- ple yet surprisingly effective \u201cwait-k\u201d policy trained to generate the target sentence concur- rently with the source sentence, but always k words behind. experiments show our strat- egy achieves low latency and reasonable qual- ity (compared to full-sentence translation) on 4 directions: zh\u2194en and de\u2194en."], "machine translation and multilinguality"], [["contextual neural machine translation improves translation of cataphoric pronouns", "kayyen wong | sameen maruf | gholamreza haffari", "the advent of context-aware nmt has resulted in promising improvements in the overall translation quality and specifically in the translation of discourse phenomena such as pronouns. previous works have mainly focused on the use of past sentences as context with a focus on anaphora translation. in this work, we investigate the effect of future sentences as context by comparing the performance of a contextual nmt model trained with the future context to the one trained with the past context. our experiments and evaluation, using generic and pronoun-focused automatic metrics, show that the use of future context not only achieves significant improvements over the context-agnostic transformer, but also demonstrates comparable and in some cases improved performance over its counterpart trained on past context. we also perform an evaluation on a targeted cataphora test suite and report significant gains over the context-agnostic transformer in terms of bleu."], "machine translation and multilinguality"], [["active learning for coreference resolution using discrete annotation", "belinda z. li | gabriel stanovsky | luke zettlemoyer", "we improve upon pairwise annotation for active learning in coreference resolution, by asking annotators to identify mention antecedents if a presented mention pair is deemed not coreferent. this simple modification, when combined with a novel mention clustering algorithm for selecting which examples to label, is much more efficient in terms of the performance obtained per annotation budget. in experiments with existing benchmark coreference datasets, we show that the signal from this additional question leads to significant performance gains per human-annotation hour. future work can use our annotation protocol to effectively develop coreference models for new domains. our code is publicly available."], "semantics"], [["hat: hardware-aware transformers for efficient natural language processing", "hanrui wang | zhanghao wu | zhijian liu | han cai | ligeng zhu | chuang gan | song han", "transformers are ubiquitous in natural language processing (nlp) tasks, but they are difficult to be deployed on hardware due to the intensive computation. to enable low-latency inference on resource-constrained hardware platforms, we propose to design hardware-aware transformers (hat) with neural architecture search. we first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. then we train a supertransformer that covers all candidates in the design space, and efficiently produces many subtransformers with weight sharing. finally, we perform an evolutionary search with a hardware latency constraint to find a specialized subtransformer dedicated to run fast on the target hardware. extensive experiments on four machine translation tasks demonstrate that hat can discover efficient models for different hardware (cpu, gpu, iot device). when running wmt\u201914 translation task on raspberry pi-4, hat can achieve 3\u00d7 speedup, 3.7\u00d7 smaller size over baseline transformer; 2.7\u00d7 speedup, 3.6\u00d7 smaller size over evolved transformer with 12,041\u00d7 less search cost and no performance loss. hat is open-sourced at https://github.com/mit-han-lab/hardware-aware-transformers."], "machine translation and multilinguality"], [["multi-task deep neural networks for natural language understanding", "xiaodong liu | pengcheng he | weizhu chen | jianfeng gao", "in this paper, we present a multi-task deep neural network (mt-dnn) for learning representations across multiple natural language understanding (nlu) tasks. mt-dnn not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. mt-dnn extends the model proposed in liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as bert (devlin et al., 2018). mt-dnn obtains new state-of-the-art results on ten nlu tasks, including snli, scitail, and eight out of nine glue tasks, pushing the glue benchmark to 82.7% (2.2% absolute improvement) as of february 25, 2019 on the latest glue test set. we also demonstrate using the snli and scitail datasets that the representations learned by mt-dnn allow domain adaptation with substantially fewer in-domain labels than the pre-trained bert representations. our code and pre-trained models will be made publicly available."], "semantics"], [["in neural machine translation, what does transfer learning transfer?", "alham fikri aji | nikolay bogoychev | kenneth heafield | rico sennrich", "transfer learning improves quality for low-resource machine translation, but it is unclear what exactly it transfers. we perform several ablation studies that limit information transfer, then measure the quality impact across three language pairs to gain a black-box understanding of transfer learning. word embeddings play an important role in transfer learning, particularly if they are properly aligned. although transfer learning can be performed without embeddings, results are sub-optimal. in contrast, transferring only the embeddings but nothing else yields catastrophic results. we then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. finally, transfer learning can eliminate the need for a warm-up phase when training transformer models in high resource language pairs."], "machine translation and multilinguality"], [["hierarchical entity typing via multi-level learning to rank", "tongfei chen | yunmo chen | benjamin van durme", "we propose a novel method for hierarchical entity classification that embraces ontological structure at both training and during prediction. at training, our novel multi-level learning-to-rank loss compares positive types against negative siblings according to the type tree. during prediction, we define a coarse-to-fine decoder that restricts viable candidates at each level of the ontology based on already predicted parent type(s). our approach significantly outperform prior work on strict accuracy, demonstrating the effectiveness of our method."], "information extraction, retrieval and text mining"], [["generating hierarchical explanations on text classification via feature interaction detection", "hanjie chen | guangtao zheng | yangfeng ji", "generating explanations for neural networks has become crucial for their applications in real-world with respect to reliability and trustworthiness. in natural language processing, existing methods usually provide important features which are words or phrases selected from an input text as an explanation, but ignore the interactions between them. it poses challenges for humans to interpret an explanation and connect it to model prediction. in this work, we build hierarchical explanations by detecting feature interactions. such explanations visualize how words and phrases are combined at different levels of the hierarchy, which can help users understand the decision-making of black-box models. the proposed method is evaluated with three neural text classifiers (lstm, cnn, and bert) on two benchmark datasets, via both automatic and human evaluations. experiments show the effectiveness of the proposed method in providing explanations that are both faithful to models and interpretable to humans."], "interpretability and analysis of models for nlp"], [["improving event detection via open-domain trigger knowledge", "meihan tong | bin xu | shuai wang | yixin cao | lei hou | juanzi li | jun xie", "event detection (ed) is a fundamental task in automatically structuring texts. due to the small scale of training data, previous methods perform poorly on unseen/sparsely labeled trigger words and are prone to overfitting densely labeled trigger words. to address the issue, we propose a novel enrichment knowledge distillation (ekd) model to leverage external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations. experiments on benchmark ace2005 show that our model outperforms nine strong baselines, is especially effective for unseen/sparsely labeled trigger words. the source code is released on https://github.com/shuaiwa16/ekd.git."], "information extraction, retrieval and text mining"], [["craftassist instruction parsing: semantic parsing for a voxel-world assistant", "kavya srinet | yacine jernite | jonathan gray | arthur szlam", "we propose a semantic parsing dataset focused on instruction-driven communication with an agent in the game minecraft. the dataset consists of 7k human utterances and their corresponding parses. given proper world state, the parses can be interpreted and executed in game. we report the performance of baseline models, and analyze their successes and failures."], "dialogue and interactive systems"], [["evaluation of thematic coherence in microblogs", "iman munire bilal | bo wang | maria liakata | rob procter | adam tsakalidis", "collecting together microblogs representing opinions about the same topics within the same timeframe is useful to a number of different tasks and practitioners. a major question is how to evaluate the quality of such thematic clusters. here we create a corpus of microblog clusters from three different domains and time windows and define the task of evaluating thematic coherence. we provide annotation guidelines and human annotations of thematic coherence by journalist experts. we subsequently investigate the efficacy of different automated evaluation metrics for the task. we consider a range of metrics including surface level metrics, ones for topic model coherence and text generation metrics (tgms). while surface level metrics perform well, outperforming topic coherence metrics, they are not as consistent as tgms. tgms are more reliable than all other metrics considered for capturing thematic coherence in microblog clusters due to being less sensitive to the effect of time windows."], "resources and evaluation"], [["multi-label and multilingual news framing analysis", "afra feyza aky\u00fcrek | lei guo | randa elanwar | prakash ishwar | margrit betke | derry tanti wijaya", "news framing refers to the practice in which aspects of specific issues are highlighted in the news to promote a particular interpretation. in nlp, although recent works have studied framing in english news, few have studied how the analysis can be extended to other languages and in a multi-label setting. in this work, we explore multilingual transfer learning to detect multiple frames from just the news headline in a genuinely low-resource context where there are few/no frame annotations in the target language. we propose a novel method that can leverage elementary resources consisting of a dictionary and few annotations to detect frames in the target language. our method performs comparably or better than translating the entire target language headline to the source language for which we have annotated data. this work opens up an exciting new capability of scaling up frame analysis to many languages, even those without existing translation technologies. lastly, we apply our method to detect frames on the issue of u.s. gun violence in multiple languages and obtain exciting insights on the relationship between different frames of the same problem across different countries with different languages."], "nlp applications"], [["clubert: a cluster-based approach for learning sense distributions in multiple languages", "tommaso pasini | federico scozzafava | bianca scarlini", "knowing the most frequent sense (mfs) of a word has been proved to help word sense disambiguation (wsd) models significantly. however, the scarcity of sense-annotated data makes it difficult to induce a reliable and high-coverage distribution of the meanings in a language vocabulary. to address this issue, in this paper we present clubert, an automatic and multilingual approach for inducing the distributions of word senses from a corpus of raw sentences. our experiments show that clubert learns distributions over english senses that are of higher quality than those extracted by alternative approaches. when used to induce the mfs of a lemma, clubert attains state-of-the-art results on the english word sense disambiguation tasks and helps to improve the disambiguation performance of two off-the-shelf wsd models. moreover, our distributions also prove to be effective in other languages, beating all their alternatives for computing the mfs on the multilingual wsd tasks. we release our sense distributions in five different languages at https://github.com/sapienzanlp/clubert."], "semantics"], [["estimating predictive uncertainty for rumour verification models", "elena kochkina | maria liakata", "the inability to correctly resolve rumours circulating online can have harmful real-world consequences. we present a method for incorporating model and data uncertainty estimates into natural language processing models for automatic rumour verification. we show that these estimates can be used to filter out model predictions likely to be erroneous so that these difficult instances can be prioritised by a human fact-checker. we propose two methods for uncertainty-based instance rejection, supervised and unsupervised. we also show how uncertainty estimates can be used to interpret model performance as a rumour unfolds."], "nlp applications"], [["word2sense: sparse interpretable word embeddings", "abhishek panigrahi | harsha vardhan simhadri | chiranjib bhattacharyya", "we present an unsupervised method to generate word2sense word embeddings that are interpretable \u2014 each dimension of the embedding space corresponds to a fine-grained sense, and the non-negative value of the embedding along the j-th dimension represents the relevance of the j-th sense to the word. the underlying lda-based generative model can be extended to refine the representation of a polysemous word in a short context, allowing us to use the embedings in contextual tasks. on computational nlp tasks, word2sense embeddings compare well with other word embeddings generated by unsupervised methods. across tasks such as word similarity, entailment, sense induction, and contextual interpretation, word2sense is competitive with the state-of-the-art method for that task. word2sense embeddings are at least as sparse and fast to compute as prior art."], "semantics"], [["human attention maps for text classification: do humans and neural networks focus on the same words?", "cansu sen | thomas hartvigsen | biao yin | xiangnan kong | elke rundensteiner", "motivated by human attention, computational attention mechanisms have been designed to help neural networks adjust their focus on specific parts of the input data. while attention mechanisms are claimed to achieve interpretability, little is known about the actual relationships between machine and human attention. in this work, we conduct the first quantitative assessment of human versus computational attention mechanisms for the text classification task. to achieve this, we design and conduct a large-scale crowd-sourcing study to collect human attention maps that encode the parts of a text that humans focus on when conducting text classification. based on this new resource of human attention dataset for text classification, yelp-hat, collected on the publicly available yelp dataset, we perform a quantitative comparative analysis of machine attention maps created by deep learning models and human attention maps. our analysis offers insights into the relationships between human versus machine attention maps along three dimensions: overlap in word selections, distribution over lexical categories, and context-dependency of sentiment polarity. our findings open promising future research opportunities ranging from supervised attention to the design of human-centric attention-based explanations."], "interpretability and analysis of models for nlp"], [["learning morphosyntactic analyzers from the bible via iterative annotation projection across 26 languages", "garrett nicolai | david yarowsky", "a large percentage of computational tools are concentrated in a very small subset of the planet\u2019s languages. compounding the issue, many languages lack the high-quality linguistic annotation necessary for the construction of such tools with current machine learning methods. in this paper, we address both issues simultaneously: leveraging the high accuracy of english taggers and parsers, we project morphological information onto translations of the bible in 26 varied test languages. using an iterative discovery, constraint, and training process, we build inflectional lexica in the target languages. through a combination of iteration, ensembling, and reranking, we see double-digit relative error reductions in lemmatization and morphological analysis over a strong initial system."], "machine translation and multilinguality"], [["why overfitting isn\u2019t always bad: retrofitting cross-lingual word embeddings to dictionaries", "mozhi zhang | yoshinari fujinuma | michael j. paul | jordan boyd-graber", "cross-lingual word embeddings (clwe) are often evaluated on bilingual lexicon induction (bli). recent clwe methods use linear projections, which underfit the training dictionary, to generalize on bli. however, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary. we address this limitation by retrofitting clwe to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary. this simple post-processing step often improves accuracy on two downstream tasks, despite lowering bli test accuracy. we also retrofit to both the training dictionary and a synthetic dictionary induced from clwe, which sometimes generalizes even better on downstream tasks. our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why bli is a flawed clwe evaluation."], "machine learning for nlp"], [["a spreading activation framework for tracking conceptual complexity of texts", "ioana hulpu\u0219 | sanja \u0161tajner | heiner stuckenschmidt", "we propose an unsupervised approach for assessing conceptual complexity of texts, based on spreading activation. using dbpedia knowledge graph as a proxy to long-term memory, mentioned concepts become activated and trigger further activation as the text is sequentially traversed. drawing inspiration from psycholinguistic theories of reading comprehension, we model memory processes such as semantic priming, sentence wrap-up, and forgetting. we show that our models capture various aspects of conceptual text complexity and significantly outperform current state of the art."], "linguistic theories, cognitive modeling and psycholinguistics"], [["supervised grapheme-to-phoneme conversion of orthographic schwas in hindi and punjabi", "aryaman arora | luke gessler | nathan schneider", "hindi grapheme-to-phoneme (g2p) conversion is mostly trivial, with one exception: whether a schwa represented in the orthography is pronounced or unpronounced (deleted). previous work has attempted to predict schwa deletion in a rule-based fashion using prosodic or phonetic analysis. we present the first statistical schwa deletion classifier for hindi, which relies solely on the orthography as the input and outperforms previous approaches. we trained our model on a newly-compiled pronunciation lexicon extracted from various online dictionaries. our best hindi model achieves state of the art performance, and also achieves good performance on a closely related language, punjabi, without modification."], "phonology, morphology and word segmentation"], [["divide, conquer and combine: hierarchical feature fusion network with local and global perspectives for multimodal affective computing", "sijie mai | haifeng hu | songlong xing", "we propose a general strategy named \u2018divide, conquer and combine\u2019 for multimodal fusion. instead of directly fusing features at holistic level, we conduct fusion hierarchically so that both local and global interactions are considered for a comprehensive interpretation of multimodal embeddings. in the \u2018divide\u2019 and \u2018conquer\u2019 stages, we conduct local fusion by exploring the interaction of a portion of the aligned feature vectors across various modalities lying within a sliding window, which ensures that each part of multimodal embeddings are explored sufficiently. on its basis, global fusion is conducted in the \u2018combine\u2019 stage to explore the interconnection across local interactions, via an attentive bi-directional skip-connected lstm that directly connects distant local interactions and integrates two levels of attention mechanism. in this way, local interactions can exchange information sufficiently and thus obtain an overall view of multimodal information. our method achieves state-of-the-art performance on multimodal affective computing with higher efficiency."], "sentiment analysis, stylistic analysis, and argument mining"], [["hooks in the headline: learning to generate headlines with controlled styles", "di jin | zhijing jin | joey tianyi zhou | lisa orii | peter szolovits", "current summarization systems only produce plain, factual headlines, far from the practical needs for the exposure and memorableness of the articles. we propose a new task, stylistic headline generation (shg), to enrich the headlines with three style options (humor, romance and clickbait), thus attracting more readers. with no style-specific article-headline pair (only a standard headline summarization dataset and mono-style corpora), our method titlestylist generates stylistic headlines by combining the summarization and reconstruction tasks into a multitasking framework. we also introduced a novel parameter sharing scheme to further disentangle the style from text. through both automatic and human evaluation, we demonstrate that titlestylist can generate relevant, fluent headlines with three target styles: humor, romance, and clickbait. the attraction score of our model generated headlines outperforms the state-of-the-art summarization model by 9.68%, even outperforming human-written references."], "summarization"], [["learning representation mapping for relation detection in knowledge base question answering", "peng wu | shujian huang | rongxiang weng | zaixiang zheng | jianbing zhang | xiaohui yan | jiajun chen", "relation detection is a core step in many natural language process applications including knowledge base question answering. previous efforts show that single-fact questions could be answered with high accuracy. however, one critical problem is that current approaches only get high accuracy for questions whose relations have been seen in the training data. but for unseen relations, the performance will drop rapidly. the main reason for this problem is that the representations for unseen relations are missing. in this paper, we propose a simple mapping method, named representation adapter, to learn the representation mapping for both seen and unseen relations based on previously learned relation embedding. we employ the adversarial objective and the reconstruction objective to improve the mapping performance. we re-organize the popular simplequestion dataset to reveal and evaluate the problem of detecting unseen relations. experiments show that our method can greatly improve the performance of unseen relations while the performance for those seen part is kept comparable to the state-of-the-art."], "question answering"], [["distilling discrimination and generalization knowledge for event detection via delta-representation learning", "yaojie lu | hongyu lin | xianpei han | le sun", "event detection systems rely on discrimination knowledge to distinguish ambiguous trigger words and generalization knowledge to detect unseen/sparse trigger words. current neural event detection approaches focus on trigger-centric representations, which work well on distilling discrimination knowledge, but poorly on learning generalization knowledge. to address this problem, this paper proposes a delta-learning approach to distill discrimination and generalization knowledge by effectively decoupling, incrementally learning and adaptively fusing event representation. experiments show that our method significantly outperforms previous approaches on unseen/sparse trigger words, and achieves state-of-the-art performance on both ace2005 and kbp2017 datasets."], "information extraction, retrieval and text mining"], [["jointly learning semantic parser and natural language generator via dual information maximization", "hai ye | wenjie li | lu wang", "semantic parsing aims to transform natural language (nl) utterances into formal meaning representations (mrs), whereas an nl generator achieves the reverse: producing an nl description for some given mrs. despite this intrinsic connection, the two tasks are often studied separately in prior work. in this paper, we model the duality of these two tasks via a joint learning framework, and demonstrate its effectiveness of boosting the performance on both tasks. concretely, we propose a novel method of dual information maximization (dim) to regularize the learning process, where dim empirically maximizes the variational lower bounds of expected joint distributions of nl and mrs. we further extend dim to a semi-supervision setup (semidim), which leverages unlabeled data of both tasks. experiments on three datasets of dialogue management and code generation (and summarization) show that performance on both semantic parsing and nl generation can be consistently improved by dim, in both supervised and semi-supervised setups."], "generation"], [["low-resource generation of multi-hop reasoning questions", "jianxing yu | wei liu | shuang qiu | qinliang su | kai wang | xiaojun quan | jian yin", "this paper focuses on generating multi-hop reasoning questions from the raw text in a low resource circumstance. such questions have to be syntactically valid and need to logically correlate with the answers by deducing over multiple relations on several sentences in the text. specifically, we first build a multi-hop generation model and guide it to satisfy the logical rationality by the reasoning chain extracted from a given text. since the labeled data is limited and insufficient for training, we propose to learn the model with the help of a large scale of unlabeled data that is much easier to obtain. such data contains rich expressive forms of the questions with structural patterns on syntax and semantics. these patterns can be estimated by the neural hidden semi-markov model using latent variables. with latent patterns as a prior, we can regularize the generation model and produce the optimal results. experimental results on the hotpotqa data set demonstrate the effectiveness of our model. moreover, we apply the generated results to the task of machine reading comprehension and achieve significant performance improvements."], "question answering"], [["learning to discover, ground and use words with segmental neural language models", "kazuya kawakami | chris dyer | phil blunsom", "we propose a segmental neural language model that combines the generalization power of neural networks with the ability to discover word-like units that are latent in unsegmented character sequences. in contrast to previous segmentation models that treat word segmentation as an isolated task, our model unifies word discovery, learning how words fit together to form sentences, and, by conditioning the model on visual context, how words\u2019 meanings ground in representations of nonlinguistic modalities. experiments show that the unconditional model learns predictive distributions better than character lstm models, discovers words competitively with nonparametric bayesian word segmentation models, and that modeling language conditional on visual context improves performance on both."], "language grounding to vision, robotics and beyond"], [["from english to code-switching: transfer learning with strong morphological clues", "gustavo aguilar | thamar solorio", "linguistic code-switching (cs) is still an understudied phenomenon in natural language processing. the nlp community has mostly focused on monolingual and multi-lingual scenarios, but little attention has been given to cs in particular. this is partly because of the lack of resources and annotated data, despite its increasing occurrence in social media platforms. in this paper, we aim at adapting monolingual models to code-switched text in various tasks. specifically, we transfer english knowledge from a pre-trained elmo model to different code-switched language pairs (i.e., nepali-english, spanish-english, and hindi-english) using the task of language identification. our method, cs-elmo, is an extension of elmo with a simple yet effective position-aware attention mechanism inside its character convolutions. we show the effectiveness of this transfer learning step by outperforming multilingual bert and homologous cs-unaware elmo models and establishing a new state of the art in cs tasks, such as ner and pos tagging. our technique can be expanded to more english-paired code-switched languages, providing more resources to the cs community."], "information extraction, retrieval and text mining"], [["video-grounded dialogues with pretrained generation language models", "hung le | steven c.h. hoi", "pre-trained language models have shown remarkable success in improving various downstream nlp tasks due to their ability to capture dependencies in textual data and generate natural responses. in this paper, we leverage the power of pre-trained language models for improving video-grounded dialogue, which is very challenging and involves complex features of different dynamics: (1) video features which can extend across both spatial and temporal dimensions; and (2) dialogue features which involve semantic dependencies over multiple dialogue turns. we propose a framework by extending gpt-2 models to tackle these challenges by formulating video-grounded dialogue tasks as a sequence-to-sequence task, combining both visual and textual representation into a structured sequence, and fine-tuning a large pre-trained gpt-2 network. our framework allows fine-tuning language models to capture dependencies across multiple modalities over different levels of information: spatio-temporal level in video and token-sentence level in dialogue context. we achieve promising improvement on the audio-visual scene-aware dialogues (avsd) benchmark from dstc7, which supports a potential direction in this line of research."], "dialogue and interactive systems"], [["improving low-resource named entity recognition using joint sentence and token labeling", "canasai kruengkrai | thien hai nguyen | sharifah mahani aljunied | lidong bing", "exploiting sentence-level labels, which are easy to obtain, is one of the plausible methods to improve low-resource named entity recognition (ner), where token-level labels are costly to annotate. current models for jointly learning sentence and token labeling are limited to binary classification. we present a joint model that supports multi-class classification and introduce a simple variant of self-attention that allows the model to learn scaling factors. our model produces 3.78%, 4.20%, 2.08% improvements in f1 over the bilstm-crf baseline on e-commerce product titles in three different low-resource languages: vietnamese, thai, and indonesian, respectively."], "information extraction, retrieval and text mining"], [["transfer capsule network for aspect level sentiment classification", "zhuang chen | tieyun qian", "aspect-level sentiment classification aims to determine the sentiment polarity of a sentence towards an aspect. due to the high cost in annotation, the lack of aspect-level labeled data becomes a major obstacle in this area. on the other hand, document-level labeled data like reviews are easily accessible from online websites. these reviews encode sentiment knowledge in abundant contexts. in this paper, we propose a transfer capsule network (transcap) model for transferring document-level knowledge to aspect-level sentiment classification. to this end, we first develop an aspect routing approach to encapsulate the sentence-level semantic representations into semantic capsules from both the aspect-level and document-level data. we then extend the dynamic routing approach to adaptively couple the semantic capsules with the class capsules under the transfer learning framework. experiments on semeval datasets demonstrate the effectiveness of transcap."], "sentiment analysis, stylistic analysis, and argument mining"], [["know what you don\u2019t know: modeling a pragmatic speaker that refers to objects of unknown categories", "sina zarrie\u00df | david schlangen", "zero-shot learning in language & vision is the task of correctly labelling (or naming) objects of novel categories. another strand of work in l&v aims at pragmatically informative rather than \u201ccorrect\u201d object descriptions, e.g. in reference games. we combine these lines of research and model zero-shot reference games, where a speaker needs to successfully refer to a novel object in an image. inspired by models of \u201crational speech acts\u201d, we extend a neural generator to become a pragmatic speaker reasoning about uncertain object categories. as a result of this reasoning, the generator produces fewer nouns and names of distractor categories as compared to a literal speaker. we show that this conversational strategy for dealing with novel objects often improves communicative success, in terms of resolution accuracy of an automatic listener."], "discourse and pragmatics"], [["retrieve, read, rerank: towards end-to-end multi-document reading comprehension", "minghao hu | yuxing peng | zhen huang | dongsheng li", "this paper considers the reading comprehension task in which multiple documents are given as input. prior work has shown that a pipeline of retriever, reader, and reranker can improve the overall performance. however, the pipeline system is inefficient since the input is re-encoded within each module, and is unable to leverage upstream components to help downstream training. in this work, we present re3qa, a unified question answering model that combines context retrieving, reading comprehension, and answer reranking to predict the final answer. unlike previous pipelined approaches, re3qa shares contextualized text representation across different components, and is carefully designed to use high-quality upstream outputs (e.g., retrieved context or candidate answers) for directly supervising downstream modules (e.g., the reader or the reranker). as a result, the whole network can be trained end-to-end to avoid the context inconsistency problem. experiments show that our model outperforms the pipelined baseline and achieves state-of-the-art results on two versions of triviaqa and two variants of squad."], "question answering"], [["uncertain natural language inference", "tongfei chen | zhengping jiang | adam poliak | keisuke sakaguchi | benjamin van durme", "we introduce uncertain natural language inference (unli), a refinement of natural language inference (nli) that shifts away from categorical labels, targeting instead the direct prediction of subjective probability assessments. we demonstrate the feasibility of collecting annotations for unli by relabeling a portion of the snli dataset under a probabilistic scale, where items even with the same categorical label differ in how likely people judge them to be true given a premise. we describe a direct scalar regression modeling approach, and find that existing categorically-labeled nli data can be used in pre-training. our best models correlate well with humans, demonstrating models are capable of more subtle inferences than the categorical bin assignment employed in current nli tasks."], "semantics"], [["attention-based conditioning methods for external knowledge integration", "katerina margatina | christos baziotis | alexandros potamianos", "in this paper, we present a novel approach for incorporating external knowledge in recurrent neural networks (rnns). we propose the integration of lexicon features into the self-attention mechanism of rnn-based architectures. this form of conditioning on the attention distribution, enforces the contribution of the most salient words for the task at hand. we introduce three methods, namely attentional concatenation, feature-based gating and affine transformation. experiments on six benchmark datasets show the effectiveness of our methods. attentional feature-based gating yields consistent performance improvement across tasks. our approach is implemented as a simple add-on module for rnn-based models with minimal computational overhead and can be adapted to any deep neural architecture."], "linguistic theories, cognitive modeling and psycholinguistics"], [["understanding undesirable word embedding associations", "kawin ethayarajh | david duvenaud | graeme hirst", "word embeddings are often criticized for capturing undesirable word associations such as gender stereotypes. however, methods for measuring and removing such biases remain poorly understood. we show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection (bolukbasi et al., 2016) is, under certain conditions, equivalent to training on an unbiased corpus. we also prove that weat, the most common association test for word embeddings, systematically overestimates bias. given that the subspace projection method is provably effective, we use it to derive a new measure of association called the relational inner product association (ripa). experiments with ripa reveal that, on average, skipgram with negative sampling (sgns) does not make most words any more gendered than they are in the training corpus. however, for gender-stereotyped words, sgns actually amplifies the gender association in the corpus."], "semantics"], [["social bias frames: reasoning about social and power implications of language", "maarten sap | saadia gabriel | lianhui qin | dan jurafsky | noah a. smith | yejin choi", "warning: this paper contains content that may be offensive or upsetting. language has the power to reinforce stereotypes and project social biases onto others. at the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people\u2019s judgments about others. for example, given a statement that \u201cwe shouldn\u2019t lower our standards to hire more women,\u201d most listeners will infer the implicature intended by the speaker - that \u201cwomen (candidates) are less qualified.\u201d most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. we introduce social bias frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. in addition, we introduce the social bias inference corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. we then establish baseline approaches that learn to recover social bias frames from unstructured text. we find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% f1), they are not effective at spelling out more detailed explanations in terms of social bias frames. our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications."], "ethics in nlp"], [["matinf: a jointly labeled large-scale dataset for classification, question answering and summarization", "canwen xu | jiaxin pei | hongtao wu | yiyu liu | chenliang li", "recently, large-scale datasets have vastly facilitated the development in nearly all domains of natural language processing. however, there is currently no cross-task dataset in nlp, which hinders the development of multi-task learning. we propose matinf, the first jointly labeled large-scale dataset for classification, question answering and summarization. matinf contains 1.07 million question-answer pairs with human-labeled categories and user-generated question descriptions. based on such rich information, matinf is applicable for three major nlp tasks, including classification, question answering, and summarization. we benchmark existing methods and a novel multi-task baseline over matinf to inspire further research. our comprehensive comparison and experiments over matinf and other datasets demonstrate the merits held by matinf."], "resources and evaluation"], [["how to ask good questions? try to leverage paraphrases", "xin jia | wenjie zhou | xu sun | yunfang wu", "given a sentence and its relevant answer, how to ask good questions is a challenging task, which has many real applications. inspired by human\u2019s paraphrasing capability to ask questions of the same meaning but with diverse expressions, we propose to incorporate paraphrase knowledge into question generation(qg) to generate human-like questions. specifically, we present a two-hand hybrid model leveraging a self-built paraphrase resource, which is automatically conducted by a simple back-translation method. on the one hand, we conduct multi-task learning with sentence-level paraphrase generation (pg) as an auxiliary task to supplement paraphrase knowledge to the task-share encoder. on the other hand, we adopt a new loss function for diversity training to introduce more question patterns to qg. extensive experimental results show that our proposed model obtains obvious performance gain over several strong baselines, and further human evaluation validates that our model can ask questions of high quality by leveraging paraphrase knowledge."], "semantics"], [["cross-linguistic syntactic evaluation of word prediction models", "aaron mueller | garrett nicolai | panayiota petrou-zeniou | natalia talmina | tal linzen", "a range of studies have concluded that neural word prediction models can distinguish grammatical from ungrammatical sentences with high accuracy. however, these studies are based primarily on monolingual evidence from english. to investigate how these models\u2019 ability to learn syntax varies by language, we introduce clams (cross-linguistic assessment of models on syntax), a syntactic evaluation suite for monolingual and multilingual models. clams includes subject-verb agreement challenge sets for english, french, german, hebrew and russian, generated from grammars we develop. we use clams to evaluate lstm language models as well as monolingual and multilingual bert. across languages, monolingual lstms achieved high accuracy on dependencies without attractors, and generally poor accuracy on agreement across object relative clauses. on other constructions, agreement accuracy was generally higher in languages with richer morphology. multilingual models generally underperformed monolingual models. multilingual bert showed high syntactic accuracy on english, but noticeable deficiencies in other languages."], "interpretability and analysis of models for nlp"], [["generating responses with a specific emotion in dialog", "zhenqiao song | xiaoqing zheng | lu liu | mu xu | xuanjing huang", "it is desirable for dialog systems to have capability to express specific emotions during a conversation, which has a direct, quantifiable impact on improvement of their usability and user satisfaction. after a careful investigation of real-life conversation data, we found that there are at least two ways to express emotions with language. one is to describe emotional states by explicitly using strong emotional words; another is to increase the intensity of the emotional experiences by implicitly combining neutral words in distinct ways. we propose an emotional dialogue system (emods) that can generate the meaningful responses with a coherent structure for a post, and meanwhile express the desired emotion explicitly or implicitly within a unified framework. experimental results showed emods performed better than the baselines in bleu, diversity and the quality of emotional expression."], "dialogue and interactive systems"], [["the (non-)utility of structural features in bilstm-based dependency parsers", "agnieszka falenska | jonas kuhn", "classical non-neural dependency parsers put considerable effort on the design of feature functions. especially, they benefit from information coming from structural features, such as features drawn from neighboring tokens in the dependency tree. in contrast, their bilstm-based successors achieve state-of-the-art performance without explicit information about the structural context. in this paper we aim to answer the question: how much structural context are the bilstm representations able to capture implicitly? we show that features drawn from partial subtrees become redundant when the bilstms are used. we provide a deep insight into information flow in transition- and graph-based neural architectures to demonstrate where the implicit information comes from when the parsers make their decisions. finally, with model ablations we demonstrate that the structural context is not only present in the models, but it significantly influences their performance."], "tagging, chunking, syntax and parsing"], [["automatic machine translation evaluation using source language inputs and cross-lingual language model", "kosuke takahashi | katsuhito sudoh | satoshi nakamura", "we propose an automatic evaluation method of machine translation that uses source language sentences regarded as additional pseudo references. the proposed method evaluates a translation hypothesis in a regression model. the model takes the paired source, reference, and hypothesis sentence all together as an input. a pretrained large scale cross-lingual language model encodes the input to sentence-pair vectors, and the model predicts a human evaluation score with those vectors. our experiments show that our proposed method using cross-lingual language model (xlm) trained with a translation language modeling (tlm) objective achieves a higher correlation with human judgments than a baseline method that uses only hypothesis and reference sentences. additionally, using source sentences in our proposed method is confirmed to improve the evaluation performance."], "resources and evaluation"], [["massively multilingual transfer for ner", "afshin rahimi | yuan li | trevor cohn", "in cross-lingual transfer, nlp models over one or more source languages are applied to a low-resource target language. while most prior work has used a single source model or a few carefully selected models, here we consider a \u201cmassive\u201d setting with many such models. this setting raises the problem of poor transfer, particularly from distant languages. we propose two techniques for modulating the transfer, suitable for zero-shot or few-shot learning, respectively. evaluating on named entity recognition, we show that our techniques are much more effective than strong baselines, including standard ensembling, and our unsupervised method rivals oracle selection of the single best individual model."], "tagging, chunking, syntax and parsing"], [["budgeted policy learning for task-oriented dialogue systems", "zhirui zhang | xiujun li | jianfeng gao | enhong chen", "this paper presents a new approach that extends deep dyna-q (ddq) by incorporating a budget-conscious scheduling (bcs) to best utilize a fixed, small amount of user interactions (budget) for learning task-oriented dialogue agents. bcs consists of (1) a poisson-based global scheduler to allocate budget over different stages of training; (2) a controller to decide at each training step whether the agent is trained using real or simulated experiences; (3) a user goal sampling module to generate the experiences that are most effective for policy learning. experiments on a movie-ticket booking task with simulated and real users show that our approach leads to significant improvements in success rate over the state-of-the-art baselines given the fixed budget."], "dialogue and interactive systems"], [["weight poisoning attacks on pretrained models", "keita kurita | paul michel | graham neubig", "recently, nlp has seen a surge in the usage of large pre-trained models. users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. this raises the question of whether downloading untrusted pre-trained weights can pose a security threat. in this paper, we show that it is possible to construct \u201cweight poisoning\u201d attacks where pre-trained weights are injected with vulnerabilities that expose \u201cbackdoors\u201d after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. we show that by applying a regularization method which we call ripple and an initialization procedure we call embedding surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. finally, we outline practical defenses against such attacks."], "machine learning for nlp"], [["determining relative argument specificity and stance for complex argumentative structures", "esin durmus | faisal ladhak | claire cardie", "systems for automatic argument generation and debate require the ability to (1) determine the stance of any claims employed in the argument and (2) assess the specificity of each claim relative to the argument context. existing work on understanding claim specificity and stance, however, has been limited to the study of argumentative structures that are relatively shallow, most often consisting of a single claim that directly supports or opposes the argument thesis. in this paper, we tackle these tasks in the context of complex arguments on a diverse set of topics. in particular, our dataset consists of manually curated argument trees for 741 controversial topics covering 95,312 unique claims; lines of argument are generally of depth 2 to 6. we find that as the distance between a pair of claims increases along the argument path, determining the relative specificity of a pair of claims becomes easier and determining their relative stance becomes harder."], "sentiment analysis, stylistic analysis, and argument mining"], [["reading turn by turn: hierarchical attention architecture for spoken dialogue comprehension", "zhengyuan liu | nancy chen", "comprehending multi-turn spoken conversations is an emerging research area, presenting challenges different from reading comprehension of passages due to the interactive nature of information exchange from at least two speakers. unlike passages, where sentences are often the default semantic modeling unit, in multi-turn conversations, a turn is a topically coherent unit embodied with immediately relevant context, making it a linguistically intuitive segment for computationally modeling verbal interactions. therefore, in this work, we propose a hierarchical attention neural network architecture, combining turn-level and word-level attention mechanisms, to improve spoken dialogue comprehension performance. experiments are conducted on a multi-turn conversation dataset, where nurses inquire and discuss symptom information with patients. we empirically show that the proposed approach outperforms standard attention baselines, achieves more efficient learning outcomes, and is more robust to lengthy and out-of-distribution test samples."], "dialogue and interactive systems"], [["ccmatrix: mining billions of high-quality parallel sentences on the web", "holger schwenk | guillaume wenzek | sergey edunov | edouard grave | armand joulin | angela fan", "we show that margin-based bitext mining in a multilingual sentence space can be successfully scaled to operate on monolingual corpora of billions of sentences. we use 32 snapshots of a curated common crawl corpus (wenzel et al, 2019) totaling 71 billion unique sentences. using one unified approach for 90 languages, we were able to mine 10.8 billion parallel sentences, out of which only 2.9 billions are aligned with english. we illustrate the capability of our scalable mining system to create high quality training sets from one language to any other by training hundreds of different machine translation models and evaluating them on the many-to-many ted benchmark. further, we evaluate on competitive translation benchmarks such as wmt and wat. using only mined bitext, we set a new state of the art for a single system on the wmt\u201919 test set for english-german/russian/chinese. in particular, our english/german and english/russian systems outperform the best single ones by over 4 bleu points and are on par with best wmt\u201919 systems, which train on the wmt training data and augment it with backtranslation. we also achieve excellent results for distant languages pairs like russian/japanese, outperforming the best submission at the 2020 wat workshop. all of the mined bitext will be freely available."], "machine translation and multilinguality"], [["misleading failures of partial-input baselines", "shi feng | eric wallace | jordan boyd-graber", "recent work establishes dataset difficulty and removes annotation artifacts via partial-input baselines (e.g., hypothesis-only model for snli or question-only model for vqa). a successful partial-input baseline indicates that the dataset is cheatable. but the converse is not necessarily true: failures of partial-input baselines do not mean the dataset is free of artifacts. we first design artificial datasets to illustrate how the trivial patterns that are only visible in the full input can evade any partial-input baseline. next, we identify such artifacts in the snli dataset\u2014a hypothesis-only model augmented with trivial patterns in the premise can solve 15% of previously-thought \u201chard\u201d examples. our work provides a caveat for the use and creation of partial-input baselines for datasets."], "machine learning for nlp"], [["pre-learning environment representations for data-efficient neural instruction following", "david gaddy | dan klein", "we consider the problem of learning to map from natural language instructions to state transitions (actions) in a data-efficient manner. our method takes inspiration from the idea that it should be easier to ground language to concepts that have already been formed through pre-linguistic observation. we augment a baseline instruction-following learner with an initial environment-learning phase that uses observations of language-free state transitions to induce a suitable latent representation of actions before processing the instruction-following training data. we show that mapping to pre-learned representations substantially improves performance over systems whose representations are learned from limited instructional data alone."], "machine learning for nlp"], [["from surrogacy to adoption; from bitcoin to cryptocurrency: debate topic expansion", "roy bar-haim | dalia krieger | orith toledo-ronen | lilach edelstein | yonatan bilu | alon halfon | yoav katz | amir menczel | ranit aharonov | noam slonim", "when debating a controversial topic, it is often desirable to expand the boundaries of discussion. for example, we may consider the pros and cons of possible alternatives to the debate topic, make generalizations, or give specific examples. we introduce the task of debate topic expansion - finding such related topics for a given debate topic, along with a novel annotated dataset for the task. we focus on relations between wikipedia concepts, and show that they differ from well-studied lexical-semantic relations such as hypernyms, hyponyms and antonyms. we present algorithms for finding both consistent and contrastive expansions and demonstrate their effectiveness empirically. we suggest that debate topic expansion may have various use cases in argumentation mining."], "sentiment analysis, stylistic analysis, and argument mining"], [["learning latent trees with stochastic perturbations and differentiable dynamic programming", "caio corro | ivan titov", "we treat projective dependency trees as latent variables in our probabilistic model and induce them in such a way as to be beneficial for a downstream task, without relying on any direct tree supervision. our approach relies on gumbel perturbations and differentiable dynamic programming. unlike previous approaches to latent tree learning, we stochastically sample global structures and our parser is fully differentiable. we illustrate its effectiveness on sentiment analysis and natural language inference tasks. we also study its properties on a synthetic structure induction task. ablation studies emphasize the importance of both stochasticity and constraining latent structures to be projective trees."], "machine learning for nlp"], [["modeling transitions of focal entities for conversational knowledge base question answering", "yunshi lan | jing jiang", "conversational kbqa is about answering a sequence of questions related to a kb. follow-up questions in conversational kbqa often have missing information referring to entities from the conversation history. in this paper, we propose to model these implied entities, which we refer to as the focal entities of the conversation. we propose a novel graph-based model to capture the transitions of focal entities and apply a graph neural network to derive a probability distribution of focal entities for each question, which is then combined with a standard kbqa module to perform answer ranking. our experiments on two datasets demonstrate the effectiveness of our proposed method."], "question answering"], [["self-supervised neural machine translation", "dana ruiter | cristina espa\u00f1a-bonet | josef van genabith", "we present a simple new method where an emergent nmt system is used for simultaneously selecting training data and learning internal nmt representations. this is done in a self-supervised way without parallel data, in such a way that both tasks enhance each other during training. the method is language independent, introduces no additional hyper-parameters, and achieves bleu scores of 29.21 (en2fr) and 27.36 (fr2en) on newstest2014 using english and french wikipedia data for training."], "machine translation and multilinguality"], [["efficient strategies for hierarchical text classification: external knowledge and auxiliary tasks", "kervy rivas rojas | gina bustamante | arturo oncevay | marco antonio sobrevilla cabezudo", "in hierarchical text classification, we perform a sequence of inference steps to predict the category of a document from top to bottom of a given class taxonomy. most of the studies have focused on developing novels neural network architectures to deal with the hierarchical structure, but we prefer to look for efficient ways to strengthen a baseline model. we first define the task as a sequence-to-sequence problem. afterwards, we propose an auxiliary synthetic task of bottom-up-classification. then, from external dictionaries, we retrieve textual definitions for the classes of all the hierarchy\u2019s layers, and map them into the word vector space. we use the class-definition embeddings as an additional input to condition the prediction of the next layer and in an adapted beam search. whereas the modified search did not provide large gains, the combination of the auxiliary task and the additional input of class-definitions significantly enhance the classification accuracy. with our efficient approaches, we outperform previous studies, using a drastically reduced number of parameters, in two well-known english datasets."], "nlp applications"], [["improving neural machine translation with soft template prediction", "jian yang | shuming ma | dongdong zhang | zhoujun li | ming zhou", "although neural machine translation (nmt) has achieved significant progress in recent years, most previous nmt models only depend on the source text to generate translation. inspired by the success of template-based and syntax-based approaches in other fields, we propose to use extracted templates from tree structures as soft target templates to guide the translation procedure. in order to learn the syntactic structure of the target sentences, we adopt constituency-based parse tree to generate candidate templates. we incorporate the template information into the encoder-decoder framework to jointly utilize the templates and source text. experiments show that our model significantly outperforms the baseline models on four benchmarks and demonstrates the effectiveness of soft target templates."], "machine translation and multilinguality"], [["simple, interpretable and stable method for detecting words with usage change across corpora", "hila gonen | ganesh jawahar | djam\u00e9 seddah | yoav goldberg", "the problem of comparing two bodies of text and searching for words that differ in their usage between them arises often in digital humanities and computational social science. this is commonly approached by training word embeddings on each corpus, aligning the vector spaces, and looking for words whose cosine distance in the aligned space is large. however, these methods often require extensive filtering of the vocabulary to perform well, and - as we show in this work - result in unstable, and hence less reliable, results. we propose an alternative approach that does not use vector space alignment, and instead considers the neighbors of each word. the method is simple, interpretable and stable. we demonstrate its effectiveness in 9 different setups, considering different corpus splitting criteria (age, gender and profession of tweet authors, time of tweet) and different languages (english, french and hebrew)."], "computational social science, social media and cultural analytics"], [["conan - counter narratives through nichesourcing: a multilingual dataset of responses to fight online hate speech", "yi-ling chung | elizaveta kuzmenko | serra sinem tekiroglu | marco guerini", "although there is an unprecedented effort to provide adequate responses in terms of laws and policies to hate content on social media platforms, dealing with hatred online is still a tough problem. tackling hate speech in the standard way of content deletion or user suspension may be charged with censorship and overblocking. one alternate strategy, that has received little attention so far by the research community, is to actually oppose hate content with counter-narratives (i.e. informed textual responses). in this paper, we describe the creation of the first large-scale, multilingual, expert-based dataset of hate-speech/counter-narrative pairs. this dataset has been built with the effort of more than 100 operators from three different ngos that applied their training and expertise to the task. together with the collected data we also provide additional annotations about expert demographics, hate and response type, and data augmentation through translation and paraphrasing. finally, we provide initial experiments to assess the quality of our data."], "computational social science, social media and cultural analytics"], [["on the summarization of consumer health questions", "asma ben abacha | dina demner-fushman", "question understanding is one of the main challenges in question answering. in real world applications, users often submit natural language questions that are longer than needed and include peripheral information that increases the complexity of the question, leading to substantially more false positives in answer retrieval. in this paper, we study neural abstractive models for medical question summarization. we introduce the meqsum corpus of 1,000 summarized consumer health questions. we explore data augmentation methods and evaluate state-of-the-art neural abstractive models on this new task. in particular, we show that semantic augmentation from question datasets improves the overall performance, and that pointer-generator networks outperform sequence-to-sequence attentional models on this task, with a rouge-1 score of 44.16%. we also present a detailed error analysis and discuss directions for improvement that are specific to question summarization."], "summarization"], [["observing dialogue in therapy: categorizing and forecasting behavioral codes", "jie cao | michael tanana | zac imel | eric poitras | david atkins | vivek srikumar", "automatically analyzing dialogue can help understand and guide behavior in domains such as counseling, where interactions are largely mediated by conversation. in this paper, we study modeling behavioral codes used to asses a psychotherapy treatment style called motivational interviewing (mi), which is effective for addressing substance abuse and related problems. specifically, we address the problem of providing real-time guidance to therapists with a dialogue observer that (1) categorizes therapist and client mi behavioral codes and, (2) forecasts codes for upcoming utterances to help guide the conversation and potentially alert the therapist. for both tasks, we define neural network models that build upon recent successes in dialogue modeling. our experiments demonstrate that our models can outperform several baselines for both tasks. we also report the results of a careful analysis that reveals the impact of the various network design tradeoffs for modeling therapy dialogue."], "dialogue and interactive systems"], [["biomedical entity representations with synonym marginalization", "mujeen sung | hwisang jeon | jinhyuk lee | jaewoo kang", "biomedical named entities often play important roles in many biomedical text mining tools. however, due to the incompleteness of provided synonyms and numerous variations in their surface forms, normalization of biomedical entities is very challenging. in this paper, we focus on learning representations of biomedical entities solely based on the synonyms of entities. to learn from the incomplete synonyms, we use a model-based candidate selection and maximize the marginal likelihood of the synonyms present in top candidates. our model-based candidates are iteratively updated to contain more difficult negative samples as our model evolves. in this way, we avoid the explicit pre-selection of negative samples from more than 400k candidates. on four biomedical entity normalization datasets having three different entity types (disease, chemical, adverse reaction), our model biosyn consistently outperforms previous state-of-the-art models almost reaching the upper bound on each dataset."], "semantics"], [["conversational word embedding for retrieval-based dialog system", "wentao ma | yiming cui | ting liu | dong wang | shijin wang | guoping hu", "human conversations contain many types of information, e.g., knowledge, common sense, and language habits. in this paper, we propose a conversational word embedding method named pr-embedding, which utilizes the conversation pairs <post, reply> to learn word embedding. different from previous works, pr-embedding uses the vectors from two different semantic spaces to represent the words in post and reply.to catch the information among the pair, we first introduce the word alignment model from statistical machine translation to generate the cross-sentence window, then train the embedding on word-level and sentence-level.we evaluate the method on single-turn and multi-turn response selection tasks for retrieval-based dialog systems.the experiment results show that pr-embedding can improve the quality of the selected response."], "dialogue and interactive systems"], [["vocabulary pyramid network: multi-pass encoding and decoding with multi-level vocabularies for response generation", "cao liu | shizhu he | kang liu | jun zhao", "we study the task of response generation. conventional methods employ a fixed vocabulary and one-pass decoding, which not only make them prone to safe and general responses but also lack further refining to the first generated raw sequence. to tackle the above two problems, we present a vocabulary pyramid network (vpn) which is able to incorporate multi-pass encoding and decoding with multi-level vocabularies into response generation. specifically, the dialogue input and output are represented by multi-level vocabularies which are obtained from hierarchical clustering of raw words. then, multi-pass encoding and decoding are conducted on the multi-level vocabularies. since vpn is able to leverage rich encoding and decoding information with multi-level vocabularies, it has the potential to generate better responses. experiments on english twitter and chinese weibo datasets demonstrate that vpn remarkably outperforms strong baselines."], "dialogue and interactive systems"], [["sentence-level evidence embedding for claim verification with hierarchical attention networks", "jing ma | wei gao | shafiq joty | kam-fai wong", "claim verification is generally a task of verifying the veracity of a given claim, which is critical to many downstream applications. it is cumbersome and inefficient for human fact-checkers to find consistent pieces of evidence, from which solid verdict could be inferred against the claim. in this paper, we propose a novel end-to-end hierarchical attention network focusing on learning to represent coherent evidence as well as their semantic relatedness with the claim. our model consists of three main components: 1) a coherence-based attention layer embeds coherent evidence considering the claim and sentences from relevant articles; 2) an entailment-based attention layer attends on sentences that can semantically infer the claim on top of the first attention; and 3) an output layer predicts the verdict based on the embedded evidence. experimental results on three public benchmark datasets show that our proposed model outperforms a set of state-of-the-art baselines."], "computational social science, social media and cultural analytics"], [["masking actor information leads to fairer political claims detection", "erenay dayanik | sebastian pad\u00f3", "a central concern in computational social sciences (css) is fairness: where the role of nlp is to scale up text analysis to large corpora, the quality of automatic analyses should be as independent as possible of textual properties. we analyze the performance of a state-of-the-art neural model on the task of political claims detection (i.e., the identification of forward-looking statements made by political actors) and identify a strong frequency bias: claims made by frequent actors are recognized better. we propose two simple debiasing methods which mask proper names and pronouns during training of the model, thus removing personal information bias. we find that (a) these methods significantly decrease frequency bias while keeping the overall performance stable; and (b) the resulting models improve when evaluated in an out-of-domain setting."], "computational social science, social media and cultural analytics"], [["multi-domain named entity recognition with genre-aware and agnostic inference", "jing wang | mayank kulkarni | daniel preotiuc-pietro", "named entity recognition is a key component of many text processing pipelines and it is thus essential for this component to be robust to different types of input. however, domain transfer of ner models with data from multiple genres has not been widely studied. to this end, we conduct ner experiments in three predictive setups on data from: a) multiple domains; b) multiple domains where the genre label is unknown at inference time; c) domains not encountered in training. we introduce a new architecture tailored to this task by using shared and private domain parameters and multi-task learning. this consistently outperforms all other baseline and competitive methods on all three experimental setups, with differences ranging between +1.95 to +3.11 average f1 across multiple genres when compared to standard approaches. these results illustrate the challenges that need to be taken into account when building real-world nlp applications that are robust to various types of text and the methods that can help, at least partially, alleviate these issues."], "information extraction, retrieval and text mining"], [["a novel graph-based multi-modal fusion encoder for neural machine translation", "yongjing yin | fandong meng | jinsong su | chulun zhou | zhengyuan yang | jie zhou | jiebo luo", "multi-modal neural machine translation (nmt) aims to translate source sentences into a target language paired with images. however, dominant multi-modal nmt models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning. to deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for nmt. specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects). we then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations. finally, these representations provide an attention-based context vector for the decoder. we evaluate our proposed encoder on the multi30k datasets. experimental results and in-depth analysis show the superiority of our multi-modal nmt model."], "machine translation and multilinguality"], [["social biases in nlp models as barriers for persons with disabilities", "ben hutchinson | vinodkumar prabhakaran | emily denton | kellie webster | yu zhong | stephen denuyl", "building equitable and inclusive nlp technologies demands consideration of whether and how social attitudes are represented in ml models. in particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. in this paper, we present evidence of such undesirable biases towards mentions of disability in two different english language models: toxicity prediction and sentiment analysis. next, we demonstrate that the neural embeddings that are the critical first step in most nlp pipelines similarly contain undesirable biases towards mentions of disability. we end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness."], "ethics in nlp"], [["tan-ntm: topic attention networks for neural topic modeling", "madhur panwar | shashank shailabh | milan aggarwal | balaji krishnamurthy", "topic models have been widely used to learn text representations and gain insight into document corpora. to perform topic discovery, most existing neural models either take document bag-of-words (bow) or sequence of tokens as input followed by variational inference and bow reconstruction to learn topic-word distribution. however, leveraging topic-word distribution for learning better features during document encoding has not been explored much. to this end, we develop a framework tan-ntm, which processes document as a sequence of tokens through a lstm whose contextual outputs are attended in a topic-aware manner. we propose a novel attention mechanism which factors in topic-word distribution to enable the model to attend on relevant words that convey topic related cues. the output of topic attention module is then used to carry out variational inference. we perform extensive ablations and experiments resulting in ~9-15 percentage improvement over score of existing sota topic models in npmi coherence on several benchmark datasets - 20newsgroups, yelp review polarity and agnews. further, we show that our method learns better latent document-topic features compared to existing topic models through improvement on two downstream tasks: document classification and topic guided keyphrase generation."], "information extraction, retrieval and text mining"], [["improving abstractive document summarization with salient information modeling", "yongjian you | weijia jia | tianyi liu | wenmian yang", "comprehensive document encoding and salient information selection are two major difficulties for generating summaries with adequate salient information. to tackle the above difficulties, we propose a transformer-based encoder-decoder framework with two novel extensions for abstractive document summarization. specifically, (1) to encode the documents comprehensively, we design a focus-attention mechanism and incorporate it into the encoder. this mechanism models a gaussian focal bias on attention scores to enhance the perception of local context, which contributes to producing salient and informative summaries. (2) to distinguish salient information precisely, we design an independent saliency-selection network which manages the information flow from encoder to decoder. this network effectively reduces the influences of secondary information on the generated summaries. experimental results on the popular cnn/daily mail benchmark demonstrate that our model outperforms other state-of-the-art baselines on the rouge metrics."], "summarization"], [["is word segmentation child\u2019s play in all languages?", "georgia r. loukatou | steven moran | damian blasi | sabine stoll | alejandrina cristia", "when learning language, infants need to break down the flow of input speech into minimal word-like units, a process best described as unsupervised bottom-up segmentation. proposed strategies include several segmentation algorithms, but only cross-linguistically robust algorithms could be plausible candidates for human word learning, since infants have no initial knowledge of the ambient language. we report on the stability in performance of 11 conceptually diverse algorithms on a selection of 8 typologically distinct languages. the results consist evidence that some segmentation algorithms are cross-linguistically valid, thus could be considered as potential strategies employed by all infants."], "linguistic theories, cognitive modeling and psycholinguistics"], [["not all claims are created equal: choosing the right statistical approach to assess hypotheses", "erfan sadeqi azer | daniel khashabi | ashish sabharwal | dan roth", "empirical research in natural language processing (nlp) has adopted a narrow set of principles for assessing hypotheses, relying mainly on p-value computation, which suffers from several known issues. while alternative proposals have been well-debated and adopted in other fields, they remain rarely discussed or used within the nlp community. we address this gap by contrasting various hypothesis assessment techniques, especially those not commonly used in the field (such as evaluations based on bayesian inference). since these statistical techniques differ in the hypotheses they can support, we argue that practitioners should first decide their target hypothesis before choosing an assessment method. this is crucial because common fallacies, misconceptions, and misinterpretation surrounding hypothesis assessment methods often stem from a discrepancy between what one would like to claim versus what the method used actually assesses. our survey reveals that these issues are omnipresent in the nlp research community. as a step forward, we provide best practices and guidelines tailored to nlp research, as well as an easy-to-use package for bayesian assessment of hypotheses, complementing existing tools."], "resources and evaluation"], [["a unified multi-task adversarial learning framework for pharmacovigilance mining", "shweta yadav | asif ekbal | sriparna saha | pushpak bhattacharyya", "the mining of adverse drug reaction (adr) has a crucial role in the pharmacovigilance. the traditional ways of identifying adr are reliable but time-consuming, non-scalable and offer a very limited amount of adr relevant information. with the unprecedented growth of information sources in the forms of social media texts (twitter, blogs, reviews etc.), biomedical literature, and electronic medical records (emr), it has become crucial to extract the most pertinent adr related information from these free-form texts. in this paper, we propose a neural network inspired multi- task learning framework that can simultaneously extract adrs from various sources. we adopt a novel adversarial learning-based approach to learn features across multiple adr information sources. unlike the other existing techniques, our approach is capable to extracting fine-grained information (such as \u2018indications\u2019, \u2018symptoms\u2019, \u2018finding\u2019, \u2018disease\u2019, \u2018drug\u2019) which provide important cues in pharmacovigilance. we evaluate our proposed approach on three publicly available real- world benchmark pharmacovigilance datasets, a twitter dataset from psb 2016 social me- dia shared task, cadec corpus and medline adr corpus. experiments show that our unified framework achieves state-of-the-art performance on individual tasks associated with the different benchmark datasets. this establishes the fact that our proposed approach is generic, which enables it to achieve high performance on the diverse datasets."], "information extraction, retrieval and text mining"], [["breaking down the invisible wall of informal fallacies in online discussions", "saumya sahai | oana balalau | roxana horincar", "people debate on a variety of topics on online platforms such as reddit, or facebook. debates can be lengthy, with users exchanging a wealth of information and opinions. however, conversations do not always go smoothly, and users sometimes engage in unsound argumentation techniques to prove a claim. these techniques are called fallacies. fallacies are persuasive arguments that provide insufficient or incorrect evidence to support the claim. in this paper, we study the most frequent fallacies on reddit, and we present them using the pragma-dialectical theory of argumentation. we construct a new annotated dataset of fallacies, using user comments containing fallacy mentions as noisy labels, and cleaning the data via crowdsourcing. finally, we study the task of classifying fallacies using neural models. we find that generally the models perform better in the presence of conversational context.we have released the data and the code at github.com/sahaisaumya/informal_fallacies."], "computational social science, social media and cultural analytics"], [["bootstrapping techniques for polysynthetic morphological analysis", "william lane | steven bird", "polysynthetic languages have exceptionally large and sparse vocabularies, thanks to the number of morpheme slots and combinations in a word. this complexity, together with a general scarcity of written data, poses a challenge to the development of natural language technologies. to address this challenge, we offer linguistically-informed approaches for bootstrapping a neural morphological analyzer, and demonstrate its application to kunwinjku, a polysynthetic australian language. we generate data from a finite state transducer to train an encoder-decoder model. we improve the model by \u201challucinating\u201d missing linguistic structure into the training data, and by resampling from a zipf distribution to simulate a more natural distribution of morphemes. the best model accounts for all instances of reduplication in the test set and achieves an accuracy of 94.7% overall, a 10 percentage point improvement over the fst baseline. this process demonstrates the feasibility of bootstrapping a neural morph analyzer from minimal resources."], "phonology, morphology and word segmentation"], [["automated topical component extraction using neural network attention scores from source-based essay scoring", "haoran zhang | diane litman", "while automated essay scoring (aes) can reliably grade essays at scale, automated writing evaluation (awe) additionally provides formative feedback to guide essay revision. however, a neural aes typically does not provide useful feature representations for supporting awe. this paper presents a method for linking awe and neural aes, by extracting topical components (tcs) representing evidence from a source text using the intermediate output of attention layers. we evaluate performance using a feature-based aes requiring tcs. results show that performance is comparable whether using automatically or manually constructed tcs for 1) representing essays as rubric-based features, 2) grading essays."], "nlp applications"], [["generating fact checking explanations", "pepa atanasova | jakob grue simonsen | christina lioma | isabelle augenstein", "most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims. a crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process \u2013 generating justifications for verdicts on claims. this paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction. our results indicate that optimising both objectives at the same time, rather than training them separately, improves the performance of a fact checking system. the results of a manual evaluation further suggest that the informativeness, coverage and overall quality of the generated explanations are also improved in the multi-task model."], "semantics"], [["target inference in argument conclusion generation", "milad alshomary | shahbaz syed | martin potthast | henning wachsmuth", "in argumentation, people state premises to reason towards a conclusion. the conclusion conveys a stance towards some target, such as a concept or statement. often, the conclusion remains implicit, though, since it is self-evident in a discussion or left out for rhetorical reasons. however, the conclusion is key to understanding an argument and, hence, to any application that processes argumentation. we thus study the question to what extent an argument\u2019s conclusion can be reconstructed from its premises. in particular, we argue here that a decisive step is to infer a conclusion\u2019s target, and we hypothesize that this target is related to the premises\u2019 targets. we develop two complementary target inference approaches: one ranks premise targets and selects the top-ranked target as the conclusion target, the other finds a new conclusion target in a learned embedding space using a triplet neural network. our evaluation on corpora from two domains indicates that a hybrid of both approaches is best, outperforming several strong baselines. according to human annotators, we infer a reasonably adequate conclusion target in 89% of the cases."], "sentiment analysis, stylistic analysis, and argument mining"], [["sumbt: slot-utterance matching for universal and scalable belief tracking", "hwaran lee | jinsik lee | tae-yoon kim", "in goal-oriented dialog systems, belief trackers estimate the probability distribution of slot-values at every dialog turn. previous neural approaches have modeled domain- and slot-dependent belief trackers, and have difficulty in adding new slot-values, resulting in lack of flexibility of domain ontology configurations. in this paper, we propose a new approach to universal and scalable belief tracker, called slot-utterance matching belief tracker (sumbt). the model learns the relations between domain-slot-types and slot-values appearing in utterances through attention mechanisms based on contextual semantic vectors. furthermore, the model predicts slot-value labels in a non-parametric way. from our experiments on two dialog corpora, woz 2.0 and multiwoz, the proposed model showed performance improvement in comparison with slot-dependent methods and achieved the state-of-the-art joint accuracy."], "dialogue and interactive systems"], [["compositional semantic parsing across graphbanks", "matthias lindemann | jonas groschwitz | alexander koller", "most semantic parsers that map sentences to graph-based meaning representations are hand-designed for specific graphbanks. we present a compositional neural semantic parser which achieves, for the first time, competitive accuracies across a diverse range of graphbanks. incorporating bert embeddings and multi-task learning improves the accuracy further, setting new states of the art on dm, pas, psd, amr 2015 and eds."], "semantics"], [["a two-step approach for implicit event argument detection", "zhisong zhang | xiang kong | zhengzhong liu | xuezhe ma | eduard hovy", "in this work, we explore the implicit event argument detection task, which studies event arguments beyond sentence boundaries. the addition of cross-sentence argument candidates imposes great challenges for modeling. to reduce the number of candidates, we adopt a two-step approach, decomposing the problem into two sub-problems: argument head-word detection and head-to-span expansion. evaluated on the recent rams dataset (ebner et al., 2020), our model achieves overall better performance than a strong sequence labeling baseline. we further provide detailed error analysis, presenting where the model mainly makes errors and indicating directions for future improvements. it remains a challenge to detect implicit arguments, calling for more future work of document-level modeling for this task."], "information extraction, retrieval and text mining"], [["learning to customize model structures for few-shot dialogue generation tasks", "yiping song | zequn liu | wei bi | rui yan | ming zhang", "training the generative models with minimal corpus is one of the critical challenges for building open-domain dialogue systems. existing methods tend to use the meta-learning framework which pre-trains the parameters on all non-target tasks then fine-tunes on the target task. however, fine-tuning distinguishes tasks from the parameter perspective but ignores the model-structure perspective, resulting in similar dialogue models for different tasks. in this paper, we propose an algorithm that can customize a unique dialogue model for each task in the few-shot setting. in our approach, each dialogue model consists of a shared module, a gating module, and a private module. the first two modules are shared among all the tasks, while the third one will differentiate into different network structures to better capture the characteristics of the corresponding task. the extensive experiments on two datasets show that our method outperforms all the baselines in terms of task consistency, response quality, and diversity."], "dialogue and interactive systems"], [["text and causal inference: a review of using text to remove confounding from causal estimates", "katherine keith | david jensen | brendan o\u2019connor", "many applications of computational social science aim to infer causal conclusions from non-experimental data. such observational data often contains confounders, variables that influence both potential causes and potential effects. unmeasured or latent confounders can bias causal estimates, and this has motivated interest in measuring potential confounders from observed text. for example, an individual\u2019s entire history of social media posts or the content of a news article could provide a rich measurement of multiple confounders.yet, methods and applications for this problem are scattered across different communities and evaluation practices are inconsistent.this review is the first to gather and categorize these examples and provide a guide to data-processing and evaluation decisions. despite increased attention on adjusting for confounding using text, there are still many open problems, which we highlight in this paper."], "computational social science, social media and cultural analytics"], [["proactive human-machine conversation with explicit conversation goal", "wenquan wu | zhen guo | xiangyang zhou | hua wu | xiyuan zhang | rongzhong lian | haifeng wang", "though great progress has been made for human-machine conversation, current dialogue system is still in its infancy: it usually converses passively and utters words more as a matter of response, rather than on its own initiatives. in this paper, we take a radical step towards building a human-like conversational agent: endowing it with the ability of proactively leading the conversation (introducing a new topic or maintaining the current topic). to facilitate the development of such conversation systems, we create a new dataset named konv where one acts as a conversation leader and the other acts as the follower. the leader is provided with a knowledge graph and asked to sequentially change the discussion topics, following the given conversation goal, and meanwhile keep the dialogue as natural and engaging as possible. konv enables a very challenging task as the model needs to both understand dialogue and plan over the given knowledge graph. we establish baseline results on this dataset (about 270k utterances and 30k dialogues) using several state-of-the-art models. experimental results show that dialogue models that plan over the knowledge graph can make full use of related knowledge to generate more diverse multi-turn conversations. the baseline systems along with the dataset are publicly available."], "dialogue and interactive systems"], [["neural news recommendation with topic-aware news representation", "chuhan wu | fangzhao wu | mingxiao an | yongfeng huang | xing xie", "news recommendation can help users find interested news and alleviate information overload. the topic information of news is critical for learning accurate news and user representations for news recommendation. however, it is not considered in many existing news recommendation methods. in this paper, we propose a neural news recommendation approach with topic-aware news representations. the core of our approach is a topic-aware news encoder and a user encoder. in the news encoder we learn representations of news from their titles via cnn networks and apply attention networks to select important words. in addition, we propose to learn topic-aware news representations by jointly training the news encoder with an auxiliary topic classification task. in the user encoder we learn the representations of users from their browsed news and use attention networks to select informative news for user representation learning. extensive experiments on a real-world dataset validate the effectiveness of our approach."], "nlp applications"], [["single model ensemble using pseudo-tags and distinct vectors", "ryosuke kuwabara | jun suzuki | hideki nakayama", "model ensemble techniques often increase task performance in neural networks; however, they require increased time, memory, and management effort. in this study, we propose a novel method that replicates the effects of a model ensemble with a single model. our approach creates k-virtual models within a single parameter space using k-distinct pseudo-tags and k-distinct vectors. experiments on text classification and sequence labeling tasks on several datasets demonstrate that our method emulates or outperforms a traditional model ensemble with 1/k-times fewer parameters."], "machine learning for nlp"], [["a novel bi-directional interrelated model for joint intent detection and slot filling", "haihong e | peiqing niu | zhongfu chen | meina song", "a spoken language understanding (slu) system includes two main tasks, slot filling (sf) and intent detection (id). the joint model for the two tasks is becoming a tendency in slu. but the bi-directional interrelated connections between the intent and slots are not established in the existing joint models. in this paper, we propose a novel bi-directional interrelated model for joint intent detection and slot filling. we introduce an sf-id network to establish direct connections for the two tasks to help them promote each other mutually. besides, we design an entirely new iteration mechanism inside the sf-id network to enhance the bi-directional interrelated connections. the experimental results show that the relative improvement in the sentence-level semantic frame accuracy of our model is 3.79% and 5.42% on atis and snips datasets, respectively, compared to the state-of-the-art model."], "dialogue and interactive systems"], [["sentiment and emotion help sarcasm? a multi-task learning framework for multi-modal sarcasm, sentiment and emotion analysis", "dushyant singh chauhan | dhanush s r | asif ekbal | pushpak bhattacharyya", "in this paper, we hypothesize that sarcasm is closely related to sentiment and emotion, and thereby propose a multi-task deep learning framework to solve all these three problems simultaneously in a multi-modal conversational scenario. we, at first, manually annotate the recently released multi-modal mustard sarcasm dataset with sentiment and emotion classes, both implicit and explicit. for multi-tasking, we propose two attention mechanisms, viz. inter-segment inter-modal attention (ie-attention) and intra-segment inter-modal attention (ia-attention). the main motivation of ie-attention is to learn the relationship between the different segments of the sentence across the modalities. in contrast, ia-attention focuses within the same segment of the sentence across the modalities. finally, representations from both the attentions are concatenated and shared across the five classes (i.e., sarcasm, implicit sentiment, explicit sentiment, implicit emotion, explicit emotion) for multi-tasking. experimental results on the extended version of the mustard dataset show the efficacy of our proposed approach for sarcasm detection over the existing state-of-the-art systems. the evaluation also shows that the proposed multi-task framework yields better performance for the primary task, i.e., sarcasm detection, with the help of two secondary tasks, emotion and sentiment analysis."], "speech and multimodality"], [["bob: bert over bert for training persona-based dialogue models from limited personalized data", "haoyu song | yan wang | kaiyan zhang | wei-nan zhang | ting liu", "maintaining a consistent persona is essential for dialogue agents. although tremendous advancements have been brought, the limited-scale of annotated personalized dialogue datasets is still a barrier towards training robust and consistent persona-based dialogue models. this work shows how this challenge can be addressed by disentangling persona-based dialogue generation into two sub-tasks with a novel bert-over-bert (bob) model. specifically, the model consists of a bert-based encoder and two bert-based decoders, where one decoder is for response generation, and another is for consistency understanding. in particular, to learn the ability of consistency understanding from large-scale non-dialogue inference data, we train the second decoder in an unlikelihood manner. under different limited data settings, both automatic and human evaluations demonstrate that the proposed model outperforms strong baselines in response quality and persona consistency."], "dialogue and interactive systems"], [["txtract: taxonomy-aware knowledge extraction for thousands of product categories", "giannis karamanolakis | jun ma | xin luna dong", "extracting structured knowledge from product profiles is crucial for various applications in e-commerce. state-of-the-art approaches for knowledge extraction were each designed for a single category of product, and thus do not apply to real-life e-commerce scenarios, which often contain thousands of diverse categories. this paper proposes txtract, a taxonomy-aware knowledge extraction model that applies to thousands of product categories organized in a hierarchical taxonomy. through category conditional self-attention and multi-task learning, our approach is both scalable, as it trains a single model for thousands of categories, and effective, as it extracts category-specific attribute values. experiments on products from a taxonomy with 4,000 categories show that txtract outperforms state-of-the-art approaches by up to 10% in f1 and 15% in coverage across all categories."], "information extraction, retrieval and text mining"], [["entity-aware dependency-based deep graph attention network for comparative preference classification", "nianzu ma | sahisnu mazumder | hao wang | bing liu", "this paper studies the task of comparative preference classification (cpc). given two entities in a sentence, our goal is to classify whether the first (or the second) entity is preferred over the other or no comparison is expressed at all between the two entities. existing works either do not learn entity-aware representations well and fail to deal with sentences involving multiple entity pairs or use sequential modeling approaches that are unable to capture long-range dependencies between the entities. some also use traditional machine learning approaches that do not generalize well. this paper proposes a novel entity-aware dependency-based deep graph attention network (ed-gat) that employs a multi-hop graph attention over a dependency graph sentence representation to leverage both the semantic information from word embeddings and the syntactic information from the dependency graph to solve the problem. empirical evaluation shows that the proposed model achieves the state-of-the-art performance in comparative preference classification."], "sentiment analysis, stylistic analysis, and argument mining"], [["facet-aware evaluation for extractive summarization", "yuning mao | liyuan liu | qi zhu | xiang ren | jiawei han", "commonly adopted metrics for extractive summarization focus on lexical overlap at the token level. in this paper, we present a facet-aware evaluation setup for better assessment of the information coverage in extracted summaries. specifically, we treat each sentence in the reference summary as a facet, identify the sentences in the document that express the semantics of each facet as support sentences of the facet, and automatically evaluate extractive summarization methods by comparing the indices of extracted sentences and support sentences of all the facets in the reference summary. to facilitate this new evaluation setup, we construct an extractive version of the cnn/daily mail dataset and perform a thorough quantitative investigation, through which we demonstrate that facet-aware evaluation manifests better correlation with human judgment than rouge, enables fine-grained evaluation as well as comparative analysis, and reveals valuable insights of state-of-the-art summarization methods. data can be found at https://github.com/morningmoni/far."], "resources and evaluation"], [["the sofc-exp corpus and neural approaches to information extraction in the materials science domain", "annemarie friedrich | heike adel | federico tomazic | johannes hingerl | renou benteau | anika marusczyk | lukas lange", "this paper presents a new challenging information extraction task in the domain of materials science. we develop an annotation scheme for marking information on experiments related to solid oxide fuel cells in scientific publications, such as involved materials and measurement conditions. with this paper, we publish our annotation guidelines, as well as our sofc-exp corpus consisting of 45 open-access scholarly articles annotated by domain experts. a corpus and an inter-annotator agreement study demonstrate the complexity of the suggested named entity recognition and slot filling tasks as well as high annotation quality. we also present strong neural-network based models for a variety of tasks that can be addressed on the basis of our new data set. on all tasks, using bert embeddings leads to large performance gains, but with increasing task complexity, adding a recurrent neural network on top seems beneficial. our models will serve as competitive baselines in future work, and analysis of their performance highlights difficult cases when modeling the data and suggests promising research directions."], "resources and evaluation"], [["joint entity extraction and assertion detection for clinical text", "parminder bhatia | busra celikkaya | mohammed khalilia", "negative medical findings are prevalent in clinical reports, yet discriminating them from positive findings remains a challenging task for in-formation extraction. most of the existing systems treat this task as a pipeline of two separate tasks, i.e., named entity recognition (ner)and rule-based negation detection. we consider this as a multi-task problem and present a novel end-to-end neural model to jointly extract entities and negations. we extend a standard hierarchical encoder-decoder ner model and first adopt a shared encoder followed by separate decoders for the two tasks. this architecture performs considerably better than the previous rule-based and machine learning-based systems. to overcome the problem of increased parameter size especially for low-resource settings, we propose the conditional softmax shared decoder architecture which achieves state-of-art results for ner and negation detection on the 2010 i2b2/va challenge dataset and a proprietary de-identified clinical dataset."], "nlp applications"], [["gluecos: an evaluation benchmark for code-switched nlp", "simran khanuja | sandipan dandapat | anirudh srinivasan | sunayana sitaram | monojit choudhury", "code-switching is the use of more than one language in the same conversation or utterance. recently, multilingual contextual embedding models, trained on multiple monolingual corpora, have shown promising results on cross-lingual and multilingual tasks. we present an evaluation benchmark, gluecos, for code-switched languages, that spans several nlp tasks in english-hindi and english-spanish. specifically, our evaluation benchmark includes language identification from text, pos tagging, named entity recognition, sentiment analysis, question answering and a new task for code-switching, natural language inference. we present results on all these tasks using cross-lingual word embedding models and multilingual models. in addition, we fine-tune multilingual models on artificially generated code-switched data. although multilingual models perform significantly better than cross-lingual models, our results show that in most tasks, across both language pairs, multilingual models fine-tuned on code-switched data perform best, showing that multilingual models can be further optimized for code-switching tasks."], "resources and evaluation"], [["ontogum: evaluating contextualized sota coreference resolution on 12 more genres", "yilun zhu | sameer pradhan | amir zeldes", "sota coreference resolution produces increasingly impressive scores on the ontonotes benchmark. however lack of comparable data following the same scheme for more genres makes it difficult to evaluate generalizability to open domain data. this paper provides a dataset and comprehensive evaluation showing that the latest neural lm based end-to-end systems degrade very substantially out of domain. we make an ontonotes-like coreference dataset called ontogum publicly available, converted from gum, an english corpus covering 12 genres, using deterministic rules, which we evaluate. thanks to the rich syntactic and discourse annotations in gum, we are able to create the largest human-annotated coreference corpus following the ontonotes guidelines, and the first to be evaluated for consistency with the ontonotes scheme. out-of-domain evaluation across 12 genres shows nearly 15-20% degradation for both deterministic and deep learning systems, indicating a lack of generalizability or covert overfitting in existing coreference resolution models."], "resources and evaluation"], [["arnor: attention regularization based noise reduction for distant supervision relation classification", "wei jia | dai dai | xinyan xiao | hua wu", "distant supervision is widely used in relation classification in order to create large-scale training data by aligning a knowledge base with an unlabeled corpus. however, it also introduces amounts of noisy labels where a contextual sentence actually does not express the labeled relation. in this paper, we propose arnor, a novel attention regularization based noise reduction framework for distant supervision relation classification. arnor assumes that a trustable relation label should be explained by the neural attention model. specifically, our arnor framework iteratively learns an interpretable model and utilizes it to select trustable instances. we first introduce attention regularization to force the model to pay attention to the patterns which explain the relation labels, so as to make the model more interpretable. then, if the learned model can clearly locate the relation patterns of a candidate instance in the training set, we will select it as a trustable instance for further training step. according to the experiments on nyt data, our arnor framework achieves significant improvements over state-of-the-art methods in both relation classification performance and noise reduction effect."], "information extraction, retrieval and text mining"], [["duality of link prediction and entailment graph induction", "mohammad javad hosseini | shay b. cohen | mark johnson | mark steedman", "link prediction and entailment graph induction are often treated as different problems. in this paper, we show that these two problems are actually complementary. we train a link prediction model on a knowledge graph of assertions extracted from raw text. we propose an entailment score that exploits the new facts discovered by the link prediction model, and then form entailment graphs between relations. we further use the learned entailments to predict improved link prediction scores. our results show that the two tasks can benefit from each other. the new entailment score outperforms prior state-of-the-art results on a standard entialment dataset and the new link prediction scores show improvements over the raw link prediction scores."], "semantics"], [["learning representations from imperfect time series data via tensor rank regularization", "paul pu liang | zhun liu | yao-hung hubert tsai | qibin zhao | ruslan salakhutdinov | louis-philippe morency", "there has been an increased interest in multimodal language processing including multimodal dialog, question answering, sentiment analysis, and speech recognition. however, naturally occurring multimodal data is often imperfect as a result of imperfect modalities, missing entries or noise corruption. to address these concerns, we present a regularization method based on tensor rank minimization. our method is based on the observation that high-dimensional multimodal time series data often exhibit correlations across time and modalities which leads to low-rank tensor representations. however, the presence of noise or incomplete values breaks these correlations and results in tensor representations of higher rank. we design a model to learn such tensor representations and effectively regularize their rank. experiments on multimodal language data show that our model achieves good results across various levels of imperfection."], "machine learning for nlp"], [["a lightweight recurrent network for sequence modeling", "biao zhang | rico sennrich", "recurrent networks have achieved great success on various sequential tasks with the assistance of complex recurrent units, but suffer from severe computational inefficiency due to weak parallelization. one direction to alleviate this issue is to shift heavy computations outside the recurrence. in this paper, we propose a lightweight recurrent network, or lrn. lrn uses input and forget gates to handle long-range dependencies as well as gradient vanishing and explosion, with all parameter related calculations factored outside the recurrence. the recurrence in lrn only manipulates the weight assigned to each token, tightly connecting lrn with self-attention networks. we apply lrn as a drop-in replacement of existing recurrent units in several neural sequential models. extensive experiments on six nlp tasks show that lrn yields the best running efficiency with little or no loss in model performance."], "machine learning for nlp"], [["measuring forecasting skill from text", "shi zong | alan ritter | eduard hovy", "people vary in their ability to make accurate predictions about the future. prior studies have shown that some individuals can predict the outcome of future events with consistently better accuracy. this leads to a natural question: what makes some forecasters better than others? in this paper we explore connections between the language people use to describe their predictions and their forecasting skill. datasets from two different forecasting domains are explored: (1) geopolitical forecasts from good judgment open, an online prediction forum and (2) a corpus of company earnings forecasts made by financial analysts. we present a number of linguistic metrics which are computed over text associated with people\u2019s predictions about the future including: uncertainty, readability, and emotion. by studying linguistic factors associated with predictions, we are able to shed some light on the approach taken by skilled forecasters. furthermore, we demonstrate that it is possible to accurately predict forecasting skill using a model that is based solely on language. this could potentially be useful for identifying accurate predictions or potentially skilled forecasters earlier."], "computational social science, social media and cultural analytics"], [["enabling language models to fill in the blanks", "chris donahue | mina lee | percy liang", "we present a simple approach for text infilling, the task of predicting missing spans of text at any position in a document. while infilling could enable rich functionality especially for writing assistance tools, more attention has been devoted to language modeling\u2014a special case of infilling where text is predicted at the end of a document. in this paper, we aim to extend the capabilities of language models (lms) to the more general task of infilling. to this end, we train (or fine tune) off-the-shelf lms on sequences containing the concatenation of artificially-masked text and the text which was masked. we show that this approach, which we call infilling by language modeling, can enable lms to infill entire sentences effectively on three different domains: short stories, scientific abstracts, and lyrics. furthermore, we show that humans have difficulty identifying sentences infilled by our approach as machine-generated in the domain of short stories."], "generation"], [["robust neural machine translation with doubly adversarial inputs", "yong cheng | lu jiang | wolfgang macherey", "neural machine translation (nmt) often suffers from the vulnerability to noisy perturbations in the input. we propose an approach to improving the robustness of nmt models, which consists of two parts: (1) attack the translation model with adversarial source examples; (2) defend the translation model with adversarial target inputs to improve its robustness against the adversarial source inputs. for the generation of adversarial inputs, we propose a gradient-based method to craft adversarial examples informed by the translation loss over the clean inputs. experimental results on chinese-english and english-german translation tasks demonstrate that our approach achieves significant improvements (2.8 and 1.6 bleu points) over transformer on standard clean benchmarks as well as exhibiting higher robustness on noisy data."], "machine translation and multilinguality"], [["beyond possession existence: duration and co-possession", "dhivya chinnappa | srikala murugan | eduardo blanco", "this paper introduces two tasks: determining (a) the duration of possession relations and (b) co-possessions, i.e., whether multiple possessors possess a possessee at the same time. we present new annotations on top of corpora annotating possession existence and experimental results. regarding possession duration, we derive the time spans we work with empirically from annotations indicating lower and upper bounds. regarding co-possessions, we use a binary label. cohen\u2019s kappa coefficients indicate substantial agreement, and experimental results show that text is more useful than the image for solving these tasks."], "semantics"], [["reasoning over semantic-level graph for fact checking", "wanjun zhong | jingjing xu | duyu tang | zenan xu | nan duan | ming zhou | jiahai wang | jian yin", "fact checking is a challenging task because verifying the truthfulness of a claim requires reasoning about multiple retrievable evidence. in this work, we present a method suitable for reasoning about the semantic-level structure of evidence. unlike most previous works, which typically represent evidence sentences with either string concatenation or fusing the features of isolated evidence sentences, our approach operates on rich semantic structures of evidence obtained by semantic role labeling. we propose two mechanisms to exploit the structure of evidence while leveraging the advances of pre-trained models like bert, gpt or xlnet. specifically, using xlnet as the backbone, we first utilize the graph structure to re-define the relative distances of words, with the intuition that semantically related words should have short distances. then, we adopt graph convolutional network and graph attention network to propagate and aggregate information from neighboring nodes on the graph. we evaluate our system on fever, a benchmark dataset for fact checking, and find that rich structural information is helpful and both our graph-based mechanisms improve the accuracy. our model is the state-of-the-art system in terms of both official evaluation metrics, namely claim verification accuracy and fever score."], "semantics"], [["are red roses red? evaluating consistency of question-answering models", "marco tulio ribeiro | carlos guestrin | sameer singh", "although current evaluation of question-answering systems treats predictions in isolation, we need to consider the relationship between predictions to measure true understanding. a model should be penalized for answering \u201cno\u201d to \u201cis the rose red?\u201d if it answers \u201cred\u201d to \u201cwhat color is the rose?\u201d. we propose a method to automatically extract such implications for instances from two qa datasets, vqa and squad, which we then use to evaluate the consistency of models. human evaluation shows these generated implications are well formed and valid. consistency evaluation provides crucial insights into gaps in existing models, while retraining with implication-augmented data improves consistency on both synthetic and human-generated implications."], "question answering"], [["towards automating healthcare question answering in a noisy multilingual low-resource setting", "jeanne e. daniel | willie brink | ryan eloff | charles copley", "we discuss ongoing work into automating a multilingual digital helpdesk service available via text messaging to pregnant and breastfeeding mothers in south africa. our anonymized dataset consists of short informal questions, often in low-resource languages, with unreliable language labels, spelling errors and code-mixing, as well as template answers with some inconsistencies. we explore cross-lingual word embeddings, and train parametric and non-parametric models on 90k samples for answer selection from a set of 126 templates. preliminary results indicate that lstms trained end-to-end perform best, with a test accuracy of 62.13% and a recall@5 of 89.56%, and demonstrate that we can accelerate response time by several orders of magnitude."], "nlp applications"], [["knowledge graph embedding compression", "mrinmaya sachan", "knowledge graph (kg) representation learning techniques that learn continuous embeddings of entities and relations in the kg have become popular in many ai applications. with a large kg, the embeddings consume a large amount of storage and memory. this is problematic and prohibits the deployment of these techniques in many real world settings. thus, we propose an approach that compresses the kg embedding layer by representing each entity in the kg as a vector of discrete codes and then composes the embeddings from these codes. the approach can be trained end-to-end with simple modifications to any existing kg embedding technique. we evaluate the approach on various standard kg embedding evaluations and show that it achieves 50-1000x compression of embeddings with a minor loss in performance. the compressed embeddings also retain the ability to perform various reasoning tasks such as kg inference."], "machine learning for nlp"], [["named entity recognition without labelled data: a weak supervision approach", "pierre lison | jeremy barnes | aliaksandr hubin | samia touileb", "named entity recognition (ner) performance often degrades rapidly when applied to target domains that differ from the texts observed during training. when in-domain labelled data is available, transfer learning techniques can be used to adapt existing ner models to the target domain. but what should one do when there is no hand-labelled data for the target domain? this paper presents a simple but powerful approach to learn ner models in the absence of labelled data through weak supervision. the approach relies on a broad spectrum of labelling functions to automatically annotate texts from the target domain. these annotations are then merged together using a hidden markov model which captures the varying accuracies and confusions of the labelling functions. a sequence labelling model can finally be trained on the basis of this unified annotation. we evaluate the approach on two english datasets (conll 2003 and news articles from reuters and bloomberg) and demonstrate an improvement of about 7 percentage points in entity-level f1 scores compared to an out-of-domain neural ner model."], "information extraction, retrieval and text mining"], [["are you looking? grounding to multiple modalities in vision-and-language navigation", "ronghang hu | daniel fried | anna rohrbach | dan klein | trevor darrell | kate saenko", "vision-and-language navigation (vln) requires grounding instructions, such as \u201cturn right and stop at the door\u201d, to routes in a visual environment. the actual grounding can connect language to the environment through multiple modalities, e.g. \u201cstop at the door\u201d might ground into visual objects, while \u201cturn right\u201d might rely only on the geometric structure of a route. we investigate where the natural language empirically grounds under two recent state-of-the-art vln models. surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark room-to-room dataset. to better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the vln task."], "language grounding to vision, robotics and beyond"], [["estimating mutual information between dense word embeddings", "vitalii zhelezniak | aleksandar savkov | nils hammerla", "word embedding-based similarity measures are currently among the top-performing methods on unsupervised semantic textual similarity (sts) tasks. recent work has increasingly adopted a statistical view on these embeddings, with some of the top approaches being essentially various correlations (which include the famous cosine similarity). another excellent candidate for a similarity measure is mutual information (mi), which can capture arbitrary dependencies between the variables and has a simple and intuitive expression. unfortunately, its use in the context of dense word embeddings has so far been avoided due to difficulties with estimating mi for continuous data. in this work we go through a vast literature on estimating mi in such cases and single out the most promising methods, yielding a simple and elegant similarity measure for word embeddings. we show that mutual information is a viable alternative to correlations, gives an excellent signal that correlates well with human judgements of similarity and rivals existing state-of-the-art unsupervised methods."], "semantics"], [["neural response generation with meta-words", "can xu | wei wu | chongyang tao | huang hu | matt schuerman | ying wang", "we present open domain dialogue generation with meta-words. a meta-word is a structured record that describes attributes of a response, and thus allows us to explicitly model the one-to-many relationship within open domain dialogues and perform response generation in an explainable and controllable manner. to incorporate meta-words into generation, we propose a novel goal-tracking memory network that formalizes meta-word expression as a goal in response generation and manages the generation process to achieve the goal with a state memory panel and a state controller. experimental results from both automatic evaluation and human judgment on two large-scale data sets indicate that our model can significantly outperform state-of-the-art generation models in terms of response relevance, response diversity, and accuracy of meta-word expression."], "dialogue and interactive systems"], [["comet: commonsense transformers for automatic knowledge graph construction", "antoine bosselut | hannah rashkin | maarten sap | chaitanya malaviya | asli celikyilmaz | yejin choi", "we present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: atomic (sap et al., 2019) and conceptnet (speer et al., 2017). contrary to many conventional kbs that store knowledge with canonical templates, commonsense kbs only store loosely structured open-text descriptions of knowledge. we posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose commonsense transformers (comet) that learn to generate rich and diverse commonsense descriptions in natural language. despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. empirical results demonstrate that comet is able to generate novel knowledge that humans rate as high quality, with up to 77.5% (atomic) and 91.7% (conceptnet) precision at top 1, which approaches human performance for these resources. our findings suggest that using generative commonsense models for automatic commonsense kb completion could soon be a plausible alternative to extractive methods."], "semantics"], [["quantity tagger: a latent-variable sequence labeling approach to solving addition-subtraction word problems", "yanyan zou | wei lu", "an arithmetic word problem typically includes a textual description containing several constant quantities. the key to solving the problem is to reveal the underlying mathematical relations (such as addition and subtraction) among quantities, and then generate equations to find solutions. this work presents a novel approach, quantity tagger, that automatically discovers such hidden relations by tagging each quantity with a sign corresponding to one type of mathematical operation. for each quantity, we assume there exists a latent, variable-sized quantity span surrounding the quantity token in the text, which conveys information useful for determining its sign. empirical results show that our method achieves 5 and 8 points of accuracy gains on two datasets respectively, compared to prior approaches."], "information extraction, retrieval and text mining"], [["fast and accurate deep bidirectional language representations for unsupervised learning", "joongbo shin | yoonhyung lee | seunghyun yoon | kyomin jung", "even though bert has achieved successful performance improvements in various supervised learning tasks, bert is still limited by repetitive inferences on unsupervised tasks for the computation of contextual language representations. to resolve this limitation, we propose a novel deep bidirectional language model called a transformer-based text autoencoder (t-ta). the t-ta computes contextual language representations without repetition and displays the benefits of a deep bidirectional architecture, such as that of bert. in computation time experiments in a cpu environment, the proposed t-ta performs over six times faster than the bert-like model on a reranking task and twelve times faster on a semantic similarity task. furthermore, the t-ta shows competitive or even better accuracies than those of bert on the above tasks. code is available at https://github.com/joongbo/tta."], "nlp applications"], [["exploiting invertible decoders for unsupervised sentence representation learning", "shuai tang | virginia r. de sa", "encoder-decoder models for unsupervised sentence representation learning using the distributional hypothesis effectively constrain the learnt representation of a sentence to only that needed to reproduce the next sentence. while the decoder is important to constrain the representation, these models tend to discard the decoder after training since only the encoder is needed to map the input sentence into a vector representation. however, parameters learnt in the decoder also contain useful information about the language. in order to utilise the decoder after learning, we present two types of decoding functions whose inverse can be easily derived without expensive inverse calculation. therefore, the inverse of the decoding function serves as another encoder that produces sentence representations. we show that, with careful design of the decoding functions, the model learns good sentence representations, and the ensemble of the representations produced from the encoder and the inverse of the decoder demonstrate even better generalisation ability and solid transferability."], "machine learning for nlp"], [["improving massively multilingual neural machine translation and zero-shot translation", "biao zhang | philip williams | ivan titov | rico sennrich", "massively multilingual models for neural machine translation (nmt) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations. in this paper, we explore ways to improve them. we argue that multilingual nmt requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening nmt architectures. we identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs. experiments on opus-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both one-to-many and many-to-many settings, and improves zero-shot performance by ~10 bleu, approaching conventional pivot-based methods."], "machine translation and multilinguality"], [["triggerner: learning with entity triggers as explanations for named entity recognition", "bill yuchen lin | dong-ho lee | ming shen | ryan moreno | xiao huang | prashant shiralkar | xiang ren", "training neural models for named entity recognition (ner) in a new domain often requires additional human annotations (e.g., tens of thousands of labeled instances) that are usually expensive and time-consuming to collect. thus, a crucial research question is how to obtain supervision in a cost-effective way. in this paper, we introduce \u201centity triggers,\u201d an effective proxy of human explanations for facilitating label-efficient learning of ner models. an entity trigger is defined as a group of words in a sentence that helps to explain why humans would recognize an entity in the sentence. we crowd-sourced 14k entity triggers for two well-studied ner datasets. our proposed model, trigger matching network, jointly learns trigger representations and soft matching module with self-attention such that can generalize to unseen sentences easily for tagging. our framework is significantly more cost-effective than the traditional neural ner frameworks. experiments show that using only 20% of the trigger-annotated sentences results in a comparable performance as using 70% of conventional annotated sentences."], "information extraction, retrieval and text mining"], [["generating diverse translations with sentence codes", "raphael shu | hideki nakayama | kyunghyun cho", "users of machine translation systems may desire to obtain multiple candidates translated in different ways. in this work, we attempt to obtain diverse translations by using sentence codes to condition the sentence generation. we describe two methods to extract the codes, either with or without the help of syntax information. for diverse generation, we sample multiple candidates, each of which conditioned on a unique code. experiments show that the sampled translations have much higher diversity scores when using reasonable sentence codes, where the translation quality is still on par with the baselines even under strong constraint imposed by the codes. in qualitative analysis, we show that our method is able to generate paraphrase translations with drastically different structures. the proposed approach can be easily adopted to existing translation systems as no modification to the model is required."], "machine translation and multilinguality"], [["towards generating long and coherent text with multi-level latent variable models", "dinghan shen | asli celikyilmaz | yizhe zhang | liqun chen | xin wang | jianfeng gao | lawrence carin", "variational autoencoders (vaes) have received much attention recently as an end-to-end architecture for text generation with latent variables. however, previous works typically focus on synthesizing relatively short sentences (up to 20 words), and the posterior collapse issue has been widely identified in text-vaes. in this paper, we propose to leverage several multi-level structures to learn a vae model for generating long, and coherent text. in particular, a hierarchy of stochastic layers between the encoder and decoder networks is employed to abstract more informative and semantic-rich latent codes. besides, we utilize a multi-level decoder structure to capture the coherent long-term structure inherent in long-form texts, by generating intermediate sentence representations as high-level plan vectors. extensive experimental results demonstrate that the proposed multi-level vae model produces more coherent and less repetitive long text compared to baselines as well as can mitigate the posterior-collapse issue."], "generation"], [["influence paths for characterizing subject-verb number agreement in lstm language models", "kaiji lu | piotr mardziel | klas leino | matt fredrikson | anupam datta", "lstm-based recurrent neural networks are the state-of-the-art for many natural language processing (nlp) tasks. despite their performance, it is unclear whether, or how, lstms learn structural features of natural languages such as subject-verb number agreement in english. lacking this understanding, the generality of lstm performance on this task and their suitability for related tasks remains uncertain. further, errors cannot be properly attributed to a lack of structural capability, training data omissions, or other exceptional faults. we introduce *influence paths*, a causal account of structural properties as carried by paths across gates and neurons of a recurrent neural network. the approach refines the notion of influence (the subject\u2019s grammatical number has influence on the grammatical number of the subsequent verb) into a set of gate or neuron-level paths. the set localizes and segments the concept (e.g., subject-verb agreement), its constituent elements (e.g., the subject), and related or interfering elements (e.g., attractors). we exemplify the methodology on a widely-studied multi-layer lstm language model, demonstrating its accounting for subject-verb number agreement. the results offer both a finer and a more complete view of an lstm\u2019s handling of this structural aspect of the english language than prior results based on diagnostic classifiers and ablation."], "interpretability and analysis of models for nlp"], [["how to best use syntax in semantic role labelling", "yufei wang | mark johnson | stephen wan | yifang sun | wei wang", "there are many different ways in which external information might be used in a nlp task. this paper investigates how external syntactic information can be used most effectively in the semantic role labeling (srl) task. we evaluate three different ways of encoding syntactic parses and three different ways of injecting them into a state-of-the-art neural elmo-based srl sequence labelling model. we show that using a constituency representation as input features improves performance the most, achieving a new state-of-the-art for non-ensemble srl models on the in-domain conll\u201905 and conll\u201912 benchmarks."], "tagging, chunking, syntax and parsing"], [["composing elementary discourse units in abstractive summarization", "zhenwen li | wenhao wu | sujian li", "in this paper, we argue that elementary discourse unit (edu) is a more appropriate textual unit of content selection than the sentence unit in abstractive summarization. to well handle the problem of composing edus into an informative and fluent summary, we propose a novel summarization method that first designs an edu selection model to extract and group informative edus and then an edu fusion model to fuse the edus in each group into one sentence. we also design the reinforcement learning mechanism to use edu fusion results to reward the edu selection action, boosting the final summarization performance. experiments on cnn/daily mail have demonstrated the effectiveness of our model."], "summarization"], [["hierarchical modeling for user personality prediction: the role of message-level attention", "veronica lynn | niranjan balasubramanian | h. andrew schwartz", "not all documents are equally important. language processing is increasingly finding use as a supplement for questionnaires to assess psychological attributes of consenting individuals, but most approaches neglect to consider whether all documents of an individual are equally informative. in this paper, we present a novel model that uses message-level attention to learn the relative weight of users\u2019 social media posts for assessing their five factor personality traits. we demonstrate that models with message-level attention outperform those with word-level attention, and ultimately yield state-of-the-art accuracies for all five traits by using both word and message attention in combination with past approaches (an average increase in pearson r of 2.5%). in addition, examination of the high-signal posts identified by our model provides insight into the relationship between language and personality, helping to inform future work."], "computational social science, social media and cultural analytics"], [["quantifying and avoiding unfair qualification labour in crowdsourcing", "jonathan k. kummerfeld", "extensive work has argued in favour of paying crowd workers a wage that is at least equivalent to the u.s. federal minimum wage. meanwhile, research on collecting high quality annotations suggests using a qualification that requires workers to have previously completed a certain number of tasks. if most requesters who pay fairly require workers to have completed a large number of tasks already then workers need to complete a substantial amount of poorly paid work before they can earn a fair wage. through analysis of worker discussions and guidance for researchers, we estimate that workers spend approximately 2.25 months of full time effort on poorly paid tasks in order to get the qualifications needed for better paid tasks. we discuss alternatives to this qualification and conduct a study of the correlation between qualifications and work quality on two nlp tasks. we find that it is possible to reduce the burden on workers while still collecting high quality data."], "ethics in nlp"], [["grounded conversation generation as guided traverses in commonsense knowledge graphs", "houyu zhang | zhenghao liu | chenyan xiong | zhiyuan liu", "human conversations naturally evolve around related concepts and hop to distant concepts. this paper presents a new conversation generation model, conceptflow, which leverages commonsense knowledge graphs to explicitly model conversation flows. by grounding conversations to the concept space, conceptflow represents the potential conversation flow as traverses in the concept space along commonsense relations. the traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the concept space, in order to generate more semantic and informative responses. experiments on reddit conversations demonstrate conceptflow\u2019s effectiveness over previous knowledge-aware conversation models and gpt-2 based models while using 70% fewer parameters, confirming the advantage of explicit modeling conversation structures. all source codes of this work are available at https://github.com/thunlp/conceptflow."], "dialogue and interactive systems"], [["manipulating the difficulty of c-tests", "ji-ung lee | erik schwan | christian m. meyer", "we propose two novel manipulation strategies for increasing and decreasing the difficulty of c-tests automatically. this is a crucial step towards generating learner-adaptive exercises for self-directed language learning and preparing language assessment tests. to reach the desired difficulty level, we manipulate the size and the distribution of gaps based on absolute and relative gap difficulty predictions. we evaluate our approach in corpus-based experiments and in a user study with 60 participants. we find that both strategies are able to generate c-tests with the desired difficulty level."], "nlp applications"], [["non-linear instance-based cross-lingual mapping for non-isomorphic embedding spaces", "goran glava\u0161 | ivan vuli\u0107", "we present instamap, an instance-based method for learning projection-based cross-lingual word embeddings. unlike prior work, it deviates from learning a single global linear projection. instamap is a non-parametric model that learns a non-linear projection by iteratively: (1) finding a globally optimal rotation of the source embedding space relying on the kabsch algorithm, and then (2) moving each point along an instance-specific translation vector estimated from the translation vectors of the point\u2019s nearest neighbours in the training dictionary. we report performance gains with instamap over four representative state-of-the-art projection-based models on bilingual lexicon induction across a set of 28 diverse language pairs. we note prominent improvements, especially for more distant language pairs (i.e., languages with non-isomorphic monolingual spaces)."], "semantics"], [["bilingual lexicon induction with semi-supervision in non-isometric embedding spaces", "barun patra | joel ruben antony moniz | sarthak garg | matthew r. gormley | graham neubig", "recent work on bilingual lexicon induction (bli) has frequently depended either on aligned bilingual lexicons or on distribution matching, often with an assumption about the isometry of the two spaces. we propose a technique to quantitatively estimate this assumption of the isometry between two embedding spaces and empirically show that this assumption weakens as the languages in question become increasingly etymologically distant. we then propose bilingual lexicon induction with semi-supervision (bliss) \u2014 a semi-supervised approach that relaxes the isometric assumption while leveraging both limited aligned bilingual lexicons and a larger set of unaligned word embeddings, as well as a novel hubness filtering technique. our proposed method obtains state of the art results on 15 of 18 language pairs on the muse dataset, and does particularly well when the embedding spaces don\u2019t appear to be isometric. in addition, we also show that adding supervision stabilizes the learning procedure, and is effective even with minimal supervision."], "machine translation and multilinguality"], [["a relaxed matching procedure for unsupervised bli", "xu zhao | zihao wang | yong zhang | hao wu", "recently unsupervised bilingual lexicon induction(bli) without any parallel corpus has attracted much research interest. one of the crucial parts in methods for the bli task is the matching procedure. previous works impose a too strong constraint on the matching and lead to many counterintuitive translation pairings. thus we propose a relaxed matching procedure to find a more precise matching between two languages. we also find that aligning source and target language embedding space bidirectionally will bring significant improvement. we follow the previous iterative framework to conduct experiments. results on standard benchmark demonstrate the effectiveness of our proposed method, which substantially outperforms previous unsupervised methods."], "machine translation and multilinguality"], [["infotabs: inference on tables as semi-structured data", "vivek gupta | maitrey mehta | pegah nokhiz | vivek srikumar", "in this paper, we observe that semi-structured tabulated text is ubiquitous; understanding them requires not only comprehending the meaning of text fragments, but also implicit relationships between them. we argue that such data can prove as a testing ground for understanding how we reason about information. to study this, we introduce a new dataset called infotabs, comprising of human-written textual hypotheses based on premises that are tables extracted from wikipedia info-boxes. our analysis shows that the semi-structured, multi-domain and heterogeneous nature of the premises admits complex, multi-faceted reasoning. experiments reveal that, while human annotators agree on the relationships between a table-hypothesis pair, several standard modeling strategies are unsuccessful at the task, suggesting that reasoning about tables can pose a difficult modeling challenge."], "semantics"], [["bipartite flat-graph network for nested named entity recognition", "ying luo | hai zhao", "in this paper, we propose a novel bipartite flat-graph network (biflag) for nested named entity recognition (ner), which contains two subgraph modules: a flat ner module for outermost entities and a graph module for all the entities located in inner layers. bidirectional lstm (bilstm) and graph convolutional network (gcn) are adopted to jointly learn flat entities and their inner dependencies. different from previous models, which only consider the unidirectional delivery of information from innermost layers to outer ones (or outside-to-inside), our model effectively captures the bidirectional interaction between them. we first use the entities recognized by the flat ner module to construct an entity graph, which is fed to the next graph module. the richer representation learned from graph module carries the dependencies of inner entities and can be exploited to improve outermost entity predictions. experimental results on three standard nested ner datasets demonstrate that our biflag outperforms previous state-of-the-art models."], "information extraction, retrieval and text mining"], [["breaking through the 80% glass ceiling: raising the state of the art in word sense disambiguation by incorporating knowledge graph information", "michele bevilacqua | roberto navigli", "neural architectures are the current state of the art in word sense disambiguation (wsd). however, they make limited use of the vast amount of relational information encoded in lexical knowledge bases (lkb). we present enhanced wsd integrating synset embeddings and relations (ewiser), a neural supervised architecture that is able to tap into this wealth of knowledge by embedding information from the lkb graph within the neural architecture, and to exploit pretrained synset embeddings, enabling the network to predict synsets that are not in the training set. as a result, we set a new state of the art on almost all the evaluation settings considered, also breaking through, for the first time, the 80% ceiling on the concatenation of all the standard all-words english wsd evaluation benchmarks. on multilingual all-words wsd, we report state-of-the-art results by training on nothing but english."], "semantics"], [["joint type inference on entities and relations via graph convolutional networks", "changzhi sun | yeyun gong | yuanbin wu | ming gong | daxin jiang | man lan | shiliang sun | nan duan", "we develop a new paradigm for the task of joint entity relation extraction. it first identifies entity spans, then performs a joint inference on entity types and relation types. to tackle the joint type inference task, we propose a novel graph convolutional network (gcn) running on an entity-relation bipartite graph. by introducing a binary relation classification task, we are able to utilize the structure of entity-relation bipartite graph in a more efficient and interpretable way. experiments on ace05 show that our model outperforms existing joint models in entity performance and is competitive with the state-of-the-art in relation performance."], "information extraction, retrieval and text mining"], [["pretraining methods for dialog context representation learning", "shikib mehri | evgeniia razumovskaia | tiancheng zhao | maxine eskenazi", "this paper examines various unsupervised pretraining objectives for learning dialog context representations. two novel methods of pretraining dialog context encoders are proposed, and a total of four methods are examined. each pretraining objective is fine-tuned and evaluated on a set of downstream dialog tasks using the multiwoz dataset and strong performance improvement is observed. further evaluation shows that our pretraining objectives result in not only better performance, but also better convergence, models that are less data hungry and have better domain generalizability."], "dialogue and interactive systems"], [["neural text simplification of clinical letters with a domain specific phrase table", "matthew shardlow | raheel nawaz", "clinical letters are infamously impenetrable for the lay patient. this work uses neural text simplification methods to automatically improve the understandability of clinical letters for patients. we take existing neural text simplification software and augment it with a new phrase table that links complex medical terminology to simpler vocabulary by mining snomed-ct. in an evaluation task using crowdsourcing, we show that the results of our new system are ranked easier to understand (average rank 1.93) than using the original system (2.34) without our phrase table. we also show improvement against baselines including the original text (2.79) and using the phrase table without the neural text simplification software (2.94). our methods can easily be transferred outside of the clinical domain by using domain-appropriate resources to provide effective neural text simplification for any domain without the need for costly annotation."], "nlp applications"], [["using human attention to extract keyphrase from microblog post", "yingyi zhang | chengzhi zhang", "this paper studies automatic keyphrase extraction on social media. previous works have achieved promising results on it, but they neglect human reading behavior during keyphrase annotating. the human attention is a crucial element of human reading behavior. it reveals the relevance of words to the main topics of the target text. thus, this paper aims to integrate human attention into keyphrase extraction models. first, human attention is represented by the reading duration estimated from eye-tracking corpus. then, we merge human attention with neural network models by an attention mechanism. in addition, we also integrate human attention into unsupervised models. to the best of our knowledge, we are the first to utilize human attention on keyphrase extraction tasks. the experimental results show that our models have significant improvements on two twitter datasets."], "information extraction, retrieval and text mining"], [["usr: an unsupervised and reference free evaluation metric for dialog generation", "shikib mehri | maxine eskenazi", "the lack of meaningful automatic evaluation metrics for dialog has impeded open-domain dialog research. standard language generation metrics have been shown to be ineffective for evaluating dialog models. to this end, this paper presents usr, an unsupervised and reference-free evaluation metric for dialog. usr is a reference-free metric that trains unsupervised models to measure several desirable qualities of dialog. usr is shown to strongly correlate with human judgment on both topical-chat (turn-level: 0.42, system-level: 1.0) and personachat (turn-level: 0.48 and system-level: 1.0). usr additionally produces interpretable measures for several desirable properties of dialog."], "dialogue and interactive systems"], [["r4c: a benchmark for evaluating rc systems to get the right answer for the right reason", "naoya inoue | pontus stenetorp | kentaro inui", "recent studies have revealed that reading comprehension (rc) systems learn to exploit annotation artifacts and other biases in current datasets. this prevents the community from reliably measuring the progress of rc systems. to address this issue, we introduce r4c, a new task for evaluating rc systems\u2019 internal reasoning. r4c requires giving not only answers but also derivations: explanations that justify predicted answers. we present a reliable, crowdsourced framework for scalably annotating rc datasets with derivations. we create and publicly release the r4c dataset, the first, quality-assured dataset consisting of 4.6k questions, each of which is annotated with 3 reference derivations (i.e. 13.8k derivations). experiments show that our automatic evaluation metrics using multiple reference derivations are reliable, and that r4c assesses different skills from an existing benchmark."], "question answering"], [["mutual: a dataset for multi-turn dialogue reasoning", "leyang cui | yu wu | shujie liu | yue zhang | ming zhou", "non-task oriented dialogue systems have achieved great success in recent years due to largely accessible conversation data and the development of deep learning techniques. given a context, current systems are able to yield a relevant and fluent response, but sometimes make logical mistakes because of weak reasoning capabilities. to facilitate the conversation reasoning research, we introduce mutual, a novel dataset for multi-turn dialogue reasoning, consisting of 8,860 manually annotated dialogues based on chinese student english listening comprehension exams. compared to previous benchmarks for non-task oriented dialogue systems, mutual is much more challenging since it requires a model that be able to handle various reasoning problems. empirical results show that state-of-the-art methods only reach 71%, which is far behind human performance of 94%, indicating that there is ample room for improving reasoning ability."], "dialogue and interactive systems"], [["ensuring readability and data-fidelity using head-modifier templates in deep type description generation", "jiangjie chen | ao wang | haiyun jiang | suo feng | chenguang li | yanghua xiao", "a type description is a succinct noun compound which helps human and machines to quickly grasp the informative and distinctive information of an entity. entities in most knowledge graphs (kgs) still lack such descriptions, thus calling for automatic methods to supplement such information. however, existing generative methods either overlook the grammatical structure or make factual mistakes in generated texts. to solve these problems, we propose a head-modifier template based method to ensure the readability and data fidelity of generated type descriptions. we also propose a new dataset and two metrics for this task. experiments show that our method improves substantially compared with baselines and achieves state-of-the-art performance on both datasets."], "generation"], [["a joint model for document segmentation and segment labeling", "joe barrow | rajiv jain | vlad morariu | varun manjunatha | douglas oard | philip resnik", "text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. we introduce segment pooling lstm (s-lstm), which is capable of jointly segmenting a document and labeling segments. in support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. we show that s-lstm reduces segmentation error by 30% on average, while also improving segment labeling."], "information extraction, retrieval and text mining"], [["supert: towards new frontiers in unsupervised evaluation metrics for multi-document summarization", "yang gao | wei zhao | steffen eger", "we study unsupervised multi-document summarization evaluation metrics, which require neither human-written reference summaries nor human annotations (e.g. preferences, ratings, etc.). we propose supert, which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary, i.e. selected salient sentences from the source documents, using contextualized embeddings and soft token alignment techniques. compared to the state-of-the-art unsupervised evaluation metrics, supert correlates better with human ratings by 18- 39%. furthermore, we use supert as rewards to guide a neural-based reinforcement learning summarizer, yielding favorable performance compared to the state-of-the-art unsupervised summarizers. all source code is available at https://github.com/yg211/acl20-ref-free-eval."], "summarization"], [["discrete latent variable representations for low-resource text classification", "shuning jin | sam wiseman | karl stratos | karen livescu", "while much work on deep latent variable models of text uses continuous latent variables, discrete latent variables are interesting because they are more interpretable and typically more space efficient. we consider several approaches to learning discrete latent variable models for text in the case where exact marginalization over these variables is intractable. we compare the performance of the learned representations as features for low-resource document and sentence classification. our best models outperform the previous best reported results with continuous representations in these low-resource settings, while learning significantly more compressed representations. interestingly, we find that an amortized variant of hard em performs particularly well in the lowest-resource regimes."], "machine learning for nlp"], [["improving disfluency detection by self-training a self-attentive model", "paria jamshid lou | mark johnson", "self-attentive neural syntactic parsers using contextualized word embeddings (e.g. elmo or bert) currently produce state-of-the-art results in joint parsing and disfluency detection in speech transcripts. since the contextualized word embeddings are pre-trained on a large amount of unlabeled data, using additional unlabeled data to train a neural model might seem redundant. however, we show that self-training \u2014 a semi-supervised technique for incorporating unlabeled data \u2014 sets a new state-of-the-art for the self-attentive parser on disfluency detection, demonstrating that self-training provides benefits orthogonal to the pre-trained contextualized word representations. we also show that ensembling self-trained parsers provides further gains for disfluency detection."], "speech and multimodality"], [["structure-level knowledge distillation for multilingual sequence labeling", "xinyu wang | yong jiang | nguyen bach | tao wang | fei huang | kewei tu", "multilingual sequence labeling is a task of predicting label sequences using a single unified model for multiple languages. compared with relying on multiple monolingual models, using a multilingual model has the benefit of a smaller model size, easier in online serving, and generalizability to low-resource languages. however, current multilingual models still underperform individual monolingual models significantly due to model capacity limitations. in this paper, we propose to reduce the gap between monolingual models and the unified multilingual model by distilling the structural knowledge of several monolingual models (teachers) to the unified multilingual model (student). we propose two novel kd methods based on structure-level information: (1) approximately minimizes the distance between the student\u2019s and the teachers\u2019 structure-level probability distributions, (2) aggregates the structure-level knowledge to local distributions and minimizes the distance between two local probability distributions. our experiments on 4 multilingual tasks with 25 datasets show that our approaches outperform several strong baselines and have stronger zero-shot generalizability than both the baseline model and teacher models."], "tagging, chunking, syntax and parsing"], [["learning spoken language representations with neural lattice language modeling", "chao-wei huang | yun-nung chen", "pre-trained language models have achieved huge improvement on many nlp tasks. however, these methods are usually designed for written text, so they do not consider the properties of spoken language. therefore, this paper aims at generalizing the idea of language model pre-training to lattices generated by recognition systems. we propose a framework that trains neural lattice language models to provide contextualized representations for spoken language understanding tasks. the proposed two-stage pre-training approach reduces the demands of speech data and has better efficiency. experiments on intent detection and dialogue act recognition datasets demonstrate that our proposed method consistently outperforms strong baselines when evaluated on spoken inputs. the code is available at https://github.com/miulab/lattice-elmo."], "speech and multimodality"], [["towards empathetic open-domain conversation models: a new benchmark and dataset", "hannah rashkin | eric michael smith | margaret li | y-lan boureau", "one challenge for dialogue agents is recognizing feelings in the conversation partner and replying accordingly, a key communicative skill. while it is straightforward for humans to recognize and acknowledge others\u2019 feelings in a conversation, this is a significant challenge for ai systems due to the paucity of suitable publicly-available datasets for training and evaluation. this work proposes a new benchmark for empathetic dialogue generation and empatheticdialogues, a novel dataset of 25k conversations grounded in emotional situations. our experiments indicate that dialogue models that use our dataset are perceived to be more empathetic by human evaluators, compared to models merely trained on large-scale internet conversation data. we also present empirical comparisons of dialogue model adaptations for empathetic responding, leveraging existing models or datasets without requiring lengthy re-training of the full model."], "dialogue and interactive systems"], [["on the distribution of deep clausal embeddings: a large cross-linguistic study", "damian blasi | ryan cotterell | lawrence wolf-sonkin | sabine stoll | balthasar bickel | marco baroni", "embedding a clause inside another (\u201cthe girl [who likes cars [that run fast]] has arrived\u201d) is a fundamental resource that has been argued to be a key driver of linguistic expressiveness. as such, it plays a central role in fundamental debates on what makes human language unique, and how they might have evolved. empirical evidence on the prevalence and the limits of embeddings has however been based on either laboratory setups or corpus data of relatively limited size. we introduce here a collection of large, dependency-parsed written corpora in 17 languages, that allow us, for the first time, to capture clausal embedding through dependency graphs and assess their distribution. our results indicate that there is no evidence for hard constraints on embedding depth: the tail of depth distributions is heavy. moreover, although deeply embedded clauses tend to be shorter, suggesting processing load issues, complex sentences with many embeddings do not display a bias towards less deep embeddings. taken together, the results suggest that deep embeddings are not disfavoured in written language. more generally, our study illustrates how resources and methods from latest-generation big-data nlp can provide new perspectives on fundamental questions in theoretical linguistics."], "linguistic theories, cognitive modeling and psycholinguistics"], [["treebank embedding vectors for out-of-domain dependency parsing", "joachim wagner | james barry | jennifer foster", "a recent advance in monolingual dependency parsing is the idea of a treebank embedding vector, which allows all treebanks for a particular language to be used as training data while at the same time allowing the model to prefer training data from one treebank over others and to select the preferred treebank at test time. we build on this idea by 1) introducing a method to predict a treebank vector for sentences that do not come from a treebank used in training, and 2) exploring what happens when we move away from predefined treebank embedding vectors during test time and instead devise tailored interpolations. we show that 1) there are interpolated vectors that are superior to the predefined ones, and 2) treebank vectors can be predicted with sufficient accuracy, for nine out of ten test languages, to match the performance of an oracle approach that knows the most suitable predefined treebank embedding for the test set."], "tagging, chunking, syntax and parsing"], [["topic tensor network for implicit discourse relation recognition in chinese", "sheng xu | peifeng li | fang kong | qiaoming zhu | guodong zhou", "in the literature, most of the previous studies on english implicit discourse relation recognition only use sentence-level representations, which cannot provide enough semantic information in chinese due to its unique paratactic characteristics. in this paper, we propose a topic tensor network to recognize chinese implicit discourse relations with both sentence-level and topic-level representations. in particular, besides encoding arguments (discourse units) using a gated convolutional network to obtain sentence-level representations, we train a simplified topic model to infer the latent topic-level representations. moreover, we feed the two pairs of representations to two factored tensor networks, respectively, to capture both the sentence-level interactions and topic-level relevance using multi-slice tensors. experimentation on cdtb, a chinese discourse corpus, shows that our proposed model significantly outperforms several state-of-the-art baselines in both micro and macro f1-scores."], "discourse and pragmatics"], [["semantic expressive capacity with bounded memory", "antoine venant | alexander koller", "we investigate the capacity of mechanisms for compositional semantic parsing to describe relations between sentences and semantic representations. we prove that in order to represent certain relations, mechanisms which are syntactically projective must be able to remember an unbounded number of locations in the semantic representations, where nonprojective mechanisms need not. this is the first result of this kind, and has consequences both for grammar-based and for neural systems."], "semantics"], [["orthogonal relation transforms with graph context modeling for knowledge graph embedding", "yun tang | jing huang | guangtao wang | xiaodong he | bowen zhou", "distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from transe to the latest state-of-the-art rotate. however, complex relations such as n-to-1, 1-to-n and n-to-n still remain challenging to predict. in this work, we propose a novel distance-based approach for knowledge graph link prediction. first, we extend the rotate from 2d complex domain to high dimensional space with orthogonal transforms to model relations. the orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. second, the graph context is integrated into distance scoring functions directly. specifically, graph context is explicitly modeled via two directed context representations. each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. the proposed approach improves prediction accuracy on the difficult n-to-1, 1-to-n and n-to-n cases. our experimental results show that it achieves state-of-the-art results on two common benchmarks fb15k-237 and wnrr-18, especially on fb15k-237 which has many high in-degree nodes."], "machine learning for nlp"], [["reinceptione: relation-aware inception network with joint local-global structural information for knowledge graph embedding", "zhiwen xie | guangyou zhou | jin liu | jimmy xiangji huang", "the goal of knowledge graph embedding (kge) is to learn how to represent the low dimensional vectors for entities and relations based on the observed triples. the conventional shallow models are limited to their expressiveness. conve (dettmers et al., 2018) takes advantage of cnn and improves the expressive power with parameter efficient operators by increasing the interactions between head and relation embeddings. however, there is no structural information in the embedding space of conve, and the performance is still limited by the number of interactions. the recent kbgat (nathani et al., 2019) provides another way to learn embeddings by adaptively utilizing structural information. in this paper, we take the benefits of conve and kbgat together and propose a relation-aware inception network with joint local-global structural information for knowledge graph embedding (reinceptione). specifically, we first explore the inception network to learn query embedding, which aims to further increase the interactions between head and relation embeddings. then, we propose to use a relation-aware attention mechanism to enrich the query embedding with the local neighborhood and global entity information. experimental results on both wn18rr and fb15k-237 datasets demonstrate that reinceptione achieves competitive performance compared with state-of-the-art methods."], "information extraction, retrieval and text mining"], [["towards faithfully interpretable nlp systems: how should we define and evaluate faithfulness?", "alon jacovi | yoav goldberg", "with the growing popularity of deep-learning based nlp models, comes a need for interpretable systems. but what is interpretability, and what constitutes a high-quality interpretation? in this opinion piece we reflect on the current state of interpretability evaluation research. we call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. we survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is \u201cdefined\u201d by the community. we provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. we call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility."], "interpretability and analysis of models for nlp"], [["are training samples correlated? learning to generate dialogue responses with multiple references", "lisong qiu | juntao li | wei bi | dongyan zhao | rui yan", "due to its potential applications, open-domain dialogue generation has become popular and achieved remarkable progress in recent years, but sometimes suffers from generic responses. previous models are generally trained based on 1-to-1 mapping from an input query to its response, which actually ignores the nature of 1-to-n mapping in dialogue that there may exist multiple valid responses corresponding to the same query. in this paper, we propose to utilize the multiple references by considering the correlation of different valid responses and modeling the 1-to-n mapping with a novel two-step generation architecture. the first generation phase extracts the common features of different responses which, combined with distinctive features obtained in the second phase, can generate multiple diverse and appropriate responses. experimental results show that our proposed model can effectively improve the quality of response and outperform existing neural dialogue models on both automatic and human evaluations."], "dialogue and interactive systems"], [["task refinement learning for improved accuracy and stability of unsupervised domain adaptation", "yftah ziser | roi reichart", "pivot based language modeling (pblm) (ziser and reichart, 2018a), combining lstms with pivot-based methods, has yielded significant progress in unsupervised domain adaptation. however, this approach is still challenged by the large pivot detection problem that should be solved, and by the inherent instability of lstms. in this paper we propose a task refinement learning (trl) approach, in order to solve these problems. our algorithms iteratively train the pblm model, gradually increasing the information exposed about each pivot. trl-pblm achieves stateof- the-art accuracy in six domain adaptation setups for sentiment classification. moreover, it is much more stable than plain pblm across model configurations, making the model much better fitted for practical use."], "machine learning for nlp"], [["conditional augmentation for aspect term extraction via masked sequence-to-sequence generation", "kun li | chengbo chen | xiaojun quan | qing ling | yan song", "aspect term extraction aims to extract aspect terms from review texts as opinion targets for sentiment analysis. one of the big challenges with this task is the lack of sufficient annotated data. while data augmentation is potentially an effective technique to address the above issue, it is uncontrollable as it may change aspect words and aspect labels unexpectedly. in this paper, we formulate the data augmentation as a conditional generation task: generating a new sentence while preserving the original opinion targets and labels. we propose a masked sequence-to-sequence method for conditional augmentation of aspect term extraction. unlike existing augmentation approaches, ours is controllable and allows to generate more diversified sentences. experimental results confirm that our method alleviates the data scarcity problem significantly. it also effectively boosts the performances of several current models for aspect term extraction."], "sentiment analysis, stylistic analysis, and argument mining"], [["yes, we can! mining arguments in 50 years of us presidential campaign debates", "shohreh haddadan | elena cabrio | serena villata", "political debates offer a rare opportunity for citizens to compare the candidates\u2019 positions on the most controversial topics of the campaign. thus they represent a natural application scenario for argument mining. as existing research lacks solid empirical investigation of the typology of argument components in political debates, we fill this gap by proposing an argument mining approach to political debates. we address this task in an empirical manner by annotating 39 political debates from the last 50 years of us presidential campaigns, creating a new corpus of 29k argument components, labeled as premises and claims. we then propose two tasks: (1) identifying the argumentative components in such debates, and (2) classifying them as premises and claims. we show that feature-rich svm learners and neural network architectures outperform standard baselines in argument mining over such complex data. we release the new corpus uselecdeb60to16 and the accompanying software under free licenses to the research community."], "sentiment analysis, stylistic analysis, and argument mining"], [["scaling up open tagging from tens to thousands: comprehension empowered attribute value extraction from product title", "huimin xu | wenting wang | xin mao | xinyu jiang | man lan", "supplementing product information by extracting attribute values from title is a crucial task in e-commerce domain. previous studies treat each attribute only as an entity type and build one set of ner tags (e.g., bio) for each of them, leading to a scalability issue which unfits to the large sized attribute system in real world e-commerce. in this work, we propose a novel approach to support value extraction scaling up to thousands of attributes without losing performance: (1) we propose to regard attribute as a query and adopt only one global set of bio tags for any attributes to reduce the burden of attribute tag or model explosion; (2) we explicitly model the semantic representations for attribute and title, and develop an attention mechanism to capture the interactive semantic relations in-between to enforce our framework to be attribute comprehensive. we conduct extensive experiments in real-life datasets. the results show that our model not only outperforms existing state-of-the-art ner tagging models, but also is robust and generates promising results for up to 8,906 attributes."], "information extraction, retrieval and text mining"], [["dscorer: a fast evaluation metric for discourse representation structure parsing", "jiangming liu | shay b. cohen | mirella lapata", "discourse representation structures (drss) are scoped semantic representations for texts of arbitrary length. evaluating the accuracy of predicted drss plays a key role in developing semantic parsers and improving their performance. drss are typically visualized as boxes which are not straightforward to process automatically. counter transforms drss to clauses and measures clause overlap by searching for variable mappings between two drss. however, this metric is computationally costly (with respect to memory and cpu time) and does not scale with longer texts. we introduce dscorer, an efficient new metric which converts box-style drss to graphs and then measures the overlap of n-grams. experiments show that dscorer computes accuracy scores that are correlated with counter at a fraction of the time."], "resources and evaluation"], [["low resource sequence tagging using sentence reconstruction", "tal perl | sriram chaudhury | raja giryes", "this work revisits the task of training sequence tagging models with limited resources using transfer learning. we investigate several proposed approaches introduced in recent works and suggest a new loss that relies on sentence reconstruction from normalized embeddings. specifically, our method demonstrates how by adding a decoding layer for sentence reconstruction, we can improve the performance of various baselines. we show improved results on the conll02 ner and ud 1.2 pos datasets and demonstrate the power of the method for transfer learning with low-resources achieving 0.6 f1 score in dutch using only one sample from it."], "machine learning for nlp"], [["screenplay summarization using latent narrative structure", "pinelopi papalampidi | frank keller | lea frermann | mirella lapata", "most general-purpose extractive summarization models are trained on news articles, which are short and present all important information upfront. as a result, such models are biased on position and often perform a smart selection of sentences from the beginning of the document. when summarizing long narratives, which have complex structure and present information piecemeal, simple position heuristics are not sufficient. in this paper, we propose to explicitly incorporate the underlying structure of narratives into general unsupervised and supervised extractive summarization models. we formalize narrative structure in terms of key narrative events (turning points) and treat it as latent in order to summarize screenplays (i.e., extract an optimal sequence of scenes). experimental results on the csi corpus of tv screenplays, which we augment with scene-level summarization labels, show that latent turning points correlate with important aspects of a csi episode and improve summarization performance over general extractive algorithms leading to more complete and diverse summaries."], "summarization"], [["bridging the structural gap between encoding and decoding for data-to-text generation", "chao zhao | marilyn walker | snigdha chaturvedi", "generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text. hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task. graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult. to narrow this gap, we propose dualenc, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text. empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text."], "generation"], [["give me convenience and give her death: who should decide what uses of nlp are appropriate, and on what basis?", "kobi leins | jey han lau | timothy baldwin", "as part of growing nlp capabilities, coupled with an awareness of the ethical dimensions of research, questions have been raised about whether particular datasets and tasks should be deemed off-limits for nlp research. we examine this question with respect to a paper on automatic legal sentencing from emnlp 2019 which was a source of some debate, in asking whether the paper should have been allowed to be published, who should have been charged with making such a decision, and on what basis. we focus in particular on the role of data statements in ethically assessing research, but also discuss the topic of dual use, and examine the outcomes of similar debates in other scientific disciplines."], "ethics in nlp"], [["lattice transformer for speech translation", "pei zhang | niyu ge | boxing chen | kai fan", "recent advances in sequence modeling have highlighted the strengths of the transformer architecture, especially in achieving state-of-the-art machine translation results. however, depending on the up-stream systems, e.g., speech recognition, or word segmentation, the input to translation system can vary greatly. the goal of this work is to extend the attention mechanism of the transformer to naturally consume the lattice in addition to the traditional sequential input. we first propose a general lattice transformer for speech translation where the input is the output of the automatic speech recognition (asr) which contains multiple paths and posterior scores. to leverage the extra information from the lattice structure, we develop a novel controllable lattice attention mechanism to obtain latent representations. on the ldc spanish-english speech translation corpus, our experiments show that lattice transformer generalizes significantly better and outperforms both a transformer baseline and a lattice lstm. additionally, we validate our approach on the wmt 2017 chinese-english translation task with lattice inputs from different bpe segmentations. in this task, we also observe the improvements over strong baselines."], "language grounding to vision, robotics and beyond"], [["a relational memory-based embedding model for triple classification and search personalization", "dai quoc nguyen | tu nguyen | dinh phung", "knowledge graph embedding methods often suffer from a limitation of memorizing valid triples to predict new ones for triple classification and search personalization problems. to this end, we introduce a novel embedding model, named r-men, that explores a relational memory network to encode potential dependencies in relationship triples. r-men considers each triple as a sequence of 3 input vectors that recurrently interact with a memory using a transformer self-attention mechanism. thus r-men encodes new information from interactions between the memory and each input vector to return a corresponding vector. consequently, r-men feeds these 3 returned vectors to a convolutional neural network-based decoder to produce a scalar score for the triple. experimental results show that our proposed r-men obtains state-of-the-art results on search17 for the search personalization task, and on wn11 and fb13 for the triple classification task."], "machine learning for nlp"], [["you only need attention to traverse trees", "mahtab ahmed | muhammad rifayat samee | robert e. mercer", "in recent nlp research, a topic of interest is universal sentence encoding, sentence representations that can be used in any supervised task. at the word sequence level, fully attention-based models suffer from two problems: a quadratic increase in memory consumption with respect to the sentence length and an inability to capture and use syntactic information. recursive neural nets can extract very good syntactic information by traversing a tree structure. to this end, we propose tree transformer, a model that captures phrase level syntax for constituency trees as well as word-level dependencies for dependency trees by doing recursive traversal only with attention. evaluation of this model on four tasks gets noteworthy results compared to the standard transformer and lstm-based models as well as tree-structured lstms. ablation studies to find whether positional information is inherently encoded in the trees and which type of attention is suitable for doing the recursive traversal are provided."], "machine learning for nlp"], [["translating translationese: a two-step approach to unsupervised machine translation", "nima pourdamghani | nada aldarrab | marjan ghazvininejad | kevin knight | jonathan may", "given a rough, word-by-word gloss of a source language sentence, target language natives can uncover the latent, fully-fluent rendering of the translation. in this work we explore this intuition by breaking translation into a two step process: generating a rough gloss by means of a dictionary and then \u2018translating\u2019 the resulting pseudo-translation, or \u2018translationese\u2019 into a fully fluent translation. we build our translationese decoder once from a mish-mash of parallel data that has the target language in common and then can build dictionaries on demand using unsupervised techniques, resulting in rapidly generated unsupervised neural mt systems for many source languages. we apply this process to 14 test languages, obtaining better or comparable translation results on high-resource languages than previously published unsupervised mt studies, and obtaining good quality results for low-resource languages that have never been used in an unsupervised mt scenario."], "machine translation and multilinguality"], [["probabilistic assumptions matter: improved models for distantly-supervised document-level question answering", "hao cheng | ming-wei chang | kenton lee | kristina toutanova", "we address the problem of extractive question answering using document-level distant super-vision, pairing questions and relevant documents with answer strings. we compare previously used probability space and distant supervision assumptions (assumptions on the correspondence between the weak answer string labels and possible answer mention spans). we show that these assumptions interact, and that different configurations provide complementary benefits. we demonstrate that a multi-objective model can efficiently combine the advantages of multiple assumptions and outperform the best individual formulation. our approach outperforms previous state-of-the-art models by 4.3 points in f1 on triviaqa-wiki and 1.7 points in rouge-l on narrativeqa summaries."], "question answering"], [["large-scale transfer learning for natural language generation", "sergey golovanov | rauf kurbanov | sergey nikolenko | kyryl truskovskyi | alexander tselousov | thomas wolf", "large-scale pretrained language models define state of the art in natural language processing, achieving outstanding performance on a variety of tasks. we study how these architectures can be applied and adapted for natural language generation, comparing a number of architectural and training schemes. we focus in particular on open-domain dialog as a typical high entropy generation task, presenting and comparing different architectures for adapting pretrained models with state of the art results."], "generation"], [["on exposure bias, hallucination and domain shift in neural machine translation", "chaojun wang | rico sennrich", "the standard training algorithm in neural machine translation (nmt) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. however, the practical impact of exposure bias is under debate. in this paper, we link exposure bias to another well-known problem in nmt, namely the tendency to generate hallucinations under domain shift. in experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with minimum risk training, which avoids exposure bias, can mitigate this. our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift."], "machine translation and multilinguality"], [["personalizing dialogue agents via meta-learning", "andrea madotto | zhaojiang lin | chien-sheng wu | pascale fung", "existing personalized dialogue models use human designed persona descriptions to improve dialogue consistency. collecting such descriptions from existing dialogues is expensive and requires hand-crafted feature designs. in this paper, we propose to extend model-agnostic meta-learning (maml) (finn et al., 2017) to personalized dialogue learning without using any persona descriptions. our model learns to quickly adapt to new personas by leveraging only a few dialogue samples collected from the same user, which is fundamentally different from conditioning the response on the persona descriptions. empirical results on persona-chat dataset (zhang et al., 2018) indicate that our solution outperforms non-meta-learning baselines using automatic evaluation metrics, and in terms of human-evaluated fluency and consistency."], "dialogue and interactive systems"], [["sequence labeling parsing by learning across representations", "michalina strzyz | david vilares | carlos g\u00f3mez-rodr\u00edguez", "we use parsing as sequence labeling as a common framework to learn across constituency and dependency syntactic abstractions.to do so, we cast the problem as multitask learning (mtl). first, we show that adding a parsing paradigm as an auxiliary loss consistently improves the performance on the other paradigm. secondly, we explore an mtl sequence labeling model that parses both representations, at almost no cost in terms of performance and speed. the results across the board show that on average mtl models with auxiliary losses for constituency parsing outperform single-task ones by 1.05 f1 points, and for dependency parsing by 0.62 uas points."], "tagging, chunking, syntax and parsing"], [["evaluating robustness to input perturbations for neural machine translation", "xing niu | prashant mathur | georgiana dinu | yaser al-onaizan", "neural machine translation (nmt) models are sensitive to small perturbations in the input. robustness to such perturbations is typically measured using translation quality metrics such as bleu on the noisy input. this paper proposes additional metrics which measure the relative degradation and changes in translation when small perturbations are added to the input. we focus on a class of models employing subword regularization to address robustness and perform extensive evaluations of these models using the robustness measures proposed. results show that our proposed metrics reveal a clear trend of improved robustness to perturbations when subword regularization methods are used."], "machine translation and multilinguality"], [["hubless nearest neighbor search for bilingual lexicon induction", "jiaji huang | qiang qiu | kenneth church", "bilingual lexicon induction (bli) is the task of translating words from corpora in two languages. recent advances in bli work by aligning the two word embedding spaces. following that, a key step is to retrieve the nearest neighbor (nn) in the target space given the source word. however, a phenomenon called hubness often degrades the accuracy of nn. hubness appears as some data points, called hubs, being extra-ordinarily close to many of the other data points. reducing hubness is necessary for retrieval tasks. one successful example is inverted softmax (isf), recently proposed to improve nn. this work proposes a new method, hubless nearest neighbor (hnn), to mitigate hubness. hnn differs from nn by imposing an additional equal preference assumption. moreover, the hnn formulation explains why isf works as well as it does. empirical results demonstrate that hnn outperforms nn, isf and other state-of-the-art. for reproducibility and follow-ups, we have published all code."], "machine learning for nlp"], [["jointly learning to align and summarize for neural cross-lingual summarization", "yue cao | hui liu | xiaojun wan", "cross-lingual summarization is the task of generating a summary in one language given a text in a different language. previous works on cross-lingual summarization mainly focus on using pipeline methods or training an end-to-end model using the translated parallel data. however, it is a big challenge for the model to directly learn cross-lingual summarization as it requires learning to understand different languages and learning how to summarize at the same time. in this paper, we propose to ease the cross-lingual summarization training by jointly learning to align and summarize. we design relevant loss functions to train this framework and propose several methods to enhance the isomorphism and cross-lingual transfer between languages. experimental results show that our model can outperform competitive models in most cases. in addition, we show that our model even has the ability to generate cross-lingual summaries without access to any cross-lingual corpus."], "summarization"], [["learning to ask unanswerable questions for machine reading comprehension", "haichao zhu | li dong | furu wei | wenhui wang | bing qin | ting liu", "machine reading comprehension with unanswerable questions is a challenging task. in this work, we propose a data augmentation technique by automatically generating relevant unanswerable questions according to an answerable question paired with its corresponding paragraph that contains the answer. we introduce a pair-to-sequence model for unanswerable question generation, which effectively captures the interactions between the question and the paragraph. we also present a way to construct training data for our question generation models by leveraging the existing reading comprehension dataset. experimental results show that the pair-to-sequence model performs consistently better compared with the sequence-to-sequence baseline. we further use the automatically generated unanswerable questions as a means of data augmentation on the squad 2.0 dataset, yielding 1.9 absolute f1 improvement with bert-base model and 1.7 absolute f1 improvement with bert-large model."], "question answering"], [["dialogue-based relation extraction", "dian yu | kai sun | claire cardie | dong yu", "we present the first human-annotated dialogue-based relation extraction (re) dataset dialogre, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. we further offer dialogre as a platform for studying cross-sentence re as most facts span multiple sentences. we argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional re tasks. considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of re methods in a conversational setting and investigate the performance of several representative re methods on dialogre. experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. dialogre is available at https://dataset.org/dialogre/."], "resources and evaluation"], [["predicting the focus of negation: model and error analysis", "md mosharaf hossain | kathleen hamilton | alexis palmer | eduardo blanco", "the focus of a negation is the set of tokens intended to be negated, and a key component for revealing affirmative alternatives to negated utterances. in this paper, we experiment with neural networks to predict the focus of negation. our main novelty is leveraging a scope detector to introduce the scope of negation as an additional input to the network. experimental results show that doing so obtains the best results to date. additionally, we perform a detailed error analysis providing insights into the main error categories, and analyze errors depending on whether the model takes into account scope and context information."], "semantics"], [["a re-evaluation of knowledge graph completion methods", "zhiqing sun | shikhar vashishth | soumya sanyal | partha talukdar | yiming yang", "knowledge graph completion (kgc) aims at automatically predicting missing links for large-scale knowledge graphs. a vast number of state-of-the-art kgc techniques have got published at top conferences in several research fields, including data mining, machine learning, and natural language processing. however, we notice that several recent papers report very high performance, which largely outperforms previous state-of-the-art methods. in this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem. the proposed protocol is robust to handle bias in the model, which can substantially affect the final results. we conduct extensive experiments and report performance of several existing methods using our protocol. the reproducible code has been made publicly available."], "interpretability and analysis of models for nlp"], [["predicting depression in screening interviews from latent categorization of interview prompts", "alex rinaldi | jean fox tree | snigdha chaturvedi", "accurately diagnosing depression is difficult\u2013 requiring time-intensive interviews, assessments, and analysis. hence, automated methods that can assess linguistic patterns in these interviews could help psychiatric professionals make faster, more informed decisions about diagnosis. we propose jlpc, a model that analyzes interview transcripts to identify depression while jointly categorizing interview prompts into latent categories. this latent categorization allows the model to define high-level conversational contexts that influence patterns of language in depressed individuals. we show that the proposed model not only outperforms competitive baselines, but that its latent prompt categories provide psycholinguistic insights about depression."], "linguistic theories, cognitive modeling and psycholinguistics"], [["domain adaptive inference for neural machine translation", "danielle saunders | felix stahlberg | adri\u00e0 de gispert | bill byrne", "we investigate adaptive ensemble weighting for neural machine translation, addressing the case of improving performance on a new and potentially unknown domain without sacrificing performance on the original domain. we adapt sequentially across two spanish-english and three english-german tasks, comparing unregularized fine-tuning, l2 and elastic weight consolidation. we then report a novel scheme for adaptive nmt ensemble decoding by extending bayesian interpolation with source information, and report strong improvements across test domains without access to the domain label."], "machine translation and multilinguality"], [["verbal multiword expressions for identification of metaphor", "omid rohanian | marek rei | shiva taslimipoor | le an ha", "metaphor is a linguistic device in which a concept is expressed by mentioning another. identifying metaphorical expressions, therefore, requires a non-compositional understanding of semantics. multiword expressions (mwes), on the other hand, are linguistic phenomena with varying degrees of semantic opacity and their identification poses a challenge to computational models. this work is the first attempt at analysing the interplay of metaphor and mwes processing through the design of a neural architecture whereby classification of metaphors is enhanced by informing the model of the presence of mwes. to the best of our knowledge, this is the first \u201cmwe-aware\u201d metaphor identification system paving the way for further experiments on the complex interactions of these phenomena. the results and analyses show that this proposed architecture reach state-of-the-art on two different established metaphor datasets."], "semantics"], [["accelerating bert inference for sequence labeling via early-exit", "xiaonan li | yunfan shao | tianxiang sun | hang yan | xipeng qiu | xuanjing huang", "both performance and efficiency are crucial factors for sequence labeling tasks in many real-world scenarios. although the pre-trained models (ptms) have significantly improved the performance of various sequence labeling tasks, their computational cost is expensive. to alleviate this problem, we extend the recent successful early-exit mechanism to accelerate the inference of ptms for sequence labeling tasks. however, existing early-exit mechanisms are specifically designed for sequence-level tasks, rather than sequence labeling. in this paper, we first propose a simple extension of sentence-level early-exit for sequence labeling tasks. to further reduce the computational cost, we also propose a token-level early-exit mechanism that allows partial tokens to exit early at different layers. considering the local dependency inherent in sequence labeling, we employed a window-based criterion to decide for a token whether or not to exit. the token-level early-exit brings the gap between training and inference, so we introduce an extra self-sampling fine-tuning stage to alleviate it. the extensive experiments on three popular sequence labeling tasks show that our approach can save up to 66%\u223c75% inference cost with minimal performance degradation. compared with competitive compressed models such as distilbert, our approach can achieve better performance under the same speed-up ratios of 2\u00d7, 3\u00d7, and 4\u00d7."], "information extraction, retrieval and text mining"], [["cosy: counterfactual syntax for cross-lingual understanding", "sicheng yu | hao zhang | yulei niu | qianru sun | jing jiang", "pre-trained multilingual language models, e.g., multilingual-bert, are widely used in cross-lingual tasks, yielding the state-of-the-art performance. however, such models suffer from a large performance gap between source and target languages, especially in the zero-shot setting, where the models are fine-tuned only on english but tested on other languages for the same task. we tackle this issue by incorporating language-agnostic information, specifically, universal syntax such as dependency relations and pos tags, into language models, based on the observation that universal syntax is transferable across different languages. our approach, called counterfactual syntax (cosy), includes the design of syntax-aware networks as well as a counterfactual training method to implicitly force the networks to learn not only the semantics but also the syntax. to evaluate cosy, we conduct cross-lingual experiments on natural language inference and question answering using mbert and xlm-r as network backbones. our results show that cosy achieves the state-of-the-art performance for both tasks, without using auxiliary training data."], "machine learning for nlp"], [["tree-structured neural topic model", "masaru isonuma | junichiro mori | danushka bollegala | ichiro sakata", "this paper presents a tree-structured neural topic model, which has a topic distribution over a tree with an infinite number of branches. our model parameterizes an unbounded ancestral and fraternal topic distribution by applying doubly-recurrent neural networks. with the help of autoencoding variational bayes, our model improves data scalability and achieves competitive performance when inducing latent topics and tree structures, as compared to a prior tree-structured topic model (blei et al., 2010). this work extends the tree-structured topic model such that it can be incorporated with neural models for downstream tasks."], "information extraction, retrieval and text mining"], [["gender bias amplification during speed-quality optimization in neural machine translation", "adithya renduchintala | denise diaz | kenneth heafield | xian li | mona diab", "is bias amplified when neural machine translation (nmt) models are optimized for speed and evaluated on generic test sets using bleu? we investigate architectures and techniques commonly used to speed up decoding in transformer-based models, such as greedy search, quantization, average attention networks (aans) and shallow decoder models and show their effect on gendered noun translation. we construct a new gender bias test set, simplegen, based on gendered noun phrases in which there is a single, unambiguous, correct answer. while we find minimal overall bleu degradation as we apply speed optimizations, we observe that gendered noun translation performance degrades at a much faster rate."], "machine translation and multilinguality"], [["distantly supervised named entity recognition using positive-unlabeled learning", "minlong peng | xiaoyu xing | qi zhang | jinlan fu | xuanjing huang", "in this work, we explore the way to perform named entity recognition (ner) using only unlabeled data and named entity dictionaries. to this end, we formulate the task as a positive-unlabeled (pu) learning problem and accordingly propose a novel pu learning algorithm to perform the task. we prove that the proposed algorithm can unbiasedly and consistently estimate the task loss as if there is fully labeled data. a key feature of the proposed method is that it does not require the dictionaries to label every entity within a sentence, and it even does not require the dictionaries to label all of the words constituting an entity. this greatly reduces the requirement on the quality of the dictionaries and makes our method generalize well with quite simple dictionaries. empirical studies on four public ner datasets demonstrate the effectiveness of our proposed method. we have published the source code at https://github.com/v-mipeng/lexiconner."], "tagging, chunking, syntax and parsing"], [["cross-sentence grammatical error correction", "shamil chollampatt | weiqi wang | hwee tou ng", "automatic grammatical error correction (gec) research has made remarkable progress in the past decade. however, all existing approaches to gec correct errors by considering a single sentence alone and ignoring crucial cross-sentence context. some errors can only be corrected reliably using cross-sentence context and models can also benefit from the additional contextual information in correcting other errors. in this paper, we address this serious limitation of existing approaches and improve strong neural encoder-decoder models by appropriately modeling wider contexts. we employ an auxiliary encoder that encodes previous sentences and incorporate the encoding in the decoder via attention and gating mechanisms. our approach results in statistically significant improvements in overall gec performance over strong baselines across multiple test sets. analysis of our cross-sentence gec model on a synthetic dataset shows high performance in verb tense corrections that require cross-sentence context."], "nlp applications"], [["a cross-sentence latent variable model for semi-supervised text sequence matching", "jihun choi | taeuk kim | sang-goo lee", "we present a latent variable model for predicting the relationship between a pair of text sequences. unlike previous auto-encoding\u2013based approaches that consider each sequence separately, our proposed framework utilizes both sequences within a single model by generating a sequence that has a given relationship with a source sequence. we further extend the cross-sentence generating framework to facilitate semi-supervised training. we also define novel semantic constraints that lead the decoder network to generate semantically plausible and diverse sequences. we demonstrate the effectiveness of the proposed model from quantitative and qualitative experiments, while achieving state-of-the-art results on semi-supervised natural language inference and paraphrase identification."], "semantics"], [["synthetic qa corpora generation with roundtrip consistency", "chris alberti | daniel andor | emily pitler | jacob devlin | michael collins", "we introduce a novel method of generating synthetic question answering corpora by combining models of question generation and answer extraction, and by filtering the results to ensure roundtrip consistency. by pretraining on the resulting corpora we obtain significant improvements on squad2 and nq, establishing a new state-of-the-art on the latter. our synthetic data generation models, for both question generation and answer extraction, can be fully reproduced by finetuning a publicly available bert model on the extractive subsets of squad2 and nq. we also describe a more powerful variant that does full sequence-to-sequence pretraining for question generation, obtaining exact match and f1 at less than 0.1% and 0.4% from human performance on squad2."], "question answering"], [["will-they-won\u2019t-they: a very large dataset for stance detection on twitter", "costanza conforti | jakob berndt | mohammad taher pilehvar | chryssi giannitsarou | flavio toxvaerd | nigel collier", "we present a new challenging stance detection dataset, called will-they-won\u2019t-they (wt--wt), which contains 51,284 tweets in english, making it by far the largest available dataset of the type. all the annotations are carried out by experts; therefore, the dataset constitutes a high-quality and reliable benchmark for future research in stance detection. our experiments with a wide range of recent state-of-the-art stance detection systems show that the dataset poses a strong challenge to existing models in this domain."], "resources and evaluation"], [["selective knowledge distillation for neural machine translation", "fusheng wang | jianhao yan | fandong meng | jie zhou", "neural machine translation (nmt) models achieve state-of-the-art performance on many translation benchmarks. as an active research field in nmt, knowledge distillation is widely applied to enhance the model\u2019s performance by transferring teacher model\u2019s knowledge on each training sample. however, previous work rarely discusses the different impacts and connections among these samples, which serve as the medium for transferring teacher knowledge. in this paper, we design a novel protocol that can effectively analyze the different impacts of samples by comparing various samples\u2019 partitions. based on above protocol, we conduct extensive experiments and find that the teacher\u2019s knowledge is not the more, the better. knowledge over specific samples may even hurt the whole performance of knowledge distillation. finally, to address these issues, we propose two simple yet effective strategies, i.e., batch-level and global-level selections, to pick suitable samples for distillation. we evaluate our approaches on two large-scale machine translation tasks, wmt\u201914 english-german and wmt\u201919 chinese-english. experimental results show that our approaches yield up to +1.28 and +0.89 bleu points improvements over the transformer baseline, respectively."], "machine translation and multilinguality"], [["competence-based multimodal curriculum learning for medical report generation", "fenglin liu | shen ge | xian wu", "medical report generation task, which targets to produce long and coherent descriptions of medical images, has attracted growing research interests recently. different from the general image captioning tasks, medical report generation is more challenging for data-driven neural models. this is mainly due to 1) the serious data bias and 2) the limited medical data. to alleviate the data bias and make best use of available data, we propose a competence-based multimodal curriculum learning framework (cmcl). specifically, cmcl simulates the learning process of radiologists and optimizes the model in a step by step manner. firstly, cmcl estimates the difficulty of each training instance and evaluates the competence of current model; secondly, cmcl selects the most suitable batch of training instances considering current model competence. by iterating above two steps, cmcl can gradually improve the model\u2019s performance. the experiments on the public iu-xray and mimic-cxr datasets show that cmcl can be incorporated into existing models to improve their performance."], "nlp applications"], [["interpolated spectral ngram language models", "ariadna quattoni | xavier carreras", "spectral models for learning weighted non-deterministic automata have nice theoretical and algorithmic properties. despite this, it has been challenging to obtain competitive results in language modeling tasks, for two main reasons. first, in order to capture long-range dependencies of the data, the method must use statistics from long substrings, which results in very large matrices that are difficult to decompose. the second is that the loss function behind spectral learning, based on moment matching, differs from the probabilistic metrics used to evaluate language models. in this work we employ a technique for scaling up spectral learning, and use interpolated predictions that are optimized to maximize perplexity. our experiments in character-based language modeling show that our method matches the performance of state-of-the-art ngram models, while being very fast to train."], "machine learning for nlp"], [["continual relation learning via episodic memory activation and reconsolidation", "xu han | yi dai | tianyu gao | yankai lin | zhiyuan liu | peng li | maosong sun | jie zhou", "continual relation learning aims to continually train a model on new data to learn incessantly emerging novel relations while avoiding catastrophically forgetting old relations. some pioneering work has proved that storing a handful of historical relation examples in episodic memory and replaying them in subsequent training is an effective solution for such a challenging problem. however, these memory-based methods usually suffer from overfitting the few memorized examples of old relations, which may gradually cause inevitable confusion among existing relations. inspired by the mechanism in human long-term memory formation, we introduce episodic memory activation and reconsolidation (emar) to continual relation learning. every time neural models are activated to learn both new and memorized data, emar utilizes relation prototypes for memory reconsolidation exercise to keep a stable understanding of old relations. the experimental results show that emar could get rid of catastrophically forgetting old relations and outperform the state-of-the-art continual learning models."], "information extraction, retrieval and text mining"], [["neural-based chinese idiom recommendation for enhancing elegance in essay writing", "yuanchao liu | bo pang | bingquan liu", "although the proper use of idioms can enhance the elegance of writing, the active use of various expressions is a challenge because remembering idioms is difficult. in this study, we address the problem of idiom recommendation by leveraging a neural machine translation framework, in which we suppose that idioms are written with one pseudo target language. two types of real-life datasets are collected to support this study. experimental results show that the proposed approach achieves promising performance compared with other baseline methods."], "machine learning for nlp"], [["efficient second-order treecrf for neural dependency parsing", "yu zhang | zhenghua li | min zhang", "in the deep learning (dl) era, parsing models are extremely simplified with little hurt on performance, thanks to the remarkable capability of multi-layer bilstms in context representation. as the most popular graph-based dependency parser due to its high efficiency and performance, the biaffine parser directly scores single dependencies under the arc-factorization assumption, and adopts a very simple local token-wise cross-entropy training loss. this paper for the first time presents a second-order treecrf extension to the biaffine parser. for a long time, the complexity and inefficiency of the inside-outside algorithm hinder the popularity of treecrf. to address this issue, we propose an effective way to batchify the inside and viterbi algorithms for direct large matrix operation on gpus, and to avoid the complex outside algorithm via efficient back-propagation. experiments and analysis on 27 datasets from 13 languages clearly show that techniques developed before the dl era, such as structural learning (global treecrf loss) and high-order modeling are still useful, and can further boost parsing performance over the state-of-the-art biaffine parser, especially for partially annotated training data. we release our code at https://github.com/yzhangcs/crfpar."], "tagging, chunking, syntax and parsing"], [["generating fluent adversarial examples for natural languages", "huangzhao zhang | hao zhou | ning miao | lei li", "efficiently building an adversarial attacker for natural language processing (nlp) tasks is a real challenge. firstly, as the sentence space is discrete, it is difficult to make small perturbations along the direction of gradients. secondly, the fluency of the generated examples cannot be guaranteed. in this paper, we propose mha, which addresses both problems by performing metropolis-hastings sampling, whose proposal is designed with the guidance of gradients. experiments on imdb and snli show that our proposed mhaoutperforms the baseline model on attacking capability. adversarial training with mha also leads to better robustness and performance."], "machine learning for nlp"], [["query graph generation for answering multi-hop complex questions from knowledge bases", "yunshi lan | jing jiang", "previous work on answering complex questions from knowledge bases usually separately addresses two types of complexity: questions with constraints and questions with multiple hops of relations. in this paper, we handle both types of complexity at the same time. motivated by the observation that early incorporation of constraints into query graphs can more effectively prune the search space, we propose a modified staged query graph generation method with more flexible ways to generate query graphs. our experiments clearly show that our method achieves the state of the art on three benchmark kbqa datasets."], "question answering"], [["representations of syntax [mask] useful: effects of constituency and dependency structure in recursive lstms", "michael lepori | tal linzen | r. thomas mccoy", "sequence-based neural networks show significant sensitivity to syntactic structure, but they still perform less well on syntactic tasks than tree-based networks. such tree-based networks can be provided with a constituency parse, a dependency parse, or both. we evaluate which of these two representational schemes more effectively introduces biases for syntactic structure that increase performance on the subject-verb agreement prediction task. we find that a constituency-based network generalizes more robustly than a dependency-based one, and that combining the two types of structure does not yield further improvement. finally, we show that the syntactic robustness of sequential models can be substantially improved by fine-tuning on a small amount of constructed data, suggesting that data augmentation is a viable alternative to explicit constituency structure for imparting the syntactic biases that sequential models are lacking."], "tagging, chunking, syntax and parsing"], [["multi-domain neural machine translation with word-level adaptive layer-wise domain mixing", "haoming jiang | chen liang | chong wang | tuo zhao", "many multi-domain neural machine translation (nmt) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains. however, this design lacks adaptation to individual domains. to overcome this limitation, we propose a novel multi-domain nmt model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing. we first observe that words in a sentence are often related to multiple domains. hence, we assume each word has a domain proportion, which indicates its domain preference. then word representations are obtained by mixing their embedding in individual domains based on their domain proportions. we show this can be achieved by carefully designing multi-head dot-product attention modules for different domains, and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions. through this, we can achieve effective domain knowledge sharing and capture fine-grained domain-specific knowledge as well. our experiments show that our proposed model outperforms existing ones in several nmt tasks."], "machine translation and multilinguality"], [["words aren\u2019t enough, their order matters: on the robustness of grounding visual referring expressions", "arjun akula | spandana gella | yaser al-onaizan | song-chun zhu | siva reddy", "visual referring expression recognition is a challenging task that requires natural language understanding in the context of an image. we critically examine refcocog, a standard benchmark for this task, using a human study and show that 83.7% of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the word order doesn\u2019t matter. to measure the true progress of existing models, we split the test set into two sets, one which requires reasoning on linguistic structure and the other which doesn\u2019t. additionally, we create an out-of-distribution dataset ref-adv by asking crowdworkers to perturb in-domain examples such that the target object changes. using these datasets, we empirically show that existing methods fail to exploit linguistic structure and are 12% to 23% lower in performance than the established progress for this task. we also propose two methods, one based on contrastive learning and the other based on multi-task learning, to increase the robustness of vilbert, the current state-of-the-art model for this task. our datasets are publicly available at https://github.com/aws/aws-refcocog-adv."], "language grounding to vision, robotics and beyond"], [["unsupervised multilingual word embedding with limited resources using neural language models", "takashi wada | tomoharu iwata | yuji matsumoto", "recently, a variety of unsupervised methods have been proposed that map pre-trained word embeddings of different languages into the same space without any parallel data. these methods aim to find a linear transformation based on the assumption that monolingual word embeddings are approximately isomorphic between languages. however, it has been demonstrated that this assumption holds true only on specific conditions, and with limited resources, the performance of these methods decreases drastically. to overcome this problem, we propose a new unsupervised multilingual embedding method that does not rely on such assumption and performs well under resource-poor scenarios, namely when only a small amount of monolingual data (i.e., 50k sentences) are available, or when the domains of monolingual data are different across languages. our proposed model, which we call \u2018multilingual neural language models\u2019, shares some of the network parameters among multiple languages, and encodes sentences of multiple languages into the same space. the model jointly learns word embeddings of different languages in the same space, and generates multilingual embeddings without any parallel data or pre-training. our experiments on word alignment tasks have demonstrated that, on the low-resource condition, our model substantially outperforms existing unsupervised and even supervised methods trained with 500 bilingual pairs of words. our model also outperforms unsupervised methods given different-domain corpora across languages. our code is publicly available."], "machine translation and multilinguality"], [["spatial aggregation facilitates discovery of spatial topics", "aniruddha maiti | slobodan vucetic", "spatial aggregation refers to merging of documents created at the same spatial location. we show that by spatial aggregation of a large collection of documents and applying a traditional topic discovery algorithm on the aggregated data we can efficiently discover spatially distinct topics. by looking at topic discovery through matrix factorization lenses we show that spatial aggregation allows low rank approximation of the original document-word matrix, in which spatially distinct topics are preserved and non-spatial topics are aggregated into a single topic. our experiments on synthetic data confirm this observation. our experiments on 4.7 million tweets collected during the sandy hurricane in 2012 show that spatial and temporal aggregation allows rapid discovery of relevant spatial and temporal topics during that period. our work indicates that different forms of document aggregation might be effective in rapid discovery of various types of distinct topics from large collections of documents."], "information extraction, retrieval and text mining"], [["one size does not fit all: generating and evaluating variable number of keyphrases", "xingdi yuan | tong wang | rui meng | khushboo thaker | peter brusilovsky | daqing he | adam trischler", "different texts shall by nature correspond to different number of keyphrases. this desideratum is largely missing from existing neural keyphrase generation models. in this study, we address this problem from both modeling and evaluation perspectives. we first propose a recurrent generative model that generates multiple keyphrases as delimiter-separated sequences. generation diversity is further enhanced with two novel techniques by manipulating decoder hidden states. in contrast to previous approaches, our model is capable of generating diverse keyphrases and controlling number of outputs. we further propose two evaluation metrics tailored towards the variable-number generation. we also introduce a new dataset stackex that expands beyond the only existing genre (i.e., academic writing) in keyphrase generation tasks. with both previous and new evaluation metrics, our model outperforms strong baselines on all datasets."], "generation"], [["aspect sentiment classification towards question-answering with reinforced bidirectional attention network", "jingjing wang | changlong sun | shoushan li | xiaozhong liu | luo si | min zhang | guodong zhou", "in the literature, existing studies on aspect sentiment classification (asc) focus on individual non-interactive reviews. this paper extends the research to interactive reviews and proposes a new research task, namely aspect sentiment classification towards question-answering (asc-qa), for real-world applications. this new task aims to predict sentiment polarities for specific aspects from interactive qa style reviews. in particular, a high-quality annotated corpus is constructed for asc-qa to facilitate corresponding research. on this basis, a reinforced bidirectional attention network (rban) approach is proposed to address two inherent challenges in asc-qa, i.e., semantic matching between question and answer, and data noise. experimental results demonstrate the great advantage of the proposed approach to asc-qa against several state-of-the-art baselines."], "sentiment analysis, stylistic analysis, and argument mining"], [["spying on your neighbors: fine-grained probing of contextual embeddings for information about surrounding words", "josef klafka | allyson ettinger", "although models using contextual word embeddings have achieved state-of-the-art results on a host of nlp tasks, little is known about exactly what information these embeddings encode about the context words that they are understood to reflect. to address this question, we introduce a suite of probing tasks that enable fine-grained testing of contextual embeddings for encoding of information about surrounding words. we apply these tasks to examine the popular bert, elmo and gpt contextual encoders, and find that each of our tested information types is indeed encoded as contextual information across tokens, often with near-perfect recoverability\u2014but the encoders vary in which features they distribute to which tokens, how nuanced their distributions are, and how robust the encoding of each feature is to distance. we discuss implications of these results for how different types of models break down and prioritize word-level context information when constructing token embeddings."], "interpretability and analysis of models for nlp"], [["what is learned in visually grounded neural syntax acquisition", "noriyuki kojima | hadar averbuch-elor | alexander rush | yoav artzi", "visual features are a promising signal for learning bootstrap textual models. however, blackbox learning models make it difficult to isolate the specific contribution of visual components. in this analysis, we consider the case study of the visually grounded neural syntax learner (shi et al., 2019), a recent approach for learning syntax from a visual training signal. by constructing simplified versions of the model, we isolate the core factors that yield the model\u2019s strong performance. contrary to what the model might be capable of learning, we find significantly less expressive versions produce similar predictions and perform just as well, or even better. we also find that a simple lexical signal of noun concreteness plays the main role in the model\u2019s predictions as opposed to more complex syntactic reasoning."], "language grounding to vision, robotics and beyond"], [["learning interpretable relationships between entities, relations and concepts via bayesian structure learning on open domain facts", "jingyuan zhang | mingming sun | yue feng | ping li", "concept graphs are created as universal taxonomies for text understanding in the open-domain knowledge. the nodes in concept graphs include both entities and concepts. the edges are from entities to concepts, showing that an entity is an instance of a concept. in this paper, we propose the task of learning interpretable relationships from open-domain facts to enrich and refine concept graphs. the bayesian network structures are learned from open-domain facts as the interpretable relationships between relations of facts and concepts of entities. we conduct extensive experiments on public english and chinese datasets. compared to the state-of-the-art methods, the learned network structures help improving the identification of concepts for entities based on the relations of entities on both datasets."], "information extraction, retrieval and text mining"], [["review-based question generation with adaptive instance transfer and augmentation", "qian yu | lidong bing | qiong zhang | wai lam | luo si", "while online reviews of products and services become an important information source, it remains inefficient for potential consumers to exploit verbose reviews for fulfilling their information need. we propose to explore question generation as a new way of review information exploitation, namely generating questions that can be answered by the corresponding review sentences. one major challenge of this generation task is the lack of training data, i.e. explicit mapping relation between the user-posed questions and review sentences. to obtain proper training instances for the generation model, we propose an iterative learning framework with adaptive instance transfer and augmentation. to generate to the point questions about the major aspects in reviews, related features extracted in an unsupervised manner are incorporated without the burden of aspect annotation. experiments on data from various categories of a popular e-commerce site demonstrate the effectiveness of the framework, as well as the potentials of the proposed review-based question generation task."], "generation"], [["tbert: topic models and bert joining forces for semantic similarity detection", "nicole peinelt | dong nguyen | maria liakata", "semantic similarity detection is a fundamental task in natural language understanding. adding topic information has been useful for previous feature-engineered semantic similarity models as well as neural models for other tasks. there is currently no standard way of combining topics with pretrained contextual representations such as bert. we propose a novel topic-informed bert-based architecture for pairwise semantic similarity detection and show that our model improves performance over strong neural baselines across a variety of english language datasets. we find that the addition of topics to bert helps particularly with resolving domain-specific cases."], "semantics"], [["neural temporality adaptation for document classification: diachronic word embeddings and domain adaptation models", "xiaolei huang | michael j. paul", "language usage can change across periods of time, but document classifiers models are usually trained and tested on corpora spanning multiple years without considering temporal variations. this paper describes two complementary ways to adapt classifiers to shifts across time. first, we show that diachronic word embeddings, which were originally developed to study language change, can also improve document classification, and we show a simple method for constructing this type of embedding. second, we propose a time-driven neural classification model inspired by methods for domain adaptation. experiments on six corpora show how these methods can make classifiers more robust over time."], "machine learning for nlp"], [["null it out: guarding protected attributes by iterative nullspace projection", "shauli ravfogel | yanai elazar | hila gonen | michael twiton | yoav goldberg", "the ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. we present iterative null-space projection (inlp), a novel method for removing information from neural representations. our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. by doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. while applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification."], "machine learning for nlp"], [["multimodal transformer for unaligned multimodal language sequences", "yao-hung hubert tsai | shaojie bai | paul pu liang | j. zico kolter | louis-philippe morency | ruslan salakhutdinov", "human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. however, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. in this paper, we introduce the multimodal transformer (mult) to generically address the above issues in an end-to-end manner without explicitly aligning the data. at the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. in addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in mult."], "language grounding to vision, robotics and beyond"], [["mitigating gender bias amplification in distribution by posterior regularization", "shengyu jia | tao meng | jieyu zhao | kai-wei chang", "advanced machine learning techniques have boosted the performance of natural language processing. nevertheless, recent studies, e.g., (citation) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. however, their analysis is conducted only on models\u2019 top predictions. in this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. we further propose a bias mitigation approach based on posterior regularization. with little performance loss, our method can almost remove the bias amplification in the distribution. our study sheds the light on understanding the bias amplification."], "ethics in nlp"], [["injecting numerical reasoning skills into language models", "mor geva | ankit gupta | jonathan berant", "large pre-trained language models (lms) are known to encode substantial amounts of linguistic information. however, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. in this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained lms, by generating large amounts of data, and training in a multi-task setup. we show that pre-training our model, genbert, on this data, dramatically improves performance on drop (49.3 \u2013> 72.3 f1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. moreover, genbert generalizes well to math word problem datasets, while maintaining high performance on standard rc tasks. our approach provides a general recipe for injecting skills into large pre-trained lms, whenever the skill is amenable to automatic data augmentation."], "question answering"], [["the case for translation-invariant self-attention in transformer-based language models", "ulme wennberg | gustav eje henter", "mechanisms for encoding positional information are central for transformer-based language models. in this paper, we analyze the position embeddings of existing language models, finding strong evidence of translation invariance, both for the embeddings themselves and for their effect on self-attention. the degree of translation invariance increases during training and correlates positively with model performance. our findings lead us to propose translation-invariant self-attention (tisa), which accounts for the relative position between tokens in an interpretable fashion without needing conventional position embeddings. our proposal has several theoretical advantages over existing position-representation approaches. proof-of-concept experiments show that it improves on regular albert on glue tasks, while only adding orders of magnitude less positional parameters."], "interpretability and analysis of models for nlp"], [["generate, delete and rewrite: a three-stage framework for improving persona consistency of dialogue generation", "haoyu song | yan wang | wei-nan zhang | xiaojiang liu | ting liu", "maintaining a consistent personality in conversations is quite natural for human beings, but is still a non-trivial task for machines. the persona-based dialogue generation task is thus introduced to tackle the personality-inconsistent problem by incorporating explicit persona text into dialogue generation models. despite the success of existing persona-based models on generating human-like responses, their one-stage decoding framework can hardly avoid the generation of inconsistent persona words. in this work, we introduce a three-stage framework that employs a generate-delete-rewrite mechanism to delete inconsistent words from a generated response prototype and further rewrite it to a personality-consistent one. we carry out evaluations by both human and automatic metrics. experiments on the persona-chat dataset show that our approach achieves good performance."], "dialogue and interactive systems"], [["identifying principals and accessories in a complex case based on the comprehension of fact description", "yakun hu | zhunchen luo | wenhan chao", "in this paper, we study the problem of identifying the principals and accessories from the fact description with multiple defendants in a criminal case. we treat the fact descriptions as narrative texts and the defendants as roles over the narrative story. we propose to model the defendants with behavioral semantic information and statistical characteristics, then learning the importances of defendants within a learning-to-rank framework. experimental results on a real-world dataset demonstrate the behavior analysis can effectively model the defendants\u2019 impacts in a complex case."], "nlp applications"], [["winowhy: a deep diagnosis of essential commonsense knowledge for answering winograd schema challenge", "hongming zhang | xinran zhao | yangqiu song", "in this paper, we present the first comprehensive categorization of essential commonsense knowledge for answering the winograd schema challenge (wsc). for each of the questions, we invite annotators to first provide reasons for making correct decisions and then categorize them into six major knowledge categories. by doing so, we better understand the limitation of existing methods (i.e., what kind of knowledge cannot be effectively represented or inferred with existing methods) and shed some light on the commonsense knowledge that we need to acquire in the future for better commonsense reasoning. moreover, to investigate whether current wsc models can understand the commonsense or they simply solve the wsc questions based on the statistical bias of the dataset, we leverage the collected reasons to develop a new task called winowhy, which requires models to distinguish plausible reasons from very similar but wrong reasons for all wsc questions. experimental results prove that even though pre-trained language representation models have achieved promising progress on the original wsc dataset, they are still struggling at winowhy. further experiments show that even though supervised models can achieve better performance, the performance of these models can be sensitive to the dataset distribution. winowhy and all codes are available at: https://github.com/hkust-knowcomp/winowhy."], "resources and evaluation"], [["topic-aware neural keyphrase generation for social media language", "yue wang | jing li | hou pong chan | irwin king | michael r. lyu | shuming shi", "a huge volume of user-generated content is daily produced on social media. to facilitate automatic language understanding, we study keyphrase prediction, distilling salient information from massive posts. while most existing methods extract words from source posts to form keyphrases, we propose a sequence-to-sequence (seq2seq) based neural keyphrase generation framework, enabling absent keyphrases to be created. moreover, our model, being topic-aware, allows joint modeling of corpus-level latent topic representations, which helps alleviate data sparsity widely exhibited in social media language. experiments on three datasets collected from english and chinese social media platforms show that our model significantly outperforms both extraction and generation models without exploiting latent topics. further discussions show that our model learns meaningful topics, which interprets its superiority in social media keyphrase generation."], "computational social science, social media and cultural analytics"], [["cnns found to jump around more skillfully than rnns: compositional generalization in seq2seq convolutional networks", "roberto dess\u00ec | marco baroni", "lake and baroni (2018) introduced the scan dataset probing the ability of seq2seq models to capture compositional generalizations, such as inferring the meaning of \u201cjump around\u201d 0-shot from the component words. recurrent networks (rnns) were found to completely fail the most challenging generalization cases. we test here a convolutional network (cnn) on these tasks, reporting hugely improved performance with respect to rnns. despite the big improvement, the cnn has however not induced systematic rules, suggesting that the difference between compositional and non-compositional behaviour is not clear-cut."], "linguistic theories, cognitive modeling and psycholinguistics"], [["analyzing multi-head self-attention: specialized heads do the heavy lifting, the rest can be pruned", "elena voita | david talbot | fedor moiseev | rico sennrich | ivan titov", "multi-head self-attention is a key component of the transformer, a state-of-the-art architecture for neural machine translation. in this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. we find that the most important and confident heads play consistent and often linguistically-interpretable roles. when pruning heads using a method based on stochastic gates and a differentiable relaxation of the l0 penalty, we observe that specialized heads are last to be pruned. our novel pruning method removes the vast majority of heads without seriously affecting performance. for example, on the english-russian wmt dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 bleu."], "machine translation and multilinguality"], [["integrating semantic and structural information with graph convolutional network for controversy detection", "lei zhong | juan cao | qiang sheng | junbo guo | ziang wang", "identifying controversial posts on social media is a fundamental task for mining public sentiment, assessing the influence of events, and alleviating the polarized views. however, existing methods fail to 1) effectively incorporate the semantic information from content-related posts; 2) preserve the structural information for reply relationship modeling; 3) properly handle posts from topics dissimilar to those in the training set. to overcome the first two limitations, we propose topic-post-comment graph convolutional network (tpc-gcn), which integrates the information from the graph structure and content of topics, posts, and comments for post-level controversy detection. as to the third limitation, we extend our model to disentangled tpc-gcn (dtpc-gcn), to disentangle topic-related and topic-unrelated features and then fuse dynamically. extensive experiments on two real-world datasets demonstrate that our models outperform existing methods. analysis of the results and cases proves that our models can integrate both semantic and structural information with significant generalizability."], "computational social science, social media and cultural analytics"], [["open-domain targeted sentiment analysis via span-based extraction and classification", "minghao hu | yuxing peng | zhen huang | dongsheng li | yiwei lv", "open-domain targeted sentiment analysis aims to detect opinion targets along with their sentiment polarities from a sentence. prior work typically formulates this task as a sequence tagging problem. however, such formulation suffers from problems such as huge search space and sentiment inconsistency. to address these problems, we propose a span-based extract-then-classify framework, where multiple opinion targets are directly extracted from the sentence under the supervision of target span boundaries, and corresponding polarities are then classified using their span representations. we further investigate three approaches under this framework, namely the pipeline, joint, and collapsed models. experiments on three benchmark datasets show that our approach consistently outperforms the sequence tagging baseline. moreover, we find that the pipeline model achieves the best performance compared with the other two models."], "sentiment analysis, stylistic analysis, and argument mining"], [["a study of non-autoregressive model for sequence generation", "yi ren | jinglin liu | xu tan | zhou zhao | sheng zhao | tie-yan liu", "non-autoregressive (nar) models generate all the tokens of a sequence in parallel, resulting in faster generation speed compared to their autoregressive (ar) counterparts but at the cost of lower accuracy. different techniques including knowledge distillation and source-target alignment have been proposed to bridge the gap between ar and nar models in various tasks such as neural machine translation (nmt), automatic speech recognition (asr), and text to speech (tts). with the help of those techniques, nar models can catch up with the accuracy of ar models in some tasks but not in some others. in this work, we conduct a study to understand the difficulty of nar sequence generation and try to answer: (1) why nar models can catch up with ar models in some tasks but not all? (2) why techniques like knowledge distillation and source-target alignment can help nar models. since the main difference between ar and nar models is that nar models do not use dependency among target tokens while ar models do, intuitively the difficulty of nar sequence generation heavily depends on the strongness of dependency among target tokens. to quantify such dependency, we propose an analysis model called comma to characterize the difficulty of different nar sequence generation tasks. we have several interesting findings: 1) among the nmt, asr and tts tasks, asr has the most target-token dependency while tts has the least. 2) knowledge distillation reduces the target-token dependency in target sequence and thus improves the accuracy of nar models. 3) source-target alignment constraint encourages dependency of a target token on source tokens and thus eases the training of nar models."], "generation"], [["simulspeech: end-to-end simultaneous speech to text translation", "yi ren | jinglin liu | xu tan | chen zhang | tao qin | zhou zhao | tie-yan liu", "in this work, we develop simulspeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently. simulspeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (ctc) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait-k strategy for simultaneous translation. simulspeech is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (asr) and simultaneous neural machine translation (nmt)). we introduce two novel knowledge distillation methods to ensure the performance: 1) attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous nmt and asr models to help the training of the attention mechanism in simulspeech; 2) data-level knowledge distillation transfers the knowledge from the full-sentence nmt model and also reduces the complexity of data distribution to help on the optimization of simulspeech. experiments on must-c english-spanish and english-german spoken language translation datasets show that simulspeech achieves reasonable bleu scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of bleu scores and translation delay."], "speech and multimodality"], [["learning to segment actions from observation and narration", "daniel fried | jean-baptiste alayrac | phil blunsom | chris dyer | stephen clark | aida nematzadeh", "we apply a generative segmental model of task structure, guided by narration, to action segmentation in video. we focus on unsupervised and weakly-supervised settings where no action labels are known during training. despite its simplicity, our model performs competitively with previous work on a dataset of naturalistic instructional videos. our model allows us to vary the sources of supervision used in training, and we find that both task structure and narrative language provide large benefits in segmentation quality."], "language grounding to vision, robotics and beyond"], [["better exploiting latent variables in text modeling", "canasai kruengkrai", "we show that sampling latent variables multiple times at a gradient step helps in improving a variational autoencoder and propose a simple and effective method to better exploit these latent variables through hidden state averaging. consistent gains in performance on two different datasets, penn treebank and yahoo, indicate the generalizability of our method."], "machine learning for nlp"], [["embedding time expressions for deep temporal ordering models", "tanya goyal | greg durrett", "data-driven models have demonstrated state-of-the-art performance in inferring the temporal ordering of events in text. however, these models often overlook explicit temporal signals, such as dates and time windows. rule-based methods can be used to identify the temporal links between these time expressions (timexes), but they fail to capture timexes\u2019 interactions with events and are hard to integrate with the distributed representations of neural net models. in this paper, we introduce a framework to infuse temporal awareness into such models by learning a pre-trained model to embed timexes. we generate synthetic data consisting of pairs of timexes, then train a character lstm to learn embeddings and classify the timexes\u2019 temporal relation. we evaluate the utility of these embeddings in the context of a strong neural model for event temporal ordering, and show a small increase in performance on the matres dataset and more substantial gains on an automatically collected dataset with more frequent event-timex interactions."], "information extraction, retrieval and text mining"], [["neural generation of dialogue response timings", "matthew roddy | naomi harte", "the timings of spoken response offsets in human dialogue have been shown to vary based on contextual elements of the dialogue. we propose neural models that simulate the distributions of these response offsets, taking into account the response turn as well as the preceding turn. the models are designed to be integrated into the pipeline of an incremental spoken dialogue system (sds). we evaluate our models using offline experiments as well as human listening tests. we show that human listeners consider certain response timings to be more natural based on the dialogue context. the introduction of these models into sds pipelines could increase the perceived naturalness of interactions."], "dialogue and interactive systems"], [["interpreting twitter user geolocation", "ting zhong | tianliang wang | fan zhou | goce trajcevski | kunpeng zhang | yi yang", "identifying user geolocation in online social networks is an essential task in many location-based applications. existing methods rely on the similarity of text and network structure, however, they suffer from a lack of interpretability on the corresponding results, which is crucial for understanding model behavior. in this work, we adopt influence functions to interpret the behavior of gnn-based models by identifying the importance of training users when predicting the locations of the testing users. this methodology helps with providing meaningful explanations on prediction results. furthermore, it also initiates an attempt to uncover the so-called \u201cblack-box\u201d gnn-based models by investigating the effect of individual nodes."], "nlp applications"], [["learning to tag oov tokens by integrating contextual representation and background knowledge", "keqing he | yuanmeng yan | weiran xu", "neural-based context-aware models for slot tagging have achieved state-of-the-art performance. however, the presence of oov(out-of-vocab) words significantly degrades the performance of neural-based models, especially in a few-shot scenario. in this paper, we propose a novel knowledge-enhanced slot tagging model to integrate contextual representation of input text and the large-scale lexical background knowledge. besides, we use multi-level graph attention to explicitly model lexical relations. the experiments show that our proposed knowledge integration mechanism achieves consistent improvements across settings with different sizes of training data on two public benchmark datasets."], "dialogue and interactive systems"], [["human vs. muppet: a conservative estimate of human performance on the glue benchmark", "nikita nangia | samuel r. bowman", "the glue benchmark (wang et al., 2019b) is a suite of language understanding tasks which has seen dramatic progress in the past year, with average performance moving from 70.0 at launch to 83.9, state of the art at the time of writing (may 24, 2019). here, we measure human performance on the benchmark, in order to learn whether significant headroom remains for further progress. we provide a conservative estimate of human performance on the benchmark through crowdsourcing: our annotators are non-experts who must learn each task from a brief set of instructions and 20 examples. in spite of limited training, these annotators robustly outperform the state of the art on six of the nine glue tasks and achieve an average score of 87.1. given the fast pace of progress however, the headroom we observe is quite limited. to reproduce the data-poor setting that our annotators must learn in, we also train the bert model (devlin et al., 2019) in limited-data regimes, and conclude that low-resource sentence classification remains a challenge for modern neural network approaches to text understanding."], "semantics"], [["evidence-aware inferential text generation with vector quantised variational autoencoder", "daya guo | duyu tang | nan duan | jian yin | daxin jiang | ming zhou", "generating inferential texts about an event in different perspectives requires reasoning over different contexts that the event occurs. existing works usually ignore the context that is not explicitly provided, resulting in a context-independent semantic representation that struggles to support the generation. to address this, we propose an approach that automatically finds evidence for an event from a large text corpus, and leverages the evidence to guide the generation of inferential texts. our approach works in an encoderdecoder manner and is equipped with vector quantised-variational autoencoder, where the encoder outputs representations from a distribution over discrete variables. such discrete representations enable automatically selecting relevant evidence, which not only facilitates evidence-aware generation, but also provides a natural way to uncover rationales behind the generation. our approach provides state-of-the-art performance on both event2mind and atomic datasets. more importantly, we find that with discrete representations, our model selectively uses evidence to generate different inferential texts."], "semantics"], [["generalized entropy regularization or: there\u2019s nothing special about label smoothing", "clara meister | elizabeth salesky | ryan cotterell", "prior work has explored directly regularizing the output distributions of probabilistic models to alleviate peaky (i.e. over-confident) predictions, a common sign of overfitting. this class of techniques, of which label smoothing is one, has a connection to entropy regularization. despite the consistent success of label smoothing across architectures and data sets in language generation tasks, two problems remain open: (1) there is little understanding of the underlying effects entropy regularizers have on models, and (2) the full space of entropy regularization techniques is largely unexplored. we introduce a parametric family of entropy regularizers, which includes label smoothing as a special case, and use it to gain a better understanding of the relationship between the entropy of a model and its performance on language generation tasks. we also find that variance in model performance can be explained largely by the resulting entropy of the model. lastly, we find that label smoothing provably does not allow for sparsity in an output distribution, an undesirable property for language generation models, and therefore advise the use of other entropy regularization methods in its place."], "machine learning for nlp"], [["putting evaluation in context: contextual embeddings improve machine translation evaluation", "nitika mathur | timothy baldwin | trevor cohn", "accurate, automatic evaluation of machine translation is critical for system tuning, and evaluating progress in the field. we proposed a simple unsupervised metric, and additional supervised metrics which rely on contextual word embeddings to encode the translation and reference sentences. we find that these models rival or surpass all existing metrics in the wmt 2017 sentence-level and system-level tracks, and our trained model has a substantially higher correlation with human judgements than all existing metrics on the wmt 2017 to-english sentence level dataset."], "resources and evaluation"], [["lipschitz constrained parameter initialization for deep transformers", "hongfei xu | qiuhui liu | josef van genabith | deyi xiong | jingyi zhang", "the transformer translation model employs residual connection and layer normalization to ease the optimization difficulties caused by its multi-layer encoder/decoder structure. previous research shows that even with residual connection and layer normalization, deep transformers still have difficulty in training, and particularly transformer models with more than 12 encoder/decoder layers fail to converge. in this paper, we first empirically demonstrate that a simple modification made in the official implementation, which changes the computation order of residual connection and layer normalization, can significantly ease the optimization of deep transformers. we then compare the subtle differences in computation order in considerable detail, and present a parameter initialization method that leverages the lipschitz constraint on the initialization of transformer parameters that effectively ensures training convergence. in contrast to findings in previous research we further demonstrate that with lipschitz parameter initialization, deep transformers with the original computation order can converge, and obtain significant bleu improvements with up to 24 layers. in contrast to previous research which focuses on deep encoders, our approach additionally enables transformers to also benefit from deep decoders."], "machine translation and multilinguality"], [["generating counter narratives against online hate speech: data and strategies", "serra sinem tekiro\u011flu | yi-ling chung | marco guerini", "recently research has started focusing on avoiding undesired effects that come with content moderation, such as censorship and overblocking, when dealing with hatred online. the core idea is to directly intervene in the discussion with textual responses that are meant to counter the hate content and prevent it from further spreading. accordingly, automation strategies, such as natural language generation, are beginning to be investigated. still, they suffer from the lack of sufficient amount of quality data and tend to produce generic/repetitive responses. being aware of the aforementioned limitations, we present a study on how to collect responses to hate effectively, employing large scale unsupervised language models such as gpt-2 for the generation of silver data, and the best annotation strategies/neural architectures that can be used for data filtering before expert validation/post-editing."], "resources and evaluation"], [["neural mixed counting models for dispersed topic discovery", "jiemin wu | yanghui rao | zusheng zhang | haoran xie | qing li | fu lee wang | ziye chen", "mixed counting models that use the negative binomial distribution as the prior can well model over-dispersed and hierarchically dependent random variables; thus they have attracted much attention in mining dispersed document topics. however, the existing parameter inference method like monte carlo sampling is quite time-consuming. in this paper, we propose two efficient neural mixed counting models, i.e., the negative binomial-neural topic model (nb-ntm) and the gamma negative binomial-neural topic model (gnb-ntm) for dispersed topic discovery. neural variational inference algorithms are developed to infer model parameters by using the reparameterization of gamma distribution and the gaussian approximation of poisson distribution. experiments on real-world datasets indicate that our models outperform state-of-the-art baseline models in terms of perplexity and topic coherence. the results also validate that both nb-ntm and gnb-ntm can produce explainable intermediate variables by generating dispersed proportions of document topics."], "semantics"], [["one time of interaction may not be enough: go deep with an interaction-over-interaction network for response selection in dialogues", "chongyang tao | wei wu | can xu | wenpeng hu | dongyan zhao | rui yan", "currently, researchers have paid great attention to retrieval-based dialogues in open-domain. in particular, people study the problem by investigating context-response matching for multi-turn response selection based on publicly recognized benchmark data sets. state-of-the-art methods require a response to interact with each utterance in a context from the beginning, but the interaction is performed in a shallow way. in this work, we let utterance-response interaction go deep by proposing an interaction-over-interaction network (ioi). the model performs matching by stacking multiple interaction blocks in which residual information from one time of interaction initiates the interaction process again. thus, matching information within an utterance-response pair is extracted from the interaction of the pair in an iterative fashion, and the information flows along the chain of the blocks via representations. evaluation results on three benchmark data sets indicate that ioi can significantly outperform state-of-the-art methods in terms of various matching metrics. through further analysis, we also unveil how the depth of interaction affects the performance of ioi."], "dialogue and interactive systems"], [["on the cross-lingual transferability of monolingual representations", "mikel artetxe | sebastian ruder | dani yogatama", "state-of-the-art unsupervised multilingual models (e.g., multilingual bert) have been shown to generalize in a zero-shot cross-lingual setting. this generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. we evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. more concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. this approach does not rely on a shared vocabulary or joint training. however, we show that it is competitive with multilingual bert on standard cross-lingual classification benchmarks and on a new cross-lingual question answering dataset (xquad). our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. we also release xquad as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from squad v1.1 translated into ten languages by professional translators."], "interpretability and analysis of models for nlp"], [["you impress me: dialogue generation via mutual persona perception", "qian liu | yihong chen | bei chen | jian-guang lou | zixuan chen | bin zhou | dongmei zhang", "despite the continuing efforts to improve the engagingness and consistency of chit-chat dialogue systems, the majority of current work simply focus on mimicking human-like responses, leaving understudied the aspects of modeling understanding between interlocutors. the research in cognitive science, instead, suggests that understanding is an essential signal for a high-quality chit-chat conversation. motivated by this, we propose p\u02c62 bot, a transmitter-receiver based framework with the aim of explicitly modeling understanding. specifically, p\u02c62 bot incorporates mutual persona perception to enhance the quality of personalized dialogue generation. experiments on a large public dataset, persona-chat, demonstrate the effectiveness of our approach, with a considerable boost over the state-of-the-art baselines across both automatic metrics and human evaluations."], "dialogue and interactive systems"], [["neural fuzzy repair: integrating fuzzy matches into neural machine translation", "bram bulte | arda tezcan", "we present a simple yet powerful data augmentation method for boosting neural machine translation (nmt) performance by leveraging information retrieved from a translation memory (tm). we propose and test two methods for augmenting nmt training data with fuzzy tm matches. tests on the dgt-tm data set for two language pairs show consistent and substantial improvements over a range of baseline systems. the results suggest that this method is promising for any translation environment in which a sizeable tm is available and a certain amount of repetition across translations is to be expected, especially considering its ease of implementation."], "machine translation and multilinguality"], [["semi-supervised contextual historical text normalization", "peter makarov | simon clematide", "historical text normalization, the task of mapping historical word forms to their modern counterparts, has recently attracted a lot of interest (bollmann, 2019; tang et al., 2018; lusetti et al., 2018; bollmann et al., 2018;robertson and goldwater, 2018; bollmannet al., 2017; korchagina, 2017). yet, virtually all approaches suffer from the two limitations: 1) they consider a fully supervised setup, often with impractically large manually normalized datasets; 2) normalization happens on words in isolation. by utilizing a simple generative normalization model and obtaining powerful contextualization from the target-side language model, we train accurate models with unlabeled historical data. in realistic training scenarios, our approach often leads to reduction in manually normalized data at the same accuracy levels."], "phonology, morphology and word segmentation"], [["interpretable question answering on knowledge bases and text", "alona sydorova | nina poerner | benjamin roth", "interpretability of machine learning (ml) models becomes more relevant with their increasing adoption. in this work, we address the interpretability of ml based question answering (qa) models on a combination of knowledge bases (kb) and text documents. we adapt post hoc explanation methods such as lime and input perturbation (ip) and compare them with the self-explanatory attention mechanism of the model. for this purpose, we propose an automatic evaluation paradigm for explanation methods in the context of qa. we also conduct a study with human annotators to evaluate whether explanations help them identify better qa models. our results suggest that ip provides better explanations than lime or attention, according to both automatic and human evaluation. we obtain the same ranking of methods in both experiments, which supports the validity of our automatic evaluation paradigm."], "question answering"], [["differentiable window for dynamic local attention", "thanh-tung nguyen | xuan-phi nguyen | shafiq joty | xiaoli li", "we propose differentiable window, a new neural module and general purpose component for dynamic window selection. while universally applicable, we demonstrate a compelling use case of utilizing differentiable window to improve standard attention modules by enabling more focused attentions over the input regions. we propose two variants of differentiable window, and integrate them within the transformer architecture in two novel ways. we evaluate our proposed approach on a myriad of nlp tasks, including machine translation, sentiment analysis, subject-verb agreement and language modeling. our experimental results demonstrate consistent and sizable improvements across all tasks."], "machine learning for nlp"], [["every child should have parents: a taxonomy refinement algorithm based on hyperbolic term embeddings", "rami aly | shantanu acharya | alexander ossa | arne k\u00f6hn | chris biemann | alexander panchenko", "we introduce the use of poincar\u00e9 embeddings to improve existing state-of-the-art approaches to domain-specific taxonomy induction from text as a signal for both relocating wrong hyponym terms within a (pre-induced) taxonomy as well as for attaching disconnected terms in a taxonomy. this method substantially improves previous state-of-the-art results on the semeval-2016 task 13 on taxonomy extraction. we demonstrate the superiority of poincar\u00e9 embeddings over distributional semantic representations, supporting the hypothesis that they can better capture hierarchical lexical-semantic relationships than embeddings in the euclidean space."], "semantics"], [["cdl: curriculum dual learning for emotion-controllable response generation", "lei shen | yang feng", "emotion-controllable response generation is an attractive and valuable task that aims to make open-domain conversations more empathetic and engaging. existing methods mainly enhance the emotion expression by adding regularization terms to standard cross-entropy loss and thus influence the training process. however, due to the lack of further consideration of content consistency, the common problem of response generation tasks, safe response, is intensified. besides, query emotions that can help model the relationship between query and response are simply ignored in previous models, which would further hurt the coherence. to alleviate these problems, we propose a novel framework named curriculum dual learning (cdl) which extends the emotion-controllable response generation to a dual task to generate emotional responses and emotional queries alternatively. cdl utilizes two rewards focusing on emotion and content to improve the duality. additionally, it applies curriculum learning to gradually generate high-quality responses based on the difficulties of expressing various emotions. experimental results show that cdl significantly outperforms the baselines in terms of coherence, diversity, and relation to emotion factors."], "dialogue and interactive systems"], [["mitigating bias in session-based cyberbullying detection: a non-compromising approach", "lu cheng | ahmadreza mosallanezhad | yasin silva | deborah hall | huan liu", "the element of repetition in cyberbullying behavior has directed recent computational studies toward detecting cyberbullying based on a social media session. in contrast to a single text, a session may consist of an initial post and an associated sequence of comments. yet, emerging efforts to enhance the performance of session-based cyberbullying detection have largely overlooked unintended social biases in existing cyberbullying datasets. for example, a session containing certain demographic-identity terms (e.g., \u201cgay\u201d or \u201cblack\u201d) is more likely to be classified as an instance of cyberbullying. in this paper, we first show evidence of such bias in models trained on sessions collected from different social media platforms (e.g., instagram). we then propose a context-aware and model-agnostic debiasing strategy that leverages a reinforcement learning technique, without requiring any extra resources or annotations apart from a pre-defined set of sensitive triggers commonly used for identifying cyberbullying instances. empirical evaluations show that the proposed strategy can simultaneously alleviate the impacts of the unintended biases and improve the detection performance."], "nlp applications"], [["attend to medical ontologies: content selection for clinical abstractive summarization", "sajad sotudeh gharebagh | nazli goharian | ross filice", "sequence-to-sequence (seq2seq) network is a well-established model for text summarization task. it can learn to produce readable content; however, it falls short in effectively identifying key regions of the source. in this paper, we approach the content selection problem for clinical abstractive summarization by augmenting salient ontological terms into the summarizer. our experiments on two publicly available clinical data sets (107,372 reports of mimic-cxr, and 3,366 reports of openi) show that our model statistically significantly boosts state-of-the-art results in terms of rouge metrics (with improvements: 2.9% rg-1, 2.5% rg-2, 1.9% rg-l), in the healthcare domain where any range of improvement impacts patients\u2019 welfare."], "summarization"], [["relating simple sentence representations in deep neural networks and the brain", "sharmistha jat | hao tang | partha talukdar | tom mitchell", "what is the relationship between sentence representations learned by deep recurrent models against those encoded by the brain? is there any correspondence between hidden layers of these recurrent models and brain regions when processing sentences? can these deep models be used to synthesize brain data which can then be utilized in other extrinsic tasks? we investigate these questions using sentences with simple syntax and semantics (e.g., the bone was eaten by the dog.). we consider multiple neural network architectures, including recently proposed elmo and bert. we use magnetoencephalography (meg) brain recording data collected from human subjects when they were reading these simple sentences. overall, we find that bert\u2019s activations correlate the best with meg brain data. we also find that the deep network representation can be used to generate brain data from new sentences to augment existing brain data. to the best of our knowledge, this is the first work showing that the meg brain recording when reading a word in a sentence can be used to distinguish earlier words in the sentence. our exploration is also the first to use deep neural network representations to generate synthetic brain data and to show that it helps in improving subsequent stimuli decoding task accuracy."], "linguistic theories, cognitive modeling and psycholinguistics"], [["topic sensitive attention on generic corpora corrects sense bias in pretrained embeddings", "vihari piratla | sunita sarawagi | soumen chakrabarti", "given a small corpus d_t pertaining to a limited set of focused topics, our goal is to train embeddings that accurately capture the sense of words in the topic in spite of the limited size of d_t. these embeddings may be used in various tasks involving d_t. a popular strategy in limited data settings is to adapt pretrained embeddings e trained on a large corpus. to correct for sense drift, fine-tuning, regularization, projection, and pivoting have been proposed recently. among these, regularization informed by a word\u2019s corpus frequency performed well, but we improve upon it using a new regularizer based on the stability of its cooccurrence with other words. however, a thorough comparison across ten topics, spanning three tasks, with standardized settings of hyper-parameters, reveals that even the best embedding adaptation strategies provide small gains beyond well-tuned baselines, which many earlier comparisons ignored. in a bold departure from adapting pretrained embeddings, we propose using d_t to probe, attend to, and borrow fragments from any large, topic-rich source corpus (such as wikipedia), which need not be the corpus used to pretrain embeddings. this step is made scalable and practical by suitable indexing. we reach the surprising conclusion that even limited corpus augmentation is more useful than adapting embeddings, which suggests that non-dominant sense information may be irrevocably obliterated from pretrained embeddings and cannot be salvaged by adaptation."], "semantics"], [["incorporating external knowledge through pre-training for natural language to code generation", "frank f. xu | zhengbao jiang | pengcheng yin | bogdan vasilescu | graham neubig", "open-domain code generation aims to generate code in a general-purpose programming language (such as python) from natural language (nl) intents. motivated by the intuition that developers usually retrieve resources on the web when writing code, we explore the effectiveness of incorporating two varieties of external knowledge into nl-to-code generation: automatically mined nl-code pairs from the online programming qa forum stackoverflow and programming language api documentation. our evaluations show that combining the two sources with data augmentation and retrieval-based data re-sampling improves the current state-of-the-art by up to 2.2% absolute bleu score on the code generation testbed conala. the code and resources are available at https://github.com/neulab/external-knowledge-codegen."], "semantics"], [["phone features improve speech translation", "elizabeth salesky | alan w black", "end-to-end models for speech translation (st) more tightly couple speech recognition (asr) and machine translation (mt) than a traditional cascade of separate asr and mt models, with simpler model architectures and the potential for reduced error propagation. their performance is often assumed to be superior, though in many conditions this is not yet the case. we compare cascaded and end-to-end models across high, medium, and low-resource conditions, and show that cascades remain stronger baselines. further, we introduce two methods to incorporate phone features into st models. we show that these features improve both architectures, closing the gap between end-to-end models and cascades, and outperforming previous academic work \u2013 by up to 9 bleu on our low-resource setting."], "speech and multimodality"], [["informative image captioning with external sources of information", "sanqiang zhao | piyush sharma | tomer levinboim | radu soricut", "an image caption should fluently present the essential information in a given image, including informative, fine-grained entity mentions and the manner in which these entities interact. however, current captioning models are usually trained to generate captions that only contain common object names, thus falling short on an important \u201cinformativeness\u201d dimension. we present a mechanism for integrating image information together with fine-grained labels (assumed to be generated by some upstream models) into a caption that describes the image in a fluent and informative manner. we introduce a multimodal, multi-encoder model based on transformer that ingests both image features and multiple sources of entity labels. we demonstrate that we can learn to control the appearance of these entity labels in the output, resulting in captions that are both fluent and informative."], "language grounding to vision, robotics and beyond"], [["kdconv: a chinese multi-domain dialogue dataset towards multi-turn knowledge-driven conversation", "hao zhou | chujie zheng | kaili huang | minlie huang | xiaoyan zhu", "the research of knowledge-driven conversational systems is largely limited due to the lack of dialog data which consists of multi-turn conversations on multiple topics and with knowledge annotations. in this paper, we propose a chinese multi-domain knowledge-driven conversation dataset, kdconv, which grounds the topics in multi-turn conversations to knowledge graphs. our corpus contains 4.5k conversations from three domains (film, music, and travel), and 86k utterances with an average turn number of 19.0. these conversations contain in-depth discussions on related topics and natural transition between multiple topics. to facilitate the following research on this corpus, we provide several benchmark models. comparative results show that the models can be enhanced by introducing background knowledge, yet there is still a large space for leveraging knowledge to model multi-turn conversations for further research. results also show that there are obvious performance differences between different domains, indicating that it is worth further explore transfer learning and domain adaptation. the corpus and benchmark models are publicly available."], "dialogue and interactive systems"], [["max-margin incremental ccg parsing", "milo\u0161 stanojevi\u0107 | mark steedman", "incremental syntactic parsing has been an active research area both for cognitive scientists trying to model human sentence processing and for nlp researchers attempting to combine incremental parsing with language modelling for asr and mt. most effort has been directed at designing the right transition mechanism, but less has been done to answer the question of what a probabilistic model for those transition parsers should look like. a very incremental transition mechanism of a recently proposed ccg parser when trained in straightforward locally normalised discriminative fashion produces very bad results on english ccgbank. we identify three biases as the causes of this problem: label bias, exposure bias and imbalanced probabilities bias. while known techniques for tackling these biases improve results, they still do not make the parser state of the art. instead, we tackle all of these three biases at the same time using an improved version of beam search optimisation that minimises all beam search violations instead of minimising only the biggest violation. the new incremental parser gives better results than all previously published incremental ccg parsers, and outperforms even some widely used non-incremental ccg parsers."], "tagging, chunking, syntax and parsing"], [["complex word identification as a sequence labelling task", "sian gooding | ekaterina kochmar", "complex word identification (cwi) is concerned with detection of words in need of simplification and is a crucial first step in a simplification pipeline. it has been shown that reliable cwi systems considerably improve text simplification. however, most cwi systems to date address the task on a word-by-word basis, not taking the context into account. in this paper, we present a novel approach to cwi based on sequence modelling. our system is capable of performing cwi in context, does not require extensive feature engineering and outperforms state-of-the-art systems on this task."], "nlp applications"], [["distant learning for entity linking with automatic noise detection", "phong le | ivan titov", "accurate entity linkers have been produced for domains and languages where annotated data (i.e., texts linked to a knowledge base) is available. however, little progress has been made for the settings where no or very limited amounts of labeled data are present (e.g., legal or most scientific domains). in this work, we show how we can learn to link mentions without having any labeled examples, only a knowledge base and a collection of unannotated texts from the corresponding domain. in order to achieve this, we frame the task as a multi-instance learning problem and rely on surface matching to create initial noisy labels. as the learning signal is weak and our surrogate labels are noisy, we introduce a noise detection component in our model: it lets the model detect and disregard examples which are likely to be noisy. our method, jointly learning to detect noise and link entities, greatly outperforms the surface matching baseline. for a subset of entity categories, it even approaches the performance of supervised learning."], "machine learning for nlp"], [["jw300: a wide-coverage parallel corpus for low-resource languages", "\u017eeljko agi\u0107 | ivan vuli\u0107", "viable cross-lingual transfer critically depends on the availability of parallel texts. shortage of such resources imposes a development and evaluation bottleneck in multilingual processing. we introduce jw300, a parallel corpus of over 300 languages with around 100 thousand parallel sentences per language pair on average. in this paper, we present the resource and showcase its utility in experiments with cross-lingual word embedding induction and multi-source part-of-speech projection."], "machine translation and multilinguality"], [["relabel the noise: joint extraction of entities and relations via cooperative multiagents", "daoyuan chen | yaliang li | kai lei | ying shen", "distant supervision based methods for entity and relation extraction have received increasing popularity due to the fact that these methods require light human annotation efforts. in this paper, we consider the problem of shifted label distribution, which is caused by the inconsistency between the noisy-labeled training set subject to external knowledge graph and the human-annotated test set, and exacerbated by the pipelined entity-then-relation extraction manner with noise propagation. we propose a joint extraction approach to address this problem by re-labeling noisy instances with a group of cooperative multiagents. to handle noisy instances in a fine-grained manner, each agent in the cooperative group evaluates the instance by calculating a continuous confidence score from its own perspective; to leverage the correlations between these two extraction tasks, a confidence consensus module is designed to gather the wisdom of all agents and re-distribute the noisy training set with confidence-scored labels. further, the confidences are used to adjust the training losses of extractors. experimental results on two real-world datasets verify the benefits of re-labeling noisy instance, and show that the proposed model significantly outperforms the state-of-the-art entity and relation extraction methods."], "information extraction, retrieval and text mining"], [["chid: a large-scale chinese idiom dataset for cloze test", "chujie zheng | minlie huang | aixin sun", "cloze-style reading comprehension in chinese is still limited due to the lack of various corpora. in this paper we propose a large-scale chinese cloze test dataset chid, which studies the comprehension of idiom, a unique language phenomenon in chinese. in this corpus, the idioms in a passage are replaced by blank symbols and the correct answer needs to be chosen from well-designed candidate idioms. we carefully study how the design of candidate idioms and the representation of idioms affect the performance of state-of-the-art models. results show that the machine accuracy is substantially worse than that of human, indicating a large space for further research."], "resources and evaluation"], [["enhancing cross-target stance detection with transferable semantic-emotion knowledge", "bowen zhang | min yang | xutao li | yunming ye | xiaofei xu | kuai dai", "stance detection is an important task, which aims to classify the attitude of an opinionated text towards a given target. remarkable success has been achieved when sufficient labeled training data is available. however, annotating sufficient data is labor-intensive, which establishes significant barriers for generalizing the stance classifier to the data with new targets. in this paper, we proposed a semantic-emotion knowledge transferring (sekt) model for cross-target stance detection, which uses the external knowledge (semantic and emotion lexicons) as a bridge to enable knowledge transfer across different targets. specifically, a semantic-emotion heterogeneous graph is constructed from external semantic and emotion lexicons, which is then fed into a graph convolutional network to learn multi-hop semantic connections between words and emotion tags. then, the learned semantic-emotion graph representation, which serves as prior knowledge bridging the gap between the source and target domains, is fully integrated into the bidirectional long short-term memory (bilstm) stance classifier by adding a novel knowledge-aware memory unit to the bilstm cell. extensive experiments on a large real-world dataset demonstrate the superiority of sekt against the state-of-the-art baseline methods."], "sentiment analysis, stylistic analysis, and argument mining"], [["improving document representations by generating pseudo query embeddings for dense retrieval", "hongyin tang | xingwu sun | beihong jin | jingang wang | fuzheng zhang | wei wu", "recently, the retrieval models based on dense representations have been gradually applied in the first stage of the document retrieval tasks, showing better performance than traditional sparse vector space models. to obtain high efficiency, the basic structure of these models is bi-encoder in most cases. however, this simple structure may cause serious information loss during the encoding of documents since the queries are agnostic. to address this problem, we design a method to mimic the queries to each of the documents by an iterative clustering process and represent the documents by multiple pseudo queries (i.e., the cluster centroids). to boost the retrieval process using approximate nearest neighbor search library, we also optimize the matching function with a two-step score calculation procedure. experimental results on several popular ranking and qa datasets show that our model can achieve state-of-the-art results while still remaining high efficiency."], "information extraction, retrieval and text mining"], [["feature projection for improved text classification", "qi qin | wenpeng hu | bing liu", "in classification, there are usually some good features that are indicative of class labels. for example, in sentiment classification, words like good and nice are indicative of the positive sentiment and words like bad and terrible are indicative of the negative sentiment. however, there are also many common features (e.g., words) that are not indicative of any specific class (e.g., voice and screen, which are common to both sentiment classes and are not discriminative for classification). although deep learning has made significant progresses in generating discriminative features through its powerful representation learning, we believe there is still room for improvement. in this paper, we propose a novel angle to further improve this representation learning, i.e., feature projection. this method projects existing features into the orthogonal space of the common features. the resulting projection is thus perpendicular to the common features and more discriminative for classification. we apply this new method to improve cnn, rnn, transformer, and bert based text classification and obtain markedly better results."], "information extraction, retrieval and text mining"], [["mapping natural language instructions to mobile ui action sequences", "yang li | jiacong he | xin zhou | yuan zhang | jason baldridge", "we present a new problem: grounding natural language instructions to mobile user interface actions, and create three new datasets for it. for full task evaluation, we create pixelhelp, a corpus that pairs english instructions with actions performed by people on a mobile ui emulator. to scale training, we decouple the language and action data by (a) annotating action phrase spans in how-to instructions and (b) synthesizing grounded descriptions of actions for mobile user interfaces. we use a transformer to extract action phrase tuples from long-range natural language instructions. a grounding transformer then contextually represents ui objects using both their content and screen position and connects them to object descriptions. given a starting screen and instruction, our model achieves 70.59% accuracy on predicting complete ground-truth action sequences in pixelhelp."], "language grounding to vision, robotics and beyond"], [["enhancing topic-to-essay generation with external commonsense knowledge", "pengcheng yang | lei li | fuli luo | tianyu liu | xu sun", "automatic topic-to-essay generation is a challenging task since it requires generating novel, diverse, and topic-consistent paragraph-level text with a set of topics as input. previous work tends to perform essay generation based solely on the given topics while ignoring massive commonsense knowledge. however, this commonsense knowledge provides additional background information, which can help to generate essays that are more novel and diverse. towards filling this gap, we propose to integrate commonsense from the external knowledge base into the generator through dynamic memory mechanism. besides, the adversarial training based on a multi-label discriminator is employed to further improve topic-consistency. we also develop a series of automatic evaluation metrics to comprehensively assess the quality of the generated essay. experiments show that with external commonsense knowledge and adversarial training, the generated essays are more novel, diverse, and topic-consistent than existing methods in terms of both automatic and human evaluation."], "generation"], [["recurrent chunking mechanisms for long-text machine reading comprehension", "hongyu gong | yelong shen | dian yu | jianshu chen | dong yu", "in this paper, we study machine reading comprehension (mrc) on long texts: where a model takes as inputs a lengthy document and a query, extracts a text span from the document as an answer. state-of-the-art models (e.g., bert) tend to use a stack of transformer layers that are pre-trained from a large number of unlabeled language corpora to encode the joint contextual information of query and document. however, these transformer models can only take as input a fixed-length (e.g., 512) text. to deal with even longer text inputs, previous approaches usually chunk them into equally-spaced segments and predict answers based on each segment independently without considering the information from other segments. as a result, they may form segments that fail to cover complete answers or retain insufficient contexts around the correct answer required for question answering. moreover, they are less capable of answering questions that need cross-segment information. we propose to let a model learn to chunk in a more flexible way via reinforcement learning: a model can decide the next segment that it wants to process in either direction. we also apply recurrent mechanisms to enable information to flow across segments. experiments on three mrc tasks \u2013 coqa, quac, and triviaqa \u2013 demonstrate the effectiveness of our proposed recurrent chunking mechanisms: we can obtain segments that are more likely to contain complete answers and at the same time provide sufficient contexts around the ground truth answers for better predictions."], "question answering"], [["advaug: robust adversarial augmentation for neural machine translation", "yong cheng | lu jiang | wolfgang macherey | jacob eisenstein", "in this paper, we propose a new adversarial augmentation method for neural machine translation (nmt). the main idea is to minimize the vicinal risk over virtual sentences sampled from two vicinity distributions, in which the crucial one is a novel vicinity distribution for adversarial sentences that describes a smooth interpolated embedding space centered around observed training sentence pairs. we then discuss our approach, advaug, to train nmt models using the embeddings of virtual sentences in sequence-to-sequence learning. experiments on chinese-english, english-french, and english-german translation benchmarks show that advaug achieves significant improvements over thetransformer (up to 4.9 bleu points), and substantially outperforms other data augmentation techniques (e.g.back-translation) without using extra corpora."], "machine translation and multilinguality"], [["privacy at scale: introducing the privaseer corpus of web privacy policies", "mukund srinath | shomir wilson | c lee giles", "organisations disclose their privacy practices by posting privacy policies on their websites. even though internet users often care about their digital privacy, they usually do not read privacy policies, since understanding them requires a significant investment of time and effort. natural language processing has been used to create experimental tools to interpret privacy policies, but there has been a lack of large privacy policy corpora to facilitate the creation of large-scale semi-supervised and unsupervised models to interpret and simplify privacy policies. thus, we present the privaseer corpus of 1,005,380 english language website privacy policies collected from the web. the number of unique websites represented in privaseer is about ten times larger than the next largest public collection of web privacy policies, and it surpasses the aggregate of unique websites represented in all other publicly available privacy policy corpora combined. we describe a corpus creation pipeline with stages that include a web crawler, language detection, document classification, duplicate and near-duplicate removal, and content extraction. we employ an unsupervised topic modelling approach to investigate the contents of policy documents in the corpus and discuss the distribution of topics in privacy policies at web scale. we further investigate the relationship between privacy policy domain pageranks and text features of the privacy policies. finally, we use the corpus to pretrain privbert, a transformer-based privacy policy language model, and obtain state of the art results on the data practice classification and question answering tasks."], "resources and evaluation"], [["distinguish confusing law articles for legal judgment prediction", "nuo xu | pinghui wang | long chen | li pan | xiaoyan wang | junzhou zhao", "legal judgement prediction (ljp) is the task of automatically predicting a law case\u2019s judgment results given a text describing the case\u2019s facts, which has great prospects in judicial assistance systems and handy services for the public. in practice, confusing charges are often presented, because law cases applicable to similar law articles are easily misjudged. to address this issue, existing work relies heavily on domain experts, which hinders its application in different law systems. in this paper, we present an end-to-end model, ladan, to solve the task of ljp. to distinguish confusing charges, we propose a novel graph neural network, gdl, to automatically learn subtle differences between confusing law articles, and also design a novel attention mechanism that fully exploits the learned differences to attentively extract effective discriminative features from fact descriptions. experiments conducted on real-world datasets demonstrate the superiority of our ladan."], "nlp applications"], [["camouflaged chinese spam content detection with semi-supervised generative active learning", "zhuoren jiang | zhe gao | yu duan | yangyang kang | changlong sun | qiong zhang | xiaozhong liu", "we propose a semi-supervised generative active learning (signal) model to address the imbalance, efficiency, and text camouflage problems of chinese text spam detection task. a \u201cself-diversity\u201d criterion is proposed for measuring the \u201cworthiness\u201d of a candidate for annotation. a semi-supervised variational autoencoder with masked attention learning approach and a character variation graph-enhanced augmentation procedure are proposed for data augmentation. the preliminary experiment demonstrates the proposed signal model is not only sensitive to spam sample selection, but also can improve the performance of a series of conventional active learning models for chinese spam detection task. to the best of our knowledge, this is the first work to integrate active learning and semi-supervised generative learning for text spam detection."], "nlp applications"], [["dataset creation for ranking constructive news comments", "soichiro fujita | hayato kobayashi | manabu okumura", "ranking comments on an online news service is a practically important task for the service provider, and thus there have been many studies on this task. however, most of them considered users\u2019 positive feedback, such as \u201clike\u201d-button clicks, as a quality measure. in this paper, we address directly evaluating the quality of comments on the basis of \u201cconstructiveness,\u201d separately from user feedback. to this end, we create a new dataset including 100k+ japanese comments with constructiveness scores (c-scores). our experiments clarify that c-scores are not always related to users\u2019 positive feedback, and the performance of pairwise ranking models tends to be enhanced by the variation of comments rather than articles."], "computational social science, social media and cultural analytics"], [["emerging cross-lingual structure in pretrained language models", "alexis conneau | shijie wu | haoran li | luke zettlemoyer | veselin stoyanov", "we study the problem of multilingual masked language modeling, i.e. the training of a single model on concatenated text from multiple languages, and present a detailed study of several factors that influence why these models are so effective for cross-lingual transfer. we show, contrary to what was previously hypothesized, that transfer is possible even when there is no shared vocabulary across the monolingual corpora and also when the text comes from very different domains. the only requirement is that there are some shared parameters in the top layers of the multi-lingual encoder. to better understand this result, we also show that representations from monolingual bert models in different languages can be aligned post-hoc quite effectively, strongly suggesting that, much like for non-contextual word embeddings, there are universal latent symmetries in the learned embedding spaces. for multilingual masked language modeling, these symmetries are automatically discovered and aligned during the joint training process."], "semantics"], [["hierarchical transformers for multi-document summarization", "yang liu | mirella lapata", "in this paper, we develop a neural summarization model which can effectively process multiple input documents and distill transformer architecture with the ability to encode documents in a hierarchical manner. we represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. empirical results on the wikisum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines."], "summarization"], [["improved language modeling by decoding the past", "siddhartha brahma", "highly regularized lstms achieve impressive results on several benchmark datasets in language modeling. we propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token. this biases the model towards retaining more contextual information, in turn improving its ability to predict the next token. with negligible overhead in the number of parameters and training time, our past decode regularization (pdr) method improves perplexity on the penn treebank dataset by up to 1.8 points and by up to 2.3 points on the wikitext-2 dataset, over strong regularized baselines using a single softmax. with a mixture-of-softmax model, we show gains of up to 1.0 perplexity points on these datasets. in addition, our method achieves 1.169 bits-per-character on the penn treebank character dataset for character level language modeling."], "machine learning for nlp"], [["when a good translation is wrong in context: context-aware machine translation improves on deixis, ellipsis, and lexical cohesion", "elena voita | rico sennrich | ivan titov", "though machine translation errors caused by the lack of context beyond one sentence have long been acknowledged, the development of context-aware nmt systems is hampered by several problems. firstly, standard metrics are not sensitive to improvements in consistency in document-level translations. secondly, previous work on context-aware nmt assumed that the sentence-aligned parallel data consisted of complete documents while in most practical scenarios such document-level data constitutes only a fraction of the available parallel data. to address the first issue, we perform a human study on an english-russian subtitles dataset and identify deixis, ellipsis and lexical cohesion as three main sources of inconsistency. we then create test sets targeting these phenomena. to address the second shortcoming, we consider a set-up in which a much larger amount of sentence-level data is available compared to that aligned at the document level. we introduce a model that is suitable for this scenario and demonstrate major gains over a context-agnostic baseline on our new benchmarks without sacrificing performance as measured with bleu."], "machine translation and multilinguality"], [["multi-hop reading comprehension through question decomposition and rescoring", "sewon min | victor zhong | luke zettlemoyer | hannaneh hajishirzi", "multi-hop reading comprehension (rc) requires reasoning and aggregation across several paragraphs. we propose a system for multi-hop rc that decomposes a compositional question into simpler sub-questions that can be answered by off-the-shelf single-hop rc models. since annotations for such decomposition are expensive, we recast subquestion generation as a span prediction problem and show that our method, trained using only 400 labeled examples, generates sub-questions that are as effective as human-authored sub-questions. we also introduce a new global rescoring approach that considers each decomposition (i.e. the sub-questions and their answers) to select the best final answer, greatly improving overall performance. our experiments on hotpotqa show that this approach achieves the state-of-the-art results, while providing explainable evidence for its decision making in the form of sub-questions."], "question answering"], [["soft contextual data augmentation for neural machine translation", "fei gao | jinhua zhu | lijun wu | yingce xia | tao qin | xueqi cheng | wengang zhou | tie-yan liu", "while data augmentation is an important trick to boost the accuracy of deep learning methods in computer vision tasks, its study in natural language tasks is still very limited. in this paper, we present a novel data augmentation method for neural machine translation.different from previous augmentation methods that randomly drop, swap or replace words with other words in a sentence, we softly augment a randomly chosen word in a sentence by its contextual mixture of multiple related words. more accurately, we replace the one-hot representation of a word by a distribution (provided by a language model) over the vocabulary, i.e., replacing the embedding of this word by a weighted combination of multiple semantically similar words. since the weights of those words depend on the contextual information of the word to be replaced,the newly generated sentences capture much richer information than previous augmentation methods. experimental results on both small scale and large scale machine translation data sets demonstrate the superiority of our method over strong baselines."], "machine learning for nlp"], [["double-hard debias: tailoring word embeddings for gender bias mitigation", "tianlu wang | xi victoria lin | nazneen fatema rajani | bryan mccann | vicente ordonez | caiming xiong", "word embeddings derived from human-generated corpora inherit strong gender bias which can be further amplified by downstream models. some commonly adopted debiasing approaches, including the seminal hard debias algorithm, apply post-processing procedures that project pre-trained word embeddings into a subspace orthogonal to an inferred gender subspace. we discover that semantic-agnostic corpus regularities such as word frequency captured by the word embeddings negatively impact the performance of these algorithms. we propose a simple but effective technique, double hard debias, which purifies the word embeddings against such corpus regularities prior to inferring and removing the gender subspace. experiments on three bias mitigation benchmarks show that our approach preserves the distributional semantics of the pre-trained word embeddings while reducing gender bias to a significantly larger degree than prior approaches."], "ethics in nlp"], [["tetra-tagging: word-synchronous parsing with linear-time inference", "nikita kitaev | dan klein", "we present a constituency parsing algorithm that, like a supertagger, works by assigning labels to each word in a sentence. in order to maximally leverage current neural architectures, the model scores each word\u2019s tags in parallel, with minimal task-specific structure. after scoring, a left-to-right reconciliation phase extracts a tree in (empirically) linear time. our parser achieves 95.4 f1 on the wsj test set while also achieving substantial speedups compared to current state-of-the-art parsers with comparable accuracies."], "tagging, chunking, syntax and parsing"], [["unsupervised neural text simplification", "sai surya | abhijit mishra | anirban laha | parag jain | karthik sankaranarayanan", "the paper presents a first attempt towards unsupervised neural text simplification that relies only on unlabeled text corpora. the core framework is composed of a shared encoder and a pair of attentional-decoders, crucially assisted by discrimination-based losses and denoising. the framework is trained using unlabeled text collected from en-wikipedia dump. our analysis (both quantitative and qualitative involving human evaluators) on public test data shows that the proposed model can perform text-simplification at both lexical and syntactic levels, competitive to existing supervised methods. it also outperforms viable unsupervised baselines. adding a few labeled pairs helps improve the performance further."], "generation"], [["visually grounded neural syntax acquisition", "haoyue shi | jiayuan mao | kevin gimpel | karen livescu", "we present the visually grounded neural syntax learner (vg-nsl), an approach for learning syntactic representations and structures without any explicit supervision. the model learns by looking at natural images and reading paired captions. vg-nsl generates constituency parse trees of texts, recursively composes representations for constituents, and matches them with images. we define concreteness of constituents by their matching scores with images, and use it to guide the parsing of text. experiments on the mscoco data set show that vg-nsl outperforms various unsupervised parsing approaches that do not use visual grounding, in terms of f1 scores against gold parse trees. we find that vgnsl is much more stable with respect to the choice of random initialization and the amount of training data. we also find that the concreteness acquired by vg-nsl correlates well with a similar measure defined by linguists. finally, we also apply vg-nsl to multiple languages in the multi30k data set, showing that our model consistently outperforms prior unsupervised approaches."], "language grounding to vision, robotics and beyond"], [["detecting perceived emotions in hurricane disasters", "shrey desai | cornelia caragea | junyi jessy li", "natural disasters (e.g., hurricanes) affect millions of people each year, causing widespread destruction in their wake. people have recently taken to social media websites (e.g., twitter) to share their sentiments and feelings with the larger community. consequently, these platforms have become instrumental in understanding and perceiving emotions at scale. in this paper, we introduce hurricaneemo, an emotion dataset of 15,000 english tweets spanning three hurricanes: harvey, irma, and maria. we present a comprehensive study of fine-grained emotions and propose classification tasks to discriminate between coarse-grained emotion groups. our best bert model, even after task-guided pre-training which leverages unlabeled twitter data, achieves only 68% accuracy (averaged across all groups). hurricaneemo serves not only as a challenging benchmark for models but also as a valuable resource for analyzing emotions in disaster-centric domains."], "computational social science, social media and cultural analytics"], [["graph based neural networks for event factuality prediction using syntactic and semantic structures", "amir pouran ben veyseh | thien huu nguyen | dejing dou", "event factuality prediction (efp) is the task of assessing the degree to which an event mentioned in a sentence has happened. for this task, both syntactic and semantic information are crucial to identify the important context words. the previous work for efp has only combined these information in a simple way that cannot fully exploit their coordination. in this work, we introduce a novel graph-based neural network for efp that can integrate the semantic and syntactic information more effectively. our experiments demonstrate the advantage of the proposed model for efp."], "information extraction, retrieval and text mining"], [["quase: question-answer driven sentence encoding", "hangfeng he | qiang ning | dan roth", "question-answering (qa) data often encodes essential information in many facets. this paper studies a natural question: can we get supervision from qa data for other tasks (typically, non-qa ones)? for example, can we use qamr (michael et al., 2017) to improve named entity recognition? we suggest that simply further pre-training bert is often not the best option, and propose the question-answer driven sentence encoding (quase) framework. quase learns representations from qa data, using bert or other state-of-the-art contextual language models. in particular, we observe the need to distinguish between two types of sentence encodings, depending on whether the target task is a single- or multi-sentence input; in both cases, the resulting encoding is shown to be an easy-to-use plugin for many downstream tasks. this work may point out an alternative way to supervise nlp tasks."], "semantics"], [["paperrobot: incremental draft generation of scientific ideas", "qingyun wang | lifu huang | zhiying jiang | kevin knight | heng ji | mohit bansal | yi luan", "we present a paperrobot who performs as an automatic research assistant by (1) conducting deep understanding of a large collection of human-written papers in a target domain and constructing comprehensive background knowledge graphs (kgs); (2) creating new ideas by predicting links from the background kgs, by combining graph attention and contextual text attention; (3) incrementally writing some key elements of a new paper based on memory-attention networks: from the input title along with predicted related entities to generate a paper abstract, from the abstract to generate conclusion and future work, and finally from future work to generate a title for a follow-on paper. turing tests, where a biomedical domain expert is asked to compare a system output and a human-authored string, show paperrobot generated abstracts, conclusion and future work sections, and new titles are chosen over human-written ones up to 30%, 24% and 12% of the time, respectively."], "generation"], [["tchebycheff procedure for multi-task text classification", "yuren mao | shuang yun | weiwei liu | bo du", "multi-task learning methods have achieved great progress in text classification. however, existing methods assume that multi-task text classification problems are convex multiobjective optimization problems, which is unrealistic in real-world applications. to address this issue, this paper presents a novel tchebycheff procedure to optimize the multi-task classification problems without convex assumption. the extensive experiments back up our theoretical analysis and validate the superiority of our proposals."], "machine learning for nlp"], [["a systematic assessment of syntactic generalization in neural language models", "jennifer hu | jon gauthier | peng qian | ethan wilcox | roger levy", "while state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge. furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations. we present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 english-language syntactic test suites. we find substantial differences in syntactic generalization performance by model architecture, with sequential models underperforming other architectures. factorially manipulating model architecture and training dataset size (1m-40m words), we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments. our results also reveal a dissociation between perplexity and syntactic generalization performance."], "linguistic theories, cognitive modeling and psycholinguistics"], [["know more about each other: evolving dialogue strategy via compound assessment", "siqi bao | huang he | fan wang | rongzhong lian | hua wu", "in this paper, a novel generation-evaluation framework is developed for multi-turn conversations with the objective of letting both participants know more about each other. for the sake of rational knowledge utilization and coherent conversation flow, a dialogue strategy which controls knowledge selection is instantiated and continuously adapted via reinforcement learning. under the deployed strategy, knowledge grounded conversations are conducted with two dialogue agents. the generated dialogues are comprehensively evaluated on aspects like informativeness and coherence, which are aligned with our objective and human instinct. these assessments are integrated as a compound reward to guide the evolution of dialogue strategy via policy gradient. comprehensive experiments have been carried out on the publicly available dataset, demonstrating that the proposed method outperforms the other state-of-the-art approaches significantly."], "dialogue and interactive systems"], [["meta-transfer learning for code-switched speech recognition", "genta indra winata | samuel cahyawijaya | zhaojiang lin | zihan liu | peng xu | pascale fung", "an increasing number of people in the world today speak a mixed-language as a result of being multilingual. however, building a speech recognition system for code-switching remains difficult due to the availability of limited resources and the expense and significant effort required to collect mixed-language data. we therefore propose a new learning method, meta-transfer learning, to transfer learn on a code-switched speech recognition system in a low-resource setting by judiciously extracting information from high-resource monolingual datasets. our model learns to recognize individual languages, and transfer them so as to better recognize mixed-language speech by conditioning the optimization on the code-switching data. based on experimental results, our model outperforms existing baselines on speech recognition and language modeling tasks, and is faster to converge."], "speech and multimodality"], [["benefits of intermediate annotations in reading comprehension", "dheeru dua | sameer singh | matt gardner", "complex compositional reading comprehension datasets require performing latent sequential decisions that are learned via supervision from the final answer. a large combinatorial space of possible decision paths that result in the same answer, compounded by the lack of intermediate supervision to help choose the right path, makes the learning particularly hard for this task. in this work, we study the benefits of collecting intermediate reasoning supervision along with the answer during data collection. we find that these intermediate annotations can provide two-fold benefits. first, we observe that for any collection budget, spending a fraction of it on intermediate annotations results in improved model performance, for two complex compositional datasets: drop and quoref. second, these annotations encourage the model to learn the correct latent reasoning steps, helping combat some of the biases introduced during the data collection process."], "question answering"], [["can sequence-to-sequence models crack substitution ciphers?", "nada aldarrab | jonathan may", "decipherment of historical ciphers is a challenging problem. the language of the target plaintext might be unknown, and ciphertext can have a lot of noise. state-of-the-art decipherment methods use beam search and a neural language model to score candidate plaintext hypotheses for a given cipher, assuming the plaintext language is known. we propose an end-to-end multilingual model for solving simple substitution ciphers. we test our model on synthetic and real historical ciphers and show that our proposed method can decipher text without explicit language identification while still being robust to noise."], "machine translation and multilinguality"], [["expbert: representation engineering with natural language explanations", "shikhar murty | pang wei koh | percy liang", "suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text. in this paper, we allow model developers to specify these types of inductive biases as natural language explanations. we use bert fine-tuned on multinli to \u201cinterpret\u201d these explanations with respect to the input sentence, producing explanation-guided representations of the input. across three relation extraction tasks, our method, expbert, matches a bert baseline but with 3\u201320x less labeled data and improves on the baseline by 3\u201310 f1 points with the same amount of labeled data."], "machine learning for nlp"], [["a joint neural model for information extraction with global features", "ying lin | heng ji | fei huang | lingfei wu", "most existing joint neural models for information extraction (ie) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. for example, a victim of a die event is likely to be a victim of an attack event in the same sentence. in order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, oneie, that aims to extract the globally optimal ie result as a graph from an input sentence. oneie performs end-to-end ie in four stages: (1) encoding a given sentence as contextualized word representations; (2) identifying entity mentions and event triggers as nodes; (3) computing label scores for all nodes and their pairwise links using local classifiers; (4) searching for the globally optimal graph with a beam decoder. at the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks. in addition, as oneie does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner."], "information extraction, retrieval and text mining"], [["learning constraints for structured prediction using rectifier networks", "xingyuan pan | maitrey mehta | vivek srikumar", "various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions. past work has shown that domain knowledge, framed as constraints over the output space, can help improve predictive accuracy. however, designing good constraints often relies on domain expertise. in this paper, we study the problem of learning such constraints. we frame the problem as that of training a two-layer rectifier network to identify valid structures or substructures, and show a construction for converting a trained network into a system of linear constraints over the inference variables. our experiments on several nlp tasks show that the learned constraints can improve the prediction accuracy, especially when the number of training examples is small."], "machine learning for nlp"], [["robust encodings: a framework for combating adversarial typos", "erik jones | robin jia | aditi raghunathan | percy liang", "despite excellent performance on many tasks, nlp systems are easily fooled by small adversarial perturbations of inputs. existing procedures to defend against such perturbations are either (i) heuristic in nature and susceptible to stronger attacks or (ii) provide guaranteed robustness to worst-case attacks, but are incompatible with state-of-the-art models like bert. in this work, we introduce robust encodings (roben): a simple framework that confers guaranteed robustness, without making compromises on model architecture. the core component of roben is an encoding function, which maps sentences to a smaller, discrete space of encodings. systems using these encodings as a bottleneck confer guaranteed robustness with standard training, and the same encodings can be used across multiple tasks. we identify two desiderata to construct robust encoding functions: perturbations of a sentence should map to a small set of encodings (stability), and models using encodings should still perform well (fidelity). we instantiate roben to defend against a large family of adversarial typos. across six tasks from glue, our instantiation of roben paired with bert achieves an average robust accuracy of 71.3% against all adversarial typos in the family considered, while previous work using a typo-corrector achieves only 35.3% accuracy against a simple greedy attack."], "machine learning for nlp"], [["exploiting entity bio tag embeddings and multi-task learning for relation extraction with imbalanced data", "wei ye | bo li | rui xie | zhonghao sheng | long chen | shikun zhang", "in practical scenario, relation extraction needs to first identify entity pairs that have relation and then assign a correct relation class. however, the number of non-relation entity pairs in context (negative instances) usually far exceeds the others (positive instances), which negatively affects a model\u2019s performance. to mitigate this problem, we propose a multi-task architecture which jointly trains a model to perform relation identification with cross-entropy loss and relation classification with ranking loss. meanwhile, we observe that a sentence may have multiple entities and relation mentions, and the patterns in which the entities appear in a sentence may contain useful semantic information that can be utilized to distinguish between positive and negative instances. thus we further incorporate the embeddings of character-wise/word-wise bio tag from the named entity recognition task into character/word embeddings to enrich the input representation. experiment results show that our proposed approach can significantly improve the performance of a baseline model with more than 10% absolute increase in f1-score, and outperform the state-of-the-art models on ace 2005 chinese and english corpus. moreover, bio tag embeddings are particularly effective and can be used to improve other models as well."], "information extraction, retrieval and text mining"], [["towards improving neural named entity recognition with gazetteers", "tianyu liu | jin-ge yao | chin-yew lin", "most of the recently proposed neural models for named entity recognition have been purely data-driven, with a strong emphasis on getting rid of the efforts for collecting external resources or designing hand-crafted features. this could increase the chance of overfitting since the models cannot access any supervision signal beyond the small amount of annotated data, limiting their power to generalize beyond the annotated entities. in this work, we show that properly utilizing external gazetteers could benefit segmental neural ner models. we add a simple module on the recently proposed hybrid semi-markov crf architecture and observe some promising results."], "information extraction, retrieval and text mining"], [["domain adaptation of neural machine translation by lexicon induction", "junjie hu | mengzhou xia | graham neubig | jaime carbonell", "it has been previously noted that neural machine translation (nmt) is very sensitive to domain shift. in this paper, we argue that this is a dual effect of the highly lexicalized nature of nmt, resulting in failure for sentences with large numbers of unknown words, and lack of supervision for domain-specific words. to remedy this problem, we propose an unsupervised adaptation method which fine-tunes a pre-trained out-of-domain nmt model using a pseudo-in-domain corpus. specifically, we perform lexicon induction to extract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences. in five domains over twenty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 bleu over unadapted models, and up to 2 bleu over strong back-translation baselines."], "machine translation and multilinguality"], [["selection bias explorations and debias methods for natural language sentence matching datasets", "guanhua zhang | bing bai | jian liang | kun bai | shiyu chang | mo yu | conghui zhu | tiejun zhao", "natural language sentence matching (nlsm) has gained substantial attention from both academics and the industry, and rich public datasets contribute a lot to this process. however, biased datasets can also hurt the generalization performance of trained models and give untrustworthy evaluation results. for many nlsm datasets, the providers select some pairs of sentences into the datasets, and this sampling procedure can easily bring unintended pattern, i.e., selection bias. one example is the quoraqp dataset, where some content-independent naive features are unreasonably predictive. such features are the reflection of the selection bias and termed as the \u201cleakage features.\u201d in this paper, we investigate the problem of selection bias on six nlsm datasets and find that four out of them are significantly biased. we further propose a training and evaluation framework to alleviate the bias. experimental results on quoraqp suggest that the proposed framework can improve the generalization ability of trained models, and give more trustworthy evaluation results for real-world adoptions."], "machine learning for nlp"], [["joint modelling of emotion and abusive language detection", "santhosh rajamanickam | pushkar mishra | helen yannakoudakis | ekaterina shutova", "the rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. aiming to tackle this problem, the natural language processing (nlp) community has experimented with a range of techniques for abuse detection. while achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language. the latter is, however, inextricably linked to abusive behaviour. in this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other. our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets."], "nlp applications"], [["unified semantic parsing with weak supervision", "priyanka agrawal | ayushi dalmia | parag jain | abhishek bansal | ashish mittal | karthik sankaranarayanan", "semantic parsing over multiple knowledge bases enables a parser to exploit structural similarities of programs across the multiple domains. however, the fundamental challenge lies in obtaining high-quality annotations of (utterance, program) pairs across various domains needed for training such models. to overcome this, we propose a novel framework to build a unified multi-domain enabled semantic parser trained only with weak supervision (denotations). weakly supervised training is particularly arduous as the program search space grows exponentially in a multi-domain setting. to solve this, we incorporate a multi-policy distillation mechanism in which we first train domain-specific semantic parsers (teachers) using weak supervision in the absence of the ground truth programs, followed by training a single unified parser (student) from the domain specific policies obtained from these teachers. the resultant semantic parser is not only compact but also generalizes better, and generates more accurate programs. it further does not require the user to provide a domain label while querying. on the standard overnight dataset (containing multiple domains), we demonstrate that the proposed model improves performance by 20% in terms of denotation accuracy in comparison to baseline techniques."], "semantics"], [["sparse sequence-to-sequence models", "ben peters | vlad niculae | andr\u00e9 f. t. martins", "sequence-to-sequence models are a powerful workhorse of nlp. most variants employ a softmax transformation in both their attention mechanism and output layer, leading to dense alignments and strictly positive output probabilities. this density is wasteful, making models less interpretable and assigning probability mass to many implausible outputs. in this paper, we propose sparse sequence-to-sequence models, rooted in a new family of \ud835\udefc-entmax transformations, which includes softmax and sparsemax as particular cases, and is sparse for any \ud835\udefc > 1. we provide fast algorithms to evaluate these transformations and their gradients, which scale well for large vocabulary sizes. our models are able to produce sparse alignments and to assign nonzero probability to a short list of plausible outputs, sometimes rendering beam search exact. experiments on morphological inflection and machine translation reveal consistent gains over dense models."], "machine learning for nlp"], [["fluent response generation for conversational question answering", "ashutosh baheti | alan ritter | kevin small", "question answering (qa) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational qa (convqa) subtask. one notable limitation of recent convqa efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (nlg) aspect of high-quality conversational agents. in this work, we propose a method for situating qa responses within a seq2seq nlg approach to generate fluent grammatical answer responses while maintaining correctness. from a technical perspective, we use data augmentation to generate training data for an end-to-end system. specifically, we develop syntactic transformations (sts) to produce question-specific candidate answer responses and rank them using a bert-based classifier (devlin et al., 2019). human evaluation on squad 2.0 data (rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline coqa and quac models in generating conversational responses. we further show our model\u2019s scalability by conducting tests on the coqa dataset. the code and data are available at https://github.com/abaheti95/qadialogsystem."], "generation"], [["shaping visual representations with language for few-shot classification", "jesse mu | percy liang | noah goodman", "by describing the features and abstractions of our world, language is a crucial tool for human learning and a promising source of supervision for machine learning models. we use language to improve few-shot visual classification in the underexplored scenario where natural language task descriptions are available during training, but unavailable for novel tasks at test time. existing models for this setting sample new descriptions at test time and use those to classify images. instead, we propose language-shaped learning (lsl), an end-to-end model that regularizes visual representations to predict language. lsl is conceptually simpler, more data efficient, and outperforms baselines in two challenging few-shot domains."], "language grounding to vision, robotics and beyond"], [["semantically conditioned dialog response generation via hierarchical disentangled self-attention", "wenhu chen | jianshu chen | pengda qin | xifeng yan | william yang wang", "semantically controlled neural response generation on limited-domain has achieved great performance. however, moving towards multi-domain large-scale scenarios are shown to be difficult because the possible combinations of semantic inputs grow exponentially with the number of domains. to alleviate such scalability issue, we exploit the structure of dialog acts to build a multi-layer hierarchical graph, where each act is represented as a root-to-leaf route on the graph. then, we incorporate such graph structure prior as an inductive bias to build a hierarchical disentangled self-attention network, where we disentangle attention heads to model designated nodes on the dialog act graph. by activating different (disentangled) heads at each layer, combinatorially many dialog act semantics can be modeled to control the neural response generation. on the large-scale multi-domain-woz dataset, our model can yield a significant improvement over the baselines on various automatic and human evaluation metrics."], "dialogue and interactive systems"], [["chinesebert: chinese pretraining enhanced by glyph and pinyin information", "zijun sun | xiaoya li | xiaofei sun | yuxian meng | xiang ao | qing he | fei wu | jiwei li", "recent pretraining models in chinese neglect two important aspects specific to the chinese language: glyph and pinyin, which carry significant syntax and semantic information for language understanding. in this work, we propose chinesebert, which incorporates both the glyph and pinyin information of chinese characters into language model pretraining. the glyph embedding is obtained based on different fonts of a chinese character, being able to capture character semantics from the visual features, and the pinyin embedding characterizes the pronunciation of chinese characters, which handles the highly prevalent heteronym phenomenon in chinese (the same character has different pronunciations with different meanings). pretrained on large-scale unlabeled chinese corpus, the proposed chinesebert model yields significant performance boost over baseline models with fewer training steps. the proposed model achieves new sota performances on a wide range of chinese nlp tasks, including machine reading comprehension, natural language inference, text classification, sentence pair matching, and competitive performances in named entity recognition and word segmentation."], "machine learning for nlp"], [["highway transformer: self-gating enhanced self-attentive networks", "yekun chai | shuo jin | xinwen hou", "self-attention mechanisms have made striking state-of-the-art (sota) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations. through a pseudo information highway, we introduce a gated component self-dependency units (sdu) that incorporates lstm-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. the subsidiary content-based sdu gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms. we may unveil the role of gating mechanism to aid in the context-based transformer modules, with hypothesizing that sdu gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process."], "machine learning for nlp"], [["quantifying similarity between relations with fact distribution", "weize chen | hao zhu | xu han | zhiyuan liu | maosong sun", "we introduce a conceptually simple and effective method to quantify the similarity between relations in knowledge bases. specifically, our approach is based on the divergence between the conditional probability distributions over entity pairs. in this paper, these distributions are parameterized by a very simple neural network. although computing the exact similarity is in-tractable, we provide a sampling-based method to get a good approximation. we empirically show the outputs of our approach significantly correlate with human judgments. by applying our method to various tasks, we also find that (1) our approach could effectively detect redundant relations extracted by open information extraction (open ie) models, that (2) even the most competitive models for relational classification still make mistakes among very similar relations, and that (3) our approach could be incorporated into negative sampling and softmax classification to alleviate these mistakes."], "information extraction, retrieval and text mining"], [["bleurt: learning robust metrics for text generation", "thibault sellam | dipanjan das | ankur parikh", "text generation has made significant advances in the last few years. yet, evaluation metrics have lagged behind, as the most popular choices (e.g., bleu and rouge) may correlate poorly with human judgment. we propose bleurt, a learned evaluation metric for english based on bert. bleurt can model human judgment with a few thousand possibly biased training examples. a key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. bleurt provides state-of-the-art results on the last three years of the wmt metrics shared task and the webnlg data set. in contrast to a vanilla bert-based approach, it yields superior results even when the training data is scarce and out-of-distribution."], "generation"], [["hyperbolic capsule networks for multi-label classification", "boli chen | xin huang | lin xiao | liping jing", "although deep neural networks are effective at extracting high-level features, classification methods usually encode an input into a vector representation via simple feature aggregation operations (e.g. pooling). such operations limit the performance. for instance, a multi-label document may contain several concepts. in this case, one vector can not sufficiently capture its salient and discriminative content. thus, we propose hyperbolic capsule networks (hypercaps) for multi-label classification (mlc), which have two merits. first, hyperbolic capsules are designed to capture fine-grained document information for each label, which has the ability to characterize complicated structures among labels and documents. second, hyperbolic dynamic routing (hdr) is introduced to aggregate hyperbolic capsules in a label-aware manner, so that the label-level discriminative information can be preserved along the depth of neural networks. to efficiently handle large-scale mlc datasets, we additionally present a new routing method to adaptively adjust the capsule number during routing. extensive experiments are conducted on four benchmark datasets. compared with the state-of-the-art methods, hypercaps significantly improves the performance of mlc especially on tail labels."], "nlp applications"], [["dynamic programming encoding for subword segmentation in neural machine translation", "xuanli he | gholamreza haffari | mohammad norouzi", "this paper introduces dynamic programming encoding (dpe), a new segmentation algorithm for tokenizing sentences into subword units. we view the subword segmentation of output sentences as a latent variable that should be marginalized out for learning and inference. a mixed character-subword transformer is proposed, which enables exact log marginal likelihood estimation and exact map inference to find target segmentations with maximum posterior probability. dpe uses a lightweight mixed character-subword transformer as a means of pre-processing parallel data to segment output sentences using dynamic programming. empirical results on machine translation suggest that dpe is effective for segmenting output sentences and can be combined with bpe dropout for stochastic segmentation of source sentences. dpe achieves an average improvement of 0.9 bleu over bpe (sennrich et al., 2016) and an average improvement of 0.55 bleu over bpe dropout (provilkov et al., 2019) on several wmt datasets including english <=> (german, romanian, estonian, finnish, hungarian)."], "machine translation and multilinguality"], [["attention is (not) all you need for commonsense reasoning", "tassilo klein | moin nabi", "the recently introduced bert model exhibits strong performance on several language understanding benchmarks. in this paper, we describe a simple re-implementation of bert for commonsense reasoning. we show that the attentions produced by bert can be directly utilized for tasks such as the pronoun disambiguation problem and winograd schema challenge. our proposed attention-guided commonsense reasoning method is conceptually simple yet empirically powerful. experimental analysis on multiple datasets demonstrates that our proposed system performs remarkably well on all cases while outperforming the previously reported state of the art by a margin. while results suggest that bert seems to implicitly learn to establish complex relationships between entities, solving commonsense reasoning tasks might require more than unsupervised models learned from huge text corpora."], "semantics"], [["language modelling makes sense: propagating representations through wordnet for full-coverage word sense disambiguation", "daniel loureiro | al\u00edpio jorge", "contextual embeddings represent a new generation of semantic representations learned from neural language modelling (nlm) that addresses the issue of meaning conflation hampering traditional word embeddings. in this work, we show that contextual embeddings can be used to achieve unprecedented gains in word sense disambiguation (wsd) tasks. our approach focuses on creating sense-level embeddings with full-coverage of wordnet, and without recourse to explicit knowledge of sense distributions or task-specific modelling. as a result, a simple nearest neighbors (k-nn) method using our representations is able to consistently surpass the performance of previous systems using powerful neural sequencing models. we also analyse the robustness of our approach when ignoring part-of-speech and lemma features, requiring disambiguation against the full sense inventory, and revealing shortcomings to be improved. finally, we explore applications of our sense embeddings for concept-level analyses of contextual embeddings and their respective nlms."], "semantics"], [["stolen probability: a structural weakness of neural language models", "david demeter | gregory kimmel | doug downey", "neural network language models (nnlms) generate probability distributions by applying a softmax function to a distance metric formed by taking the dot product of a prediction vector with all word vectors in a high-dimensional embedding space. the dot-product distance metric forms part of the inductive bias of nnlms. although nnlms optimize well with this inductive bias, we show that this results in a sub-optimal ordering of the embedding space that structurally impoverishes some words at the expense of others when assigning probability. we present numerical, theoretical and empirical analyses which show that words on the interior of the convex hull in the embedding space have their probability bounded by the probabilities of the words on the hull."], "machine learning for nlp"], [["are we there yet? encoder-decoder neural networks as cognitive models of english past tense inflection", "maria corkery | yevgen matusevych | sharon goldwater", "the cognitive mechanisms needed to account for the english past tense have long been a subject of debate in linguistics and cognitive science. neural network models were proposed early on, but were shown to have clear flaws. recently, however, kirov and cotterell (2018) showed that modern encoder-decoder (ed) models overcome many of these flaws. they also presented evidence that ed models demonstrate humanlike performance in a nonce-word task. here, we look more closely at the behaviour of their model in this task. we find that (1) the model exhibits instability across multiple simulations in terms of its correlation with human data, and (2) even when results are aggregated across simulations (treating each simulation as an individual human participant), the fit to the human data is not strong\u2014worse than an older rule-based model. these findings hold up through several alternative training regimes and evaluation measures. although other neural architectures might do better, we conclude that there is still insufficient evidence to claim that neural nets are a good cognitive model for this task."], "linguistic theories, cognitive modeling and psycholinguistics"], [["can you tell me how to get past sesame street? sentence-level pretraining beyond language modeling", "alex wang | jan hula | patrick xia | raghavendra pappagari | r. thomas mccoy | roma patel | najoung kim | ian tenney | yinghui huang | katherin yu | shuning jin | berlin chen | benjamin van durme | edouard grave | ellie pavlick | samuel r. bowman", "natural language understanding has recently seen a surge of progress with the use of sentence encoders like elmo (peters et al., 2018a) and bert (devlin et al., 2019) which are pretrained on variants of language modeling. we conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. however, our results are mixed across pretraining tasks and show some concerning trends: in elmo\u2019s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target tasks. in addition, fine-tuning bert on an intermediate task often negatively impacts downstream transfer. in a more positive trend, we see modest gains from multitask training, suggesting the development of more sophisticated multitask and transfer learning techniques as an avenue for further research."], "semantics"], [["masked language model scoring", "julian salazar | davis liang | toan q. nguyen | katrin kirchhoff", "pretrained masked language models (mlms) require finetuning for most nlp tasks. instead, we evaluate mlms out of the box via their pseudo-log-likelihood scores (plls), which are computed by masking tokens one by one. we show that plls outperform scores from autoregressive language models like gpt-2 in a variety of tasks. by rescoring asr and nmt hypotheses, roberta reduces an end-to-end librispeech model\u2019s wer by 30% relative and adds up to +1.7 bleu on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation. we attribute this success to pll\u2019s unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from gpt-2 (+10 points on island effects, npi licensing in blimp). one can finetune mlms to give scores without masking, enabling computation in a single inference pass. in all, plls and their associated pseudo-perplexities (pppls) enable plug-and-play use of the growing number of pretrained mlms; e.g., we use a single cross-lingual model to rescore translations in multiple languages. we release our library for language model scoring at https://github.com/awslabs/mlm-scoring."], "machine learning for nlp"], [["multi-granularity interaction network for extractive and abstractive multi-document summarization", "hanqi jin | tianming wang | xiaojun wan", "in this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. the word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. we employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the multi-news dataset."], "summarization"], [["closing the gap: joint de-identification and concept extraction in the clinical domain", "lukas lange | heike adel | jannik str\u00f6tgen", "exploiting natural language processing in the clinical domain requires de-identification, i.e., anonymization of personal information in texts. however, current research considers de-identification and downstream tasks, such as concept extraction, only in isolation and does not study the effects of de-identification on other tasks. in this paper, we close this gap by reporting concept extraction performance on automatically anonymized data and investigating joint models for de-identification and concept extraction. in particular, we propose a stacked model with restricted access to privacy sensitive information and a multitask model. we set the new state of the art on benchmark datasets in english (96.1% f1 for de-identification and 88.9% f1 for concept extraction) and spanish (91.4% f1 for concept extraction)."], "nlp applications"], [["dissent: learning sentence representations from explicit discourse relations", "allen nie | erin bennett | noah goodman", "learning effective representations of sentences is one of the core missions of natural language understanding. existing models either train on a vast amount of text, or require costly, manually curated sentence relation datasets. we show that with dependency parsing and rule-based rubrics, we can curate a high quality sentence relation task by leveraging explicit discourse relations. we show that our curated dataset provides an excellent signal for learning vector representations of sentence meaning, representing relations that can only be determined when the meanings of two sentences are combined. we demonstrate that the automatically curated corpus allows a bidirectional lstm sentence encoder to yield high quality sentence embeddings and can serve as a supervised fine-tuning dataset for larger models such as bert. our fixed sentence embeddings achieve high performance on a variety of transfer tasks, including senteval, and we achieve state-of-the-art results on penn discourse treebank\u2019s implicit relation prediction task."], "semantics"], [["location attention for extrapolation to longer sequences", "yann dubois | gautier dagan | dieuwke hupkes | elia bruni", "neural networks are surprisingly good at interpolating and perform remarkably well when the training set examples resemble those in the test set. however, they are often unable to extrapolate patterns beyond the seen data, even when the abstractions required for such patterns are simple. in this paper, we first review the notion of extrapolation, why it is important and how one could hope to tackle it. we then focus on a specific type of extrapolation which is especially useful for natural language processing: generalization to sequences that are longer than the training ones. we hypothesize that models with a separate content- and location-based attention are more likely to extrapolate than those with common attention mechanisms. we empirically support our claim for recurrent seq2seq models with our proposed attention on variants of the lookup table task. this sheds light on some striking failures of neural models for sequences and on possible methods to approaching such issues."], "machine translation and multilinguality"], [["learning to rank for plausible plausibility", "zhongyang li | tongfei chen | benjamin van durme", "researchers illustrate improvements in contextual encoding strategies via resultant performance on a battery of shared natural language understanding (nlu) tasks. many of these tasks are of a categorical prediction variety: given a conditioning context (e.g., an nli premise), provide a label based on an associated prompt (e.g., an nli hypothesis). the categorical nature of these tasks has led to common use of a cross entropy log-loss objective during training. we suggest this loss is intuitively wrong when applied to plausibility tasks, where the prompt by design is neither categorically entailed nor contradictory given the context. log-loss naturally drives models to assign scores near 0.0 or 1.0, in contrast to our proposed use of a margin-based loss. following a discussion of our intuition, we describe a confirmation study based on an extreme, synthetically curated task derived from multinli. we find that a margin-based loss leads to a more plausible model of plausibility. finally, we illustrate improvements on the choice of plausible alternative (copa) task through this change in loss."], "semantics"], [["using context in neural machine translation training objectives", "danielle saunders | felix stahlberg | bill byrne", "we present neural machine translation (nmt) training using document-level metrics with batch-level documents. previous sequence-objective approaches to nmt training focus exclusively on sentence-level metrics like sentence bleu which do not correspond to the desired evaluation metric, typically document bleu. meanwhile research into document-level nmt training focuses on data or model architecture rather than training procedure. we find that each of these lines of research has a clear space in it for the other, and propose merging them with a scheme that allows a document-level evaluation metric to be used in the nmt training objective. we first sample pseudo-documents from sentence samples. we then approximate the expected document bleu gradient with monte carlo sampling for use as a cost function in minimum risk training (mrt). this two-level sampling procedure gives nmt performance gains over sequence mrt and maximum-likelihood training. we demonstrate that training is more robust for document-level metrics than with sequence metrics. we further demonstrate improvements on nmt with ter and grammatical error correction (gec) using gleu, both metrics used at the document level for evaluations."], "machine translation and multilinguality"], [["a graph auto-encoder model of derivational morphology", "valentin hofmann | hinrich sch\u00fctze | janet pierrehumbert", "there has been little work on modeling the morphological well-formedness (mwf) of derivatives, a problem judged to be complex and difficult in linguistics. we present a graph auto-encoder that learns embeddings capturing information about the compatibility of affixes and stems in derivation. the auto-encoder models mwf in english surprisingly well by combining syntactic and semantic information with associative information from the mental lexicon."], "phonology, morphology and word segmentation"], [["cognet: a large-scale cognate database", "khuyagbaatar batsuren | gabor bella | fausto giunchiglia", "this paper introduces cognet, a new, large-scale lexical database that provides cognates -words of common origin and meaning- across languages. the database currently contains 3.1 million cognate pairs across 338 languages using 35 writing systems. the paper also describes the automated method by which cognates were computed from publicly available wordnets, with an accuracy evaluated to 94%. finally, it presents statistics about the cognate data and some initial insights into it, hinting at a possible future exploitation of the resource by various fields of lingustics."], "machine translation and multilinguality"], [["fatality killed the cat or: babelpic, a multimodal dataset for non-concrete concepts", "agostina calabrese | michele bevilacqua | roberto navigli", "thanks to the wealth of high-quality annotated images available in popular repositories such as imagenet, multimodal language-vision research is in full bloom. however, events, feelings and many other kinds of concepts which can be visually grounded are not well represented in current datasets. nevertheless, we would expect a wide-coverage language understanding system to be able to classify images depicting recess and remorse, not just cats, dogs and bridges. we fill this gap by presenting babelpic, a hand-labeled dataset built by cleaning the image-synset association found within the babelnet lexical knowledge base (lkb). babelpic explicitly targets non-concrete concepts, thus providing refreshing new data for the community. we also show that pre-trained language-vision systems can be used to further expand the resource by exploiting natural language knowledge available in the lkb. babelpic is available for download at http://babelpic.org."], "resources and evaluation"], [["clarq: a large-scale and diverse dataset for clarification question generation", "vaibhav kumar | alan w black", "question answering and conversational systems are often baffled and need help clarifying certain ambiguities. however, limitations of existing datasets hinder the development of large-scale models capable of generating and utilising clarification questions. in order to overcome these limitations, we devise a novel bootstrapping framework (based on self-supervision) that assists in the creation of a diverse, large-scale dataset of clarification questions based on post-comment tuples extracted from stackexchange. the framework utilises a neural network based architecture for classifying clarification questions. it is a two-step method where the first aims to increase the precision of the classifier and second aims to increase its recall. we quantitatively demonstrate the utility of the newly created dataset by applying it to the downstream task of question-answering. the final dataset, clarq, consists of ~2m examples distributed across 173 domains of stackexchange. we release this dataset in order to foster research into the field of clarification question generation with the larger goal of enhancing dialog and question answering systems."], "question answering"], [["extracting headless mwes from dependency parse trees: parsing, tagging, and joint modeling approaches", "tianze shi | lillian lee", "an interesting and frequent type of multi-word expression (mwe) is the headless mwe, for which there are no true internal syntactic dominance relations; examples include many named entities (\u201cwells fargo\u201d) and dates (\u201cjuly 5, 2020\u201d) as well as certain productive constructions (\u201cblow for blow\u201d, \u201cday after day\u201d). despite their special status and prevalence, current dependency-annotation schemes require treating such flat structures as if they had internal syntactic heads, and most current parsers handle them in the same fashion as headed constructions. meanwhile, outside the context of parsing, taggers are typically used for identifying mwes, but taggers might benefit from structural information. we empirically compare these two common strategies\u2014parsing and tagging\u2014for predicting flat mwes. additionally, we propose an efficient joint decoding algorithm that combines scores from both strategies. experimental results on the mwe-aware english dependency corpus and on six non-english dependency treebanks with frequent flat structures show that: (1) tagging is more accurate than parsing for identifying flat-structure mwes, (2) our joint decoder reconciles the two different views and, for non-bert features, leads to higher accuracies, and (3) most of the gains result from feature sharing between the parsers and taggers."], "tagging, chunking, syntax and parsing"], [["what context features can transformer language models use?", "joe o\u2019connor | jacob andreas", "transformer-based language models benefit from conditioning on contexts of hundreds to thousands of previous tokens. what aspects of these contexts contribute to accurate model prediction? we describe a series of experiments that measure usable information by selectively ablating lexical and structural information in transformer language models trained on english wikipedia. in both mid- and long-range contexts, we find that several extremely destructive context manipulations\u2014including shuffling word order within sentences and deleting all words other than nouns\u2014remove less than 15% of the usable information. our results suggest that long contexts, but not their detailed syntactic and propositional content, are important for the low perplexity of current transformer language models."], "interpretability and analysis of models for nlp"], [["improving dialog systems for negotiation with personality modeling", "runzhe yang | jingxiao chen | karthik narasimhan", "in this paper, we explore the ability to model and infer personality types of opponents, predict their responses, and use this information to adapt a dialog agent\u2019s high-level strategy in negotiation tasks. inspired by the idea of incorporating a theory of mind (tom) into machines, we introduce a probabilistic formulation to encapsulate the opponent\u2019s personality type during both learning and inference. we test our approach on the craigslistbargain dataset (he et al. 2018) and show that our method using tom inference achieves a 20% higher dialog agreement rate compared to baselines on a mixed population of opponents. we also demonstrate that our model displays diverse negotiation behavior with different types of opponents."], "dialogue and interactive systems"], [["poetry to prose conversion in sanskrit as a linearisation task: a case for low-resource languages", "amrith krishna | vishnu sharma | bishal santra | aishik chakraborty | pavankumar satuluri | pawan goyal", "the word ordering in a sanskrit verse is often not aligned with its corresponding prose order. conversion of the verse to its corresponding prose helps in better comprehension of the construction. owing to the resource constraints, we formulate this task as a word ordering (linearisation) task. in doing so, we completely ignore the word arrangement at the verse side. k\u0101vya guru, the approach we propose, essentially consists of a pipeline of two pretraining steps followed by a seq2seq model. the first pretraining step learns task-specific token embeddings from pretrained embeddings. in the next step, we generate multiple possible hypotheses for possible word arrangements of the input %using another pretraining step. we then use them as inputs to a neural seq2seq model for the final prediction. we empirically show that the hypotheses generated by our pretraining step result in predictions that consistently outperform predictions based on the original order in the verse. overall, k\u0101vya guru outperforms current state of the art models in linearisation for the poetry to prose conversion task in sanskrit."], "nlp applications"], [["multi-task pairwise neural ranking for hashtag segmentation", "mounica maddela | wei xu | daniel preo\u0163iuc-pietro", "hashtags are often employed on social media and beyond to add metadata to a textual utterance with the goal of increasing discoverability, aiding search, or providing additional semantics. however, the semantic content of hashtags is not straightforward to infer as these represent ad-hoc conventions which frequently include multiple words joined together and can include abbreviations and unorthodox spellings. we build a dataset of 12,594 hashtags split into individual segments and propose a set of approaches for hashtag segmentation by framing it as a pairwise ranking problem between candidate segmentations. our novel neural approaches demonstrate 24.6% error reduction in hashtag segmentation accuracy compared to the current state-of-the-art method. finally, we demonstrate that a deeper understanding of hashtag semantics obtained through segmentation is useful for downstream applications such as sentiment analysis, for which we achieved a 2.6% increase in average recall on the semeval 2017 sentiment analysis dataset."], "computational social science, social media and cultural analytics"], [["fine-grained interest matching for neural news recommendation", "heyuan wang | fangzhao wu | zheng liu | xing xie", "personalized news recommendation is a critical technology to improve users\u2019 online news reading experience. the core of news recommendation is accurate matching between user\u2019s interests and candidate news. the same user usually has diverse interests that are reflected in different news she has browsed. meanwhile, important semantic features of news are implied in text segments of different granularities. existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation. in this paper, we propose fim, a fine-grained interest matching method for neural news recommendation. instead of aggregating user\u2019s all historical browsed news into a unified vector, we hierarchically construct multi-level representations for each news via stacked dilated convolutions. then we perform fine-grained matching between segment pairs of each browsed news and the candidate news at each semantic level. high-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction. extensive experiments on a real-world dataset from msn news validate the effectiveness of our model on news recommendation."], "nlp applications"], [["generating long and informative reviews with aspect-aware coarse-to-fine decoding", "junyi li | wayne xin zhao | ji-rong wen | yang song", "generating long and informative review text is a challenging natural language generation task. previous work focuses on word-level generation, neglecting the importance of topical and syntactic characteristics from natural languages. in this paper, we propose a novel review generation model by characterizing an elaborately designed aspect-aware coarse-to-fine generation process. first, we model the aspect transitions to capture the overall content flow. then, to generate a sentence, an aspect-aware sketch will be predicted using an aspect-aware decoder. finally, another decoder fills in the semantic slots by generating corresponding words. our approach is able to jointly utilize aspect semantics, syntactic sketch, and context information. extensive experiments results have demonstrated the effectiveness of the proposed model."], "generation"], [["improving image captioning evaluation by considering inter references variance", "yanzhi yi | hangyu deng | jinglu hu", "evaluating image captions is very challenging partially due to the fact that there are multiple correct captions for every single image. most of the existing one-to-one metrics operate by penalizing mismatches between reference and generative caption without considering the intrinsic variance between ground truth captions. it usually leads to over-penalization and thus a bad correlation to human judgment. recently, the latest one-to-one metric bertscore can achieve high human correlation in system-level tasks while some issues can be fixed for better performance. in this paper, we propose a novel metric based on bertscore that could handle such a challenge and extend bertscore with a few new features appropriately for image captioning evaluation. the experimental results show that our metric achieves state-of-the-art human judgment correlation."], "resources and evaluation"], [["optimal transport-based alignment of learned character representations for string similarity", "derek tam | nicholas monath | ari kobren | aaron traylor | rajarshi das | andrew mccallum", "string similarity models are vital for record linkage, entity resolution, and search. in this work, we present stance\u2013a learned model for computing the similarity of two strings. our approach encodes the characters of each string, aligns the encodings using sinkhorn iteration (alignment is posed as an instance of optimal transport) and scores the alignment with a convolutional neural network. we evaluate stance\u2019s ability to detect whether two strings can refer to the same entity\u2013a task we term alias detection. we construct five new alias detection datasets (and make them publicly available). we show that stance (or one of its variants) outperforms both state-of-the-art and classic, parameter-free similarity models on four of the five datasets. we also demonstrate stance\u2019s ability to improve downstream tasks by applying it to an instance of cross-document coreference and show that it leads to a 2.8 point improvement in b\u02c63 f1 over the previous state-of-the-art approach."], "machine learning for nlp"], [["named entity recognition as dependency parsing", "juntao yu | bernd bohnet | massimo poesio", "named entity recognition (ner) is a fundamental task in natural language processing, concerned with identifying spans of text expressing references to entities. ner research is often focused on flat entities only (flat ner), ignoring the fact that entity references can be nested, as in [bank of [china]] (finkel and manning, 2009). in this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (dozat and manning, 2017). the biaffine model scores pairs of start and end tokens in a sentence which we use to explore all spans, so that the model is able to predict named entities accurately. we show that the model works well for both nested and flat ner through evaluation on 8 corpora and achieving sota performance on all of them, with accuracy gains of up to 2.2 percentage points."], "information extraction, retrieval and text mining"], [["neural keyphrase generation via reinforcement learning with adaptive rewards", "hou pong chan | wang chen | lu wang | irwin king", "generating keyphrases that summarize the main points of a document is a fundamental task in natural language processing. although existing generative models are capable of predicting multiple keyphrases for an input document as well as determining the number of keyphrases to generate, they still suffer from the problem of generating too few keyphrases. to address this problem, we propose a reinforcement learning (rl) approach for keyphrase generation, with an adaptive reward function that encourages a model to generate both sufficient and accurate keyphrases. furthermore, we introduce a new evaluation method that incorporates name variations of the ground-truth keyphrases using the wikipedia knowledge base. thus, our evaluation method can more robustly evaluate the quality of predicted keyphrases. extensive experiments on five real-world datasets of different scales demonstrate that our rl approach consistently and significantly improves the performance of the state-of-the-art generative models with both conventional and new evaluation methods."], "summarization"], [["tail-to-tail non-autoregressive sequence prediction for chinese grammatical error correction", "piji li | shuming shi", "we investigate the problem of chinese grammatical error correction (cgec) and present a new framework named tail-to-tail (ttt) non-autoregressive sequence prediction to address the deep issues hidden in cgec. considering that most tokens are correct and can be conveyed directly from source to target, and the error positions can be estimated and corrected based on the bidirectional context information, thus we employ a bert-initialized transformer encoder as the backbone model to conduct information modeling and conveying. considering that only relying on the same position substitution cannot handle the variable-length correction cases, various operations such substitution, deletion, insertion, and local paraphrasing are required jointly. therefore, a conditional random fields (crf) layer is stacked on the up tail to conduct non-autoregressive sequence prediction by modeling the token dependencies. since most tokens are correct and easily to be predicted/conveyed to the target, then the models may suffer from a severe class imbalance issue. to alleviate this problem, focal loss penalty strategies are integrated into the loss functions. moreover, besides the typical fix-length error correction datasets, we also construct a variable-length corpus to conduct experiments. experimental results on standard datasets, especially on the variable-length datasets, demonstrate the effectiveness of ttt in terms of sentence-level accuracy, precision, recall, and f1-measure on tasks of error detection and correction."], "nlp applications"], [["norm-based curriculum learning for neural machine translation", "xuebo liu | houtim lai | derek f. wong | lidia s. chao", "a neural machine translation (nmt) system is expensive to train, especially with high-resource settings. as the nmt architectures become deeper and wider, this issue gets worse and worse. in this paper, we aim to improve the efficiency of training an nmt by introducing a novel norm-based curriculum learning method. we use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence. the norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties. it is easy to determine and contains learning-dependent features. the norm-based model competence makes nmt learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the nmt. experimental results for the wmt\u201914 english-german and wmt\u201917 chinese-english translation tasks demonstrate that the proposed method outperforms strong baselines in terms of bleu score (+1.17/+1.56) and training speedup (2.22x/3.33x)."], "machine translation and multilinguality"], [["glyph2vec: learning chinese out-of-vocabulary word embedding from glyphs", "hong-you chen | sz-han yu | shou-de lin", "chinese nlp applications that rely on large text often contain huge amounts of vocabulary which are sparse in corpus. we show that characters\u2019 written form, glyphs, in ideographic languages could carry rich semantics. we present a multi-modal model, glyph2vec, to tackle chinese out-of-vocabulary word embedding problem. glyph2vec extracts visual features from word glyphs to expand current word embedding space for out-of-vocabulary word embedding, without the need of accessing any corpus, which is useful for improving chinese nlp systems, especially for low-resource scenarios. experiments across different applications show the significant effectiveness of our model."], "semantics"], [["efficient pairwise annotation of argument quality", "lukas gienapp | benno stein | matthias hagen | martin potthast", "we present an efficient annotation framework for argument quality, a feature difficult to be measured reliably as per previous work. a stochastic transitivity model is combined with an effective sampling strategy to infer high-quality labels with low effort from crowdsourced pairwise judgments. the model\u2019s capabilities are showcased by compiling webis-argquality-20, an argument quality corpus that comprises scores for rhetorical, logical, dialectical, and overall quality inferred from a total of 41,859 pairwise judgments among 1,271 arguments. with up to 93% cost savings, our approach significantly outperforms existing annotation procedures. furthermore, novel insight into argument quality is provided through statistical analysis, and a new aggregation method to infer overall quality from individual quality dimensions is proposed."], "sentiment analysis, stylistic analysis, and argument mining"], [["sp-10k: a large-scale evaluation set for selectional preference acquisition", "hongming zhang | hantian ding | yangqiu song", "selectional preference (sp) is a commonly observed language phenomenon and proved to be useful in many natural language processing tasks. to provide a better evaluation method for sp models, we introduce sp-10k, a large-scale evaluation set that provides human ratings for the plausibility of 10,000 sp pairs over five sp relations, covering 2,500 most frequent verbs, nouns, and adjectives in american english. three representative sp acquisition methods based on pseudo-disambiguation are evaluated with sp-10k. to demonstrate the importance of our dataset, we investigate the relationship between sp-10k and the commonsense knowledge in conceptnet5 and show the potential of using sp to represent the commonsense knowledge. we also use the winograd schema challenge to prove that the proposed new sp relations are essential for the hard pronoun coreference resolution problem."], "resources and evaluation"], [["frugal paradigm completion", "alexander erdmann | tom kenter | markus becker | christian schallhart", "lexica distinguishing all morphologically related forms of each lexeme are crucial to many language technologies, yet building them is expensive. we propose a frugal paradigm completion approach that predicts all related forms in a morphological paradigm from as few manually provided forms as possible. it induces typological information during training which it uses to determine the best sources at test time. we evaluate our language-agnostic approach on 7 diverse languages. compared to popular alternative approaches, ours reduces manual labor by 16-63% and is the most robust to typological variation."], "phonology, morphology and word segmentation"], [["a compact and language-sensitive multilingual translation method", "yining wang | long zhou | jiajun zhang | feifei zhai | jingfang xu | chengqing zong", "multilingual neural machine translation (multi-nmt) with one encoder-decoder model has made remarkable progress due to its simple deployment. however, this multilingual translation paradigm does not make full use of language commonality and parameter sharing between encoder and decoder. furthermore, this kind of paradigm cannot outperform the individual models trained on bilingual corpus in most cases. in this paper, we propose a compact and language-sensitive method for multilingual translation. to maximize parameter sharing, we first present a universal representor to replace both encoder and decoder models. to make the representor sensitive for specific languages, we further introduce language-sensitive embedding, attention, and discriminator with the ability to enhance model performance. we verify our methods on various translation scenarios, including one-to-many, many-to-many and zero-shot. extensive experiments demonstrate that our proposed methods remarkably outperform strong standard multilingual translation systems on wmt and iwslt datasets. moreover, we find that our model is especially helpful in low-resource and zero-shot translation scenarios."], "machine translation and multilinguality"], [["choosing transfer languages for cross-lingual learning", "yu-hsiang lin | chian-yu chen | jean lee | zirui li | yuyan zhang | mengzhou xia | shruti rijhwani | junxian he | zhisong zhang | xuezhe ma | antonios anastasopoulos | patrick littell | graham neubig", "cross-lingual transfer, where a high-resource transfer language is used to improve the accuracy of a low-resource task language, is now an invaluable tool for improving performance of natural language processing (nlp) on low-resource languages. however, given a particular task language, it is not clear which language to transfer from, and the standard strategy is to select languages based on ad hoc criteria, usually the intuition of the experimenter. since a large number of features contribute to the success of cross-lingual transfer (including phylogenetic similarity, typological properties, lexical overlap, or size of available data), even the most enlightened experimenter rarely considers all these factors for the particular task at hand. in this paper, we consider this task of automatically selecting optimal transfer languages as a ranking problem, and build models that consider the aforementioned features to perform this prediction. in experiments on representative nlp tasks, we demonstrate that our model predicts good transfer languages much better than ad hoc baselines considering single features in isolation, and glean insights on what features are most informative for each different nlp tasks, which may inform future ad hoc selection even without use of our method."], "machine translation and multilinguality"], [["hiring now: a skill-aware multi-attention model for job posting generation", "liting liu | jie liu | wenzheng zhang | ziming chi | wenxuan shi | yalou huang", "writing a good job posting is a critical step in the recruiting process, but the task is often more difficult than many people think. it is challenging to specify the level of education, experience, relevant skills per the company information and job description. to this end, we propose a novel task of job posting generation (jpg) which is cast as a conditional text generation problem to generate job requirements according to the job descriptions. to deal with this task, we devise a data-driven global skill-aware multi-attention generation model, named sama. specifically, to model the complex mapping relationships between input and output, we design a hierarchical decoder that we first label the job description with multiple skills, then we generate a complete text guided by the skill labels. at the same time, to exploit the prior knowledge about the skills, we further construct a skill knowledge graph to capture the global prior knowledge of skills and refine the generated results. the proposed approach is evaluated on real-world job posting data. experimental results clearly demonstrate the effectiveness of the proposed method."], "nlp applications"], [["synchronous double-channel recurrent network for aspect-opinion pair extraction", "shaowei chen | jie liu | yu wang | wenzheng zhang | ziming chi", "opinion entity extraction is a fundamental task in fine-grained opinion mining. related studies generally extract aspects and/or opinion expressions without recognizing the relations between them. however, the relations are crucial for downstream tasks, including sentiment classification, opinion summarization, etc. in this paper, we explore aspect-opinion pair extraction (aope) task, which aims at extracting aspects and opinion expressions in pairs. to deal with this task, we propose synchronous double-channel recurrent network (sdrn) mainly consisting of an opinion entity extraction unit, a relation detection unit, and a synchronization unit. the opinion entity extraction unit and the relation detection unit are developed as two channels to extract opinion entities and relations simultaneously. furthermore, within the synchronization unit, we design entity synchronization mechanism (esm) and relation synchronization mechanism (rsm) to enhance the mutual benefit on the above two channels. to verify the performance of sdrn, we manually build three datasets based on semeval 2014 and 2015 benchmarks. extensive experiments demonstrate that sdrn achieves state-of-the-art performances."], "information extraction, retrieval and text mining"], [["do you know that florence is packed with visitors? evaluating state-of-the-art models of speaker commitment", "nanjiang jiang | marie-catherine de marneffe", "when a speaker, mary, asks \u201cdo you know that florence is packed with visitors?\u201d, we take her to believe that florence is packed with visitors, but not if she asks \u201cdo you think that florence is packed with visitors?\u201d. inferring speaker commitment (aka event factuality) is crucial for information extraction and question answering. here, we explore the hypothesis that linguistic deficits drive the error patterns of existing speaker commitment models by analyzing the linguistic correlates of model error on a challenging naturalistic dataset. we evaluate two state-of-the-art speaker commitment models on the commitmentbank, an english dataset of naturally occurring discourses. the commitmentbank is annotated with speaker commitment towards the content of the complement (\u201cflorence is packed with visitors\u201d in our example) of clause-embedding verbs (\u201cknow\u201d, \u201cthink\u201d) under four entailment-canceling environments (negation, modal, question, conditional). a breakdown of items by linguistic features reveals asymmetrical error patterns: while the models achieve good performance on some classes (e.g., negation), they fail to generalize to the diverse linguistic constructions (e.g., conditionals) in natural language, highlighting directions for improvement."], "discourse and pragmatics"], [["syntax-infused variational autoencoder for text generation", "xinyuan zhang | yi yang | siyang yuan | dinghan shen | lawrence carin", "we present a syntax-infused variational autoencoder (sivae), that integrates sentences with their syntactic trees to improve the grammar of generated sentences. distinct from existing vae-based text generative models, sivae contains two separate latent spaces, for sentences and syntactic trees. the evidence lower bound objective is redesigned correspondingly, by optimizing a joint distribution that accommodates two encoders and two decoders. sivae works with long short-term memory architectures to simultaneously generate sentences and syntactic trees. two versions of sivae are proposed: one captures the dependencies between the latent variables through a conditional prior network, and the other treats the latent variables independently such that syntactically-controlled sentence generation can be performed. experimental results demonstrate the generative superiority of sivae on both reconstruction and targeted syntactic evaluations. finally, we show that the proposed models can be used for unsupervised paraphrasing given different syntactic tree templates."], "generation"], [["simultaneous translation policies: from fixed to adaptive", "baigong zheng | kaibo liu | renjie zheng | mingbo ma | hairong liu | liang huang", "adaptive policies are better than fixed policies for simultaneous translation, since they can flexibly balance the tradeoff between translation quality and latency based on the current context information. but previous methods on obtaining adaptive policies either rely on complicated training process, or underperform simple fixed policies. we design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed policies. experiments on chinese -> english and german -> english show that our adaptive policies can outperform fixed ones by up to 4 bleu points for the same latency, and more surprisingly, it even surpasses the bleu score of full-sentence translation in the greedy mode (and very close to beam mode), but with much lower latency."], "machine translation and multilinguality"], [["identifying visible actions in lifestyle vlogs", "oana ignat | laura burdick | jia deng | rada mihalcea", "we consider the task of identifying human actions visible in online videos. we focus on the widely spread genre of lifestyle vlogs, which consist of videos of people performing actions while verbally describing them. our goal is to identify if actions mentioned in the speech description of a video are visually present. we construct a dataset with crowdsourced manual annotations of visible actions, and introduce a multimodal algorithm that leverages information derived from visual and linguistic clues to automatically infer which actions are visible in a video."], "language grounding to vision, robotics and beyond"], [["pre-train and plug-in: flexible conditional text generation with variational auto-encoders", "yu duan | canwen xu | jiaxin pei | jialong han | chenliang li", "conditional text generation has drawn much attention as a topic of natural language generation (nlg) which provides the possibility for humans to control the properties of generated contents. current conditional generation models cannot handle emerging conditions due to their joint end-to-end learning fashion. when a new condition added, these techniques require full retraining. in this paper, we present a new framework named pre-train and plug-in variational auto-encoder (ppvae) towards flexible conditional text generation. ppvae decouples the text generation module from the condition representation module to allow \u201cone-to-many\u201d conditional generation. when a fresh condition emerges, only a lightweight network needs to be trained and works as a plug-in for ppvae, which is efficient and desirable for real-world applications. extensive experiments demonstrate the superiority of ppvae against the existing alternatives with better conditionality and diversity but less training effort."], "generation"], [["recurrent neural network language models always learn english-like relative clause attachment", "forrest davis | marten van schijndel", "a standard approach to evaluating language models analyzes how models assign probabilities to valid versus invalid syntactic constructions (i.e. is a grammatical sentence more probable than an ungrammatical sentence). our work uses ambiguous relative clause attachment to extend such evaluations to cases of multiple simultaneous valid interpretations, where stark grammaticality differences are absent. we compare model performance in english and spanish to show that non-linguistic biases in rnn lms advantageously overlap with syntactic structure in english but not spanish. thus, english models may appear to acquire human-like syntactic preferences, while models trained on spanish fail to acquire comparable human-like preferences. we conclude by relating these results to broader concerns about the relationship between comprehension (i.e. typical language model use cases) and production (which generates the training data for language models), suggesting that necessary linguistic biases are not present in the training signal at all."], "linguistic theories, cognitive modeling and psycholinguistics"], [["amalgamation of protein sequence, structure and textual information for improving protein-protein interaction identification", "pratik dutta | sriparna saha", "an in-depth exploration of protein-protein interactions (ppi) is essential to understand the metabolism in addition to the regulations of biological entities like proteins, carbohydrates, and many more. most of the recent ppi tasks in bionlp domain have been carried out solely using textual data. in this paper, we argue that incorporating multimodal cues can improve the automatic identification of ppi. as a first step towards enabling the development of multimodal approaches for ppi identification, we have developed two multi-modal datasets which are extensions and multi-modal versions of two popular benchmark ppi corpora (bioinfer and hrpd50). besides, existing textual modalities, two new modalities, 3d protein structure and underlying genomic sequence, are also added to each instance. further, a novel deep multi-modal architecture is also implemented to efficiently predict the protein interactions from the developed datasets. a detailed experimental analysis reveals the superiority of the multi-modal approach in comparison to the strong baselines including unimodal approaches and state-of the-art methods over both the generated multi-modal datasets. the developed multi-modal datasets are available for use at https://github.com/sduttap16/mm_ppi_nlp."], "information extraction, retrieval and text mining"], [["span-convert: few-shot span extraction for dialog with pretrained conversational representations", "samuel coope | tyler farghly | daniela gerz | ivan vuli\u0107 | matthew henderson", "we introduce span-convert, a light-weight model for dialog slot-filling which frames the task as a turn-based span extraction task. this formulation allows for a simple integration of conversational knowledge coded in large pretrained conversational models such as convert (henderson et al., 2019). we show that leveraging such knowledge in span-convert is especially useful for few-shot learning scenarios: we report consistent gains over 1) a span extractor that trains representations from scratch in the target domain, and 2) a bert-based span extractor. in order to inspire more work on span extraction for the slot-filling task, we also release restaurants-8k, a new challenging data set of 8,198 utterances, compiled from actual conversations in the restaurant booking domain."], "dialogue and interactive systems"], [["cross-modal language generation using pivot stabilization for web-scale language coverage", "ashish v. thapliyal | radu soricut", "cross-modal language generation tasks such as image captioning are directly hurt in their ability to support non-english languages by the trend of data-hungry models combined with the lack of non-english annotations. we investigate potential solutions for combining existing language-generation annotations in english with translation capabilities in order to create solutions at web-scale in both domain and language coverage. we describe an approach called pivot-language generation stabilization (plugs), which leverages directly at training time both existing english annotations (gold data) as well as their machine-translated versions (silver data); at run-time, it generates first an english caption and then a corresponding target-language caption. we show that plugs models outperform other candidate solutions in evaluations performed over 5 different target languages, under a large-domain testset using images from the open images dataset. furthermore, we find an interesting effect where the english captions generated by the plugs models are better than the captions generated by the original, monolingual english model."], "generation"], [["relation-aware collaborative learning for unified aspect-based sentiment analysis", "zhuang chen | tieyun qian", "aspect-based sentiment analysis (absa) involves three subtasks, i.e., aspect term extraction, opinion term extraction, and aspect-level sentiment classification. most existing studies focused on one of these subtasks only. several recent researches made successful attempts to solve the complete absa problem with a unified framework. however, the interactive relations among three subtasks are still under-exploited. we argue that such relations encode collaborative signals between different subtasks. for example, when the opinion term is \u201cdelicious\u201d, the aspect term must be \u201cfood\u201d rather than \u201cplace\u201d. in order to fully exploit these relations, we propose a relation-aware collaborative learning (racl) framework which allows the subtasks to work coordinately via the multi-task learning and relation propagation mechanisms in a stacked multi-layer network. extensive experiments on three real-world datasets demonstrate that racl significantly outperforms the state-of-the-art methods for the complete absa task."], "sentiment analysis, stylistic analysis, and argument mining"], [["generative semantic hashing enhanced via boltzmann machines", "lin zheng | qinliang su | dinghan shen | changyou chen", "generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. for the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. from the perspectives of both model representation and code space size, independence is always not the best assumption. in this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of boltzmann machine as the variational posterior. to address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a boltzmann machine by augmenting it as a hierarchical concatenation of a gaussian-like distribution and a bernoulli distribution. based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (elbo). with these novel techniques, the entire model can be optimized efficiently. extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains."], "information extraction, retrieval and text mining"], [["multimodal and multiresolution speech recognition with transformers", "georgios paraskevopoulos | srinivas parthasarathy | aparna khare | shiva sundaram", "this paper presents an audio visual automatic speech recognition (av-asr) system using a transformer-based architecture. we particularly focus on the scene context provided by the visual information, to ground the asr. we extract representations for audio features in the encoder layers of the transformer and fuse video features using an additional crossmodal multihead attention layer. additionally, we incorporate a multitask training criterion for multiresolution asr, where we train the model to generate both character and subword level transcriptions. experimental results on the how2 dataset, indicate that multiresolution training can speed up convergence by around 50% and relatively improves word error rate (wer) performance by upto 18% over subword prediction models. further, incorporating visual information improves performance with relative gains upto 3.76% over audio only models. our results are comparable to state-of-the-art listen, attend and spell-based architectures."], "speech and multimodality"], [["hypernymy detection for low-resource languages via meta learning", "changlong yu | jialong han | haisong zhang | wilfred ng", "hypernymy detection, a.k.a, lexical entailment, is a fundamental sub-task of many natural language understanding tasks. previous explorations mostly focus on monolingual hypernymy detection on high-resource languages, e.g., english, but few investigate the low-resource scenarios. this paper addresses the problem of low-resource hypernymy detection by combining high-resource languages. we extensively compare three joint training paradigms and for the first time propose applying meta learning to relieve the low-resource issue. experiments demonstrate the superiority of our method among the three settings, which substantially improves the performance of extremely low-resource languages by preventing over-fitting on small datasets."], "semantics"], [["controlled crowdsourcing for high-quality qa-srl annotation", "paul roit | ayal klein | daniela stepanov | jonathan mamou | julian michael | gabriel stanovsky | luke zettlemoyer | ido dagan", "question-answer driven semantic role labeling (qa-srl) was proposed as an attractive open and natural flavour of srl, potentially attainable from laymen. recently, a large-scale crowdsourced qa-srl corpus and a trained parser were released. trying to replicate the qa-srl annotation for new texts, we found that the resulting annotations were lacking in quality, particularly in coverage, making them insufficient for further research and evaluation. in this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. applying this protocol to qa-srl yielded high-quality annotation with drastically higher coverage, producing a new gold evaluation dataset. we believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations."], "semantics"], [["adaptive nearest neighbor machine translation", "xin zheng | zhirui zhang | junliang guo | shujian huang | boxing chen | weihua luo | jiajun chen", "knn-mt, recently proposed by khandelwal et al. (2020a), successfully combines pre-trained neural machine translation (nmt) model with token-level k-nearest-neighbor (knn) retrieval to improve the translation accuracy. however, the traditional knn algorithm used in knn-mt simply retrieves a same number of nearest neighbors for each target token, which may cause prediction errors when the retrieved neighbors include noises. in this paper, we propose adaptive knn-mt to dynamically determine the number of k for each target token. we achieve this by introducing a light-weight meta-k network, which can be efficiently trained with only a few training samples. on four benchmark machine translation datasets, we demonstrate that the proposed method is able to effectively filter out the noises in retrieval results and significantly outperforms the vanilla knn-mt model. even more noteworthy is that the meta-k network learned on one domain could be directly applied to other domains and obtain consistent improvements, illustrating the generality of our method. our implementation is open-sourced at https://github.com/zhengxxn/adaptive-knn-mt."], "machine translation and multilinguality"], [["an interactive multi-task learning network for end-to-end aspect-based sentiment analysis", "ruidan he | wee sun lee | hwee tou ng | daniel dahlmeier", "aspect-based sentiment analysis produces a list of aspect terms and their corresponding sentiments for a natural language sentence. this task is usually done in a pipeline manner, with aspect term extraction performed first, followed by sentiment predictions toward the extracted aspect terms. while easier to develop, such an approach does not fully exploit joint information from the two subtasks and does not use all available sources of training information that might be helpful, such as document-level labeled sentiment corpus. in this paper, we propose an interactive multi-task learning network (imn) which is able to jointly learn multiple related tasks simultaneously at both the token level as well as the document level. unlike conventional multi-task learning methods that rely on learning common features for the different tasks, imn introduces a message passing architecture where information is iteratively passed to different tasks through a shared set of latent variables. experimental results demonstrate superior performance of the proposed method against multiple baselines on three benchmark datasets."], "sentiment analysis, stylistic analysis, and argument mining"], [["multidirectional associative optimization of function-specific word representations", "daniela gerz | ivan vuli\u0107 | marek rei | roi reichart | anna korhonen", "we present a neural framework for learning associations between interrelated groups of words such as the ones found in subject-verb-object (svo) structures. our model induces a joint function-specific word vector space, where vectors of e.g. plausible svo compositions lie close together. the model retains information about word group membership even in the joint space, and can thereby effectively be applied to a number of tasks reasoning over the svo structure. we show the robustness and versatility of the proposed framework by reporting state-of-the-art results on the tasks of estimating selectional preference and event similarity. the results indicate that the combinations of representations learned with our task-independent model outperform task-specific architectures from prior work, while reducing the number of parameters by up to 95%."], "semantics"], [["uxla: a robust unsupervised data augmentation framework for zero-resource cross-lingual nlp", "m saiful bari | tasnim mohiuddin | shafiq joty", "transfer learning has yielded state-of-the-art (sota) results in many supervised nlp tasks. however, annotated data for every target task in every target language is rare, especially for low-resource languages. we propose uxla, a novel unsupervised data augmentation framework for zero-resource transfer learning scenarios. in particular, uxla aims to solve cross-lingual adaptation problems from a source language task distribution to an unknown target language task distribution, assuming no training label in the target language. at its core, uxla performs simultaneous self-training with data augmentation and unsupervised sample selection. to show its effectiveness, we conduct extensive experiments on three diverse zero-resource cross-lingual transfer tasks. uxla achieves sota results in all the tasks, outperforming the baselines by a good margin. with an in-depth framework dissection, we demonstrate the cumulative contributions of different components to its success."], "machine translation and multilinguality"], [["bam! born-again multi-task networks for natural language understanding", "kevin clark | minh-thang luong | urvashi khandelwal | christopher d. manning | quoc v. le", "it can be challenging to train multi-task neural networks that outperform or even match their single-task counterparts. to help address this, we propose using knowledge distillation where single-task models teach a multi-task model. we enhance this training with teacher annealing, a novel method that gradually transitions the model from distillation to supervised learning, helping the multi-task model surpass its single-task teachers. we evaluate our approach by multi-task fine-tuning bert on the glue benchmark. our method consistently improves over standard single-task and multi-task training."], "machine learning for nlp"], [["classification-based self-learning for weakly supervised bilingual lexicon induction", "mladen karan | ivan vuli\u0107 | anna korhonen | goran glava\u0161", "effective projection-based cross-lingual word embedding (clwe) induction critically relies on the iterative self-learning procedure. it gradually expands the initial small seed dictionary to learn improved cross-lingual mappings. in this work, we present classymap, a classification-based approach to self-learning, yielding a more robust and a more effective induction of projection-based clwes. unlike prior self-learning methods, our approach allows for integration of diverse features into the iterative process. we show the benefits of classymap for bilingual lexicon induction: we report consistent improvements in a weakly supervised setup (500 seed translation pairs) on a benchmark with 28 language pairs."], "machine translation and multilinguality"], [["ernie: enhanced language representation with informative entities", "zhengyan zhang | xu han | zhiyuan liu | xin jiang | maosong sun | qun liu", "neural language representation models such as bert pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various nlp tasks. however, the existing pre-trained language models rarely consider incorporating knowledge graphs (kgs), which can provide rich structured knowledge facts for better language understanding. we argue that informative entities in kgs can enhance language representation with external knowledge. in this paper, we utilize both large-scale textual corpora and kgs to train an enhanced language representation model (ernie), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. the experimental results have demonstrated that ernie achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model bert on other common nlp tasks. the code and datasets will be available in the future."], "information extraction, retrieval and text mining"], [["a negative case analysis of visual grounding methods for vqa", "robik shrestha | kushal kafle | christopher kanan", "existing visual question answering (vqa) methods tend to exploit dataset biases and spurious statistical correlations, instead of producing right answers for the right reasons. to address this issue, recent bias mitigation methods for vqa propose to incorporate visual cues (e.g., human attention maps) to better ground the vqa models, showcasing impressive gains. however, we show that the performance improvements are not a result of improved visual grounding, but a regularization effect which prevents over-fitting to linguistic priors. for instance, we find that it is not actually necessary to provide proper, human-based cues; random, insensible cues also result in similar improvements. based on this observation, we propose a simpler regularization scheme that does not require any external annotations and yet achieves near state-of-the-art performance on vqa-cpv2."], "language grounding to vision, robotics and beyond"], [["modeling semantic compositionality with sememe knowledge", "fanchao qi | junjie huang | chenghao yang | zhiyuan liu | xiao chen | qun liu | maosong sun", "semantic compositionality (sc) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. most related works focus on using complicated compositionality functions to model sc while few works consider external knowledge in models. in this paper, we verify the effectiveness of sememes, the minimum semantic units of human languages, in modeling sc by a confirmatory experiment. furthermore, we make the first attempt to incorporate sememe knowledge into sc models, and employ the sememe-incorporated models in learning representations of multiword expressions, a typical task of sc. in experiments, we implement our models by incorporating knowledge from a famous sememe knowledge base hownet and perform both intrinsic and extrinsic evaluations. experimental results show that our models achieve significant performance boost as compared to the baseline methods without considering sememe knowledge. we further conduct quantitative analysis and case studies to demonstrate the effectiveness of applying sememe knowledge in modeling sc.all the code and data of this paper can be obtained on https://github.com/thunlp/sememe-sc."], "semantics"], [["multilingual constituency parsing with self-attention and pre-training", "nikita kitaev | steven cao | dan klein", "we show that constituency parsing benefits from unsupervised pre-training across a variety of languages and a range of pre-training conditions. we first compare the benefits of no pre-training, fasttext, elmo, and bert for english and find that bert outperforms elmo, in large part due to increased model capacity, whereas elmo in turn outperforms the non-contextual fasttext embeddings. we also find that pre-training is beneficial across all 11 languages tested; however, large model sizes (more than 100 million parameters) make it computationally expensive to train separate models for each language. to address this shortcoming, we show that joint multilingual pre-training and fine-tuning allows sharing all but a small number of parameters between ten languages in the final model. the 10x reduction in model size compared to fine-tuning one model per language causes only a 3.2% relative error increase in aggregate. we further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages. finally, we demonstrate new state-of-the-art results for 11 languages, including english (95.8 f1) and chinese (91.8 f1)."], "tagging, chunking, syntax and parsing"], [["multi-agent task-oriented dialog policy learning with role-aware reward decomposition", "ryuichi takanobu | runze liang | minlie huang", "many studies have applied reinforcement learning to train a dialog policy and show great promise these years. one common approach is to employ a user simulator to obtain a large number of simulated user experiences for reinforcement learning algorithms. however, modeling a realistic user simulator is challenging. a rule-based simulator requires heavy domain expertise for complex tasks, and a data-driven simulator requires considerable data and it is even unclear how to evaluate a simulator. to avoid explicitly building a user simulator beforehand, we propose multi-agent dialog policy learning, which regards both the system and the user as the dialog agents. two agents interact with each other and are jointly learned simultaneously. the method uses the actor-critic framework to facilitate pretraining and improve scalability. we also propose hybrid value network for the role-aware reward decomposition to integrate role-specific domain knowledge of each agent in the task-oriented dialog. results show that our method can successfully build a system policy and a user policy simultaneously, and two agents can achieve a high task success rate through conversational interaction."], "dialogue and interactive systems"], [["on the importance of diversity in question generation for qa", "md arafat sultan | shubham chandel | ram\u00f3n fernandez astudillo | vittorio castelli", "automatic question generation (qg) has shown promise as a source of synthetic training data for question answering (qa). in this paper we ask: is textual diversity in qg beneficial for downstream qa? using top-p nucleus sampling to derive samples from a transformer-based question generator, we show that diversity-promoting qg indeed provides better qa training than likelihood maximization approaches such as beam search. we also show that standard qg evaluation metrics such as bleu, rouge and meteor are inversely correlated with diversity, and propose a diversity-aware intrinsic measure of overall qg quality that correlates well with extrinsic evaluation on qa."], "question answering"], [["tvqa+: spatio-temporal grounding for video question answering", "jie lei | licheng yu | tamara berg | mohit bansal", "we present the task of spatio-temporal video question answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos. we first augment the tvqa dataset with 310.8k bounding boxes, linking depicted objects to visual concepts in questions and answers. we name this augmented version as tvqa+. we then propose spatio-temporal answerer with grounded evidence (stage), a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos. comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our tvqa+ dataset can contribute to the question answering task. moreover, by performing this joint task, our model is able to produce insightful and interpretable spatio-temporal attention visualizations."], "language grounding to vision, robotics and beyond"], [["compound probabilistic context-free grammars for grammar induction", "yoon kim | chris dyer | alexander rush", "we study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilistic context free grammar. in contrast to traditional formulations which learn a single stochastic grammar, our context-free rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. inference in this context-dependent grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized with dynamic programming. experiments on english and chinese show the effectiveness of our approach compared to recent state-of-the-art methods for grammar induction from words with neural language models."], "tagging, chunking, syntax and parsing"], [["self-regulated interactive sequence-to-sequence learning", "julia kreutzer | stefan riezler", "not all types of supervision signals are created equal: different types of feedback have different costs and effects on learning. we show how self-regulation strategies that decide when to ask for which kind of feedback from a teacher (or from oneself) can be cast as a learning-to-learn problem leading to improved cost-aware sequence-to-sequence learning. in experiments on interactive neural machine translation, we find that the self-regulator discovers an \ud835\udf16-greedy strategy for the optimal cost-quality trade-off by mixing different feedback types including corrections, error markups, and self-supervision. furthermore, we demonstrate its robustness under domain shift and identify it as a promising alternative to active learning."], "machine learning for nlp"], [["probing neural network comprehension of natural language arguments", "timothy niven | hung-yu kao", "we are surprised to find that bert\u2019s peak performance of 77% on the argument reasoning comprehension task reaches just three points below the average untrained human baseline. however, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. we analyze the nature of these cues and demonstrate that a range of models all exploit them. this analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work."], "sentiment analysis, stylistic analysis, and argument mining"], [["hierarchy-aware global model for hierarchical text classification", "jie zhou | chunping ma | dingkun long | guangwei xu | ning ding | haoyu zhang | pengjun xie | gongshen liu", "hierarchical text classification is an essential yet challenging subtask of multi-label text classification with a taxonomic hierarchy. existing methods have difficulties in modeling the hierarchical label structure in a global view. furthermore, they cannot make full use of the mutual interactions between the text feature space and the label space. in this paper, we formulate the hierarchy as a directed graph and introduce hierarchy-aware structure encoders for modeling label dependencies. based on the hierarchy encoder, we propose a novel end-to-end hierarchy-aware global model (hiagm) with two variants. a multi-label attention variant (hiagm-la) learns hierarchy-aware label embeddings through the hierarchy encoder and conducts inductive fusion of label-aware text features. a text feature propagation model (hiagm-tp) is proposed as the deductive variant that directly feeds text features into hierarchy encoders. compared with previous works, both hiagm-la and hiagm-tp achieve significant and consistent improvements on three benchmark datasets."], "information extraction, retrieval and text mining"], [["towards unsupervised language understanding and generation by joint dual learning", "shang-yu su | chao-wei huang | yun-nung chen", "in modular dialogue systems, natural language understanding (nlu) and natural language generation (nlg) are two critical components, where nlu extracts the semantics from the given texts and nlg is to construct corresponding natural language sentences based on the input semantic representations. however, the dual property between understanding and generation has been rarely explored. the prior work is the first attempt that utilized the duality between nlu and nlg to improve the performance via a dual supervised learning framework. however, the prior work still learned both components in a supervised manner; instead, this paper introduces a general learning framework to effectively exploit such duality, providing flexibility of incorporating both supervised and unsupervised learning algorithms to train language understanding and generation models in a joint fashion. the benchmark experiments demonstrate that the proposed approach is capable of boosting the performance of both nlu and nlg. the source code is available at: https://github.com/miulab/dualug."], "dialogue and interactive systems"], [["good-enough compositional data augmentation", "jacob andreas", "we propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models. under this protocol, synthetic training examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. the protocol is model-agnostic and useful for a variety of tasks. applied to neural sequence-to-sequence models, it reduces error rate by as much as 87% on diagnostic tasks from the scan dataset and 16% on a semantic parsing task. applied to n-gram language models, it reduces perplexity by roughly 1% on small corpora in several languages."], "semantics"], [["reliability-aware dynamic feature composition for name tagging", "ying lin | liyuan liu | heng ji | dong yu | jiawei han", "word embeddings are widely used on a variety of tasks and can substantially improve the performance. however, their quality is not consistent throughout the vocabulary due to the long-tail distribution of word frequency. without sufficient contexts, rare word embeddings are usually less reliable than those of common words. however, current models typically trust all word embeddings equally regardless of their reliability and thus may introduce noise and hurt the performance. since names often contain rare and uncommon words, this problem is particularly critical for name tagging. in this paper, we propose a novel reliability-aware name tagging model to tackle this issue. we design a set of word frequency-based reliability signals to indicate the quality of each word embedding. guided by the reliability signals, the model is able to dynamically select and compose features such as word embedding and character-level representation using gating mechanisms. for example, if an input word is rare, the model relies less on its word embedding and assigns higher weights to its character and contextual features. experiments on ontonotes 5.0 show that our model outperforms the baseline model by 2.7% absolute gain in f-score. in cross-genre experiments on five genres in ontonotes, our model improves the performance for most genre pairs and obtains up to 5% absolute f-score gain."], "tagging, chunking, syntax and parsing"], [["relational graph attention network for aspect-based sentiment analysis", "kai wang | weizhou shen | yunyi yang | xiaojun quan | rui wang", "aspect-based sentiment analysis aims to determine the sentiment polarity towards a specific aspect in online reviews. most recent efforts adopt attention-based neural network models to implicitly connect aspects with opinion words. however, due to the complexity of language and the existence of multiple aspects in a single sentence, these models often confuse the connections. in this paper, we address this problem by means of effective encoding of syntax information. firstly, we define a unified aspect-oriented dependency tree structure rooted at a target aspect by reshaping and pruning an ordinary dependency parse tree. then, we propose a relational graph attention network (r-gat) to encode the new tree structure for sentiment prediction. extensive experiments are conducted on the semeval 2014 and twitter datasets, and the experimental results confirm that the connections between aspects and opinion words can be better established with our approach, and the performance of the graph attention network (gat) is significantly improved as a consequence."], "sentiment analysis, stylistic analysis, and argument mining"], [["deformer: decomposing pre-trained transformers for faster question answering", "qingqing cao | harsh trivedi | aruna balasubramanian | niranjan balasubramanian", "transformer-based qa models use input-wide self-attention \u2013 i.e. across both the question and the input passage \u2013 at all layers, causing them to be slow and memory-intensive. it turns out that we can get by without input-wide self-attention at all layers, especially in the lower layers. we introduce deformer, a decomposed transformer, which substitutes the full self-attention with question-wide and passage-wide self-attentions in the lower layers. this allows for question-independent processing of the input text representations, which in turn enables pre-computing passage representations reducing runtime compute drastically. furthermore, because deformer is largely similar to the original model, we can initialize deformer with the pre-training weights of a standard transformer, and directly fine-tune on the target qa dataset. we show deformer versions of bert and xlnet can be used to speed up qa by over 4.3x and with simple distillation-based losses they incur only a 1% drop in accuracy. we open source the code at https://github.com/stonybrooknlp/deformer."], "question answering"], [["aiming beyond the obvious: identifying non-obvious cases in semantic similarity datasets", "nicole peinelt | maria liakata | dong nguyen", "existing datasets for scoring text pairs in terms of semantic similarity contain instances whose resolution differs according to the degree of difficulty. this paper proposes to distinguish obvious from non-obvious text pairs based on superficial lexical overlap and ground-truth labels. we characterise existing datasets in terms of containing difficult cases and find that recently proposed models struggle to capture the non-obvious cases of semantic similarity. we describe metrics that emphasise cases of similarity which require more complex inference and propose that these are used for evaluating systems for semantic similarity."], "resources and evaluation"], [["worse wer, but better bleu? leveraging word embedding as intermediate in multitask end-to-end speech translation", "shun-po chuang | tzu-wei sung | alexander h. liu | hung-yi lee", "speech translation (st) aims to learn transformations from speech in the source language to the text in the target language. previous works show that multitask learning improves the st performance, in which the recognition decoder generates the text of the source language, and the translation decoder obtains the final translations based on the output of the recognition decoder. because whether the output of the recognition decoder has the correct semantics is more critical than its accuracy, we propose to improve the multitask st model by utilizing word embedding as the intermediate."], "machine translation and multilinguality"], [["measuring and improving bert\u2019s mathematical abilities by predicting the order of reasoning.", "piotr pi\u0119kos | mateusz malinowski | henryk michalewski", "imagine you are in a supermarket. you have two bananas in your basket and want to buy four apples. how many fruits do you have in total? this seemingly straightforward question can be challenging for data-driven language models, even if trained at scale. however, we would expect such generic language models to possess some mathematical abilities in addition to typical linguistic competence. towards this goal, we investigate if a commonly used language model, bert, possesses such mathematical abilities and, if so, to what degree. for that, we fine-tune bert on a popular dataset for word math problems, aqua-rat, and conduct several tests to understand learned representations better. since we teach models trained on natural language to do formal mathematics, we hypothesize that such models would benefit from training on semi-formal steps that explain how math results are derived. to better accommodate such training, we also propose new pretext tasks for learning mathematical rules. we call them (neighbor) reasoning order prediction (rop or nrop). with this new model, we achieve significantly better outcomes than data-driven baselines and even on-par with more tailored models."], "machine learning for nlp"], [["probing linguistic systematicity", "emily goodwin | koustuv sinha | timothy j. o\u2019donnell", "recently, there has been much interest in the question of whether deep natural language understanding (nlu) models exhibit systematicity, generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear. there is accumulating evidence that neural models do not learn systematically. we examine the notion of systematicity from a linguistic perspective, defining a set of probing tasks and a set of metrics to measure systematic behaviour. we also identify ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying. as a case study, we perform a series of experiments in the setting of natural language inference (nli). we provide evidence that current state-of-the-art nlu systems do not generalize systematically, despite overall high performance."], "linguistic theories, cognitive modeling and psycholinguistics"], [["barack\u2019s wife hillary: using knowledge graphs for fact-aware language modeling", "robert logan | nelson f. liu | matthew e. peters | matt gardner | sameer singh", "modeling human language requires the ability to not only generate fluent text but also encode factual knowledge. however, traditional language models are only capable of remembering facts seen at training time, and often have difficulty recalling them. to address this, we introduce the knowledge graph language model (kglm), a neural language model with mechanisms for selecting and copying facts from a knowledge graph that are relevant to the context. these mechanisms enable the model to render information it has never seen before, as well as generate out-of-vocabulary tokens. we also introduce the linked wikitext-2 dataset, a corpus of annotated text aligned to the wikidata knowledge graph whose contents (roughly) match the popular wikitext-2 benchmark. in experiments, we demonstrate that the kglm achieves significantly better performance than a strong baseline language model. we additionally compare different language model\u2019s ability to complete sentences requiring factual knowledge, showing that the kglm outperforms even very large language models in generating facts."], "generation"], [["multi-agent communication meets natural language: synergies between functional and structural language learning", "angeliki lazaridou | anna potapenko | olivier tieleman", "we present a method for combining multi-agent communication and traditional data-driven approaches to natural language learning, with an end goal of teaching agents to communicate with humans in natural language. our starting point is a language model that has been trained on generic, not task-specific language data. we then place this model in a multi-agent self-play environment that generates task-specific rewards used to adapt or modulate the model, turning it into a task-conditional language model. we introduce a new way for combining the two types of learning based on the idea of reranking language model samples, and show that this method outperforms others in communicating with humans in a visual referential communication task. finally, we present a taxonomy of different types of language drift that can occur alongside a set of measures to detect them."], "language grounding to vision, robotics and beyond"], [["learning faithful representations of causal graphs", "ananth balashankar | lakshminarayanan subramanian", "learning contextual text embeddings that represent causal graphs has been useful in improving the performance of downstream tasks like causal treatment effect estimation. however, existing causal embeddings which are trained to predict direct causal links, fail to capture other indirect causal links of the graph, thus leading to spurious correlations in downstream tasks. in this paper, we define the faithfulness property of contextual embeddings to capture geometric distance-based properties of directed acyclic causal graphs. by incorporating these faithfulness properties, we learn text embeddings that are 31.3% more faithful to human validated causal graphs with about 800k and 200k causal links and achieve 21.1% better precision-recall auc in a link prediction fine-tuning task. further, in a crowdsourced causal question-answering task on yahoo! answers with questions of the form \u201cwhat causes x?\u201d, our faithful embeddings achieved a precision of the first ranked answer (p@1) of 41.07%, outperforming the existing baseline by 10.2%."], "interpretability and analysis of models for nlp"], [["finding your voice: the linguistic development of mental health counselors", "justine zhang | robert filbin | christine morrison | jaclyn weiser | cristian danescu-niculescu-mizil", "mental health counseling is an enterprise with profound societal importance where conversations play a primary role. in order to acquire the conversational skills needed to face a challenging range of situations, mental health counselors must rely on training and on continued experience with actual clients. however, in the absence of large scale longitudinal studies, the nature and significance of this developmental process remain unclear. for example, prior literature suggests that experience might not translate into consequential changes in counselor behavior. this has led some to even argue that counseling is a profession without expertise. in this work, we develop a computational framework to quantify the extent to which individuals change their linguistic behavior with experience and to study the nature of this evolution. we use our framework to conduct a large longitudinal study of mental health counseling conversations, tracking over 3,400 counselors across their tenure. we reveal that overall, counselors do indeed change their conversational behavior to become more diverse across interactions, developing an individual voice that distinguishes them from other counselors. furthermore, a finer-grained investigation shows that the rate and nature of this diversification vary across functionally different conversational components."], "nlp applications"], [["logicalfactchecker: leveraging logical operations for fact checking with graph module network", "wanjun zhong | duyu tang | zhangyin feng | nan duan | ming zhou | ming gong | linjun shou | daxin jiang | jiahai wang | jian yin", "verifying the correctness of a textual statement requires not only semantic reasoning about the meaning of words, but also symbolic reasoning about logical operations like count, superlative, aggregation, etc. in this work, we propose logicalfactchecker, a neural network approach capable of leveraging logical operations for fact checking. it achieves the state-of-the-art performance on tabfact, a large-scale, benchmark dataset built for verifying a textual statement with semi-structured tables. this is achieved by a graph module network built upon the transformer-based architecture. with a textual statement and a table as the input, logicalfactchecker automatically derives a program (a.k.a. logical form) of the statement in a semantic parsing manner. a heterogeneous graph is then constructed to capture not only the structures of the table and the program, but also the connections between inputs with different modalities. such a graph reveals the related contexts of each word in the statement, the table and the program. the graph is used to obtain graph-enhanced contextual representations of words in transformer-based architecture. after that, a program-driven module network is further introduced to exploit the hierarchical structure of the program, where semantic compositionality is dynamically modeled along the program structure with a set of function-specific modules. ablation experiments suggest that both the heterogeneous graph and the module network are important to obtain strong results."], "semantics"], [["unsupervised bilingual word embedding agreement for unsupervised neural machine translation", "haipeng sun | rui wang | kehai chen | masao utiyama | eiichiro sumita | tiejun zhao", "unsupervised bilingual word embedding (ubwe), together with other technologies such as back-translation and denoising, has helped unsupervised neural machine translation (unmt) achieve remarkable results in several language pairs. in previous methods, ubwe is first trained using non-parallel monolingual corpora and then this pre-trained ubwe is used to initialize the word embedding in the encoder and decoder of unmt. that is, the training of ubwe and unmt are separate. in this paper, we first empirically investigate the relationship between ubwe and unmt. the empirical findings show that the performance of unmt is significantly affected by the performance of ubwe. thus, we propose two methods that train unmt with ubwe agreement. empirical results on several language pairs show that the proposed methods significantly outperform conventional unmt."], "machine translation and multilinguality"], [["effective adversarial regularization for neural machine translation", "motoki sato | jun suzuki | shun kiyono", "a regularization technique based on adversarial perturbation, which was initially developed in the field of image processing, has been successfully applied to text classification tasks and has yielded attractive improvements. we aim to further leverage this promising methodology into more sophisticated and critical neural models in the natural language processing field, i.e., neural machine translation (nmt) models. however, it is not trivial to apply this methodology to such models. thus, this paper investigates the effectiveness of several possible configurations of applying the adversarial perturbation and reveals that the adversarial regularization technique can significantly and consistently improve the performance of widely used nmt models, such as lstm-based and transformer-based models."], "machine translation and multilinguality"], [["2kenize: tying subword sequences for chinese script conversion", "pranav a | isabelle augenstein", "simplified chinese to traditional chinese character conversion is a common preprocessing step in chinese nlp. despite this, current approaches have insufficient performance because they do not take into account that a simplified chinese character can correspond to multiple traditional characters. here, we propose a model that can disambiguate between mappings and convert between the two scripts. the model is based on subword segmentation, two language models, as well as a method for mapping between subword sequences. we further construct benchmark datasets for topic classification and script conversion. our proposed method outperforms previous chinese character conversion approaches by 6 points in accuracy. these results are further confirmed in a downstream application, where 2kenize is used to convert pretraining dataset for topic classification. an error analysis reveals that our method\u2019s particular strengths are in dealing with code mixing and named entities."], "phonology, morphology and word segmentation"], [["interpretable operational risk classification with semi-supervised variational autoencoder", "fan zhou | shengming zhang | yi yang", "operational risk management is one of the biggest challenges nowadays faced by financial institutions. there are several major challenges of building a text classification system for automatic operational risk prediction, including imbalanced labeled/unlabeled data and lacking interpretability. to tackle these challenges, we present a semi-supervised text classification framework that integrates multi-head attention mechanism with semi-supervised variational inference for operational risk classification (semiorc). we empirically evaluate the framework on a real-world dataset. the results demonstrate that our method can better utilize unlabeled data and learn visually interpretable document representations. semiorc also outperforms other baseline methods on operational risk classification."], "nlp applications"], [["tigs: an inference algorithm for text infilling with gradient search", "dayiheng liu | jie fu | pengfei liu | jiancheng lv", "text infilling aims at filling in the missing part of a sentence or paragraph, which has been applied to a variety of real-world natural language generation scenarios. given a well-trained sequential generative model, it is challenging for its unidirectional decoder to generate missing symbols conditioned on the past and future information around the missing part. in this paper, we propose an iterative inference algorithm based on gradient search, which could be the first inference algorithm that can be broadly applied to any neural sequence generative models for text infilling tasks. extensive experimental comparisons show the effectiveness and efficiency of the proposed method on three different text infilling tasks with various mask ratios and different mask strategies, comparing with five state-of-the-art methods."], "machine learning for nlp"], [["incorporating syntactic and semantic information in word embeddings using graph convolutional networks", "shikhar vashishth | manik bhandari | prateek yadav | piyush rai | chiranjib bhattacharyya | partha talukdar", "word embeddings have been widely adopted across several nlp applications. most existing word embedding methods utilize sequential context of a word to learn its embedding. while there have been some attempts at utilizing syntactic context of a word, such methods result in an explosion of the vocabulary size. in this paper, we overcome this problem by proposing syngcn, a flexible graph convolution based method for learning word embeddings. syngcn utilizes the dependency context of a word without increasing the vocabulary size. word embeddings learned by syngcn outperform existing methods on various intrinsic and extrinsic tasks and provide an advantage when used with elmo. we also propose semgcn, an effective framework for incorporating diverse semantic knowledge for further enhancing learned word representations. we make the source code of both models available to encourage reproducible research."], "semantics"], [["crowdsourcing and validating event-focused emotion corpora for german and english", "enrica troiano | sebastian pad\u00f3 | roman klinger", "sentiment analysis has a range of corpora available across multiple languages. for emotion analysis, the situation is more limited, which hinders potential research on crosslingual modeling and the development of predictive models for other languages. in this paper, we fill this gap for german by constructing deisear, a corpus designed in analogy to the well-established english isear emotion dataset. motivated by scherer\u2019s appraisal theory, we implement a crowdsourcing experiment which consists of two steps. in step 1, participants create descriptions of emotional events for a given emotion. in step 2, five annotators assess the emotion expressed by the texts. we show that transferring an emotion classification model from the original english isear to the german crowdsourced deisear via machine translation does not, on average, cause a performance drop."], "linguistic theories, cognitive modeling and psycholinguistics"], [["historical text normalization with delayed rewards", "simon flachs | marcel bollmann | anders s\u00f8gaard", "training neural sequence-to-sequence models with simple token-level log-likelihood is now a standard approach to historical text normalization, albeit often outperformed by phrase-based models. policy gradient training enables direct optimization for exact matches, and while the small datasets in historical text normalization are prohibitive of from-scratch reinforcement learning, we show that policy gradient fine-tuning leads to significant improvements across the board. policy gradient training, in particular, leads to more accurate normalizations for long or unseen words."], "phonology, morphology and word segmentation"], [["a survey of race, racism, and anti-racism in nlp", "anjalie field | su lin blodgett | zeerak waseem | yulia tsvetkov", "despite inextricable ties between race and language, little work has considered race in nlp research and development. in this work, we survey 79 papers from the acl anthology that mention race. these papers reveal various types of race-related bias in all stages of nlp model development, highlighting the need for proactive consideration of how nlp systems can uphold racial hierarchies. however, persistent gaps in research on race and nlp remain: race has been siloed as a niche topic and remains ignored in many nlp tasks; most work operationalizes race as a fixed single-dimensional variable with a ground-truth label, which risks reinforcing differences produced by historical racism; and the voices of historically marginalized people are nearly absent in nlp literature. by identifying where and how nlp literature has and has not considered race, especially in comparison to related fields, our work calls for inclusion and racial justice in nlp research practices."], "ethics in nlp"], [["a2n: attending to neighbors for knowledge graph inference", "trapit bansal | da-cheng juan | sujith ravi | andrew mccallum", "state-of-the-art models for knowledge graph completion aim at learning a fixed embedding representation of entities in a multi-relational graph which can generalize to infer unseen entity relationships at test time. this can be sub-optimal as it requires memorizing and generalizing to all possible entity relationships using these fixed representations. we thus propose a novel attention-based method to learn query-dependent representation of entities which adaptively combines the relevant graph neighborhood of an entity leading to more accurate kg completion. the proposed method is evaluated on two benchmark datasets for knowledge graph completion, and experimental results show that the proposed model performs competitively or better than existing state-of-the-art, including recent methods for explicit multi-hop reasoning. qualitative probing offers insight into how the model can reason about facts involving multiple hops in the knowledge graph, through the use of neighborhood attention."], "information extraction, retrieval and text mining"], [["contextualized weak supervision for text classification", "dheeraj mekala | jingbo shang", "weakly supervised text classification based on a few user-provided seed words has recently attracted much attention from researchers. existing methods mainly generate pseudo-labels in a context-free manner (e.g., string matching), therefore, the ambiguous, context-dependent nature of human language has been long overlooked. in this paper, we propose a novel framework conwea, providing contextualized weak supervision for text classification. specifically, we leverage contextualized representations of word occurrences and seed word information to automatically differentiate multiple interpretations of the same word, and thus create a contextualized corpus. this contextualized corpus is further utilized to train the classifier and expand seed words in an iterative manner. this process not only adds new contextualized, highly label-indicative keywords but also disambiguates initial seed words, making our weak supervision fully contextualized. extensive experiments and case studies on real-world datasets demonstrate the necessity and significant advantages of using contextualized weak supervision, especially when the class labels are fine-grained."], "information extraction, retrieval and text mining"], [["opportunistic decoding with timely correction for simultaneous translation", "renjie zheng | mingbo ma | baigong zheng | kaibo liu | liang huang", "simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently. most existing frameworks, however, have difficulties in balancing between the translation quality and latency, i.e., the decoding policy is usually either too aggressive or too conservative. we propose an opportunistic decoding technique with timely correction ability, which always (over-)generates a certain mount of extra words at each step to keep the audience on track with the latest information. at the same time, it also corrects, in a timely fashion, the mistakes in the former overgenerated words when observing more source context to ensure high translation quality. experiments show our technique achieves substantial reduction in latency and up to +3.1 increase in bleu, with revision rate under 8% in chinese-to-english and english-to-chinese translation."], "machine translation and multilinguality"], [["deep neural model inspection and comparison via functional neuron pathways", "james fiacco | samridhi choudhary | carolyn rose", "we introduce a general method for the interpretation and comparison of neural models. the method is used to factor a complex neural model into its functional components, which are comprised of sets of co-firing neurons that cut across layers of the network architecture, and which we call neural pathways. the function of these pathways can be understood by identifying correlated task level and linguistic heuristics in such a way that this knowledge acts as a lens for approximating what the network has learned to apply to its intended task. as a case study for investigating the utility of these pathways, we present an examination of pathways identified in models trained for two standard tasks, namely named entity recognition and recognizing textual entailment."], "resources and evaluation"], [["collocation classification with unsupervised relation vectors", "luis espinosa anke | steven schockaert | leo wanner", "lexical relation classification is the task of predicting whether a certain relation holds between a given pair of words. in this paper, we explore to which extent the current distributional landscape based on word embeddings provides a suitable basis for classification of collocations, i.e., pairs of words between which idiosyncratic lexical relations hold. first, we introduce a novel dataset with collocations categorized according to lexical functions. second, we conduct experiments on a subset of this benchmark, comparing it in particular to the well known diffvec dataset. in these experiments, in addition to simple word vector arithmetic operations, we also investigate the role of unsupervised relation vectors as a complementary input. while these relation vectors indeed help, we also show that lexical function classification poses a greater challenge than the syntactic and semantic relations that are typically used for benchmarks in the literature."], "resources and evaluation"], [["joint chinese word segmentation and part-of-speech tagging via two-way attentions of auto-analyzed knowledge", "yuanhe tian | yan song | xiang ao | fei xia | xiaojun quan | tong zhang | yonggang wang", "chinese word segmentation (cws) and part-of-speech (pos) tagging are important fundamental tasks for chinese language processing, where joint learning of them is an effective one-step solution for both tasks. previous studies for joint cws and pos tagging mainly follow the character-based tagging paradigm with introducing contextual information such as n-gram features or sentential representations from recurrent neural models. however, for many cases, the joint tagging needs not only modeling from context features but also knowledge attached to them (e.g., syntactic relations among words); limited efforts have been made by existing research to meet such needs. in this paper, we propose a neural model named twasp for joint cws and pos tagging following the character-based sequence labeling paradigm, where a two-way attention mechanism is used to incorporate both context feature and their corresponding syntactic knowledge for each input character. particularly, we use existing language processing toolkits to obtain the auto-analyzed syntactic knowledge for the context, and the proposed attention module can learn and benefit from them although their quality may not be perfect. our experiments illustrate the effectiveness of the two-way attentions for joint cws and pos tagging, where state-of-the-art performance is achieved on five benchmark datasets."], "phonology, morphology and word segmentation"], [["harvesting and refining question-answer pairs for unsupervised qa", "zhongli li | wenhui wang | li dong | furu wei | ke xu", "question answering (qa) has shown great success thanks to the availability of large-scale datasets and the effectiveness of neural models. recent research works have attempted to extend these successes to the settings with few or no labeled data available. in this work, we introduce two approaches to improve unsupervised qa. first, we harvest lexically and syntactically divergent questions from wikipedia to automatically construct a corpus of question-answer pairs (named as refqa). second, we take advantage of the qa model to extract more appropriate answers, which iteratively refines data over refqa. we conduct experiments on squad 1.1, and newsqa by fine-tuning bert without access to manually annotated data. our approach outperforms previous unsupervised approaches by a large margin, and is competitive with early supervised models. we also show the effectiveness of our approach in the few-shot learning setting."], "question answering"], [["on the word alignment from neural machine translation", "xintong li | guanlin li | lemao liu | max meng | shuming shi", "prior researches suggest that neural machine translation (nmt) captures word alignment through its attention mechanism, however, this paper finds attention may almost fail to capture word alignment for some nmt models. this paper thereby proposes two methods to induce word alignment which are general and agnostic to specific nmt models. experiments show that both methods induce much better word alignment than attention. this paper further visualizes the translation through the word alignment induced by nmt. in particular, it analyzes the effect of alignment errors on translation errors at word level and its quantitative analysis over many testing examples consistently demonstrate that alignment errors are likely to lead to translation errors measured by different metrics."], "machine translation and multilinguality"], [["correlating neural and symbolic representations of language", "grzegorz chrupa\u0142a | afra alishahi", "analysis methods which enable us to better understand the representations and functioning of neural models of language are increasingly needed as deep learning becomes the dominant approach in nlp. here we present two methods based on representational similarity analysis (rsa) and tree kernels (tk) which allow us to directly quantify how strongly the information encoded in neural activation patterns corresponds to information represented by symbolic structures such as syntax trees. we first validate our methods on the case of a simple synthetic language for arithmetic expressions with clearly defined syntax and semantics, and show that they exhibit the expected pattern of results. we then our methods to correlate neural representations of english sentences with their constituency parse trees."], "machine learning for nlp"], [["data programming for learning discourse structure", "sonia badene | kate thompson | jean-pierre lorr\u00e9 | nicholas asher", "this paper investigates the advantages and limits of data programming for the task of learning discourse structure. the data programming paradigm implemented in the snorkel framework allows a user to label training data using expert-composed heuristics, which are then transformed via the \u201cgenerative step\u201d into probability distributions of the class labels given the training candidates. these results are later generalized using a discriminative model. snorkel\u2019s attractive promise to create a large amount of annotated data from a smaller set of training data by unifying the output of a set of heuristics has yet to be used for computationally difficult tasks, such as that of discourse attachment, in which one must decide where a given discourse unit attaches to other units in a text in order to form a coherent discourse structure. although approaching this problem using snorkel requires significant modifications to the structure of the heuristics, we show that weak supervision methods can be more than competitive with classical supervised learning approaches to the attachment problem."], "discourse and pragmatics"], [["difficulty-aware machine translation evaluation", "runzhe zhan | xuebo liu | derek f. wong | lidia s. chao", "the high-quality translation results produced by machine translation (mt) systems still pose a huge challenge for automatic evaluation. current mt evaluation pays the same attention to each sentence component, while the questions of real-world examinations (e.g., university examinations) have different difficulties and weightings. in this paper, we propose a novel difficulty-aware mt evaluation metric, expanding the evaluation dimension by taking translation difficulty into consideration. a translation that fails to be predicted by most mt systems will be treated as a difficult one and assigned a large weight in the final score function, and conversely. experimental results on the wmt19 english-german metrics shared tasks show that our proposed method outperforms commonly used mt metrics in terms of human correlation. in particular, our proposed method performs well even when all the mt systems are very competitive, which is when most existing metrics fail to distinguish between them. the source code is freely available at https://github.com/nlp2ct/difficulty-aware-mt-evaluation."], "machine translation and multilinguality"], [["a neural model for joint document and snippet ranking in question answering for large document collections", "dimitris pappas | ion androutsopoulos", "question answering (qa) systems for large document collections typically use pipelines that (i) retrieve possibly relevant documents, (ii) re-rank them, (iii) rank paragraphs or other snippets of the top-ranked documents, and (iv) select spans of the top-ranked snippets as exact answers. pipelines are conceptually simple, but errors propagate from one component to the next, without later components being able to revise earlier decisions. we present an architecture for joint document and snippet ranking, the two middle stages, which leverages the intuition that relevant documents have good snippets and good snippets come from relevant documents. the architecture is general and can be used with any neural text relevance ranker. we experiment with two main instantiations of the architecture, based on posit-drmm (pdrmm) and a bert-based ranker. experiments on biomedical data from bioasq show that our joint models vastly outperform the pipelines in snippet retrieval, the main goal for qa, with fewer trainable parameters, also remaining competitive in document retrieval. furthermore, our joint pdrmm-based model is competitive with bert-based models, despite using orders of magnitude fewer parameters. these claims are also supported by human evaluation on two test batches of bioasq. to test our key findings on another dataset, we modified the natural questions dataset so that it can also be used for document and snippet retrieval. our joint pdrmm-based model again outperforms the corresponding pipeline in snippet retrieval on the modified natural questions dataset, even though it performs worse than the pipeline in document retrieval. we make our code and the modified natural questions dataset publicly available."], "information extraction, retrieval and text mining"], [["sentence-level agreement for neural machine translation", "mingming yang | rui wang | kehai chen | masao utiyama | eiichiro sumita | min zhang | tiejun zhao", "the training objective of neural machine translation (nmt) is to minimize the loss between the words in the translated sentences and those in the references. in nmt, there is a natural correspondence between the source sentence and the target sentence. however, this relationship has only been represented using the entire neural network and the training objective is computed in word-level. in this paper, we propose a sentence-level agreement module to directly minimize the difference between the representation of source and target sentence. the proposed agreement module can be integrated into nmt as an additional training objective function and can also be used to enhance the representation of the source sentences. empirical results on the nist chinese-to-english and wmt english-to-german tasks show the proposed agreement module can significantly improve the nmt performance."], "machine translation and multilinguality"], [["a self-training method for machine reading comprehension with soft evidence extraction", "yilin niu | fangkai jiao | mantong zhou | ting yao | jingfang xu | minlie huang", "neural models have achieved great success on machine reading comprehension (mrc), many of which typically consist of two components: an evidence extractor and an answer predictor. the former seeks the most relevant information from a reference text, while the latter is to locate or generate answers from the extracted evidence. despite the importance of evidence labels for training the evidence extractor, they are not cheaply accessible, particularly in many non-extractive mrc tasks such as yes/no question answering and multi-choice mrc. to address this problem, we present a self-training method (stm), which supervises the evidence extractor with auto-generated evidence labels in an iterative process. at each iteration, a base mrc model is trained with golden answers and noisy evidence labels. the trained model will predict pseudo evidence labels as extra supervision in the next iteration. we evaluate stm on seven datasets over three mrc tasks. experimental results demonstrate the improvement on existing mrc models, and we also analyze how and why such a self-training method works in mrc."], "question answering"], [["look harder: a neural machine translation model with hard attention", "sathish reddy indurthi | insoo chung | sangha kim", "soft-attention based neural machine translation (nmt) models have achieved promising results on several translation tasks. these models attend all the words in the source sequence for each target token, which makes them ineffective for long sequence translation. in this work, we propose a hard-attention based nmt model which selects a subset of source tokens for each target token to effectively handle long sequence translation. due to the discrete nature of the hard-attention mechanism, we design a reinforcement learning algorithm coupled with reward shaping strategy to efficiently train it. experimental results show that the proposed model performs better on long sequences and thereby achieves significant bleu score improvement on english-german (en-de) and english-french (enfr) translation tasks compared to the soft attention based nmt."], "machine translation and multilinguality"], [["scaling up open tagging from tens to thousands: comprehension empowered attribute value extraction from product title", "huimin xu | wenting wang | xin mao | xinyu jiang | man lan", "supplementing product information by extracting attribute values from title is a crucial task in e-commerce domain. previous studies treat each attribute only as an entity type and build one set of ner tags (e.g., bio) for each of them, leading to a scalability issue which unfits to the large sized attribute system in real world e-commerce. in this work, we propose a novel approach to support value extraction scaling up to thousands of attributes without losing performance: (1) we propose to regard attribute as a query and adopt only one global set of bio tags for any attributes to reduce the burden of attribute tag or model explosion; (2) we explicitly model the semantic representations for attribute and title, and develop an attention mechanism to capture the interactive semantic relations in-between to enforce our framework to be attribute comprehensive. we conduct extensive experiments in real-life datasets. the results show that our model not only outperforms existing state-of-the-art ner tagging models, but also is robust and generates promising results for up to 8,906 attributes."], "information extraction, retrieval and text mining"], [["textbook question answering with multi-modal context graph understanding and self-supervised open-set comprehension", "daesik kim | seonhoon kim | nojun kwak", "in this work, we introduce a novel algorithm for solving the textbook question answering (tqa) task which describes more realistic qa problems compared to other recent tasks. we mainly focus on two related issues with analysis of the tqa dataset. first, solving the tqa problems requires to comprehend multi-modal contexts in complicated input data. to tackle this issue of extracting knowledge features from long text lessons and merging them with visual features, we establish a context graph from texts and images, and propose a new module f-gcn based on graph convolutional networks (gcn). second, scientific terms are not spread over the chapters and subjects are split in the tqa dataset. to overcome this so called \u2018out-of-domain\u2019 issue, before learning qa problems, we introduce a novel self-supervised open-set learning process without any annotations. the experimental results show that our model significantly outperforms prior state-of-the-art methods. moreover, ablation studies validate that both methods of incorporating f-gcn for extracting knowledge from multi-modal contexts and our newly proposed self-supervised learning process are effective for tqa problems."], "question answering"], [["bert rediscovers the classical nlp pipeline", "ian tenney | dipanjan das | ellie pavlick", "pre-trained text encoders have rapidly advanced the state of the art on many nlp tasks. we focus on one such model, bert, and aim to quantify where linguistic information is captured within the network. we find that the model represents the steps of the traditional nlp pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: pos tagging, parsing, ner, semantic roles, then coreference. qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations."], "semantics"], [["open vocabulary learning for neural chinese pinyin ime", "zhuosheng zhang | yafang huang | hai zhao", "pinyin-to-character (p2c) conversion is the core component of pinyin-based chinese input method engine (ime). however, the conversion is seriously compromised by the ambiguities of chinese characters corresponding to pinyin as well as the predefined fixed vocabularies. to alleviate such inconveniences, we propose a neural p2c conversion model augmented by an online updated vocabulary with a sampling mechanism to support open vocabulary learning during ime working. our experiments show that the proposed method outperforms commercial imes and state-of-the-art traditional models on standard corpus and true inputting history dataset in terms of multiple metrics and thus the online updated vocabulary indeed helps our ime effectively follows user inputting behavior."], "phonology, morphology and word segmentation"], [["generating question-answer hierarchies", "kalpesh krishna | mohit iyyer", "the process of knowledge acquisition can be viewed as a question-answer game between a student and a teacher in which the student typically starts by asking broad, open-ended questions before drilling down into specifics (hintikka, 1981; hakkarainen and sintonen, 2002). this pedagogical perspective motivates a new way of representing documents. in this paper, we present squash (specificity-controlled question-answer hierarchies), a novel and challenging text generation task that converts an input document into a hierarchy of question-answer pairs. users can click on high-level questions (e.g., \u201cwhy did frodo leave the fellowship?\u201d) to reveal related but more specific questions (e.g., \u201cwho did frodo leave with?\u201d). using a question taxonomy loosely based on lehnert (1978), we classify questions in existing reading comprehension datasets as either general or specific . we then use these labels as input to a pipelined system centered around a conditional neural language model. we extensively evaluate the quality of the generated qa hierarchies through crowdsourced experiments and report strong empirical results."], "question answering"], [["on-device structured and context partitioned projection networks", "sujith ravi | zornitsa kozareva", "a challenging problem in on-device text classification is to build highly accurate neural models that can fit in small memory footprint and have low latency. to address this challenge, we propose an on-device neural network sgnn++ which dynamically learns compact projection vectors from raw text using structured and context-dependent partition projections. we show that this results in accelerated inference and performance improvements. we conduct extensive evaluation on multiple conversational tasks and languages such as english, japanese, spanish and french. our sgnn++ model significantly outperforms all baselines, improves upon existing on-device neural models and even surpasses rnn, cnn and bilstm models on dialog act and intent prediction. through a series of ablation studies we show the impact of the partitioned projections and structured information leading to 10% improvement. we study the impact of the model size on accuracy and introduce quatization-aware training for sgnn++ to further reduce the model size while preserving the same quality. finally, we show fast inference on mobile phones."], "dialogue and interactive systems"], [["an imitation learning approach to unsupervised parsing", "bowen li | lili mou | frank keller", "recently, there has been an increasing interest in unsupervised parsers that optimize semantically oriented objectives, typically using reinforcement learning. unfortunately, the learned trees often do not match actual syntax trees well. shen et al. (2018) propose a structured attention mechanism for language modeling (prpn), which induces better syntactic structures but relies on ad hoc heuristics. also, their model lacks interpretability as it is not grounded in parsing actions. in our work, we propose an imitation learning approach to unsupervised parsing, where we transfer the syntactic knowledge induced by prpn to a tree-lstm model with discrete parsing actions. its policy is then refined by gumbel-softmax training towards a semantically oriented objective. we evaluate our approach on the all natural language inference dataset and show that it achieves a new state of the art in terms of parsing f-score, outperforming our base models, including prpn."], "tagging, chunking, syntax and parsing"], [["effective estimation of deep generative language models", "tom pelsmaeker | wilker aziz", "advances in variational inference enable parameterisation of probabilistic models by deep neural networks. this combines the statistical transparency of the probabilistic modelling framework with the representational power of deep learning. yet, due to a problem known as posterior collapse, it is difficult to estimate such models in the context of language modelling effectively. we concentrate on one such model, the variational auto-encoder, which we argue is an important building block in hierarchical probabilistic models of language. this paper contributes a sober view of the problem, a survey of techniques to address it, novel techniques, and extensions to the model. to establish a ranking of techniques, we perform a systematic comparison using bayesian optimisation and find that many techniques perform reasonably similar, given enough resources. still, a favourite can be named based on convenience. we also make several empirical observations and recommendations of best practices that should help researchers interested in this exciting field."], "machine learning for nlp"], [["generalizing natural language analysis through span-relation representations", "zhengbao jiang | wei xu | jun araki | graham neubig", "natural language processing covers a wide variety of tasks predicting syntax, semantics, and information content, and usually each type of output is generated with specially designed architectures. in this paper, we provide the simple insight that a great variety of tasks can be represented in a single unified format consisting of labeling spans and relations between spans, thus a single task-independent model can be used across different tasks. we perform extensive experiments to test this insight on 10 disparate tasks spanning dependency parsing (syntax), semantic role labeling (semantics), relation extraction (information content), aspect based sentiment analysis (sentiment), and many others, achieving performance comparable to state-of-the-art specialized models. we further demonstrate benefits of multi-task learning, and also show that the proposed method makes it easy to analyze differences and similarities in how the model handles different tasks. finally, we convert these datasets into a unified format to build a benchmark, which provides a holistic testbed for evaluating future models for generalized natural language analysis."], "machine learning for nlp"], [["deebert: dynamic early exiting for accelerating bert inference", "ji xin | raphael tang | jaejun lee | yaoliang yu | jimmy lin", "large-scale pre-trained language models such as bert have brought significant improvements to nlp applications. however, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications. we propose a simple but effective method, deebert, to accelerate bert inference. our approach allows samples to exit earlier without passing through the entire model. experiments show that deebert is able to save up to ~40% inference time with minimal degradation in model quality. further analyses show different behaviors in the bert transformer layers and also reveal their redundancy. our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. code is available at https://github.com/castorini/deebert."], "nlp applications"], [["towards unsupervised text classification leveraging experts and word embeddings", "zied haj-yahia | adrien sieg | l\u00e9a a. deleris", "text classification aims at mapping documents into a set of predefined categories. supervised machine learning models have shown great success in this area but they require a large number of labeled documents to reach adequate accuracy. this is particularly true when the number of target categories is in the tens or the hundreds. in this work, we explore an unsupervised approach to classify documents into categories simply described by a label. the proposed method is inspired by the way a human proceeds in this situation: it draws on textual similarity between the most relevant words in each document and a dictionary of keywords for each category reflecting its semantics and lexical field. the novelty of our method hinges on the enrichment of the category labels through a combination of human expertise and language models, both generic and domain specific. our experiments on 5 standard corpora show that the proposed method increases f1-score over relying solely on human expertise and can also be on par with simple supervised approaches. it thus provides a practical alternative to situations where low cost text categorization is needed, as we illustrate with our application to operational risk incidents classification."], "nlp applications"], [["heterogeneous graph neural networks for extractive document summarization", "danqing wang | pengfei liu | yining zheng | xipeng qiu | xuanjing huang", "as a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. an intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. in this paper, we present a heterogeneous graph-based neural network for extractive summarization (hetersumgraph), which contains semantic nodes of different granularity levels apart from sentences. these additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. to our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. the code will be released on github."], "summarization"], [["collaborative dialogue in minecraft", "anjali narayan-chen | prashant jayannavar | julia hockenmaier", "we wish to develop interactive agents that can communicate with humans to collaboratively solve tasks in grounded scenarios. since computer games allow us to simulate such tasks without the need for physical robots, we define a minecraft-based collaborative building task in which one player (a, the architect) is shown a target structure and needs to instruct the other player (b, the builder) to build this structure. both players interact via a chat interface. a can observe b but cannot place blocks. we present the minecraft dialogue corpus, a collection of 509 conversations and game logs. as a first step towards our goal of developing fully interactive agents for this task, we consider the subtask of architect utterance generation, and show how challenging it is."], "dialogue and interactive systems"], [["scde: sentence cloze dataset with high quality distractors from examinations", "xiang kong | varun gangal | eduard hovy", "we introduce scde, a dataset to evaluate the performance of computational models through sentence prediction. scde is a human created sentence cloze dataset, collected from public school english examinations. our task requires a model to fill up multiple blanks in a passage from a shared candidate set with distractors designed by english teachers. experimental results demonstrate that this task requires the use of non-local, discourse-level context beyond the immediate sentence neighborhood. the blanks require joint solving and significantly impair each other\u2019s context. furthermore, through ablations, we show that the distractors are of high quality and make the task more challenging. our experiments show that there is a significant performance gap between advanced models (72%) and humans (87%), encouraging future models to bridge this gap."], "question answering"], [["incorporating linguistic constraints into keyphrase generation", "jing zhao | yuxiang zhang", "keyphrases, that concisely describe the high-level topics discussed in a document, are very useful for a wide range of natural language processing tasks. though existing keyphrase generation methods have achieved remarkable performance on this task, they generate many overlapping phrases (including sub-phrases or super-phrases) of keyphrases. in this paper, we propose the parallel seq2seq network with the coverage attention to alleviate the overlapping phrase problem. specifically, we integrate the linguistic constraints of keyphrase into the basic seq2seq network on the source side, and employ the multi-task learning framework on the target side. in addition, in order to prevent from generating overlapping phrases of keyphrases with correct syntax, we introduce the coverage vector to keep track of the attention history and to decide whether the parts of source text have been covered by existing generated keyphrases. experimental results show that our method can outperform the state-of-the-art copyrnn on scientific datasets, and is also more effective in news domain."], "information extraction, retrieval and text mining"], [["compositionality and generalization in emergent languages", "rahma chaabouni | eugene kharitonov | diane bouchacourt | emmanuel dupoux | marco baroni", "natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as compositionality. in this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality. equipped with new ways to measure compositionality in emergent languages inspired by disentanglement in representation learning, we establish three main results: first, given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts. second, there is no correlation between the degree of compositionality of an emergent language and its ability to generalize. third, while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission: the more compositional a language is, the more easily it will be picked up by new learners, even when the latter differ in architecture from the original agents. we conclude that compositionality does not arise from simple generalization pressure, but if an emergent language does chance upon it, it will be more likely to survive and thrive."], "interpretability and analysis of models for nlp"], [["gear: graph-based evidence aggregating and reasoning for fact verification", "jie zhou | xu han | cheng yang | zhiyuan liu | lifeng wang | changcheng li | maosong sun", "fact verification (fv) is a challenging task which requires to retrieve relevant evidence from plain text and use the evidence to verify given claims. many claims require to simultaneously integrate and reason over several pieces of evidence for verification. however, previous work employs simple models to extract information from evidence without letting evidence communicate with each other, e.g., merely concatenate the evidence for processing. therefore, these methods are unable to grasp sufficient relational and logical information among the evidence. to alleviate this issue, we propose a graph-based evidence aggregating and reasoning (gear) framework which enables information to transfer on a fully-connected evidence graph and then utilizes different aggregators to collect multi-evidence information. we further employ bert, an effective pre-trained language representation model, to improve the performance. experimental results on a large-scale benchmark dataset fever have demonstrated that gear could leverage multi-evidence information for fv and thus achieves the promising result with a test fever score of 67.10%. our code is available at https://github.com/thunlp/gear."], "semantics"], [["implicit discourse relation classification: we need to talk about evaluation", "najoung kim | song feng | chulaka gunasekara | luis lastras", "implicit relation classification on penn discourse treebank (pdtb) 2.0 is a common benchmark task for evaluating the understanding of discourse relations. however, the lack of consistency in preprocessing and evaluation poses challenges to fair comparison of results in the literature. in this work, we highlight these inconsistencies and propose an improved evaluation protocol. paired with this protocol, we report strong baseline results from pretrained sentence encoders, which set the new state-of-the-art for pdtb 2.0. furthermore, this work is the first to explore fine-grained relation classification on pdtb 3.0. we expect our work to serve as a point of comparison for future work, and also as an initiative to discuss models of larger context and possible data augmentations for downstream transferability."], "discourse and pragmatics"], [["asset: a dataset for tuning and evaluation of sentence simplification models with multiple rewriting transformations", "fernando alva-manchego | louis martin | antoine bordes | carolina scarton | beno\u00eet sagot | lucia specia", "in order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. this makes it impossible to understand the ability of simplification models in more realistic settings. to alleviate this limitation, this paper introduces asset, a new dataset for assessing sentence simplification in english. asset is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. through quantitative and qualitative experiments, we show that simplifications in asset are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. furthermore, we motivate the need for developing better methods for automatic evaluation using asset, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed."], "resources and evaluation"], [["scalable syntax-aware language models using knowledge distillation", "adhiguna kuncoro | chris dyer | laura rimell | stephen clark | phil blunsom", "prior work has shown that, on small amounts of training data, syntactic neural language models learn structurally sensitive generalisations more successfully than sequential language models. however, their computational complexity renders scaling difficult, and it remains an open question whether structural biases are still necessary when sequential models have access to ever larger amounts of training data. to answer this question, we introduce an efficient knowledge distillation (kd) technique that transfers knowledge from a syntactic language model trained on a small corpus to an lstm language model, hence enabling the lstm to develop a more structurally sensitive representation of the larger training data it learns from. on targeted syntactic evaluations, we find that, while sequential lstms perform much better than previously reported, our proposed technique substantially improves on this baseline, yielding a new state of the art. our findings and analysis affirm the importance of structural biases, even in models that learn from large amounts of data."], "tagging, chunking, syntax and parsing"], [["temporally-informed analysis of named entity recognition", "shruti rijhwani | daniel preotiuc-pietro", "natural language processing models often have to make predictions on text data that evolves over time as a result of changes in language use or the information described in the text. however, evaluation results on existing data sets are seldom reported by taking the timestamp of the document into account. we analyze and propose methods that make better use of temporally-diverse training data, with a focus on the task of named entity recognition. to support these experiments, we introduce a novel data set of english tweets annotated with named entities. we empirically demonstrate the effect of temporal drift on performance, and how the temporal information of documents can be used to obtain better models compared to those that disregard temporal information. our analysis gives insights into why this information is useful, in the hope of informing potential avenues of improvement for named entity recognition as well as other nlp tasks under similar experimental setups."], "information extraction, retrieval and text mining"], [["persuasion for good: towards a personalized persuasive dialogue system for social good", "xuewei wang | weiyan shi | richard kim | yoojung oh | sijia yang | jingwen zhang | zhou yu", "developing intelligent persuasive conversational agents to change people\u2019s opinions and actions for social good is the frontier in advancing the ethical development of automated dialogue systems. to do so, the first step is to understand the intricate organization of strategic disclosures and appeals employed in human persuasion conversations. we designed an online persuasion task where one participant was asked to persuade the other to donate to a specific charity. we collected a large dataset with 1,017 dialogues and annotated emerging persuasion strategies from a subset. based on the annotation, we built a baseline classifier with context information and sentence-level features to predict the 10 persuasion strategies used in the corpus. furthermore, to develop an understanding of personalized persuasion processes, we analyzed the relationships between individuals\u2019 demographic and psychological backgrounds including personality, morality, value systems, and their willingness for donation. then, we analyzed which types of persuasion strategies led to a greater amount of donation depending on the individuals\u2019 personal backgrounds. this work lays the ground for developing a personalized persuasive dialogue system."], "dialogue and interactive systems"], [["learning deep transformer models for machine translation", "qiang wang | bei li | tong xiao | jingbo zhu | changliang li | derek f. wong | lidia s. chao", "transformer is the state-of-the-art model in recent machine translation evaluations. two strands of research are promising to improve models of this kind: the first uses wide networks (a.k.a. transformer-big) and has been the de facto standard for development of the transformer system, and the other uses deeper language representation but faces the difficulty arising from learning deep networks. here, we continue the line of research on the latter. we claim that a truly deep transformer model can surpass the transformer-big counterpart by 1) proper use of layer normalization and 2) a novel way of passing the combination of previous layers to the next. on wmt\u201916 english-german and nist openmt\u201912 chinese-english tasks, our deep system (30/25-layer encoder) outperforms the shallow transformer-big/base baseline (6-layer encoder) by 0.4-2.4 bleu points. as another bonus, the deep model is 1.6x smaller in size and 3x faster in training than transformer-big."], "machine translation and multilinguality"], [["schubert: optimizing elements of bert", "ashish khetan | zohar karnin", "transformers have gradually become a key component for many state-of-the-art natural language representation models. a recent transformer based model- bertachieved state-of-the-art results on various natural language processing tasks, including glue, squad v1.1, and squad v2.0. this model however is computationally prohibitive and has a huge number of parameters. in this work we revisit the architecture choices of bert in efforts to obtain a lighter model. we focus on reducing the number of parameters yet our methods can be applied towards other objectives such flops or latency. we show that much efficient light bert models can be obtained by reducing algorithmically chosen correct architecture design dimensions rather than reducing the number of transformer encoder layers. in particular, our schubert gives 6.6% higher average accuracy on glue and squad datasets as compared to bert with three encoder layers while having the same number of parameters."], "machine learning for nlp"], [["unsupervised discovery of gendered language through latent-variable modeling", "alexander miserlis hoyle | lawrence wolf-sonkin | hanna wallach | isabelle augenstein | ryan cotterell", "studying the ways in which language is gendered has long been an area of interest in sociolinguistics. studies have explored, for example, the speech of male and female characters in film and the language used to describe male and female politicians. in this paper, we aim not to merely study this phenomenon qualitatively, but instead to quantify the degree to which the language used to describe men and women is different and, moreover, different in a positive or negative way. to that end, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. we find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes: positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men."], "semantics"], [["a large-scale corpus for conversation disentanglement", "jonathan k. kummerfeld | sai r. gouravajhala | joseph j. peper | vignesh athreya | chulaka gunasekara | jatin ganhotra | siva sankalp patel | lazaros c polymenakos | walter lasecki", "disentangling conversations mixed together in a single stream of messages is a difficult task, made harder by the lack of large manually annotated datasets. we created a new dataset of 77,563 messages manually annotated with reply-structure graphs that both disentangle conversations and define internal conversation structure. our data is 16 times larger than all previously released datasets combined, the first to include adjudication of annotation disagreements, and the first to include context. we use our data to re-examine prior work, in particular, finding that 89% of conversations in a widely used dialogue corpus are either missing messages or contain extra messages. our manually-annotated data presents an opportunity to develop robust data-driven methods for conversation disentanglement, which will help advance dialogue research."], "dialogue and interactive systems"], [["fastbert: a self-distilling bert with adaptive inference time", "weijie liu | peng zhou | zhiruo wang | zhe zhao | haotang deng | qi ju", "pre-trained language models like bert have proven to be highly performant. however, they are often computationally expensive in many practical scenarios, for such heavy models can hardly be readily implemented with limited resources. to improve their efficiency with an assured model performance, we propose a novel speed-tunable fastbert with adaptive inference time. the speed at inference can be flexibly adjusted under varying demands, while redundant calculation of samples is avoided. moreover, this model adopts a unique self-distillation mechanism at fine-tuning, further enabling a greater computational efficacy with minimal loss in performance. our model achieves promising results in twelve english and chinese datasets. it is able to speed up by a wide range from 1 to 12 times than bert if given different speedup thresholds to make a speed-performance tradeoff."], "semantics"], [["on the encoder-decoder incompatibility in variational text modeling and beyond", "chen wu | prince zizhuang wang | william yang wang", "variational autoencoders (vaes) combine latent variables with amortized variational inference, whose optimization usually converges into a trivial local optimum termed posterior collapse, especially in text modeling. by tracking the optimization dynamics, we observe the encoder-decoder incompatibility that leads to poor parameterizations of the data manifold. we argue that the trivial local optimum may be avoided by improving the encoder and decoder parameterizations since the posterior network is part of a transition map between them. to this end, we propose coupled-vae, which couples a vae model with a deterministic autoencoder with the same structure and improves the encoder and decoder parameterizations via encoder weight sharing and decoder signal matching. we apply the proposed coupled-vae approach to various vae models with different regularization, posterior family, decoder structure, and optimization strategy. experiments on benchmark datasets (i.e., ptb, yelp, and yahoo) show consistently improved results in terms of probability estimation and richness of the latent space. we also generalize our method to conditional language modeling and propose coupled-cvae, which largely improves the diversity of dialogue generation on the switchboard dataset."], "machine learning for nlp"], [["analyzing political parody in social media", "antonis maronikolakis | danae s\u00e1nchez villegas | daniel preotiuc-pietro | nikolaos aletras", "parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. in this paper, we present the first computational study of parody. we introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. we run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. our results show that political parody tweets can be predicted with an accuracy up to 90%. finally, we identify the markers of parody through a linguistic analysis. beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances."], "computational social science, social media and cultural analytics"], [["improving lexically constrained neural machine translation with source-conditioned masked span prediction", "gyubok lee | seongjun yang | edward choi", "accurate terminology translation is crucial for ensuring the practicality and reliability of neural machine translation (nmt) systems. to address this, lexically constrained nmt explores various methods to ensure pre-specified words and phrases appear in the translation output. however, in many cases, those methods are studied on general domain corpora, where the terms are mostly uni- and bi-grams (>98%). in this paper, we instead tackle a more challenging setup consisting of domain-specific corpora with much longer n-gram and highly specialized terms. inspired by the recent success of masked span prediction models, we propose a simple and effective training strategy that achieves consistent improvements on both terminology and sentence-level translation for three domain-specific corpora in two language pairs."], "machine translation and multilinguality"], [["word and document embedding with vmf-mixture priors on context word vectors", "shoaib jameel | steven schockaert", "word embedding models typically learn two types of vectors: target word vectors and context word vectors. these vectors are normally learned such that they are predictive of some word co-occurrence statistic, but they are otherwise unconstrained. however, the words from a given language can be organized in various natural groupings, such as syntactic word classes (e.g. nouns, adjectives, verbs) and semantic themes (e.g. sports, politics, sentiment). our hypothesis in this paper is that embedding models can be improved by explicitly imposing a cluster structure on the set of context word vectors. to this end, our model relies on the assumption that context word vectors are drawn from a mixture of von mises-fisher (vmf) distributions, where the parameters of this mixture distribution are jointly optimized with the word vectors. we show that this results in word vectors which are qualitatively different from those obtained with existing word embedding models. we furthermore show that our embedding model can also be used to learn high-quality document representations."], "semantics"], [["dialogue state tracking with explicit slot connection modeling", "yawen ouyang | moxin chen | xinyu dai | yinggong zhao | shujian huang | jiajun chen", "recent proposed approaches have made promising progress in dialogue state tracking (dst). however, in multi-domain scenarios, ellipsis and reference are frequently adopted by users to express values that have been mentioned by slots from other domains. to handle these phenomena, we propose a dialogue state tracking with slot connections (dst-sc) model to explicitly consider slot correlations across different domains. given a target slot, the slot connecting mechanism in dst-sc can infer its source slot and copy the source slot value directly, thus significantly reducing the difficulty of learning and reasoning. experimental results verify the benefits of explicit slot connection modeling, and our model achieves state-of-the-art performance on multiwoz 2.0 and multiwoz 2.1 datasets."], "dialogue and interactive systems"], [["paracrawl: web-scale acquisition of parallel corpora", "marta ba\u00f1\u00f3n | pinzhen chen | barry haddow | kenneth heafield | hieu hoang | miquel espl\u00e0-gomis | mikel l. forcada | amir kamran | faheem kirefu | philipp koehn | sergio ortiz rojas | leopoldo pla sempere | gema ram\u00edrez-s\u00e1nchez | elsa sarr\u00edas | marek strelec | brian thompson | william waites | dion wiggins | jaume zaragoza", "we report on methods to create the largest publicly available parallel corpora by crawling the web, using open source software. we empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering. we also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems."], "resources and evaluation"], [["errudite: scalable, reproducible, and testable error analysis", "tongshuang wu | marco tulio ribeiro | jeffrey heer | daniel weld", "though error analysis is crucial to understanding and improving nlp models, the common practice of manual, subjective categorization of a small sample of errors can yield biased and incomplete conclusions. this paper codifies model and task agnostic principles for informative error analysis, and presents errudite, an interactive tool for better supporting this process. first, error groups should be precisely defined for reproducibility; errudite supports this with an expressive domain-specific language. second, to avoid spurious conclusions, a large set of instances should be analyzed, including both positive and negative examples; errudite enables systematic grouping of relevant instances with filtering queries. third, hypotheses about the cause of errors should be explicitly tested; errudite supports this via automated counterfactual rewriting. we validate our approach with a user study, finding that errudite (1) enables users to perform high quality and reproducible error analyses with less effort, (2) reveals substantial ambiguities in prior published error analyses practices, and (3) enhances the error analysis experience by allowing users to test and revise prior beliefs."], "resources and evaluation"], [["exploiting the syntax-model consistency for neural relation extraction", "amir pouran ben veyseh | franck dernoncourt | dejing dou | thien huu nguyen", "this paper studies the task of relation extraction (re) that aims to identify the semantic relations between two entity mentions in text. in the deep learning models for re, it has been beneficial to incorporate the syntactic structures from the dependency trees of the input sentences. in such models, the dependency trees are often used to directly structure the network architectures or to obtain the dependency relations between the word pairs to inject the syntactic information into the models via multi-task learning. the major problem with these approaches is the lack of generalization beyond the syntactic structures in the training data or the failure to capture the syntactic importance of the words for re. in order to overcome these issues, we propose a novel deep learning model for re that uses the dependency trees to extract the syntax-based importance scores for the words, serving as a tree representation to introduce syntactic information into the models with greater generalization. in particular, we leverage ordered-neuron long-short term memory networks (on-lstm) to infer the model-based importance scores for re for every word in the sentences that are then regulated to be consistent with the syntax-based scores to enable syntactic information injection. we perform extensive experiments to demonstrate the effectiveness of the proposed method, leading to the state-of-the-art performance on three re benchmark datasets."], "information extraction, retrieval and text mining"], [["automatically identifying complaints in social media", "daniel preo\u0163iuc-pietro | mihaela gaman | nikolaos aletras", "complaining is a basic speech act regularly used in human and computer mediated communication to express a negative mismatch between reality and expectations in a particular situation. automatically identifying complaints in social media is of utmost importance for organizations or brands to improve the customer experience or in developing dialogue systems for handling and responding to complaints. in this paper, we introduce the first systematic analysis of complaints in computational linguistics. we collect a new annotated data set of written complaints expressed on twitter. we present an extensive linguistic analysis of complaining as a speech act in social media and train strong feature-based and neural models of complaints across nine domains achieving a predictive performance of up to 79 f1 using distant supervision."], "computational social science, social media and cultural analytics"], [["esprit: explaining solutions to physical reasoning tasks", "nazneen fatema rajani | rui zhang | yi chern tan | stephan zheng | jeremy weiss | aadit vyas | abhijit gupta | caiming xiong | richard socher | dragomir radev", "neural networks lack the ability to reason about qualitative physics and so cannot generalize to scenarios and tasks unseen during training. we propose esprit, a framework for commonsense reasoning about qualitative physics in natural language that generates interpretable descriptions of physical events. we use a two-step approach of first identifying the pivotal physical events in an environment and then generating natural language descriptions of those events using a data-to-text approach. our framework learns to generate explanations of how the physical simulation will causally evolve so that an agent or a human can easily reason about a solution using those interpretable descriptions. human evaluations indicate that esprit produces crucial fine-grained details and has high coverage of physical concepts compared to even human annotations. dataset, code and documentation are available at https://github.com/salesforce/esprit."], "generation"], [["the effectiveness of simple hybrid systems for hypernym discovery", "william held | nizar habash", "hypernymy modeling has largely been separated according to two paradigms, pattern-based methods and distributional methods. however, recent works utilizing a mix of these strategies have yielded state-of-the-art results. this paper evaluates the contribution of both paradigms to hybrid success by evaluating the benefits of hybrid treatment of baseline models from each paradigm. even with a simple methodology for each individual system, utilizing a hybrid approach establishes new state-of-the-art results on two domain-specific english hypernym discovery tasks and outperforms all non-hybrid approaches in a general english hypernym discovery task."], "semantics"], [["exact hard monotonic attention for character-level transduction", "shijie wu | ryan cotterell", "many common character-level, string-to-string transduction tasks, e.g., grapheme-to-phoneme conversion and morphological inflection, consist almost exclusively of monotonic transduction. neural sequence-to-sequence models with soft attention, non-monotonic models, outperform popular monotonic models. in this work, we ask the following question: is monotonicity really a helpful inductive bias in these tasks? we develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns alignment jointly. with the help of dynamic programming, we are able to compute the exact marginalization over all alignments. our models achieve state-of-the-art performance on morphological inflection. furthermore, we find strong performance on two other character-level transduction tasks. code is available at https://github.com/shijie-wu/neural-transducer."], "machine learning for nlp"], [["reducing gender bias in neural machine translation as a domain adaptation problem", "danielle saunders | bill byrne", "training data for nlp tasks often exhibits gender bias in that fewer sentences refer to women than to men. in neural machine translation (nmt) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. the recent winomt challenge set allows us to measure this effect directly (stanovsky et al, 2019) ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. rather than attempt to create a \u2018balanced\u2019 dataset, we use transfer learning on a small set of trusted, gender-balanced examples. this approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. a known pitfall of transfer learning on new domains is \u2018catastrophic forgetting\u2019, which we address at adaptation and inference time. during adaptation we show that elastic weight consolidation allows a performance trade-off between general translation quality and bias reduction. at inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in stanovsky et al, 2019 on winomt with no degradation of general test set bleu. we demonstrate our approach translating from english into three languages with varied linguistic properties and data availability."], "machine translation and multilinguality"], [["temporal common sense acquisition with minimal supervision", "ben zhou | qiang ning | daniel khashabi | dan roth", "temporal common sense (e.g., duration and frequency of events) is crucial for understanding natural language. however, its acquisition is challenging, partly because such information is often not expressed explicitly in text, and human annotation on such concepts is costly. this work proposes a novel sequence modeling approach that exploits explicit and implicit mentions of temporal common sense, extracted from a large corpus, to build tacolm, a temporal common sense language model. our method is shown to give quality predictions of various dimensions of temporal common sense (on udst and a newly collected dataset from realnews). it also produces representations of events for relevant tasks such as duration comparison, parent-child relations, event coreference and temporal qa (on timebank, hieve and mctaco) that are better than using the standard bert. thus, it will be an important component of temporal nlp."], "semantics"], [["s2orc: the semantic scholar open research corpus", "kyle lo | lucy lu wang | mark neumann | rodney kinney | daniel weld", "we introduce s2orc, a large corpus of 81.1m english-language academic papers spanning many academic disciplines. the corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1m open access papers. full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. in s2orc, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. we hope this resource will facilitate research and development of tools and tasks for text mining over academic text."], "resources and evaluation"], [["universal decompositional semantic parsing", "elias stengel-eskin | aaron steven white | sheng zhang | benjamin van durme", "we introduce a transductive model for parsing into universal decompositional semantics (uds) representations, which jointly learns to map natural language utterances into uds graph structures and annotate the graph with decompositional semantic attribute scores. we also introduce a strong pipeline model for parsing into the uds graph structure, and show that our transductive parser performs comparably while additionally performing attribute prediction. by analyzing the attribute prediction errors, we find the model captures natural relationships between attribute groups."], "semantics"], [["imitation learning for non-autoregressive neural machine translation", "bingzhen wei | mingxuan wang | hao zhou | junyang lin | xu sun", "non-autoregressive translation models (nat) have achieved impressive inference speedup. a potential issue of the existing nat algorithms, however, is that the decoding is conducted in parallel, without directly considering previous context. in this paper, we propose an imitation learning framework for non-autoregressive machine translation, which still enjoys the fast translation speed but gives comparable translation performance compared to its auto-regressive counterpart. we conduct experiments on the iwslt16, wmt14 and wmt16 datasets. our proposed model achieves a significant speedup over the autoregressive models, while keeping the translation quality comparable to the autoregressive models. by sampling sentence length in parallel at inference time, we achieve the performance of 31.85 bleu on wmt16 ro\u2192en and 30.68 bleu on iwslt16 en\u2192de."], "machine translation and multilinguality"], [["a recipe for creating multimodal aligned datasets for sequential tasks", "angela lin | sudha rao | asli celikyilmaz | elnaz nouri | chris brockett | debadeepta dey | bill dolan", "many high-level procedural tasks can be decomposed into sequences of instructions that vary in their order and choice of tools. in the cooking domain, the web offers many, partially-overlapping, text and video recipes (i.e. procedures) that describe how to make the same dish (i.e. high-level task). aligning instructions for the same dish across different sources can yield descriptive visual explanations that are far richer semantically than conventional textual instructions, providing commonsense insight into how real-world procedures are structured. learning to align these different instruction sets is challenging because: a) different recipes vary in their order of instructions and use of ingredients; and b) video instructions can be noisy and tend to contain far more information than text instructions. to address these challenges, we use an unsupervised alignment algorithm that learns pairwise alignments between instructions of different recipes for the same dish. we then use a graph algorithm to derive a joint alignment between multiple text and multiple video recipes for the same dish. we release the microsoft research multimodal aligned recipe corpus containing ~150k pairwise alignments between recipes across 4262 dishes with rich commonsense information."], "resources and evaluation"], [["bayesian hierarchical words representation learning", "oren barkan | idan rejwan | avi caciularu | noam koenigstein", "this paper presents the bayesian hierarchical words representation (bhwr) learning algorithm. bhwr facilitates variational bayes word representation learning combined with semantic taxonomy modeling via hierarchical priors. by propagating relevant information between related words, bhwr utilizes the taxonomy to improve the quality of such representations. evaluation of several linguistic datasets demonstrates the advantages of bhwr over suitable alternatives that facilitate bayesian modeling with or without semantic priors. finally, we further show that bhwr produces better representations for rare words."], "machine learning for nlp"], [["improving non-autoregressive neural machine translation with monolingual data", "jiawei zhou | phillip keung", "non-autoregressive (nar) neural machine translation is usually done via knowledge distillation from an autoregressive (ar) model. under this framework, we leverage large monolingual corpora to improve the nar model\u2019s performance, with the goal of transferring the ar model\u2019s generalization ability while preventing overfitting. on top of a strong nar baseline, our experimental results on the wmt14 en-de and wmt16 en-ro news translation tasks confirm that monolingual data augmentation consistently improves the performance of the nar model to approach the teacher ar model\u2019s performance, yields comparable or better results than the best non-iterative nar methods in the literature and helps reduce overfitting in the training process."], "machine translation and multilinguality"], [["soft gazetteers for low-resource named entity recognition", "shruti rijhwani | shuyan zhou | graham neubig | jaime carbonell", "traditional named entity recognition models use gazetteers (lists of entities) as features to improve performance. although modern neural network models do not require such hand-crafted features for strong performance, recent work has demonstrated their utility for named entity recognition on english data. however, designing such features for low-resource languages is challenging, because exhaustive entity gazetteers do not exist in these languages. to address this problem, we propose a method of \u201csoft gazetteers\u201d that incorporates ubiquitously available information from english knowledge bases, such as wikipedia, into neural named entity recognition models through cross-lingual entity linking. our experiments on four low-resource languages show an average improvement of 4 points in f1 score."], "information extraction, retrieval and text mining"], [["relation extraction with explanation", "hamed shahbazi | xiaoli fern | reza ghaeini | prasad tadepalli", "recent neural models for relation extraction with distant supervision alleviate the impact of irrelevant sentences in a bag by learning importance weights for the sentences. efforts thus far have focused on improving extraction accuracy but little is known about their explanability. in this work we annotate a test set with ground-truth sentence-level explanations to evaluate the quality of explanations afforded by the relation extraction models. we demonstrate that replacing the entity mentions in the sentences with their fine-grained entity types not only enhances extraction accuracy but also improves explanation. we also propose to automatically generate \u201cdistractor\u201d sentences to augment the bags and train the model to ignore the distractors. evaluations on the widely used fb-nyt dataset show that our methods achieve new state-of-the-art accuracy while improving model explanability."], "information extraction, retrieval and text mining"], [["taxonomy construction of unseen domains via graph-based cross-domain knowledge transfer", "chao shang | sarthak dash | md. faisal mahbub chowdhury | nandana mihindukulasooriya | alfio gliozzo", "extracting lexico-semantic relations as graph-structured taxonomies, also known as taxonomy construction, has been beneficial in a variety of nlp applications. recently graph neural network (gnn) has shown to be powerful in successfully tackling many tasks. however, there has been no attempt to exploit gnn to create taxonomies. in this paper, we propose graph2taxo, a gnn-based cross-domain transfer framework for the taxonomy construction task. our main contribution is to learn the latent features of taxonomy construction from existing domains to guide the structure learning of an unseen domain. we also propose a novel method of directed acyclic graph (dag) generation for taxonomy construction. specifically, our proposed graph2taxo uses a noisy graph constructed from automatically extracted noisy hyponym hypernym candidate pairs, and a set of taxonomies for some known domains for training. the learned model is then used to generate taxonomy for a new unknown domain given a set of terms for that domain. experiments on benchmark datasets from science and environment domains show that our approach attains significant improvements correspondingly over the state of the art."], "machine learning for nlp"], [["graph neural news recommendation with unsupervised preference disentanglement", "linmei hu | siyong xu | chen li | cheng yang | chuan shi | nan duan | xing xie | ming zhou", "with the explosion of news information, personalized news recommendation has become very important for users to quickly find their interested contents. most existing methods usually learn the representations of users and news from news contents for recommendation. however, they seldom consider high-order connectivity underlying the user-news interactions. moreover, existing methods failed to disentangle a user\u2019s latent preference factors which cause her clicks on different news. in this paper, we model the user-news interactions as a bipartite graph and propose a novel graph neural news recommendation model with unsupervised preference disentanglement, named gnud. our model can encode high-order relationships into user and news representations by information propagation along the graph. furthermore, the learned representations are disentangled with latent preference factors by a neighborhood routing algorithm, which can enhance expressiveness and interpretability. a preference regularizer is also designed to force each disentangled subspace to independently reflect an isolated preference, improving the quality of the disentangled representations. experimental results on real-world news datasets demonstrate that our proposed model can effectively improve the performance of news recommendation and outperform state-of-the-art news recommendation methods."], "nlp applications"], [["give me more feedback ii: annotating thesis strength and related attributes in student essays", "zixuan ke | hrishikesh inamdar | hui lin | vincent ng", "while the vast majority of existing work on automated essay scoring has focused on holistic scoring, researchers have recently begun work on scoring specific dimensions of essay quality. nevertheless, progress on dimension-specific essay scoring is limited in part by the lack of annotated corpora. to facilitate advances in this area, we design a scoring rubric for scoring a core, yet unexplored dimension of persuasive essay quality, thesis strength, and annotate a corpus of essays with thesis strength scores. we additionally identify the attributes that could impact thesis strength and annotate the essays with the values of these attributes, which, when predicted by computational models, could provide further feedback to students on why her essay receives a particular thesis strength score."], "linguistic theories, cognitive modeling and psycholinguistics"], [["aligned dual channel graph convolutional network for visual question answering", "qingbao huang | jielong wei | yi cai | changmeng zheng | junying chen | ho-fung leung | qing li", "visual question answering aims to answer the natural language question about a given image. existing graph-based methods only focus on the relations between objects in an image and neglect the importance of the syntactic dependency relations between words in a question. to simultaneously capture the relations between objects in an image and the syntactic dependency relations between words in a question, we propose a novel dual channel graph convolutional network (dc-gcn) for better combining visual and textual advantages. the dc-gcn model consists of three parts: an i-gcn module to capture the relations between objects in an image, a q-gcn module to capture the syntactic dependency relations between words in a question, and an attention alignment module to align image representations and question representations. experimental results show that our model achieves comparable performance with the state-of-the-art approaches."], "language grounding to vision, robotics and beyond"], [["syntactically supervised transformers for faster neural machine translation", "nader akoury | kalpesh krishna | mohit iyyer", "standard decoders for neural machine translation autoregressively generate a single target token per timestep, which slows inference especially for long outputs. while architectural advances such as the transformer fully parallelize the decoder computations at training time, inference still proceeds sequentially. recent developments in non- and semi-autoregressive decoding produce multiple tokens per timestep independently of the others, which improves inference speed but deteriorates translation quality. in this work, we propose the syntactically supervised transformer (synst), which first autoregressively predicts a chunked parse tree before generating all of the target tokens in one shot conditioned on the predicted parse. a series of controlled experiments demonstrates that synst decodes sentences ~5x faster than the baseline autoregressive transformer while achieving higher bleu scores than most competing methods on en-de and en-fr datasets."], "machine translation and multilinguality"], [["evaluating explanation methods for neural machine translation", "jierui li | lemao liu | huayang li | guanlin li | guoping huang | shuming shi", "recently many efforts have been devoted to interpreting the black-box nmt models, but little progress has been made on metrics to evaluate explanation methods. word alignment error rate can be used as such a metric that matches human understanding, however, it can not measure explanation methods on those target words that are not aligned to any source word. this paper thereby makes an initial attempt to evaluate explanation methods from an alternative viewpoint. to this end, it proposes a principled metric based on fidelity in regard to the predictive behavior of the nmt model. as the exact computation for this metric is intractable, we employ an efficient approach as its approximation. on six standard translation tasks, we quantitatively evaluate several explanation methods in terms of the proposed metric and we reveal some valuable findings for these explanation methods in our experiments."], "machine translation and multilinguality"], [["adversarial learning of privacy-preserving text representations for de-identification of medical records", "max friedrich | arne k\u00f6hn | gregor wiedemann | chris biemann", "de-identification is the task of detecting protected health information (phi) in medical text. it is a critical step in sanitizing electronic health records (ehr) to be shared for research. automatic de-identification classifiers can significantly speed up the sanitization process. however, obtaining a large and diverse dataset to train such a classifier that works well across many types of medical text poses a challenge as privacy laws prohibit the sharing of raw medical records. we introduce a method to create privacy-preserving shareable representations of medical text (i.e. they contain no phi) that does not require expensive manual pseudonymization. these representations can be shared between organizations to create unified datasets for training de-identification models. our representation allows training a simple lstm-crf de-identification model to an f1 score of 97.4%, which is comparable to a strong baseline that exposes private information in its representation. a robust, widely available de-identification classifier based on our representation could potentially enable studies for which de-identification would otherwise be too costly."], "information extraction, retrieval and text mining"], [["bayes test of precision, recall, and f1 measure for comparison of two natural language processing models", "ruibo wang | jihong li", "direct comparison on point estimation of the precision (p), recall (r), and f1 measure of two natural language processing (nlp) models on a common test corpus is unreasonable and results in less replicable conclusions due to a lack of a statistical test. however, the existing t-tests in cross-validation (cv) for model comparison are inappropriate because the distributions of p, r, f1 are skewed and an interval estimation of p, r, and f1 based on a t-test may exceed [0,1]. in this study, we propose to use a block-regularized 3\u00d72 cv (3\u00d72 bcv) in model comparison because it could regularize the difference in certain frequency distributions over linguistic units between training and validation sets and yield stable estimators of p, r, and f1. on the basis of the 3\u00d72 bcv, we calibrate the posterior distributions of p, r, and f1 and derive an accurate interval estimation of p, r, and f1. furthermore, we formulate the comparison into a hypothesis testing problem and propose a novel bayes test. the test could directly compute the probabilities of the hypotheses on the basis of the posterior distributions and provide more informative decisions than the existing significance t-tests. three experiments with regard to nlp chunking tasks are conducted, and the results illustrate the validity of the bayes test."], "machine learning for nlp"], [["evaluating explainable ai: which algorithmic explanations help users predict model behavior?", "peter hase | mohit bansal", "algorithmic approaches to interpreting machine learning models have proliferated in recent years. we carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors. a model is simulatable when a person can predict its behavior on new inputs. through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) lime, (2) anchor, (3) decision boundary, (4) a prototype model, and (5) a composite approach that combines explanations from each method. clear evidence of method effectiveness is found in very few cases: lime improves simulatability in tabular classification, and our prototype method is effective in counterfactual simulation tests. we also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are. our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains. we show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods."], "interpretability and analysis of models for nlp"], [["multi-channel graph neural network for entity alignment", "yixin cao | zhiyuan liu | chengjiang li | zhiyuan liu | juanzi li | tat-seng chua", "entity alignment typically suffers from the issues of structural heterogeneity and limited seed alignments. in this paper, we propose a novel multi-channel graph neural network model (mugnn) to learn alignment-oriented knowledge graph (kg) embeddings by robustly encoding two kgs via multiple channels. each channel encodes kgs via different relation weighting schemes with respect to self-attention towards kg completion and cross-kg attention for pruning exclusive entities respectively, which are further combined via pooling techniques. moreover, we also infer and transfer rule knowledge for completing two kgs consistently. mugnn is expected to reconcile the structural differences of two kgs, and thus make better use of seed alignments. extensive experiments on five publicly available datasets demonstrate our superior performance (5% hits@1 up on average). source code and data used in the experiments can be accessed at https://github.com/thunlp/mugnn ."], "information extraction, retrieval and text mining"], [["syntactic data augmentation increases robustness to inference heuristics", "junghyun min | r. thomas mccoy | dipanjan das | emily pitler | tal linzen", "pretrained neural models such as bert, when fine-tuned to perform natural language inference (nli), often show high accuracy on standard datasets, but display a surprising lack of sensitivity to word order on controlled challenge sets. we hypothesize that this issue is not primarily caused by the pretrained model\u2019s limitations, but rather by the paucity of crowdsourced nli examples that might convey the importance of syntactic structure at the fine-tuning stage. we explore several methods to augment standard training sets with syntactically informative examples, generated by applying syntactic transformations to sentences from the mnli corpus. the best-performing augmentation method, subject/object inversion, improved bert\u2019s accuracy on controlled examples that diagnose sensitivity to word order from 0.28 to 0.73, without affecting performance on the mnli test set. this improvement generalized beyond the particular construction used for data augmentation, suggesting that augmentation causes bert to recruit abstract syntactic representations."], "semantics"], [["generating sentences from disentangled syntactic and semantic spaces", "yu bao | hao zhou | shujian huang | lei li | lili mou | olga vechtomova | xin-yu dai | jiajun chen", "variational auto-encoders (vaes) are widely used in natural language generation due to the regularization of the latent space. however, generating sentences from the continuous latent space does not explicitly model the syntactic information. in this paper, we propose to generate sentences from disentangled syntactic and semantic spaces. our proposed method explicitly models syntactic information in the vae\u2019s latent space by using the linearized tree sequence, leading to better performance of language generation. additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntax transfer generation. experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work."], "generation"], [["fine-grained temporal relation extraction", "siddharth vashishtha | benjamin van durme | aaron steven white", "we present a novel semantic framework for modeling temporal relations and event durations that maps pairs of events to real-valued scales. we use this framework to construct the largest temporal relations dataset to date, covering the entirety of the universal dependencies english web treebank. we use this dataset to train models for jointly predicting fine-grained temporal relations and event durations. we report strong results on our data and show the efficacy of a transfer-learning approach for predicting categorical relations."], "information extraction, retrieval and text mining"], [["eliciting knowledge from experts: automatic transcript parsing for cognitive task analysis", "junyi du | he jiang | jiaming shen | xiang ren", "cognitive task analysis (cta) is a type of analysis in applied psychology aimed at eliciting and representing the knowledge and thought processes of domain experts. in cta, often heavy human labor is involved to parse the interview transcript into structured knowledge (e.g., flowchart for different actions). to reduce human efforts and scale the process, automated cta transcript parsing is desirable. however, this task has unique challenges as (1) it requires the understanding of long-range context information in conversational text; and (2) the amount of labeled data is limited and indirect\u2014i.e., context-aware, noisy, and low-resource. in this paper, we propose a weakly-supervised information extraction framework for automated cta transcript parsing. we partition the parsing process into a sequence labeling task and a text span-pair relation extraction task, with distant supervision from human-curated protocol files. to model long-range context information for extracting sentence relations, neighbor sentences are involved as a part of input. different types of models for capturing context dependency are then applied. we manually annotate real-world cta transcripts to facilitate the evaluation of the parsing tasks."], "nlp applications"], [["deseption: dual sequence prediction and adversarial examples for improved fact-checking", "christopher hidey | tuhin chakrabarty | tariq alhindi | siddharth varia | kriste krstovski | mona diab | smaranda muresan", "the increased focus on misinformation has spurred development of data and systems for detecting the veracity of a claim as well as retrieving authoritative evidence. the fact extraction and verification (fever) dataset provides such a resource for evaluating endto- end fact-checking, requiring retrieval of evidence from wikipedia to validate a veracity prediction. we show that current systems for fever are vulnerable to three categories of realistic challenges for fact-checking \u2013 multiple propositions, temporal reasoning, and ambiguity and lexical variation \u2013 and introduce a resource with these types of claims. then we present a system designed to be resilient to these \u201cattacks\u201d using multiple pointer networks for document selection and jointly modeling a sequence of evidence sentences and veracity relation predictions. we find that in handling these attacks we obtain state-of-the-art results on fever, largely due to improved evidence retrieval."], "nlp applications"], [["real-time open-domain question answering with dense-sparse phrase index", "minjoon seo | jinhyuk lee | tom kwiatkowski | ankur parikh | ali farhadi | hannaneh hajishirzi", "existing open-domain question answering (qa) models are not suitable for real-time usage because they need to process several long documents on-demand for every input query, which is computationally prohibitive. in this paper, we introduce query-agnostic indexable representations of document phrases that can drastically speed up open-domain qa. in particular, our dense-sparse phrase encoding effectively captures syntactic, semantic, and lexical information of the phrases and eliminates the pipeline filtering of context documents. leveraging strategies for optimizing training and inference time, our model can be trained and deployed even in a single 4-gpu server. moreover, by representing phrases as pointers to their start and end tokens, our model indexes phrases in the entire english wikipedia (up to 60 billion phrases) using under 2tb. our experiments on squad-open show that our model is on par with or more accurate than previous models with 6000x reduced computational cost, which translates into at least 68x faster end-to-end inference benchmark on cpus. code and demo are available at nlp.cs.washington.edu/denspi"], "machine learning for nlp"], [["reference network for neural machine translation", "han fu | chenghao liu | jianling sun", "neural machine translation (nmt) has achieved notable success in recent years. such a framework usually generates translations in isolation. in contrast, human translators often refer to reference data, either rephrasing the intricate sentence fragments with common terms in source language, or just accessing to the golden translation directly. in this paper, we propose a reference network to incorporate referring process into translation decoding of nmt. to construct a reference book, an intuitive way is to store the detailed translation history with extra memory, which is computationally expensive. instead, we employ local coordinates coding (lcc) to obtain global context vectors containing monolingual and bilingual contextual information for nmt decoding. experimental results on chinese-english and english-german tasks demonstrate that our proposed model is effective in improving the translation quality with lightweight computation cost."], "machine translation and multilinguality"], [["bringing structure into summaries: a faceted summarization dataset for long scientific documents", "rui meng | khushboo thaker | lei zhang | yue dong | xingdi yuan | tong wang | daqing he", "faceted summarization provides briefings of a document from different perspectives. readers can quickly comprehend the main points of a long document with the help of a structured outline. however, little research has been conducted on this subject, partially due to the lack of large-scale faceted summarization datasets. in this study, we present facetsum, a faceted summarization benchmark built on emerald journal articles, covering a diverse range of domains. different from traditional document-summary pairs, facetsum provides multiple summaries, each targeted at specific sections of a long document, including the purpose, method, findings, and value. analyses and empirical results on our dataset reveal the importance of bringing structure into summaries. we believe facetsum will spur further advances in summarization research and foster the development of nlp systems that can leverage the structured information in both long texts and summaries."], "resources and evaluation"], [["multi-modal sarcasm detection in twitter with hierarchical fusion model", "yitao cai | huiyu cai | xiaojun wan", "sarcasm is a subtle form of language in which people express the opposite of what is implied. previous works of sarcasm detection focused on texts. however, more and more social media platforms like twitter allow users to create multi-modal messages, including texts, images, and videos. it is insufficient to detect sarcasm from multi-model messages based only on texts. in this paper, we focus on multi-modal sarcasm detection for tweets consisting of texts and images in twitter. we treat text features, image features and image attributes as three modalities and propose a multi-modal hierarchical fusion model to address this task. our model first extracts image features and attribute features, and then leverages attribute features and bidirectional lstm network to extract text features. features of three modalities are then reconstructed and fused into one feature vector for prediction. we create a multi-modal sarcasm detection dataset based on twitter. evaluation results on the dataset demonstrate the efficacy of our proposed model and the usefulness of the three modalities."], "computational social science, social media and cultural analytics"], [["depth growing for neural machine translation", "lijun wu | yiren wang | yingce xia | fei tian | fei gao | tao qin | jianhuang lai | tie-yan liu", "while very deep neural networks have shown effectiveness for computer vision and text classification applications, how to increase the network depth of the neural machine translation (nmt) models for better translation quality remains a challenging problem. directly stacking more blocks to the nmt model results in no improvement and even drop in performance. in this work, we propose an effective two-stage approach with three specially designed components to construct deeper nmt models, which result in significant improvements over the strong transformer baselines on wmt14 english\u2192german and english\u2192french translation tasks."], "machine learning for nlp"], [["improving transformer models by reordering their sublayers", "ofir press | noah a. smith | omer levy", "multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. could ordering the sublayers in a different pattern lead to better performance? we generate randomly ordered transformers and train them with the language modeling objective. we observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. we propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. however, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains."], "machine learning for nlp"], [["nne: a dataset for nested named entity recognition in english newswire", "nicky ringland | xiang dai | ben hachey | sarvnaz karimi | cecile paris | james r. curran", "named entity recognition (ner) is widely used in natural language processing applications and downstream tasks. however, most ner tools target flat annotation from popular datasets, eschewing the semantic information available in nested entity mentions. we describe nne\u2014a fine-grained, nested named entity dataset over the full wall street journal portion of the penn treebank (ptb). our annotation comprises 279,795 mentions of 114 entity types with up to 6 layers of nesting. we hope the public release of this large dataset for english newswire will encourage development of new techniques for nested ner."], "information extraction, retrieval and text mining"], [["exploring pre-trained language models for event extraction and generation", "sen yang | dawei feng | linbo qiao | zhigang kan | dongsheng li", "traditional approaches to the task of ace event extraction usually depend on manually annotated data, which is often laborious to create and limited in size. therefore, in addition to the difficulty of event extraction itself, insufficient training data hinders the learning process as well. to promote event extraction, we first propose an event extraction model to overcome the roles overlap problem by separating the argument prediction in terms of roles. moreover, to address the problem of insufficient training data, we propose a method to automatically generate labeled data by editing prototypes and screen out generated samples by ranking the quality. experiments on the ace2005 dataset demonstrate that our extraction model can surpass most existing extraction methods. besides, incorporating our generation method exhibits further significant improvement. it obtains new state-of-the-art results on the event extraction task, including pushing the f1 score of trigger classification to 81.1%, and the f1 score of argument classification to 58.9%."], "information extraction, retrieval and text mining"], [["improving multi-turn dialogue modelling with utterance rewriter", "hui su | xiaoyu shen | rongzhi zhang | fei sun | pengwei hu | cheng niu | jie zhou", "recent research has achieved impressive results in single-turn dialogue modelling. in the multi-turn setting, however, current models are still far from satisfactory. one major challenge is the frequently occurred coreference and information omission in our daily conversation, making it hard for machines to understand the real intention. in this paper, we propose rewriting the human utterance as a pre-process to help multi-turn dialgoue modelling. each utterance is first rewritten to recover all coreferred and omitted information. the next processing steps are then performed based on the rewritten utterance. to properly train the utterance rewriter, we collect a new dataset with human annotations and introduce a transformer-based utterance rewriting architecture using the pointer network. we show the proposed architecture achieves remarkably good performance on the utterance rewriting task. the trained utterance rewriter can be easily integrated into online chatbots and brings general improvement over different domains."], "dialogue and interactive systems"], [["content word aware neural machine translation", "kehai chen | rui wang | masao utiyama | eiichiro sumita", "neural machine translation (nmt) encodes the source sentence in a universal way to generate the target sentence word-by-word. however, nmt does not consider the importance of word in the sentence meaning, for example, some words (i.e., content words) express more important meaning than others (i.e., function words). to address this limitation, we first utilize word frequency information to distinguish between content and function words in a sentence, and then design a content word-aware nmt to improve translation performance. empirical results on the wmt14 english-to-german, wmt14 english-to-french, and wmt17 chinese-to-english translation tasks show that the proposed methods can significantly improve the performance of transformer-based nmt."], "machine translation and multilinguality"], [["toward comprehensive understanding of a sentiment based on human motives", "naoki otani | eduard hovy", "in sentiment detection, the natural language processing community has focused on determining holders, facets, and valences, but has paid little attention to the reasons for sentiment decisions. our work considers human motives as the driver for human sentiments and addresses the problem of motive detection as the first step. following a study in psychology, we define six basic motives that cover a wide range of topics appearing in review texts, annotate 1,600 texts in restaurant and laptop domains with the motives, and report the performance of baseline methods on this new dataset. we also show that cross-domain transfer learning boosts detection performance, which indicates that these universal motives exist across different domains."], "sentiment analysis, stylistic analysis, and argument mining"], [["multimodal transformer for multimodal machine translation", "shaowei yao | xiaojun wan", "multimodal machine translation (mmt) aims to introduce information from other modality, generally static images, to improve the translation quality. previous works propose various incorporation methods, but most of them do not consider the relative importance of multiple modalities. equally treating all modalities may encode too much useless information from less important modalities. in this paper, we introduce the multimodal self-attention in transformer to solve the issues above in mmt. the proposed method learns the representation of images based on the text, which avoids encoding irrelevant information in images. experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics."], "speech and multimodality"], [["towards near-imperceptible steganographic text", "falcon dai | zheng cai", "we show that the imperceptibility of several existing linguistic steganographic systems (fang et al., 2017; yang et al., 2018) relies on implicit assumptions on statistical behaviors of fluent text. we formally analyze them and empirically evaluate these assumptions. furthermore, based on these observations, we propose an encoding algorithm called patient-huffman with improved near-imperceptible guarantees."], "nlp applications"], [["scoring sentence singletons and pairs for abstractive summarization", "logan lebanoff | kaiqiang song | franck dernoncourt | doo soon kim | seokhwan kim | walter chang | fei liu", "when writing a summary, humans tend to choose content from one or two sentences and merge them into a single summary sentence. however, the mechanisms behind the selection of one or multiple source sentences remain poorly understood. sentence fusion assumes multi-sentence input; yet sentence selection methods only work with single sentences and not combinations of them. there is thus a crucial gap between sentence selection and fusion to support summarizing by both compressing single sentences and fusing pairs. this paper attempts to bridge the gap by ranking sentence singletons and pairs together in a unified space. our proposed framework attempts to model human methodology by selecting either a single sentence or a pair of sentences, then compressing or fusing the sentence(s) to produce a summary sentence. we conduct extensive experiments on both single- and multi-document summarization datasets and report findings on sentence selection and abstraction."], "summarization"], [["petra: a sparsely supervised memory model for people tracking", "shubham toshniwal | allyson ettinger | kevin gimpel | karen livescu", "we propose petra, a memory-augmented neural network designed to track entities in its memory slots. petra is trained using sparse annotation from the gap pronoun resolution dataset and outperforms a prior memory model on the task while using a simpler architecture. we empirically compare key modeling choices, finding that we can simplify several aspects of the design of the memory module while retaining strong performance. to measure the people tracking capability of memory models, we (a) propose a new diagnostic evaluation based on counting the number of unique entities in text, and (b) conduct a small scale human evaluation to compare evidence of people tracking in the memory logs of petra relative to a previous approach. petra is highly effective in both evaluations, demonstrating its ability to track people in its memory despite being trained with limited annotation."], "discourse and pragmatics"], [["improving the robustness of question answering systems to question paraphrasing", "wee chung gan | hwee tou ng", "despite the advancement of question answering (qa) systems and rapid improvements on held-out test sets, their generalizability is a topic of concern. we explore the robustness of qa models to question paraphrasing by creating two test sets consisting of paraphrased squad questions. paraphrased questions from the first test set are very similar to the original questions designed to test qa models\u2019 over-sensitivity, while questions from the second test set are paraphrased using context words near an incorrect answer candidate in an attempt to confuse qa models. we show that both paraphrased test sets lead to significant decrease in performance on multiple state-of-the-art qa models. using a neural paraphrasing model trained to generate multiple paraphrased questions for a given source question and a set of paraphrase suggestions, we propose a data augmentation approach that requires no human intervention to re-train the models for improved robustness to question paraphrasing."], "question answering"], [["stre: self attentive edit quality prediction in wikipedia", "soumya sarkar | bhanu prakash reddy | sandipan sikdar | animesh mukherjee", "wikipedia can easily be justified as a behemoth, considering the sheer volume of content that is added or removed every minute to its several projects. this creates an immense scope, in the field of natural language processing toward developing automated tools for content moderation and review. in this paper we propose self attentive revision encoder (stre) which leverages orthographic similarity of lexical units toward predicting the quality of new edits. in contrast to existing propositions which primarily employ features like page reputation, editor activity or rule based heuristics, we utilize the textual content of the edits which, we believe contains superior signatures of their quality. more specifically, we deploy deep encoders to generate representations of the edits from its text content, which we then leverage to infer quality. we further contribute a novel dataset containing \u223c 21m revisions across 32k wikipedia pages and demonstrate that stre outperforms existing methods by a significant margin \u2013 at least 17% and at most 103%. our pre-trained model achieves such result after retraining on a set as small as 20% of the edits in a wikipage. this, to the best of our knowledge, is also the first attempt towards employing deep language models to the enormous domain of automated content moderation and review in wikipedia."], "linguistic theories, cognitive modeling and psycholinguistics"], [["who sides with whom? towards computational construction of discourse networks for political debates", "sebastian pad\u00f3 | andre blessing | nico blokker | erenay dayanik | sebastian haunss | jonas kuhn", "understanding the structures of political debates (which actors make what claims) is essential for understanding democratic political decision making. the vision of computational construction of such discourse networks from newspaper reports brings together political science and natural language processing. this paper presents three contributions towards this goal: (a) a requirements analysis, linking the task to knowledge base population; (b) an annotated pilot corpus of migration claims based on german newspaper reports; (c) initial modeling results."], "computational social science, social media and cultural analytics"], [["cross-domain ner using cross-domain language modeling", "chen jia | xiaobo liang | yue zhang", "due to limitation of labeled resources, cross-domain named entity recognition (ner) has been a challenging task. most existing work considers a supervised setting, making use of labeled data for both the source and target domains. a disadvantage of such methods is that they cannot train for domains without ner data. to address this issue, we consider using cross-domain lm as a bridge cross-domains for ner domain adaptation, performing cross-domain and cross-task knowledge transfer by designing a novel parameter generation network. results show that our method can effectively extract domain differences from cross-domain lm contrast, allowing unsupervised domain adaptation while also giving state-of-the-art results among supervised domain adaptation methods."], "tagging, chunking, syntax and parsing"], [["semi-supervised text classification with balanced deep representation distributions", "changchun li | ximing li | jihong ouyang", "semi-supervised text classification (sstc) mainly works under the spirit of self-training. they initialize the deep classifier by training over labeled texts; and then alternatively predict unlabeled texts as their pseudo-labels and train the deep classifier over the mixture of labeled and pseudo-labeled texts. naturally, their performance is largely affected by the accuracy of pseudo-labels for unlabeled texts. unfortunately, they often suffer from low accuracy because of the margin bias problem caused by the large difference between representation distributions of labels in sstc. to alleviate this problem, we apply the angular margin loss, and perform gaussian linear transformation to achieve balanced label angle variances, i.e., the variance of label angles of texts within the same label. more accuracy of predicted pseudo-labels can be achieved by constraining all label angle variances balanced, where they are estimated over both labeled and pseudo-labeled texts during self-training loops. with this insight, we propose a novel sstc method, namely semi-supervised text classification with balanced deep representation distributions (s2tc-bdd). to evaluate s2tc-bdd, we compare it against the state-of-the-art sstc methods. empirical results demonstrate the effectiveness of s2tc-bdd, especially when the labeled texts are scarce."], "information extraction, retrieval and text mining"], [["analyzing the persuasive effect of style in news editorial argumentation", "roxanne el baff | henning wachsmuth | khalid al khatib | benno stein", "news editorials argue about political issues in order to challenge or reinforce the stance of readers with different ideologies. previous research has investigated such persuasive effects for argumentative content. in contrast, this paper studies how important the style of news editorials is to achieve persuasion. to this end, we first compare content- and style-oriented classifiers on editorials from the liberal nytimes with ideology-specific effect annotations. we find that conservative readers are resistant to nytimes style, but on liberals, style even has more impact than content. focusing on liberals, we then cluster the leads, bodies, and endings of editorials, in order to learn about writing style patterns of effective argumentation."], "sentiment analysis, stylistic analysis, and argument mining"], [["modeling semantic relationship in multi-turn conversations with hierarchical latent variables", "lei shen | yang feng | haolan zhan", "multi-turn conversations consist of complex semantic structures, and it is still a challenge to generate coherent and diverse responses given previous utterances. it\u2019s practical that a conversation takes place under a background, meanwhile, the query and response are usually most related and they are consistent in topic but also different in content. however, little work focuses on such hierarchical relationship among utterances. to address this problem, we propose a conversational semantic relationship rnn (csrr) model to construct the dependency explicitly. the model contains latent variables in three hierarchies. the discourse-level one captures the global background, the pair-level one stands for the common topic information between query and response, and the utterance-level ones try to represent differences in content. experimental results show that our model significantly improves the quality of responses in terms of fluency, coherence, and diversity compared to baseline methods."], "dialogue and interactive systems"], [["response-anticipated memory for on-demand knowledge integration in response generation", "zhiliang tian | wei bi | dongkyu lee | lanqing xue | yiping song | xiaojiang liu | nevin l. zhang", "neural conversation models are known to generate appropriate but non-informative responses in general. a scenario where informativeness can be significantly enhanced is conversing by reading (cbr), where conversations take place with respect to a given external document. in previous work, the external document is utilized by (1) creating a context-aware document memory that integrates information from the document and the conversational context, and then (2) generating responses referring to the memory. in this paper, we propose to create the document memory with some anticipated responses in mind. this is achieved using a teacher-student framework. the teacher is given the external document, the context, and the ground-truth response, and learns how to build a response-aware document memory from three sources of information. the student learns to construct a response-anticipated document memory from the first two sources, and teacher\u2019s insight on memory creation. empirical results show that our model outperforms the previous state-of-the-art for the cbr task."], "dialogue and interactive systems"], [["human-in-the-loop for data collection: a multi-target counter narrative dataset to fight online hate speech", "margherita fanton | helena bonaldi | serra sinem tekiro\u011flu | marco guerini", "undermining the impact of hateful content with informed and non-aggressive responses, called counter narratives, has emerged as a possible solution for having healthier online communities. thus, some nlp studies have started addressing the task of counter narrative generation. although such studies have made an effort to build hate speech / counter narrative (hs/cn) datasets for neural generation, they fall short in reaching either high-quality and/or high-quantity. in this paper, we propose a novel human-in-the-loop data collection methodology in which a generative language model is refined iteratively by using its own data from the previous loops to generate new training samples that experts review and/or post-edit. our experiments comprised several loops including diverse dynamic variations. results show that the methodology is scalable and facilitates diverse, novel, and cost-effective data collection. to our knowledge, the resulting dataset is the only expert-based multi-target hs/cn dataset available to the community."], "resources and evaluation"], [["a reinforced generation of adversarial examples for neural machine translation", "wei zou | shujian huang | jun xie | xinyu dai | jiajun chen", "neural machine translation systems tend to fail on less decent inputs despite its significant efficacy, which may significantly harm the credibility of these systems\u2014fathoming how and when neural-based systems fail in such cases is critical for industrial maintenance. instead of collecting and analyzing bad cases using limited handcrafted error features, here we investigate this issue by generating adversarial examples via a new paradigm based on reinforcement learning. our paradigm could expose pitfalls for a given performance metric, e.g., bleu, and could target any given neural machine translation architecture. we conduct experiments of adversarial attacks on two mainstream neural machine translation architectures, rnn-search, and transformer. the results show that our method efficiently produces stable attacks with meaning-preserving adversarial examples. we also present a qualitative and quantitative analysis for the preference pattern of the attack, demonstrating its capability of pitfall exposure."], "machine translation and multilinguality"], [["curriculum pre-training for end-to-end speech translation", "chengyi wang | yu wu | shujie liu | ming zhou | zhenglu yang", "end-to-end speech translation poses a heavy burden on the encoder because it has to transcribe, understand, and learn cross-lingual semantics simultaneously. to obtain a powerful encoder, traditional methods pre-train it on asr data to capture speech features. however, we argue that pre-training the encoder only through simple speech recognition is not enough, and high-level linguistic knowledge should be considered. inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages. the difficulty of these courses is gradually increasing. experiments show that our curriculum pre-training method leads to significant improvements on en-de and en-fr speech translation benchmarks."], "speech and multimodality"], [["multiqt: multimodal learning for real-time question tracking in speech", "jakob d. havtorn | jan latko | joakim edin | lars maal\u00f8e | lasse borgholt | lorenzo belgrano | nicolai jacobsen | regitze sdun | \u017eeljko agi\u0107", "we address a challenging and practical task of labeling questions in speech in real time during telephone calls to emergency medical services in english, which embeds within a broader decision support system for emergency call-takers. we propose a novel multimodal approach to real-time sequence labeling in speech. our model treats speech and its own textual representation as two separate modalities or views, as it jointly learns from streamed audio and its noisy transcription into text via automatic speech recognition. our results show significant gains of jointly learning from the two modalities when compared to text or audio only, under adverse noise and limited volume of training data. the results generalize to medical symptoms detection where we observe a similar pattern of improvements with multimodal learning."], "speech and multimodality"], [["fine-tuning pre-trained transformer language models to distantly supervised relation extraction", "christoph alt | marc h\u00fcbner | leonhard hennig", "distantly supervised relation extraction is widely used to extract relational facts from text, but suffers from noisy labels. current relation extraction methods try to alleviate the noise by multi-instance learning and by providing supporting linguistic and contextual information to more efficiently guide the relation classification. while achieving state-of-the-art results, we observed these models to be biased towards recognizing a limited set of relations with high precision, while ignoring those in the long tail. to address this gap, we utilize a pre-trained language model, the openai generative pre-trained transformer (gpt) (radford et al., 2018). the gpt and similar models have been shown to capture semantic and syntactic features, and also a notable amount of \u201ccommon-sense\u201d knowledge, which we hypothesize are important features for recognizing a more diverse set of relations. by extending the gpt to the distantly supervised setting, and fine-tuning it on the nyt10 dataset, we show that it predicts a larger set of distinct relation types with high confidence. manual and automated evaluation of our model shows that it achieves a state-of-the-art auc score of 0.422 on the nyt10 dataset, and performs especially well at higher recall levels."], "information extraction, retrieval and text mining"], [["dense procedure captioning in narrated instructional videos", "botian shi | lei ji | yaobo liang | nan duan | peng chen | zhendong niu | ming zhou", "understanding narrated instructional videos is important for both research and real-world web applications. motivated by video dense captioning, we propose a model to generate procedure captions from narrated instructional videos which are a sequence of step-wise clips with description. previous works on video dense captioning learn video segments and generate captions without considering transcripts. we argue that transcripts in narrated instructional videos can enhance video representation by providing fine-grained complimentary and semantic textual information. in this paper, we introduce a framework to (1) extract procedures by a cross-modality module, which fuses video content with the entire transcript; and (2) generate captions by encoding video frames as well as a snippet of transcripts within each extracted procedure. experiments show that our model can achieve state-of-the-art performance in procedure extraction and captioning, and the ablation studies demonstrate that both the video frames and the transcripts are important for the task."], "language grounding to vision, robotics and beyond"], [["when do word embeddings accurately reflect surveys on our beliefs about people?", "kenneth joseph | jonathan morgan", "social biases are encoded in word embeddings. this presents a unique opportunity to study society historically and at scale, and a unique danger when embeddings are used in downstream applications. here, we investigate the extent to which publicly-available word embeddings accurately reflect beliefs about certain kinds of people as measured via traditional survey methods. we find that biases found in word embeddings do, on average, closely mirror survey data across seventeen dimensions of social meaning. however, we also find that biases in embeddings are much more reflective of survey data for some dimensions of meaning (e.g. gender) than others (e.g. race), and that we can be highly confident that embedding-based measures reflect survey data only for the most salient biases."], "computational social science, social media and cultural analytics"], [["open domain event extraction using neural latent variable models", "xiao liu | heyan huang | yue zhang", "we consider open domain event extraction, the task of extracting unconstraint types of events from news clusters. a novel latent variable neural model is constructed, which is scalable to very large corpus. a dataset is collected and manually annotated, with task-specific evaluation metrics being designed. results show that the proposed unsupervised model gives better performance compared to the state-of-the-art method for event schema induction."], "information extraction, retrieval and text mining"], [["attend, translate and summarize: an efficient method for neural cross-lingual summarization", "junnan zhu | yu zhou | jiajun zhang | chengqing zong", "cross-lingual summarization aims at summarizing a document in one language (e.g., chinese) into another language (e.g., english). in this paper, we propose a novel method inspired by the translation pattern in the process of obtaining a cross-lingual summary. we first attend to some words in the source text, then translate them into the target language, and summarize to get the final summary. specifically, we first employ the encoder-decoder attention distribution to attend to the source words. second, we present three strategies to acquire the translation probability, which helps obtain the translation candidates for each source word. finally, each summary word is generated either from the neural distribution or from the translation candidates of source words. experimental results on chinese-to-english and english-to-chinese summarization tasks have shown that our proposed method can significantly outperform the baselines, achieving comparable performance with the state-of-the-art."], "summarization"], [["crawling and preprocessing mailing lists at scale for dialog analysis", "janek bevendorff | khalid al khatib | martin potthast | benno stein", "this paper introduces the webis gmane email corpus 2019, the largest publicly available and fully preprocessed email corpus to date. we crawled more than 153 million emails from 14,699 mailing lists and segmented them into semantically consistent components using a new neural segmentation model. with 96% accuracy on 15 classes of email segments, our model achieves state-of-the-art performance while being more efficient to train than previous ones. all data, code, and trained models are made freely available alongside the paper."], "resources and evaluation"], [["spelling error correction with soft-masked bert", "shaohua zhang | haoran huang | jicong liu | hang li", "spelling error correction is an important yet challenging task because a satisfactory solution of it essentially needs human-level language understanding ability. without loss of generality we consider chinese spelling error correction (csc) in this paper. a state-of-the-art method for the task selects a character from a list of candidates for correction (including non-correction) at each position of the sentence on the basis of bert, the language representation model. the accuracy of the method can be sub-optimal, however, because bert does not have sufficient capability to detect whether there is an error at each position, apparently due to the way of pre-training it using mask language modeling. in this work, we propose a novel neural architecture to address the aforementioned issue, which consists of a network for error detection and a network for error correction based on bert, with the former being connected to the latter with what we call soft-masking technique. our method of using \u2018soft-masked bert\u2019 is general, and it may be employed in other language detection-correction problems. experimental results on two datasets, including one large dataset which we create and plan to release, demonstrate that the performance of our proposed method is significantly better than the baselines including the one solely based on bert."], "nlp applications"], [["socaog: incremental graph parsing for social relation inference in dialogues", "liang qiu | yuan liang | yizhou zhao | pan lu | baolin peng | zhou yu | ying nian wu | song-chun zhu", "inferring social relations from dialogues is vital for building emotionally intelligent robots to interpret human language better and act accordingly. we model the social network as an and-or graph, named socaog, for the consistency of relations among a group and leveraging attributes as inference cues. moreover, we formulate a sequential structure prediction task, and propose an \ud835\udefc-\ud835\udefd-\ud835\udefe strategy to incrementally parse socaog for the dynamic inference upon any incoming utterance: (i) an \ud835\udefc process predicting attributes and relations conditioned on the semantics of dialogues, (ii) a \ud835\udefd process updating the social relations based on related attributes, and (iii) a \ud835\udefe process updating individual\u2019s attributes based on interpersonal social relations. empirical results on dialogre and moviegraph show that our model infers social relations more accurately than the state-of-the-art methods. moreover, the ablation study shows the three processes complement each other, and the case study demonstrates the dynamic relational inference."], "computational social science, social media and cultural analytics"], [["revisiting low-resource neural machine translation: a case study", "rico sennrich | biao zhang", "it has been shown that the performance of neural machine translation (nmt) drops starkly in low-resource conditions, underperforming phrase-based statistical machine translation (pbsmt) and requiring large amounts of auxiliary data to achieve competitive results. in this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings. we discuss some pitfalls to be aware of when training low-resource nmt systems, and recent techniques that have shown to be especially helpful in low-resource settings, resulting in a set of best practices for low-resource nmt. in our experiments on german\u2013english with different amounts of iwslt14 training data, we show that, without the use of any auxiliary monolingual or multilingual data, an optimized nmt system can outperform pbsmt with far less data than previously claimed. we also apply these techniques to a low-resource korean\u2013english dataset, surpassing previously reported results by 4 bleu."], "machine translation and multilinguality"], [["dynamically adjusting transformer batch size by monitoring gradient direction change", "hongfei xu | josef van genabith | deyi xiong | qiuhui liu", "the choice of hyper-parameters affects the performance of neural models. while much previous research (sutskever et al., 2013; duchi et al., 2011; kingma and ba, 2015) focuses on accelerating convergence and reducing the effects of the learning rate, comparatively few papers concentrate on the effect of batch size. in this paper, we analyze how increasing batch size affects gradient direction, and propose to evaluate the stability of gradients with their angle change. based on our observations, the angle change of gradient direction first tends to stabilize (i.e. gradually decrease) while accumulating mini-batches, and then starts to fluctuate. we propose to automatically and dynamically determine batch sizes by accumulating gradients of mini-batches and performing an optimization step at just the time when the direction of gradients starts to fluctuate. to improve the efficiency of our approach for large models, we propose a sampling approach to select gradients of parameters sensitive to the batch size. our approach dynamically determines proper and efficient batch sizes during training. in our experiments on the wmt 14 english to german and english to french tasks, our approach improves the transformer with a fixed 25k batch size by +0.73 and +0.82 bleu respectively."], "machine translation and multilinguality"], [["sentiment tagging with partial labels using modular architectures", "xiao zhang | dan goldwasser", "many nlp learning tasks can be decomposed into several distinct sub-tasks, each associated with a partial label. in this paper we focus on a popular class of learning problems, sequence prediction applied to several sentiment analysis tasks, and suggest a modular learning approach in which different sub-tasks are learned using separate functional modules, combined to perform the final task while sharing information. our experiments show this approach helps constrain the learning process and can alleviate some of the supervision efforts."], "sentiment analysis, stylistic analysis, and argument mining"], [["the techqa dataset", "vittorio castelli | rishav chakravarti | saswati dana | anthony ferritto | radu florian | martin franz | dinesh garg | dinesh khandelwal | scott mccarley | michael mccawley | mohamed nasr | lin pan | cezar pendus | john pitrelli | saurabh pujar | salim roukos | andrzej sakrajda | avi sil | rosario uceda-sosa | todd ward | rong zhang", "we introduce techqa, a domain-adaptation question answering dataset for the technical support domain. the techqa corpus highlights two real-world issues from the automated customer support domain. first, it contains actual questions posed by users on a technical forum, rather than questions generated specifically for a competition or a task. second, it has a real-world size \u2013 600 training, 310 dev, and 490 evaluation question/answer pairs \u2013 thus reflecting the cost of creating large labeled datasets with actual data. hence, techqa is meant to stimulate research in domain adaptation rather than as a resource to build qa systems from scratch. techqa was obtained by crawling the ibmdeveloper and developerworks forums for questions with accepted answers provided in an ibm technote\u2014a technical document that addresses a specific technical issue. we also release a collection of the 801,998 technotes available on the web as of april 4, 2019 as a companion resource that can be used to learn representations of the it domain language."], "resources and evaluation"], [["improving multi-hop question answering over knowledge graphs using knowledge base embeddings", "apoorv saxena | aditay tripathi | partha talukdar", "knowledge graphs (kg) are multi-relational graphs consisting of entities as nodes and relations among them as typed edges. goal of the question answering over kg (kgqa) task is to answer natural language queries posed over the kg. multi-hop kgqa requires reasoning over multiple edges of the kg to arrive at the right answer. kgs are often incomplete with many missing links, posing additional challenges for kgqa, especially for multi-hop kgqa. recent research on multi-hop kgqa has attempted to handle kg sparsity using relevant external text, which isn\u2019t always readily available. in a separate line of research, kg embedding methods have been proposed to reduce kg sparsity by performing missing link prediction. such kg embedding methods, even though highly relevant, have not been explored for multi-hop kgqa so far. we fill this gap in this paper and propose embedkgqa. embedkgqa is particularly effective in performing multi-hop kgqa over sparse kgs. embedkgqa also relaxes the requirement of answer selection from a pre-specified neighborhood, a sub-optimal constraint enforced by previous multi-hop kgqa methods. through extensive experiments on multiple benchmark datasets, we demonstrate embedkgqa\u2019s effectiveness over other state-of-the-art baselines."], "question answering"], [["simplify the usage of lexicon in chinese ner", "ruotian ma | minlong peng | qi zhang | zhongyu wei | xuanjing huang", "recently, many works have tried to augment the performance of chinese named entity recognition (ner) using word lexicons. as a representative, lattice-lstm has achieved new benchmark results on several public chinese ner datasets. however, lattice-lstm has a complex model architecture. this limits its application in many industrial areas where real-time ner responses are needed. in this work, we propose a simple but effective method for incorporating the word lexicon into the character representations. this method avoids designing a complicated sequence modeling architecture, and for any neural ner model, it requires only subtle adjustment of the character representation layer to introduce the lexicon information. experimental studies on four benchmark chinese ner datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-of-the-art methods, along with a better performance. the experimental results also show that the proposed method can be easily incorporated with pre-trained models like bert."], "information extraction, retrieval and text mining"], [["bridging the gap between training and inference for neural machine translation", "wen zhang | yang feng | fandong meng | di you | qun liu", "neural machine translation (nmt) generates target words sequentially in the way of predicting the next word conditioned on the context words. at training time, it predicts with the ground truth words as context while at inference it has to generate the entire sequence from scratch. this discrepancy of the fed context leads to error accumulation among the way. furthermore, word-level training requires strict matching between the generated sequence and the ground truth sequence which leads to overcorrection over different but reasonable translations. in this paper, we address these issues by sampling context words not only from the ground truth sequence but also from the predicted sequence by the model during training, where the predicted sequence is selected with a sentence-level optimum. experiment results on chinese->english and wmt\u201914 english->german translation tasks demonstrate that our approach can achieve significant improvements on multiple datasets."], "machine translation and multilinguality"], [["contextual embeddings: when are they worth it?", "simran arora | avner may | jian zhang | christopher r\u00e9", "we study the settings for which deep contextual embeddings (e.g., bert) give large improvements in performance relative to classic pretrained embeddings (e.g., glove), and an even simpler baseline\u2014random word embeddings\u2014focusing on the impact of the training set size and the linguistic properties of the task. surprisingly, we find that both of these simpler baselines can match contextual embeddings on industry-scale data, and often perform within 5 to 10% accuracy (absolute) on benchmark tasks. furthermore, we identify properties of data for which contextual embeddings give particularly large gains: language containing complex structure, ambiguous word usage, and words unseen in training."], "machine learning for nlp"], [["answering while summarizing: multi-task learning for multi-hop qa with evidence extraction", "kosuke nishida | kyosuke nishida | masaaki nagata | atsushi otsuka | itsumi saito | hisako asano | junji tomita", "question answering (qa) using textual sources for purposes such as reading comprehension (rc) has attracted much attention. this study focuses on the task of explainable multi-hop qa, which requires the system to return the answer with evidence sentences by reasoning and gathering disjoint pieces of the reference texts. it proposes the query focused extractor (qfe) model for evidence extraction and uses multi-task learning with the qa model. qfe is inspired by extractive summarization models; compared with the existing method, which extracts each evidence sentence independently, it sequentially extracts evidence sentences by using an rnn with an attention mechanism on the question sentence. it enables qfe to consider the dependency among the evidence sentences and cover important information in the question sentence. experimental results show that qfe with a simple rc baseline model achieves a state-of-the-art evidence extraction score on hotpotqa. although designed for rc, it also achieves a state-of-the-art evidence extraction score on fever, which is a recognizing textual entailment task on a large textual database."], "question answering"], [["negative lexically constrained decoding for paraphrase generation", "tomoyuki kajiwara", "paraphrase generation can be regarded as monolingual translation. unlike bilingual machine translation, paraphrase generation rewrites only a limited portion of an input sentence. hence, previous methods based on machine translation often perform conservatively to fail to make necessary rewrites. to solve this problem, we propose a neural model for paraphrase generation that first identifies words in the source sentence that should be paraphrased. then, these words are paraphrased by the negative lexically constrained decoding that avoids outputting these words as they are. experiments on text simplification and formality transfer show that our model improves the quality of paraphrasing by making necessary rewrites to an input sentence."], "generation"], [["zero-shot cross-lingual abstractive sentence summarization through teaching generation and attention", "xiangyu duan | mingming yin | min zhang | boxing chen | weihua luo", "abstractive sentence summarization (assum) targets at grasping the core idea of the source sentence and presenting it as the summary. it is extensively studied using statistical models or neural models based on the large-scale monolingual source-summary parallel corpus. but there is no cross-lingual parallel corpus, whose source sentence language is different to the summary language, to directly train a cross-lingual assum system. we propose to solve this zero-shot problem by using resource-rich monolingual assum system to teach zero-shot cross-lingual assum system on both summary word generation and attention. this teaching process is along with a back-translation process which simulates source-summary pairs. experiments on cross-lingual assum task show that our proposed method is significantly better than pipeline baselines and previous works, and greatly enhances the cross-lingual performances closer to the monolingual performances."], "machine translation and multilinguality"], [["knowledge distillation for multilingual unsupervised neural machine translation", "haipeng sun | rui wang | kehai chen | masao utiyama | eiichiro sumita | tiejun zhao", "unsupervised neural machine translation (unmt) has recently achieved remarkable results for several language pairs. however, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. that is, research on multilingual unmt has been limited. in this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve unmt for all language pairs. on the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual unmt performance. our experiments on a dataset with english translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-english language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs."], "machine translation and multilinguality"], [["mask-align: self-supervised neural word alignment", "chi chen | maosong sun | yang liu", "word alignment, which aims to align translationally equivalent words between source and target sentences, plays an important role in many natural language processing tasks. current unsupervised neural alignment methods focus on inducing alignments from neural machine translation models, which does not leverage the full context in the target sequence. in this paper, we propose mask-align, a self-supervised word alignment model that takes advantage of the full context on the target side. our model masks out each target token and predicts it conditioned on both source and the remaining target tokens. this two-step process is based on the assumption that the source token contributing most to recovering the masked target token should be aligned. we also introduce an attention variant called leaky attention, which alleviates the problem of unexpected high cross-attention weights on special tokens such as periods. experiments on four language pairs show that our model outperforms previous unsupervised neural aligners and obtains new state-of-the-art results."], "machine translation and multilinguality"], [["uncovering probabilistic implications in typological knowledge bases", "johannes bjerva | yova kementchedjhieva | ryan cotterell | isabelle augenstein", "the study of linguistic typology is rooted in the implications we find between linguistic features, such as the fact that languages with object-verb word ordering tend to have postpositions. uncovering such implications typically amounts to time-consuming manual processing by trained and experienced linguists, which potentially leaves key linguistic universals unexplored. in this paper, we present a computational model which successfully identifies known universals, including greenberg universals, but also uncovers new ones, worthy of further linguistic investigation. our approach outperforms baselines previously used for this problem, as well as a strong baseline from knowledge base population."], "linguistic theories, cognitive modeling and psycholinguistics"], [["unsupervised opinion summarization with noising and denoising", "reinald kim amplayo | mirella lapata", "the supervised training of high-capacity models on large datasets containing hundreds of thousands of document-summary pairs is critical to the recent success of deep learning techniques for abstractive summarization. unfortunately, in most domains (other than news) such training data is not available and cannot be easily sourced. in this paper we enable the use of supervised learning for the setting where there are only documents available (e.g., product or business reviews) without ground truth summaries. we create a synthetic dataset from a corpus of user reviews by sampling a review, pretending it is a summary, and generating noisy versions thereof which we treat as pseudo-review input. we introduce several linguistically motivated noise generation functions and a summarization model which learns to denoise the input and generate the original review. at test time, the model accepts genuine reviews and generates a summary containing salient opinions, treating those that do not reach consensus as noise. extensive automatic and human evaluation shows that our model brings substantial improvements over both abstractive and extractive baselines."], "summarization"], [["clinical concept linking with contextualized neural representations", "elliot schumacher | andriy mulyar | mark dredze", "in traditional approaches to entity linking, linking decisions are based on three sources of information \u2013 the similarity of the mention string to an entity\u2019s name, the similarity of the context of the document to the entity, and broader information about the knowledge base (kb). in some domains, there is little contextual information present in the kb and thus we rely more heavily on mention string similarity. we consider one example of this, concept linking, which seeks to link mentions of medical concepts to a medical concept ontology. we propose an approach to concept linking that leverages recent work in contextualized neural models, such as elmo (peters et al. 2018), which create a token representation that integrates the surrounding context of the mention and concept name. we find a neural ranking approach paired with contextualized embeddings provides gains over a competitive baseline (leaman et al. 2013). additionally, we find that a pre-training step using synonyms from the ontology offers a useful initialization for the ranker."], "nlp applications"], [["examining the state-of-the-art in news timeline summarization", "demian gholipour ghalandari | georgiana ifrim", "previous work on automatic news timeline summarization (tls) leaves an unclear picture about how this task can generally be approached and how well it is currently solved. this is mostly due to the focus on individual subtasks, such as date selection and date summarization, and to the previous lack of appropriate evaluation metrics for the full tls task. in this paper, we compare different tls strategies using appropriate evaluation frameworks, and propose a simple and effective combination of methods that improves over the stateof-the-art on all tested benchmarks. for a more robust evaluation, we also present a new tls dataset, which is larger and spans longer time periods than previous datasets."], "summarization"], [["continual and multi-task architecture search", "ramakanth pasunuru | mohit bansal", "architecture search is the process of automatically learning the neural model or cell structure that best suits the given task. recently, this approach has shown promising performance improvements (on language modeling and image classification) with reasonable training speed, using a weight sharing strategy called efficient neural architecture search (enas). in our work, we first introduce a novel continual architecture search (cas) approach, so as to continually evolve the model parameters during the sequential training of several tasks, without losing performance on previously learned tasks (via block-sparsity and orthogonality constraints), thus enabling life-long learning. next, we explore a multi-task architecture search (mas) approach over enas for finding a unified, single cell structure that performs well across multiple tasks (via joint controller rewards), and hence allows more generalizable transfer of the cell structure knowledge to an unseen new task. we empirically show the effectiveness of our sequential continual learning and parallel multi-task learning based architecture search approaches on diverse sentence-pair classification tasks (glue) and multimodal-generation based video captioning tasks. further, we present several ablations and analyses on the learned cell structures."], "machine learning for nlp"], [["\u201cwho said it, and why?\u201d provenance for natural language claims", "yi zhang | zachary ives | dan roth", "in an era where generating content and publishing it is so easy, we are bombarded with information and are exposed to all kinds of claims, some of which do not always rank high on the truth scale. this paper suggests that the key to a longer-term, holistic, and systematic approach to navigating this information pollution is capturing the provenance of claims. to do that, we develop a formal definition of provenance graph for a given natural language claim, aiming to understand where the claim may come from and how it has evolved. to construct the graph, we model provenance inference, formulated mainly as an information extraction task and addressed via a textual entailment model. we evaluate our approach using two benchmark datasets, showing initial success in capturing the notion of provenance and its effectiveness on the application of claim verification."], "computational social science, social media and cultural analytics"], [["what kind of language is hard to language-model?", "sabrina j. mielke | ryan cotterell | kyle gorman | brian roark | jason eisner", "how language-agnostic are current state-of-the-art nlp tools? are there some types of language that are easier to model with current methods? in prior work (cotterell et al., 2018) we attempted to address this question for language modeling, and observed that recurrent neural network language models do not perform equally well over all the high-resource european languages found in the europarl corpus. we speculated that inflectional morphology may be the primary culprit for the discrepancy. in this paper, we extend these earlier experiments to cover 69 languages from 13 language families using a multilingual bible corpus. methodologically, we introduce a new paired-sample multiplicative mixed-effects model to obtain language difficulty coefficients from at-least-pairwise parallel corpora. in other words, the model is aware of inter-sentence variation and can handle missing data. exploiting this model, we show that \u201ctranslationese\u201d is not any easier to model than natively written language in a fair comparison. trying to answer the question of what features difficult languages have in common, we try and fail to reproduce our earlier (cotterell et al., 2018) observation about morphological complexity and instead reveal far simpler statistics of the data that seem to drive complexity in a much larger sample."], "machine translation and multilinguality"], [["unsupervised learning of pcfgs with normalizing flow", "lifeng jin | finale doshi-velez | timothy miller | lane schwartz | william schuler", "unsupervised pcfg inducers hypothesize sets of compact context-free rules as explanations for sentences. pcfg induction not only provides tools for low-resource languages, but also plays an important role in modeling language acquisition (bannard et al., 2009; abend et al. 2017). however, current pcfg induction models, using word tokens as input, are unable to incorporate semantics and morphology into induction, and may encounter issues of sparse vocabulary when facing morphologically rich languages. this paper describes a neural pcfg inducer which employs context embeddings (peters et al., 2018) in a normalizing flow model (dinh et al., 2015) to extend pcfg induction to use semantic and morphological information. linguistically motivated sparsity and categorical distance constraints are imposed on the inducer as regularization. experiments show that the pcfg induction model with normalizing flow produces grammars with state-of-the-art accuracy on a variety of different languages. ablation further shows a positive effect of normalizing flow, context embeddings and proposed regularizers."], "tagging, chunking, syntax and parsing"], [["towards robustifying nli models against lexical dataset biases", "xiang zhou | mohit bansal", "while deep learning models are making fast progress on the task of natural language inference, recent studies have also shown that these models achieve high accuracy by exploiting several dataset biases, and without deep understanding of the language semantics. using contradiction-word bias and word-overlapping bias as our two bias examples, this paper explores both data-level and model-level debiasing methods to robustify models against lexical dataset biases. first, we debias the dataset through data augmentation and enhancement, but show that the model bias cannot be fully removed via this method. next, we also compare two ways of directly debiasing the model without knowing what the dataset biases are in advance. the first approach aims to remove the label bias at the embedding level. the second approach employs a bag-of-words sub-model to capture the features that are likely to exploit the bias and prevents the original model from learning these biased features by forcing orthogonality between these two sub-models. we performed evaluations on new balanced datasets extracted from the original mnli dataset as well as the nli stress tests, and show that the orthogonality approach is better at debiasing the model while maintaining competitive overall accuracy."], "semantics"], [["pretrained transformers improve out-of-distribution robustness", "dan hendrycks | xiaoyuan liu | eric wallace | adam dziedzic | rishabh krishnan | dawn song", "although pretrained transformers such as bert achieve high accuracy on in-distribution examples, do they generalize to new distributions? we systematically measure out-of-distribution (ood) generalization for seven nlp datasets by constructing a new robustness benchmark with realistic distribution shifts. we measure the generalization of previous models including bag-of-words models, convnets, and lstms, and we show that pretrained transformers\u2019 performance declines are substantially smaller. pretrained transformers are also more effective at detecting anomalous or ood examples, while many previous models are frequently worse than chance. we examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. finally, we show where future work can improve ood robustness."], "machine learning for nlp"], [["encoding social information with graph convolutional networks forpolitical perspective detection in news media", "chang li | dan goldwasser", "identifying the political perspective shaping the way news events are discussed in the media is an important and challenging task. in this paper, we highlight the importance of contextualizing social information, capturing how this information is disseminated in social networks. we use graph convolutional networks, a recently proposed neural architecture for representing relational information, to capture the documents\u2019 social context. we show that social information can be used effectively as a source of distant supervision, and when direct supervision is available, even little social information can significantly improve performance."], "computational social science, social media and cultural analytics"], [["fine-grained analysis of cross-linguistic syntactic divergences", "dmitry nikolaev | ofir arviv | taelin karidi | neta kenneth | veronika mitnik | lilja maria saeboe | omri abend", "the patterns in which the syntax of different languages converges and diverges are often used to inform work on cross-lingual transfer. nevertheless, little empirical work has been done on quantifying the prevalence of different syntactic divergences across language pairs. we propose a framework for extracting divergence patterns for any language pair from a parallel corpus, building on universal dependencies. we show that our framework provides a detailed picture of cross-language divergences, generalizes previous approaches, and lends itself to full automation. we further present a novel dataset, a manually word-aligned subset of the parallel ud corpus in five languages, and use it to perform a detailed corpus study. we demonstrate the usefulness of the resulting analysis by showing that it can help account for performance patterns of a cross-lingual parser."], "resources and evaluation"], [["annotation and automatic classification of aspectual categories", "markus egg | helena prepens | will roberts", "we present the first annotated resource for the aspectual classification of german verb tokens in their clausal context. we use aspectual features compatible with the plurality of aspectual classifications in previous work and treat aspectual ambiguity systematically. we evaluate our corpus by using it to train supervised classifiers to automatically assign aspectual categories to verbs in context, permitting favourable comparisons to previous work."], "semantics"], [["gan-bert: generative adversarial learning for robust text classification with a bunch of labeled examples", "danilo croce | giuseppe castellucci | roberto basili", "recent transformer-based architectures, e.g., bert, provide impressive results in many natural language processing tasks. however, most of the adopted benchmarks are made of (sometimes hundreds of) thousands of examples. in many real scenarios, obtaining high- quality annotated data is expensive and time consuming; in contrast, unlabeled examples characterizing the target task can be, in general, easily collected. one promising method to enable semi-supervised learning has been proposed in image processing, based on semi- supervised generative adversarial networks. in this paper, we propose gan-bert that ex- tends the fine-tuning of bert-like architectures with unlabeled data in a generative adversarial setting. experimental results show that the requirement for annotated examples can be drastically reduced (up to only 50-100 annotated examples), still obtaining good performances in several sentence classification tasks."], "machine learning for nlp"], [["evaluating discourse in structured text representations", "elisa ferracane | greg durrett | junyi jessy li | katrin erk", "discourse structure is integral to understanding a text and is helpful in many nlp tasks. learning latent representations of discourse is an attractive alternative to acquiring expensive labeled discourse data. liu and lapata (2018) propose a structured attention mechanism for text classification that derives a tree over a text, akin to an rst discourse tree. we examine this model in detail, and evaluate on additional discourse-relevant tasks and datasets, in order to assess whether the structured attention improves performance on the end task and whether it captures a text\u2019s discourse structure. we find the learned latent trees have little to no structure and instead focus on lexical cues; even after obtaining more structured trees with proposed model modifications, the trees are still far from capturing discourse structure when compared to discourse dependency trees from an existing discourse parser. finally, ablation studies show the structured attention provides little benefit, sometimes even hurting performance."], "discourse and pragmatics"], [["diag-nre: a neural pattern diagnosis framework for distantly supervised neural relation extraction", "shun zheng | xu han | yankai lin | peilin yu | lu chen | ling huang | zhiyuan liu | wei xu", "pattern-based labeling methods have achieved promising results in alleviating the inevitable labeling noises of distantly supervised neural relation extraction. however, these methods require significant expert labor to write relation-specific patterns, which makes them too sophisticated to generalize quickly. to ease the labor-intensive workload of pattern writing and enable the quick generalization to new relation types, we propose a neural pattern diagnosis framework, diag-nre, that can automatically summarize and refine high-quality relational patterns from noise data with human experts in the loop. to demonstrate the effectiveness of diag-nre, we apply it to two real-world datasets and present both significant and interpretable improvements over state-of-the-art methods."], "information extraction, retrieval and text mining"], [["multi-task networks with universe, group, and task feature learning", "shiva pentyala | mengwen liu | markus dreyer", "we present methods for multi-task learning that take advantage of natural groupings of related tasks. task groups may be defined along known properties of the tasks, such as task domain or language. such task groups represent supervised information at the inter-task level and can be encoded into the model. we investigate two variants of neural network architectures that accomplish this, learning different feature spaces at the levels of individual tasks, task groups, as well as the universe of all tasks: (1) parallel architectures encode each input simultaneously into feature spaces at different levels; (2) serial architectures encode each input successively into feature spaces at different levels in the task hierarchy. we demonstrate the methods on natural language understanding (nlu) tasks, where a grouping of tasks into different task domains leads to improved performance on atis, snips, and a large in-house dataset."], "dialogue and interactive systems"], [["end-to-end bias mitigation by modelling biases in corpora", "rabeeh karimi mahabadi | yonatan belinkov | james henderson", "several recent studies have shown that strong natural language understanding (nlu) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios. we propose two learning strategies to train neural models, which are more robust to such biases and transfer better to out-of-domain datasets. the biases are specified in terms of one or more bias-only models, which learn to leverage the dataset biases. during training, the bias-only models\u2019 predictions are used to adjust the loss of the base model to reduce its reliance on biases by down-weighting the biased examples and focusing the training on the hard examples. we experiment on large-scale natural language inference and fact verification benchmarks, evaluating on out-of-domain datasets that are specifically designed to assess the robustness of models against known biases in the training data. results show that our debiasing methods greatly improve robustness in all settings and better transfer to other textual entailment datasets. our code and data are publicly available in https://github.com/rabeehk/robust-nli."], "semantics"], [["on the robustness of language encoders against grammatical errors", "fan yin | quanyu long | tao meng | kai-wei chang", "we conduct a thorough study to diagnose the behaviors of pre-trained language encoders (elmo, bert, and roberta) when confronted with natural grammatical errors. specifically, we collect real grammatical errors from non-native speakers and conduct adversarial attacks to simulate these errors on clean text data. we use this approach to facilitate debugging models on downstream applications. results confirm that the performance of all tested models is affected but the degree of impact varies. to interpret model behaviors, we further design a linguistic acceptability task to reveal their abilities in identifying ungrammatical sentences and the position of errors. we find that fixed contextual encoders with a simple classifier trained on the prediction of sentence correctness are able to locate error positions. we also design a cloze test for bert and discover that bert captures the interaction between errors and specific tokens in context. our results shed light on understanding the robustness and behaviors of language encoders against grammatical errors."], "interpretability and analysis of models for nlp"], [["progressive self-supervised attention learning for aspect-level sentiment analysis", "jialong tang | ziyao lu | jinsong su | yubin ge | linfeng song | le sun | jiebo luo", "in aspect-level sentiment classification (asc), it is prevalent to equip dominant neural models with attention mechanisms, for the sake of acquiring the importance of each context word on the given aspect. however, such a mechanism tends to excessively focus on a few frequent words with sentiment polarities, while ignoring infrequent ones. in this paper, we propose a progressive self-supervised attention learning approach for neural asc models, which automatically mines useful attention supervision information from a training corpus to refine attention mechanisms. specifically, we iteratively conduct sentiment predictions on all training instances. particularly, at each iteration, the context word with the maximum attention weight is extracted as the one with active/misleading influence on the correct/incorrect prediction of every instance, and then the word itself is masked for subsequent iterations. finally, we augment the conventional training objective with a regularization term, which enables asc models to continue equally focusing on the extracted active context words while decreasing weights of those misleading ones. experimental results on multiple datasets show that our proposed approach yields better attention mechanisms, leading to substantial improvements over the two state-of-the-art neural asc models. source code and trained models are available at https://github.com/deeplearnxmu/pssattention."], "sentiment analysis, stylistic analysis, and argument mining"], [["addressing posterior collapse with mutual information for improved variational neural machine translation", "arya d. mccarthy | xian li | jiatao gu | ning dong", "this paper proposes a simple and effective approach to address the problem of posterior collapse in conditional variational autoencoders (cvaes). it thus improves performance of machine translation models that use noisy or monolingual data, as well as in conventional settings. extending transformer and conditional vaes, our proposed latent variable model measurably prevents posterior collapse by (1) using a modified evidence lower bound (elbo) objective which promotes mutual information between the latent variable and the target, and (2) guiding the latent variable with an auxiliary bag-of-words prediction task. as a result, the proposed model yields improved translation quality compared to existing variational nmt models on wmt ro\u2194en and de\u2194en. with latent variables being effectively utilized, our model demonstrates improved robustness over non-latent transformer in handling uncertainty: exploiting noisy source-side monolingual data (up to +3.2 bleu), and training with weakly aligned web-mined parallel data (up to +4.7 bleu)."], "machine translation and multilinguality"], [["global optimization under length constraint for neural text summarization", "takuya makino | tomoya iwakura | hiroya takamura | manabu okumura", "we propose a global optimization method under length constraint (golc) for neural text summarization models. golc increases the probabilities of generating summaries that have high evaluation scores, rouge in this paper, within a desired length. we compared golc with two optimization methods, a maximum log-likelihood and a minimum risk training, on cnn/daily mail and a japanese single document summarization data set of the mainichi shimbun newspapers. the experimental results show that a state-of-the-art neural summarization model optimized with golc generates fewer overlength summaries while maintaining the fastest processing speed; only 6.70% overlength summaries on cnn/daily and 7.8% on long summary of mainichi, compared to the approximately 20% to 50% on cnn/daily mail and 10% to 30% on mainichi with the other optimization methods. we also demonstrate the importance of the generation of in-length summaries for post-editing with the dataset mainich that is created with strict length constraints. the ex- perimental results show approximately 30% to 40% improved post-editing time by use of in-length summaries."], "summarization"], [["the dialogue dodecathlon: open-domain knowledge and image grounded conversational agents", "kurt shuster | da ju | stephen roller | emily dinan | y-lan boureau | jason weston", "we introduce dodecadialogue: a set of 12 tasks that measures if a conversational agent can communicate engagingly with personality and empathy, ask questions, answer questions by utilizing knowledge resources, discuss topics and situations, and perceive and converse about images. by multi-tasking on such a broad large-scale set of data, we hope to both move towards and measure progress in producing a single unified agent that can perceive, reason and converse with humans in an open-domain setting. we show that such multi-tasking improves over a bert pre-trained baseline, largely due to multi-tasking with very large dialogue datasets in a similar domain, and that the multi-tasking in general provides gains to both text and image-based tasks using several metrics in both the fine-tune and task transfer settings. we obtain state-of-the-art results on many of the tasks, providing a strong baseline for this challenge."], "dialogue and interactive systems"], [["don\u2019t eclipse your arts due to small discrepancies: boundary repositioning with a pointer network for aspect extraction", "zhenkai wei | yu hong | bowei zou | meng cheng | jianmin yao", "the current aspect extraction methods suffer from boundary errors. in general, these errors lead to a relatively minor difference between the extracted aspects and the ground-truth. however, they hurt the performance severely. in this paper, we propose to utilize a pointer network for repositioning the boundaries. recycling mechanism is used, which enables the training data to be collected without manual intervention. we conduct the experiments on the benchmark datasets se14 of laptop and se14-16 of restaurant. experimental results show that our method achieves substantial improvements over the baseline, and outperforms state-of-the-art methods."], "sentiment analysis, stylistic analysis, and argument mining"], [["machine reading of historical events", "or honovich | lucas torroba hennigen | omri abend | shay b. cohen", "machine reading is an ambitious goal in nlp that subsumes a wide range of text understanding capabilities. within this broad framework, we address the task of machine reading the time of historical events, compile datasets for the task, and develop a model for tackling it. given a brief textual description of an event, we show that good performance can be achieved by extracting relevant sentences from wikipedia, and applying a combination of task-specific and general-purpose feature embeddings for the classification. furthermore, we establish a link between the historical event ordering task and the event focus time task from the information retrieval literature, showing they also provide a challenging test case for machine reading algorithms."], "information extraction, retrieval and text mining"], [["reinforced training data selection for domain adaptation", "miaofeng liu | yan song | hongbin zou | tong zhang", "supervised models suffer from the problem of domain shifting where distribution mismatch in the data across domains greatly affect model performance. to solve the problem, training data selection (tds) has been proven to be a prospective solution for domain adaptation in leveraging appropriate data. however, conventional tds methods normally requires a predefined threshold which is neither easy to set nor can be applied across tasks, and models are trained separately with the tds process. to make tds self-adapted to data and task, and to combine it with model training, in this paper, we propose a reinforcement learning (rl) framework that synchronously searches for training instances relevant to the target domain and learns better representations for them. a selection distribution generator (sdg) is designed to perform the selection and is updated according to the rewards computed from the selected data, where a predictor is included in the framework to ensure a task-specific model can be trained on the selected data and provides feedback to rewards. experimental results from part-of-speech tagging, dependency parsing, and sentiment analysis, as well as ablation studies, illustrate that the proposed framework is not only effective in data selection and representation, but also generalized to accommodate different nlp tasks."], "machine learning for nlp"], [["toward gender-inclusive coreference resolution", "yang trista cao | hal daum\u00e9 iii", "correctly resolving textual mentions of people fundamentally entails making inferences about those people. such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. to better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems. through these studies, conducted on english text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we build systems that lead to many potential harms."], "ethics in nlp"], [["towards faithful neural table-to-text generation with content-matching constraints", "zhenyi wang | xiaoyang wang | bang an | dong yu | changyou chen", "text generation from a knowledge base aims to translate knowledge triples to natural language descriptions. most existing methods ignore the faithfulness between a generated text description and the original table, leading to generated information that goes beyond the content of the table. in this paper, for the first time, we propose a novel transformer-based generation framework to achieve the goal. the core techniques in our method to enforce faithfulness include a new table-text optimal-transport matching loss and a table-text embedding similarity loss based on the transformer model. furthermore, to evaluate faithfulness, we propose a new automatic metric specialized to the table-to-text generation problem. we also provide detailed analysis on each component of our model in our experiments. automatic and human evaluations show that our framework can significantly outperform state-of-the-art by a large margin."], "generation"], [["an empirical study on hyperparameter optimization for fine-tuning pre-trained language models", "xueqing liu | chi wang", "the performance of fine-tuning pre-trained language models largely depends on the hyperparameter configuration. in this paper, we investigate the performance of modern hyperparameter optimization methods (hpo) on fine-tuning pre-trained language models. first, we study and report three hpo algorithms\u2019 performances on fine-tuning two state-of-the-art language models on the glue dataset. we find that using the same time budget, hpo often fails to outperform grid search due to two reasons: insufficient time budget and overfitting. we propose two general strategies and an experimental procedure to systematically troubleshoot hpo\u2019s failure cases. by applying the procedure, we observe that hpo can succeed with more appropriate settings in the search space and time budget; however, in certain cases overfitting remains. finally, we make suggestions for future work. our implementation can be found in https://github.com/microsoft/flaml/tree/main/flaml/nlp/"], "resources and evaluation"], [["learning implicit text generation via feature matching", "inkit padhi | pierre dognin | ke bai | c\u00edcero nogueira dos santos | vijil chenthamarakshan | youssef mroueh | payel das", "generative feature matching network (gfmn) is an approach for training state-of-the-art implicit generative models for images by performing moment matching on features from pre-trained neural networks. in this paper, we present new gfmn formulations that are effective for sequential data. our experimental results show the effectiveness of the proposed method, seqgfmn, for three distinct generation tasks in english: unconditional text generation, class-conditional text generation, and unsupervised text style transfer. seqgfmn is stable to train and outperforms various adversarial approaches for text generation and text style transfer."], "generation"], [["classification and clustering of arguments with contextualized word embeddings", "nils reimers | benjamin schiller | tilman beck | johannes daxenberger | christian stab | iryna gurevych", "we experiment with two recent contextualized word embedding methods (elmo and bert) in the context of open-domain argument search. for the first time, we show how to leverage the power of contextualized word embeddings to classify and cluster topic-dependent arguments, achieving impressive results on both tasks and across multiple datasets. for argument classification, we improve the state-of-the-art for the ukp sentential argument mining corpus by 20.8 percentage points and for the ibm debater - evidence sentences dataset by 7.4 percentage points. for the understudied task of argument clustering, we propose a pre-training step which improves by 7.8 percentage points over strong baselines on a novel dataset, and by 12.3 percentage points for the argument facet similarity (afs) corpus."], "sentiment analysis, stylistic analysis, and argument mining"], [["analyzing linguistic differences between owner and staff attributed tweets", "daniel preo\u0163iuc-pietro | rita devlin marier", "research on social media has to date assumed that all posts from an account are authored by the same person. in this study, we challenge this assumption and study the linguistic differences between posts signed by the account owner or attributed to their staff. we introduce a novel data set of tweets posted by u.s. politicians who self-reported their tweets using a signature. we analyze the linguistic topics and style features that distinguish the two types of tweets. predictive results show that we are able to predict owner and staff attributed tweets with good accuracy, even when not using any training data from that account."], "computational social science, social media and cultural analytics"], [["starc: structured annotations for reading comprehension", "yevgeni berzak | jonathan malmaud | roger levy", "we present starc (structured annotations for reading comprehension), a new annotation framework for assessing reading comprehension with multiple choice questions. our framework introduces a principled structure for the answer choices and ties them to textual span annotations. the framework is implemented in onestopqa, a new high-quality dataset for evaluation and analysis of reading comprehension in english. we use this dataset to demonstrate that starc can be leveraged for a key new application for the development of sat-like reading comprehension materials: automatic annotation quality probing via span ablation experiments. we further show that it enables in-depth analyses and comparisons between machine and human reading comprehension behavior, including error distributions and guessing ability. our experiments also reveal that the standard multiple choice dataset in nlp, race, is limited in its ability to measure reading comprehension. 47% of its questions can be guessed by machines without accessing the passage, and 18% are unanimously judged by humans as not having a unique correct answer. onestopqa provides an alternative test set for reading comprehension which alleviates these shortcomings and has a substantially higher human ceiling performance."], "resources and evaluation"], [["fine-grained sentence functions for short-text conversation", "wei bi | jun gao | xiaojiang liu | shuming shi", "sentence function is an important linguistic feature referring to a user\u2019s purpose in uttering a specific sentence. the use of sentence function has shown promising results to improve the performance of conversation models. however, there is no large conversation dataset annotated with sentence functions. in this work, we collect a new short-text conversation dataset with manually annotated sentence functions (stc-sefun). classification models are trained on this dataset to (i) recognize the sentence function of new data in a large corpus of short-text conversations; (ii) estimate a proper sentence function of the response given a test query. we later train conversation models conditioned on the sentence functions, including information retrieval-based and neural generative models. experimental results demonstrate that the use of sentence functions can help improve the quality of the returned responses."], "linguistic theories, cognitive modeling and psycholinguistics"], [["handling divergent reference texts when evaluating table-to-text generation", "bhuwan dhingra | manaal faruqui | ankur parikh | ming-wei chang | dipanjan das | william cohen", "automatically constructed datasets for generating text from semi-structured data (tables), such as wikibio, often contain reference texts that diverge from the information in the corresponding semi-structured data. we show that metrics which rely solely on the reference texts, such as bleu and rouge, show poor correlation with human judgments when those references diverge. we propose a new metric, parent, which aligns n-grams from the reference and generated texts to the semi-structured data before computing their precision and recall. through a large scale human evaluation study of table-to-text models for wikibio, we show that parent correlates with human judgments better than existing text generation metrics. we also adapt and evaluate the information extraction based evaluation proposed by wiseman et al (2017), and show that parent has comparable correlation to it, while being easier to use. we show that parent is also applicable when the reference texts are elicited from humans using the data from the webnlg challenge."], "generation"], [["modeling label semantics for predicting emotional reactions", "radhika gaonkar | heeyoung kwon | mohaddeseh bastan | niranjan balasubramanian | nathanael chambers", "predicting how events induce emotions in the characters of a story is typically seen as a standard multi-label classification task, which usually treats labels as anonymous classes to predict. they ignore information that may be conveyed by the emotion labels themselves. we propose that the semantics of emotion labels can guide a model\u2019s attention when representing the input story. further, we observe that the emotions evoked by an event are often related: an event that evokes joy is unlikely to also evoke sadness. in this work, we explicitly model label classes via label embeddings, and add mechanisms that track label-label correlations both during training and inference. we also introduce a new semi-supervision strategy that regularizes for the correlations on unlabeled data. our empirical evaluations show that modeling label semantics yields consistent benefits, and we advance the state-of-the-art on an emotion inference task."], "sentiment analysis, stylistic analysis, and argument mining"], [["is attention interpretable?", "sofia serrano | noah a. smith", "attention mechanisms have recently boosted performance on a range of nlp tasks. because attention layers explicitly weight input components\u2019 representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). we test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. while we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. we conclude that while attention noisily predicts input components\u2019 overall importance to a model, it is by no means a fail-safe indicator."], "machine learning for nlp"], [["a unified mrc framework for named entity recognition", "xiaoya li | jingrong feng | yuxian meng | qinghong han | fei wu | jiwei li", "the task of named entity recognition (ner) is normally divided into nested ner and flat ner depending on whether named entities are nested or not.models are usually separately developed for the two tasks, since sequence labeling models, the most widely used backbone for flat ner, are only able to assign a single label to a particular token, which is unsuitable for nested ner where a token may be assigned several labels. in this paper, we propose a unified framework that is capable of handling both flat and nested ner tasks. instead of treating the task of ner as a sequence labeling problem, we propose to formulate it as a machine reading comprehension (mrc) task. for example, extracting entities with the per label is formalized as extracting answer spans to the question \u201cwhich person is mentioned in the text\".this formulation naturally tackles the entity overlapping issue in nested ner: the extraction of two overlapping entities with different categories requires answering two independent questions. additionally, since the query encodes informative prior knowledge, this strategy facilitates the process of entity extraction, leading to better performances for not only nested ner, but flat ner. we conduct experiments on both nested and flat ner datasets.experiment results demonstrate the effectiveness of the proposed formulation. we are able to achieve a vast amount of performance boost over current sota models on nested ner datasets, i.e., +1.28, +2.55, +5.44, +6.37,respectively on ace04, ace05, genia and kbp17, along with sota results on flat ner datasets, i.e., +0.24, +1.95, +0.21, +1.49 respectively on english conll 2003, english ontonotes 5.0, chinese msra and chinese ontonotes 4.0."], "information extraction, retrieval and text mining"], [["improving adversarial text generation by modeling the distant future", "ruiyi zhang | changyou chen | zhe gan | wenlin wang | dinghan shen | guoyin wang | zheng wen | lawrence carin", "auto-regressive text generation models usually focus on local fluency, and may cause inconsistent semantic meaning in long text generation. further, automatically generating words with similar semantics is challenging, and hand-crafted linguistic rules are difficult to apply. we consider a text planning scheme and present a model-based imitation-learning approach to alleviate the aforementioned issues. specifically, we propose a novel guider network to focus on the generative process over a longer horizon, which can assist next-word prediction and provide intermediate rewards for generator optimization. extensive experiments demonstrate that the proposed method leads to improved performance."], "generation"], [["coupling distant annotation and adversarial training for cross-domain chinese word segmentation", "ning ding | dingkun long | guangwei xu | muhua zhu | pengjun xie | xiaobin wang | haitao zheng", "fully supervised neural approaches have achieved significant progress in the task of chinese word segmentation (cws). nevertheless, the performance of supervised models always drops gravely if the domain shifts due to the distribution gap across domains and the out of vocabulary (oov) problem. in order to simultaneously alleviate the issues, this paper intuitively couples distant annotation and adversarial training for cross-domain cws. 1) we rethink the essence of \u201cchinese words\u201d and design an automatic distant annotation mechanism, which does not need any supervision or pre-defined dictionaries on the target domain. the method could effectively explore domain-specific words and distantly annotate the raw texts for the target domain. 2) we further develop a sentence-level adversarial training procedure to perform noise reduction and maximum utilization of the source domain information. experiments on multiple real-world datasets across various domains show the superiority and robustness of our model, significantly outperforming previous state-of-the-arts cross-domain cws methods."], "phonology, morphology and word segmentation"], [["e3: entailment-driven extracting and editing for conversational machine reading", "victor zhong | luke zettlemoyer", "conversational machine reading systems help users answer high-level questions (e.g. determine if they qualify for particular government benefits) when they do not know the exact rules by which the determination is made (e.g. whether they need certain income levels or veteran status). the key challenge is that these rules are only provided in the form of a procedural text (e.g. guidelines from government website) which the system must read to figure out what to ask the user. we present a new conversational machine reading model that jointly extracts a set of decision rules from the procedural text while reasoning about which are entailed by the conversational history and which still need to be edited to create questions for the user. on the recently introduced sharc conversational machine reading dataset, our entailment-driven extract and edit network (e3) achieves a new state-of-the-art, outperforming existing systems as well as a new bert-based baseline. in addition, by explicitly highlighting which information still needs to be gathered, e3 provides a more explainable alternative to prior work. we release source code for our models and experiments at https://github.com/vzhong/e3."], "question answering"], [["corefqa: coreference resolution as query-based span prediction", "wei wu | fei wang | arianna yuan | fei wu | jiwei li", "in this paper, we present corefqa, an accurate and extensible approach for the coreference resolution task. we formulate the problem as a span prediction task, like in question answering: a query is generated for each candidate mention using its surrounding context, and a span prediction module is employed to extract the text spans of the coreferences within the document using the generated query. this formulation comes with the following key advantages: (1) the span prediction strategy provides the flexibility of retrieving mentions left out at the mention proposal stage; (2) in the question answering framework, encoding the mention and its context explicitly in a query makes it possible to have a deep and thorough examination of cues embedded in the context of coreferent mentions; and (3) a plethora of existing question answering datasets can be used for data augmentation to improve the model\u2019s generalization capability. experiments demonstrate significant performance boost over previous models, with 83.1 (+3.5) f1 score on the conll-2012 benchmark and 87.5 (+2.5) f1 score on the gap benchmark."], "nlp applications"], [["bertram: improved word embeddings have big impact on contextualized model performance", "timo schick | hinrich sch\u00fctze", "pretraining deep language models has led to large performance gains in nlp. despite this success, schick and sch\u00fctze (2020) recently showed that these models struggle to understand rare words. for static word embeddings, this problem has been addressed by separately learning representations for rare words. in this work, we transfer this idea to pretrained language models: we introduce bertram, a powerful architecture based on bert that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models. this is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture. integrating bertram into bert leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks."], "semantics"], [["a multi-perspective architecture for semantic code search", "rajarshi haldar | lingfei wu | jinjun xiong | julia hockenmaier", "the ability to match pieces of code to their corresponding natural language descriptions and vice versa is fundamental for natural language search interfaces to software repositories. in this paper, we propose a novel multi-perspective cross-lingual neural framework for code\u2013text matching, inspired in part by a previous model for monolingual text-to-text matching, to capture both global and local similarities. our experiments on the conala dataset show that our proposed model yields better performance on this cross-lingual text-to-code matching task than previous approaches that map code and text to a single joint embedding space."], "nlp applications"], [["contrastive self-supervised learning for commonsense reasoning", "tassilo klein | moin nabi", "we propose a self-supervised method to solve pronoun disambiguation and winograd schema challenge problems. our approach exploits the characteristic structure of training corpora related to so-called \u201ctrigger\u201d words, which are responsible for flipping the answer in pronoun disambiguation. we achieve such commonsense reasoning by constructing pair-wise contrastive auxiliary predictions. to this end, we leverage a mutual exclusive loss regularized by a contrastive margin. our architecture is based on the recently introduced transformer networks, bert, that exhibits strong performance on many nlp benchmarks. empirical results show that our method alleviates the limitation of current supervised approaches for commonsense reasoning. this study opens up avenues for exploiting inexpensive self-supervision to achieve performance gain in commonsense reasoning tasks."], "machine learning for nlp"], [["the paradigm discovery problem", "alexander erdmann | micha elsner | shijie wu | ryan cotterell | nizar habash", "this work treats the paradigm discovery problem (pdp), the task of learning an inflectional morphological system from unannotated sentences. we formalize the pdp and develop evaluation metrics for judging systems. using currently available resources, we construct datasets for the task. we also devise a heuristic benchmark for the pdp and report empirical results on five diverse languages. our benchmark system first makes use of word embeddings and string similarity to cluster forms by cell and by paradigm. then, we bootstrap a neural transducer on top of the clustered data to predict words to realize the empty paradigm slots. an error analysis of our system suggests clustering by cell across different inflection classes is the most pressing challenge for future work."], "phonology, morphology and word segmentation"], [["single-/multi-source cross-lingual ner via teacher-student learning on unlabeled data in target language", "qianhui wu | zijia lin | b\u00f6rje karlsson | jian-guang lou | biqing huang", "to better tackle the named entity recognition (ner) problem on languages with little/no labeled data, cross-lingual ner must effectively leverage knowledge learned from source languages with rich labeled data. previous works on cross-lingual ner are mostly based on label projection with pairwise texts or direct model transfer. however, such methods either are not applicable if the labeled data in the source languages is unavailable, or do not leverage information contained in unlabeled data in the target language. in this paper, we propose a teacher-student learning method to address such limitations, where ner models in the source languages are used as teachers to train a student model on unlabeled data in the target language. the proposed method works for both single-source and multi-source cross-lingual ner. for the latter, we further propose a similarity measuring method to better weight the supervision from different teacher models. extensive experiments for 3 target languages on benchmark datasets well demonstrate that our method outperforms existing state-of-the-art methods for both single-source and multi-source cross-lingual ner."], "information extraction, retrieval and text mining"], [["a multilingual bpe embedding space for universal sentiment lexicon induction", "mengjie zhao | hinrich sch\u00fctze", "we present a new method for sentiment lexicon induction that is designed to be applicable to the entire range of typological diversity of the world\u2019s languages. we evaluate our method on parallel bible corpus+ (pbc+), a parallel corpus of 1593 languages. the key idea is to use byte pair encodings (bpes) as basic units for multilingual embeddings. through zero-shot transfer from english sentiment, we learn a seed lexicon for each language in the domain of pbc+. through domain adaptation, we then generalize the domain-specific lexicon to a general one. we show \u2013 across typologically diverse languages in pbc+ \u2013 good quality of seed and general-domain sentiment lexicons by intrinsic and extrinsic and by automatic and human evaluation. we make freely available our code, seed sentiment lexicons for all 1593 languages and induced general-domain sentiment lexicons for 200 languages."], "sentiment analysis, stylistic analysis, and argument mining"], [["diachronic sense modeling with deep contextualized word embeddings: an ecological view", "renfen hu | shen li | shichen liang", "diachronic word embeddings have been widely used in detecting temporal changes. however, existing methods face the meaning conflation deficiency by representing a word as a single vector at each time period. to address this issue, this paper proposes a sense representation and tracking framework based on deep contextualized embeddings, aiming at answering not only what and when, but also how the word meaning changes. the experiments show that our framework is effective in representing fine-grained word senses, and it brings a significant improvement in word change detection task. furthermore, we model the word change from an ecological viewpoint, and sketch two interesting sense behaviors in the process of language evolution, i.e. sense competition and sense cooperation."], "linguistic theories, cognitive modeling and psycholinguistics"], [["learning to ask more: semi-autoregressive sequential question generation under dual-graph interaction", "zi chai | xiaojun wan", "traditional question generation (tqg) aims to generate a question given an input passage and an answer. when there is a sequence of answers, we can perform sequential question generation (sqg) to produce a series of interconnected questions. since the frequently occurred information omission and coreference between questions, sqg is rather challenging. prior works regarded sqg as a dialog generation task and recurrently produced each question. however, they suffered from problems caused by error cascades and could only capture limited context dependencies. to this end, we generate questions in a semi-autoregressive way. our model divides questions into different groups and generates each group of them in parallel. during this process, it builds two graphs focusing on information from passages, answers respectively and performs dual-graph interaction to get information for generation. besides, we design an answer-aware attention mechanism and the coarse-to-fine generation scenario. experiments on our new dataset containing 81.9k questions show that our model substantially outperforms prior works."], "generation"], [["interactive classification by asking informative questions", "lili yu | howard chen | sida i. wang | tao lei | yoav artzi", "we study the potential for interaction in natural language classification. we add a limited form of interaction for intent classification, where users provide an initial query using natural language, and the system asks for additional information using binary or multi-choice questions. at each turn, our system decides between asking the most informative question or making the final classification pre-diction. the simplicity of the model allows for bootstrapping of the system without interaction data, instead relying on simple crowd-sourcing tasks. we evaluate our approach on two domains, showing the benefit of interaction and the advantage of learning to balance between asking additional questions and making the final prediction."], "machine learning for nlp"], [["a diverse corpus for evaluating and developing english math word problem solvers", "shen-yun miao | chao-chun liang | keh-yih su", "we present asdiv (academia sinica diverse mwp dataset), a diverse (in terms of both language patterns and problem types) english math word problem (mwp) corpus for evaluating the capability of various mwp solvers. existing mwp corpora for studying ai progress remain limited either in language usage patterns or in problem types. we thus present a new english mwp corpus with 2,305 mwps that cover more text patterns and most problem types taught in elementary school. each mwp is annotated with its problem type and grade level (for indicating the level of difficulty). furthermore, we propose a metric to measure the lexicon usage diversity of a given mwp corpus, and demonstrate that asdiv is more diverse than existing corpora. experiments show that our proposed corpus reflects the true capability of mwp solvers more faithfully."], "resources and evaluation"], [["amr parsing as sequence-to-graph transduction", "sheng zhang | xutai ma | kevin duh | benjamin van durme", "we propose an attention-based model that treats amr parsing as sequence-to-graph transduction. unlike most amr parsers that rely on pre-trained aligners, external semantic resources, or data augmentation, our proposed parser is aligner-free, and it can be effectively trained with limited amounts of labeled amr data. our experimental results outperform all previously reported smatch scores, on both amr 2.0 (76.3% on ldc2017t10) and amr 1.0 (70.2% on ldc2014t12)."], "semantics"], [["towards interpretable clinical diagnosis with bayesian network ensembles stacked on entity-aware cnns", "jun chen | xiaoya dai | quan yuan | chao lu | haifeng huang", "the automatic text-based diagnosis remains a challenging task for clinical use because it requires appropriate balance between accuracy and interpretability. in this paper, we attempt to propose a solution by introducing a novel framework that stacks bayesian network ensembles on top of entity-aware convolutional neural networks (cnn) towards building an accurate yet interpretable diagnosis system. the proposed framework takes advantage of the high accuracy and generality of deep neural networks as well as the interpretability of bayesian networks, which is critical for ai-empowered healthcare. the evaluation conducted on the real electronic medical record (emr) documents from hospitals and annotated by professional doctors proves that, the proposed framework outperforms the previous automatic diagnosis methods in accuracy performance and the diagnosis explanation of the framework is reasonable."], "nlp applications"], [["rat-sql: relation-aware schema encoding and linking for text-to-sql parsers", "bailin wang | richard shin | xiaodong liu | oleksandr polozov | matthew richardson", "when translating natural language questions into sql queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas. the generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query. we present a unified framework, based on the relation-aware self-attention mechanism, to address schema encoding, schema linking, and feature representation within a text-to-sql encoder. on the challenging spider dataset this framework boosts the exact match accuracy to 57.2%, surpassing its best counterparts by 8.7% absolute improvement. further augmented with bert, it achieves the new state-of-the-art performance of 65.6% on the spider leaderboard. in addition, we observe qualitative improvements in the model\u2019s understanding of schema linking and alignment. our implementation will be open-sourced at https://github.com/microsoft/rat-sql."], "semantics"], [["an effective approach to unsupervised machine translation", "mikel artetxe | gorka labaka | eneko agirre", "while machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both neural machine translation (nmt) and statistical machine translation (smt) systems using monolingual corpora only. in this paper, we identify and address several deficiencies of existing unsupervised smt approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure. moreover, we use our improved smt system to initialize a dual nmt model, which is further fine-tuned through on-the-fly back-translation. together, we obtain large improvements over the previous state-of-the-art in unsupervised machine translation. for instance, we get 22.5 bleu points in english-to-german wmt 2014, 5.5 points more than the previous best unsupervised system, and 0.5 points more than the (supervised) shared task winner back in 2014."], "machine translation and multilinguality"], [["merge and label: a novel neural network architecture for nested ner", "joseph fisher | andreas vlachos", "named entity recognition (ner) is one of the best studied tasks in natural language processing. however, most approaches are not capable of handling nested structures which are common in many applications. in this paper we introduce a novel neural network architecture that first merges tokens and/or entities into entities forming nested structures, and then labels each of them independently. unlike previous work, our merge and label approach predicts real-valued instead of discrete segmentation structures, which allow it to combine word and nested entity embeddings while maintaining differentiability. we evaluate our approach using the ace 2005 corpus, where it achieves state-of-the-art f1 of 74.6, further improved with contextual embeddings (bert) to 82.4, an overall improvement of close to 8 f1 points over previous approaches trained on the same data. additionally we compare it against bilstm-crfs, the dominant approach for flat ner structures, demonstrating that its ability to predict nested structures does not impact performance in simpler cases."], "information extraction, retrieval and text mining"], [["multi-hypothesis machine translation evaluation", "marina fomicheva | lucia specia | francisco guzm\u00e1n", "reliably evaluating machine translation (mt) through automated metrics is a long-standing problem. one of the main challenges is the fact that multiple outputs can be equally valid. attempts to minimise this issue include metrics that relax the matching of mt output and reference strings, and the use of multiple references. the latter has been shown to significantly improve the performance of evaluation metrics. however, collecting multiple references is expensive and in practice a single reference is generally used. in this paper, we propose an alternative approach: instead of modelling linguistic variation in human reference we exploit the mt model uncertainty to generate multiple diverse translations and use these: (i) as surrogates to reference translations; (ii) to obtain a quantification of translation variability to either complement existing metric scores or (iii) replace references altogether. we show that for a number of popular evaluation metrics our variability estimates lead to substantial improvements in correlation with human judgements of quality by up 15%."], "resources and evaluation"], [["language-aware interlingua for multilingual neural machine translation", "changfeng zhu | heng yu | shanbo cheng | weihua luo", "multilingual neural machine translation (nmt) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages. however, the traditional multilingual model fails to capture the diversity and specificity of different languages, resulting in inferior performance compared with individual models that are sufficiently trained. in this paper, we incorporate a language-aware interlingua into the encoder-decoder architecture. the interlingual network enables the model to learn a language-independent representation from the semantic spaces of different languages, while still allowing for language-specific specialization of a particular language-pair. experiments show that our proposed method achieves remarkable improvements over state-of-the-art multilingual nmt baselines and produces comparable performance with strong individual models."], "machine translation and multilinguality"], [["strategies for structuring story generation", "angela fan | mike lewis | yann dauphin", "writers often rely on plans or sketches to write long stories, but most current language models generate word by word from left to right. we explore coarse-to-fine models for creating narrative texts of several hundred words, and introduce new models which decompose stories by abstracting over actions and entities. the model first generates the predicate-argument structure of the text, where different mentions of the same entity are marked with placeholder tokens. it then generates a surface realization of the predicate-argument structure, and finally replaces the entity placeholders with context-sensitive names and references. human judges prefer the stories from our models to a wide range of previous approaches to hierarchical text generation. extensive analysis shows that our methods can help improve the diversity and coherence of events and entities in generated stories."], "generation"], [["paraphrase augmented task-oriented dialog generation", "silin gao | yichi zhang | zhijian ou | zhou yu", "neural generative models have achieved promising performance on dialog generation tasks if given a huge data set. however, the lack of high-quality dialog data and the expensive data annotation process greatly limit their application in real world settings. we propose a paraphrase augmented response generation (parg) framework that jointly trains a paraphrase model and a response generation model to improve the dialog generation performance. we also design a method to automatically construct paraphrase training data set based on dialog state and dialog act labels. parg is applicable to various dialog generation models, such as tscp (lei et al., 2018) and damd (zhang et al., 2019). experimental results show that the proposed framework improves these state-of-the-art dialog models further on camrest676 and multiwoz. parg also outperforms other data augmentation methods significantly in dialog generation tasks, especially under low resource settings."], "dialogue and interactive systems"], [["modeling long context for task-oriented dialogue state generation", "jun quan | deyi xiong", "based on the recently proposed transferable dialogue state generator (trade) that predicts dialogue states from utterance-concatenated dialogue context, we propose a multi-task learning model with a simple yet effective utterance tagging technique and a bidirectional language model as an auxiliary task for task-oriented dialogue state generation. by enabling the model to learn a better representation of the long dialogue context, our approaches attempt to solve the problem that the performance of the baseline significantly drops when the input dialogue context sequence is long. in our experiments, our proposed model achieves a 7.03% relative improvement over the baseline, establishing a new state-of-the-art joint goal accuracy of 52.04% on the multiwoz 2.0 dataset."], "dialogue and interactive systems"], [["programming in natural language with fuse: synthesizing methods from spoken utterances using deep natural language understanding", "sebastian weigelt | vanessa steurer | tobias hey | walter f. tichy", "the key to effortless end-user programming is natural language. we examine how to teach intelligent systems new functions, expressed in natural language. as a first step, we collected 3168 samples of teaching efforts in plain english. then we built fuse, a novel system that translates english function descriptions into code. our approach is three-tiered and each task is evaluated separately. we first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using bert). then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a bilstm). finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to api calls and inject control structures (f1: 67.0% with information retrieval and knowledge-based methods). in an end-to-end evaluation on an unseen dataset fuse synthesized 84.6% of the method signatures and 79.2% of the api calls correctly."], "nlp applications"], [["domain adaptive dialog generation via meta learning", "kun qian | zhou yu", "domain adaptation is an essential task in dialog system building because there are so many new dialog tasks created for different needs every day. collecting and annotating training data for these new tasks is costly since it involves real user interactions. we propose a domain adaptive dialog generation method based on meta-learning (daml). daml is an end-to-end trainable dialog system model that learns from multiple rich-resource tasks and then adapts to new domains with minimal training samples. we train a dialog system model using multiple rich-resource single-domain dialog data by applying the model-agnostic meta-learning algorithm to dialog domain. the model is capable of learning a competitive dialog system on a new domain with only a few training examples in an efficient manner. the two-step gradient updates in daml enable the model to learn general features across multiple tasks. we evaluate our method on a simulated dialog dataset and achieve state-of-the-art performance, which is generalizable to new tasks."], "generation"], [["the referential reader: a recurrent entity network for anaphora resolution", "fei liu | luke zettlemoyer | jacob eisenstein", "we present a new architecture for storing and accessing entity mentions during online text processing. while reading the text, entity references are identified, and may be stored by either updating or overwriting a cell in a fixed-length memory. the update operation implies coreference with the other mentions that are stored in the same cell; the overwrite operation causes these mentions to be forgotten. by encoding the memory operations as differentiable gates, it is possible to train the model end-to-end, using both a supervised anaphora resolution objective as well as a supplementary language modeling objective. evaluation on a dataset of pronoun-name anaphora demonstrates strong performance with purely incremental text processing."], "machine learning for nlp"], [["dynamic sampling strategies for multi-task reading comprehension", "ananth gottumukkala | dheeru dua | sameer singh | matt gardner", "building general reading comprehension systems, capable of solving multiple datasets at the same time, is a recent aspirational goal in the research community. prior work has focused on model architecture or generalization to held out datasets, and largely passed over the particulars of the multi-task learning set up. we show that a simple dynamic sampling strategy, selecting instances for training proportional to the multi-task model\u2019s current performance on a dataset relative to its single task performance, gives substantive gains over prior multi-task sampling strategies, mitigating the catastrophic forgetting that is common in multi-task learning. we also demonstrate that allowing instances of different tasks to be interleaved as much as possible between each epoch and batch has a clear benefit in multitask performance over forcing task homogeneity at the epoch or batch level. our final model shows greatly increased performance over the best model on orb, a recently-released multitask reading comprehension benchmark."], "question answering"], [["dependency graph enhanced dual-transformer structure for aspect-based sentiment classification", "hao tang | donghong ji | chenliang li | qiji zhou", "aspect-based sentiment classification is a popular task aimed at identifying the corresponding emotion of a specific aspect. one sentence may contain various sentiments for different aspects. many sophisticated methods such as attention mechanism and convolutional neural networks (cnn) have been widely employed for handling this challenge. recently, semantic dependency tree implemented by graph convolutional networks (gcn) is introduced to describe the inner connection between aspects and the associated emotion words. but the improvement is limited due to the noise and instability of dependency trees. to this end, we propose a dependency graph enhanced dual-transformer network (named dgedt) by jointly considering the flat representations learnt from transformer and graph-based representations learnt from the corresponding dependency graph in an iterative interaction manner. specifically, a dual-transformer structure is devised in dgedt to support mutual reinforcement between the flat representation learning and graph-based representation learning. the idea is to allow the dependency graph to guide the representation learning of the transformer encoder and vice versa. the results on five datasets demonstrate that the proposed dgedt outperforms all state-of-the-art alternatives with a large margin."], "machine learning for nlp"], [["benchmarking multimodal regex synthesis with complex structures", "xi ye | qiaochu chen | isil dillig | greg durrett", "existing datasets for regular expression (regex) generation from natural language are limited in complexity; compared to regex tasks that users post on stackoverflow, the regexes in these datasets are simple, and the language used to describe them is not diverse. we introduce structuredregex, a new regex synthesis dataset differing from prior ones in three aspects. first, to obtain structurally complex and realistic regexes, we generate the regexes using a probabilistic grammar with pre-defined macros observed from real-world stackoverflow posts. second, to obtain linguistically diverse natural language descriptions, we show crowdworkers abstract depictions of the underlying regex and ask them to describe the pattern they see, rather than having them paraphrase synthetic language. third, we augment each regex example with a collection of strings that are and are not matched by the ground truth regex, similar to how real users give examples. our quantitative and qualitative analysis demonstrates the advantages of structuredregex over prior datasets. further experimental results using various multimodal synthesis techniques highlight the challenge presented by our dataset, including non-local constraints and multi-modal inputs."], "semantics"], [["sequence tagging with contextual and non-contextual subword representations: a multilingual evaluation", "benjamin heinzerling | michael strube", "pretrained contextual and non-contextual subword embeddings have become available in over 250 languages, allowing massively multilingual nlp. however, while there is no dearth of pretrained embeddings, the distinct lack of systematic evaluations makes it difficult for practitioners to choose between them. in this work, we conduct an extensive evaluation comparing non-contextual subword embeddings, namely fasttext and bpemb, and a contextual representation method, namely bert, on multilingual named entity recognition and part-of-speech tagging. we find that overall, a combination of bert, bpemb, and character representations works best across languages and tasks. a more detailed analysis reveals different strengths and weaknesses: multilingual bert performs well in medium- to high-resource languages, but is outperformed by non-contextual subword embeddings in a low-resource setting."], "information extraction, retrieval and text mining"], [["self-attention with cross-lingual position representation", "liang ding | longyue wang | dacheng tao", "position encoding (pe), an essential part of self-attention networks (sans), is used to preserve the word order information for natural language processing tasks, generating fixed position indices for input sequences. however, in cross-lingual scenarios, machine translation, the pes of source and target sentences are modeled independently. due to word order divergences in different languages, modeling the cross-lingual positional relationships might help sans tackle this problem. in this paper, we augment sans with cross-lingual position representations to model the bilingually aware latent structure for the input sentence. specifically, we utilize bracketing transduction grammar (btg)-based reordering information to encourage sans to learn bilingual diagonal alignments. experimental results on wmt\u201914 english\u21d2german, wat\u201917 japanese\u21d2english, and wmt\u201917 chinese\u21d4english translation tasks demonstrate that our approach significantly and consistently improves translation quality over strong baselines. extensive analyses confirm that the performance gains come from the cross-lingual information."], "machine translation and multilinguality"], [["morphological irregularity correlates with frequency", "shijie wu | ryan cotterell | timothy o\u2019donnell", "we present a study of morphological irregularity. following recent work, we define an information-theoretic measure of irregularity based on the predictability of forms in a language. using a neural transduction model, we estimate this quantity for the forms in 28 languages. we first present several validatory and exploratory analyses of irregularity. we then show that our analyses provide evidence for a correlation between irregularity and frequency: higher frequency items are more likely to be irregular and irregular items are more likely be highly frequent. to our knowledge, this result is the first of its breadth and confirms longstanding proposals from the linguistics literature. the correlation is more robust when aggregated at the level of whole paradigms\u2014providing support for models of linguistic structure in which inflected forms are unified by abstract underlying stems or lexemes."], "linguistic theories, cognitive modeling and psycholinguistics"], [["improving neural conversational models with entropy-based data filtering", "rich\u00e1rd cs\u00e1ky | patrik purgai | g\u00e1bor recski", "current neural network-based conversational models lack diversity and generate boring responses to open-ended utterances. priors such as persona, emotion, or topic provide additional information to dialog models to aid response generation, but annotating a dataset with priors is expensive and such annotations are rarely available. while previous methods for improving the quality of open-domain response generation focused on either the underlying model or the training objective, we present a method of filtering dialog datasets by removing generic utterances from training data using a simple entropy-based approach that does not require human supervision. we conduct extensive experiments with different variations of our method, and compare dialog models across 17 evaluation metrics to show that training on datasets filtered this way results in better conversational quality as chatbots learn to output more diverse responses."], "dialogue and interactive systems"], [["interconnected question generation with coreference alignment and conversation flow modeling", "yifan gao | piji li | irwin king | michael r. lyu", "we study the problem of generating interconnected questions in question-answering style conversations. compared with previous works which generate questions based on a single sentence (or paragraph), this setting is different in two major aspects: (1) questions are highly conversational. almost half of them refer back to conversation history using coreferences. (2) in a coherent conversation, questions have smooth transitions between turns. we propose an end-to-end neural model with coreference alignment and conversation flow modeling. the coreference alignment modeling explicitly aligns coreferent mentions in conversation history with corresponding pronominal references in generated questions, which makes generated questions interconnected to conversation history. the conversation flow modeling builds a coherent conversation by starting questioning on the first few sentences in a text passage and smoothly shifting the focus to later parts. extensive experiments show that our system outperforms several baselines and can generate highly conversational questions. the code implementation is released at https://github.com/evan-gao/conversaional-qg."], "generation"], [["target-guided open-domain conversation", "jianheng tang | tiancheng zhao | chenyan xiong | xiaodan liang | eric xing | zhiting hu", "many real-world open-domain conversation applications have specific goals to achieve during open-ended chats, such as recommendation, psychotherapy, education, etc. we study the problem of imposing conversational goals on open-domain chat agents. in particular, we want a conversational system to chat naturally with human and proactively guide the conversation to a designated target subject. the problem is challenging as no public data is available for learning such a target-guided strategy. we propose a structured approach that introduces coarse-grained keywords to control the intended content of system responses. we then attain smooth conversation transition through turn-level supervised learning, and drive the conversation towards the target with discourse-level constraints. we further derive a keyword-augmented conversation dataset for the study. quantitative and human evaluations show our system can produce meaningful and effective conversations, significantly improving over other approaches"], "dialogue and interactive systems"], [["avoiding reasoning shortcuts: adversarial evaluation, training, and model development for multi-hop qa", "yichen jiang | mohit bansal", "multi-hop question answering requires a model to connect multiple pieces of evidence scattered in a long context to answer the question. in this paper, we show that in the multi-hop hotpotqa (yang et al., 2018) dataset, the examples often contain reasoning shortcuts through which models can directly locate the answer by word-matching the question with a sentence in the context. we demonstrate this issue by constructing adversarial documents that create contradicting answers to the shortcut but do not affect the validity of the original answer. the performance of strong baseline models drops significantly on our adversarial test, indicating that they are indeed exploiting the shortcuts rather than performing multi-hop reasoning. after adversarial training, the baseline\u2019s performance improves but is still limited on the adversarial test. hence, we use a control unit that dynamically attends to the question at different reasoning hops to guide the model\u2019s multi-hop reasoning. we show that our 2-hop model trained on the regular data is more robust to the adversaries than the baseline. after adversarial training, it not only achieves significant improvements over its counterpart trained on regular data, but also outperforms the adversarially-trained baseline significantly. finally, we sanity-check that these improvements are not obtained by exploiting potential new shortcuts in the adversarial data, but indeed due to robust multi-hop reasoning skills of the models."], "question answering"], [["mind: a large-scale dataset for news recommendation", "fangzhao wu | ying qiao | jiun-hung chen | chuhan wu | tao qi | jianxun lian | danyang liu | xing xie | jianfeng gao | winnie wu | ming zhou", "news recommendation is an important technique for personalized news service. compared with product and movie recommendations which have been comprehensively studied, the research on news recommendation is much more limited, mainly due to the lack of a high-quality benchmark dataset. in this paper, we present a large-scale dataset named mind for news recommendation. constructed from the user click logs of microsoft news, mind contains 1 million users and more than 160k english news articles, each of which has rich textual content such as title, abstract and body. we demonstrate mind a good testbed for news recommendation through a comparative study of several state-of-the-art news recommendation methods which are originally developed on different proprietary datasets. our results show the performance of news recommendation highly relies on the quality of news content understanding and user interest modeling. many natural language processing techniques such as effective text representation methods and pre-trained language models can effectively improve the performance of news recommendation. the mind dataset will be available at https://msnews.github.io."], "resources and evaluation"], [["learning an unreferenced metric for online dialogue evaluation", "koustuv sinha | prasanna parthasarathi | jasmine wang | ryan lowe | william l. hamilton | joelle pineau", "evaluating the quality of a dialogue interaction between two agents is a difficult task, especially in open-domain chit-chat style dialogue. there have been recent efforts to develop automatic dialogue evaluation metrics, but most of them do not generalize to unseen datasets and/or need a human-generated reference response during inference, making it infeasible for online evaluation. here, we propose an unreferenced automated evaluation metric that uses large pre-trained language models to extract latent representations of utterances, and leverages the temporal transitions that exist between them. we show that our model achieves higher correlation with human annotations in an online setting, while not requiring true responses for comparison during inference."], "dialogue and interactive systems"], [["context-aware embedding for targeted aspect-based sentiment analysis", "bin liang | jiachen du | ruifeng xu | binyang li | hejiao huang", "attention-based neural models were employed to detect the different aspects and sentiment polarities of the same target in targeted aspect-based sentiment analysis (tabsa). however, existing methods do not specifically pre-train reasonable embeddings for targets and aspects in tabsa. this may result in targets or aspects having the same vector representations in different contexts and losing the context-dependent information. to address this problem, we propose a novel method to refine the embeddings of targets and aspects. such pivotal embedding refinement utilizes a sparse coefficient vector to adjust the embeddings of target and aspect from the context. hence the embeddings of targets and aspects can be refined from the highly correlative words instead of using context-independent or randomly initialized vectors. experiment results on two benchmark datasets show that our approach yields the state-of-the-art performance in tabsa task."], "sentiment analysis, stylistic analysis, and argument mining"], [["using automatically extracted minimum spans to disentangle coreference evaluation from boundary detection", "nafise sadat moosavi | leo born | massimo poesio | michael strube", "the common practice in coreference resolution is to identify and evaluate the maximum span of mentions. the use of maximum spans tangles coreference evaluation with the challenges of mention boundary detection like prepositional phrase attachment. to address this problem, minimum spans are manually annotated in smaller corpora. however, this additional annotation is costly and therefore, this solution does not scale to large corpora. in this paper, we propose the mina algorithm for automatically extracting minimum spans to benefit from minimum span evaluation in all corpora. we show that the extracted minimum spans by mina are consistent with those that are manually annotated by experts. our experiments show that using minimum spans is in particular important in cross-dataset coreference evaluation, in which detected mention boundaries are noisier due to domain shift. we have integrated mina into https://github.com/ns-moosavi/coval for reporting standard coreference scores based on both maximum and automatically detected minimum spans."], "discourse and pragmatics"], [["learning how to active learn by dreaming", "thuy-trang vu | ming liu | dinh phung | gholamreza haffari", "heuristic-based active learning (al) methods are limited when the data distribution of the underlying learning problems vary. recent data-driven al policy learning methods are also restricted to learn from closely related domains. we introduce a new sample-efficient method that learns the al policy directly on the target domain of interest by using wake and dream cycles. our approach interleaves between querying the annotation of the selected datapoints to update the underlying student learner and improving al policy using simulation where the current student learner acts as an imperfect annotator. we evaluate our method on cross-domain and cross-lingual text classification and named entity recognition tasks. experimental results show that our dream-based al policy training strategy is more effective than applying the pretrained policy without further fine-tuning and better than the existing strong baseline methods that use heuristics or reinforcement learning."], "machine learning for nlp"], [["how large are lions? inducing distributions over quantitative attributes", "yanai elazar | abhijit mahabal | deepak ramachandran | tania bedrax-weiss | dan roth", "most current nlp systems have little knowledge about quantitative attributes of objects and events. we propose an unsupervised method for collecting quantitative information from large amounts of web data, and use it to create a new, very large resource consisting of distributions over physical quantities associated with objects, adjectives, and verbs which we call distributions over quantitative (doq). this contrasts with recent work in this area which has focused on making only relative comparisons such as \u201cis a lion bigger than a wolf?\u201d. our evaluation shows that doq compares favorably with state of the art results on existing datasets for relative comparisons of nouns and adjectives, and on a new dataset we introduce."], "linguistic theories, cognitive modeling and psycholinguistics"], [["a generative model for joint natural language understanding and generation", "bo-hsiang tseng | jianpeng cheng | yimai fang | david vandyke", "natural language understanding (nlu) and natural language generation (nlg) are two fundamental and related tasks in building task-oriented dialogue systems with opposite objectives: nlu tackles the transformation from natural language to formal representations, whereas nlg does the reverse. a key to success in either task is parallel training data which is expensive to obtain at a large scale. in this work, we propose a generative model which couples nlu and nlg through a shared latent variable. this approach allows us to explore both spaces of natural language and formal representations, and facilitates information sharing through the latent space to eventually benefit nlu and nlg. our model achieves state-of-the-art performance on two dialogue datasets with both flat and tree-structured formal representations. we also show that the model can be trained in a semi-supervised fashion by utilising unlabelled data to boost its performance."], "dialogue and interactive systems"], [["pyramid: a layered model for nested named entity recognition", "jue wang | lidan shou | ke chen | gang chen", "this paper presents pyramid, a novel layered model for nested named entity recognition (nested ner). in our approach, token or text region embeddings are recursively inputted into l flat ner layers, from bottom to top, stacked in a pyramid shape. each time an embedding passes through a layer of the pyramid, its length is reduced by one. its hidden state at layer l represents an l-gram in the input text, which is labeled only if its corresponding text region represents a complete entity mention. we also design an inverse pyramid to allow bidirectional interaction between layers. the proposed method achieves state-of-the-art f1 scores in nested ner on ace-2004, ace-2005, genia, and nne, which are 80.27, 79.42, 77.78, and 93.70 with conventional embeddings, and 87.74, 86.34, 79.31, and 94.68 with pre-trained contextualized embeddings. in addition, our model can be used for the more general task of overlapping named entity recognition. a preliminary experiment confirms the effectiveness of our method in overlapping ner."], "information extraction, retrieval and text mining"], [["multi-cell compositional lstm for ner domain adaptation", "chen jia | yue zhang", "cross-domain ner is a challenging yet practical problem. entity mentions can be highly different across domains. however, the correlations between entity types can be relatively more stable across domains. we investigate a multi-cell compositional lstm structure for multi-task learning, modeling each entity type using a separate cell state. with the help of entity typed units, cross-domain knowledge transfer can be made in an entity type level. theoretically, the resulting distinct feature distributions for each entity type make it more powerful for cross-domain transfer. empirically, experiments on four few-shot and zero-shot datasets show our method significantly outperforms a series of multi-task learning methods and achieves the best results."], "information extraction, retrieval and text mining"], [["on faithfulness and factuality in abstractive summarization", "joshua maynez | shashi narayan | bernd bohnet | ryan mcdonald", "it is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. in this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. we conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. our human annotators found substantial amounts of hallucinated content in all model generated summaries. however, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., rouge, but also in generating faithful and factual summaries as evaluated by humans. furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria."], "summarization"], [["learning architectures from an extended search space for language modeling", "yinqiao li | chi hu | yuhao zhang | nuo xu | yufan jiang | tong xiao | jingbo zhu | tongran liu | changliang li", "neural architecture search (nas) has advanced significantly in recent years but most nas systems restrict search to learning architectures of a recurrent or convolutional cell. in this paper, we extend the search space of nas. in particular, we present a general approach to learn both intra-cell and inter-cell architectures (call it ess). for a better search result, we design a joint learning method to perform intra-cell and inter-cell nas simultaneously. we implement our model in a differentiable architecture search system. for recurrent neural language modeling, it outperforms a strong baseline significantly on the ptb and wikitext data, with a new state-of-the-art on ptb. moreover, the learned architectures show good transferability to other systems. e.g., they improve state-of-the-art systems on the conll and wnut named entity recognition (ner) tasks and conll chunking task, indicating a promising line of research on large-scale pre-learned architectures."], "machine learning for nlp"], [["toward better storylines with sentence-level language models", "daphne ippolito | david grangier | douglas eck | chris callison-burch", "we propose a sentence-level language model which selects the next sentence in a story from a finite set of fluent alternatives. since it does not need to model fluency, the sentence-level language model can focus on longer range dependencies, which are crucial for multi-sentence coherence. rather than dealing with individual words, our method treats the story so far as a list of pre-trained sentence embeddings and predicts an embedding for the next sentence, which is more efficient than predicting word embeddings. notably this allows us to consider a large number of candidates for the next sentence during training. we demonstrate the effectiveness of our approach with state-of-the-art accuracy on the unsupervised story cloze task and with promising results on larger-scale next sentence prediction tasks."], "generation"], [["would you rather? a new benchmark for learning machine alignment with cultural values and social preferences", "yi tay | donovan ong | jie fu | alvin chan | nancy chen | anh tuan luu | chris pal", "understanding human preferences, along with cultural and social nuances, lives at the heart of natural language understanding. concretely, we present a new task and corpus for learning alignments between machine and human preferences. our newly introduced problem is concerned with predicting the preferable options from two sentences describing scenarios that may involve social and cultural situations. our problem is framed as a natural language inference task with crowd-sourced preference votes by human players, obtained from a gamified voting platform. we benchmark several state-of-the-art neural models, along with bert and friends on this task. our experimental results show that current state-of-the-art nlp models still leave much room for improvement."], "computational social science, social media and cultural analytics"], [["learning to select, track, and generate for data-to-text", "hayate iso | yui uehara | tatsuya ishigaki | hiroshi noji | eiji aramaki | ichiro kobayashi | yusuke miyao | naoaki okazaki | hiroya takamura", "we propose a data-to-text generation model with two modules, one for tracking and the other for text generation. our tracking module selects and keeps track of salient information and memorizes which record has been mentioned. our generation module generates a summary conditioned on the state of tracking module. our proposed model is considered to simulate the human-like writing process that gradually selects the information by determining the intermediate variables while writing the summary. in addition, we also explore the effectiveness of the writer information for generations. experimental results show that our proposed model outperforms existing models in all evaluation metrics even without writer information. incorporating writer information further improves the performance, contributing to content planning and surface realization."], "generation"], [["it\u2019s morphin\u2019 time! combating linguistic discrimination with inflectional perturbations", "samson tan | shafiq joty | min-yen kan | richard socher", "training on only perfect standard english corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds (e.g., african american vernacular english, colloquial singapore english, etc.). we perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popular nlp models, e.g., bert and transformer, and show that adversarially fine-tuning them for a single epoch significantly improves robustness without sacrificing performance on clean data."], "ethics in nlp"], [["cross-lingual unsupervised sentiment classification with multi-view transfer learning", "hongliang fei | ping li", "recent neural network models have achieved impressive performance on sentiment classification in english as well as other languages. their success heavily depends on the availability of a large amount of labeled data or parallel corpus. in this paper, we investigate an extreme scenario of cross-lingual sentiment classification, in which the low-resource language does not have any labels or parallel corpus. we propose an unsupervised cross-lingual sentiment classification model named multi-view encoder-classifier (mvec) that leverages an unsupervised machine translation (umt) system and a language discriminator. unlike previous language model (lm) based fine-tuning approaches that adjust parameters solely based on the classification error on training data, we employ the encoder-decoder framework of a umt as a regularization component on the shared network parameters. in particular, the cross-lingual encoder of our model learns a shared representation, which is effective for both reconstructing input sentences of two languages and generating more representative views from the input for classification. extensive experiments on five language pairs verify that our model significantly outperforms other models for 8/11 sentiment classification tasks."], "sentiment analysis, stylistic analysis, and argument mining"], [["cross-replication reliability - an empirical approach to interpreting inter-rater reliability", "ka wong | praveen paritosh | lora aroyo", "when collecting annotations and labeled data from humans, a standard practice is to use inter-rater reliability (irr) as a measure of data goodness (hallgren, 2012). metrics such as krippendorff\u2019s alpha or cohen\u2019s kappa are typically required to be above a threshold of 0.6 (landis and koch, 1977). these absolute thresholds are unreasonable for crowdsourced data from annotators with high cultural and training variances, especially on subjective topics. we present a new alternative to interpreting irr that is more empirical and contextualized. it is based upon benchmarking irr against baseline measures in a replication, one of which is a novel cross-replication reliability (xrr) measure based on cohen\u2019s (1960) kappa. we call this approach the xrr framework. we opensource a replication dataset of 4 million human judgements of facial expressions and analyze it with the proposed framework. we argue this framework can be used to measure the quality of crowdsourced datasets."], "resources and evaluation"], [["miss tools and mr fruit: emergent communication in agents learning about object affordances", "diane bouchacourt | marco baroni", "recent research studies communication emergence in communities of deep network agents assigned a joint task, hoping to gain insights on human language evolution. we propose here a new task capturing crucial aspects of the human environment, such as natural object affordances, and of human conversation, such as full symmetry among the participants. by conducting a thorough pragmatic and semantic analysis of the emergent protocol, we show that the agents solve the shared task through genuine bilateral, referential communication. however, the agents develop multiple idiolects, which makes us conclude that full symmetry is not a sufficient condition for a common language to emerge."], "linguistic theories, cognitive modeling and psycholinguistics"], [["on importance sampling-based evaluation of latent language models", "robert l. logan iv | matt gardner | sameer singh", "language models that use additional latent structures (e.g., syntax trees, coreference chains, knowledge graph links) provide several advantages over traditional language models. however, likelihood-based evaluation of these models is often intractable as it requires marginalizing over the latent space. existing works avoid this issue by using importance sampling. although this approach has asymptotic guarantees, analysis is rarely conducted on the effect of decisions such as sample size and choice of proposal distribution on the reported estimates. in this paper, we carry out this analysis for three models: rnng, entitynlm, and kglm. in addition, we elucidate subtle differences in how importance sampling is applied in these works that can have substantial effects on the final estimates, as well as provide theoretical results which reinforce the validity of this technique."], "machine learning for nlp"], [["detecting subevents using discourse and narrative features", "mohammed aldawsari | mark finlayson", "recognizing the internal structure of events is a challenging language processing task of great importance for text understanding. we present a supervised model for automatically identifying when one event is a subevent of another. building on prior work, we introduce several novel features, in particular discourse and narrative features, that significantly improve upon prior state-of-the-art performance. error analysis further demonstrates the utility of these features. we evaluate our model on the only two annotated corpora with event hierarchies: hieve and the intelligence community corpus. no prior system has been evaluated on both corpora. our model outperforms previous systems on both corpora, achieving 0.74 blanc f1 on the intelligence community corpus and 0.70 f1 on the hieve corpus, respectively a 15 and 5 percentage point improvement over previous models."], "semantics"], [["bilingual dictionary based neural machine translation without using parallel sentences", "xiangyu duan | baijun ji | hao jia | min tan | min zhang | boxing chen | weihua luo | yue zhang", "in this paper, we propose a new task of machine translation (mt), which is based on no parallel sentences but can refer to a ground-truth bilingual dictionary. motivated by the ability of a monolingual speaker learning to translate via looking up the bilingual dictionary, we propose the task to see how much potential an mt system can attain using the bilingual dictionary and large scale monolingual corpora, while is independent on parallel sentences. we propose anchored training (at) to tackle the task. at uses the bilingual dictionary to establish anchoring points for closing the gap between source language and target language. experiments on various language pairs show that our approaches are significantly better than various baselines, including dictionary-based word-by-word translation, dictionary-supervised cross-lingual word embedding transformation, and unsupervised mt. on distant language pairs that are hard for unsupervised mt to perform well, at performs remarkably better, achieving performances comparable to supervised smt trained on more than 4m parallel sentences."], "machine translation and multilinguality"], [["few-shot text ranking with meta adapted synthetic weak supervision", "si sun | yingzhuo qian | zhenghao liu | chenyan xiong | kaitao zhang | jie bao | zhiyuan liu | paul bennett", "the effectiveness of neural information retrieval (neu-ir) often depends on a large scale of in-domain relevance training signals, which are not always available in real-world ranking scenarios. to democratize the benefits of neu-ir, this paper presents metaadaptrank, a domain adaptive learning method that generalizes neu-ir models from label-rich source domains to few-shot target domains. drawing on source-domain massive relevance supervision, metaadaptrank contrastively synthesizes a large number of weak supervision signals for target domains and meta-learns to reweight these synthetic \u201cweak\u201d data based on their benefits to the target-domain ranking accuracy of neu-ir models. experiments on three trec benchmarks in the web, news, and biomedical domains show that metaadaptrank significantly improves the few-shot ranking accuracy of neu-ir models. further analyses indicate that metaadaptrank thrives from both its contrastive weak data synthesis and meta-reweighted data selection. the code and data of this paper can be obtained from https://github.com/thunlp/metaadaptrank."], "information extraction, retrieval and text mining"], [["towards end-2-end learning for predicting behavior codes from spoken utterances in psychotherapy conversations", "karan singla | zhuohao chen | david atkins | shrikanth narayanan", "spoken language understanding tasks usually rely on pipelines involving complex processing blocks such as voice activity detection, speaker diarization and automatic speech recognition (asr). we propose a novel framework for predicting utterance level labels directly from speech features, thus removing the dependency on first generating transcripts, and transcription free behavioral coding. our classifier uses a pretrained speech-2-vector encoder as bottleneck to generate word-level representations from speech features. this pretrained encoder learns to encode speech features for a word using an objective similar to word2vec. our proposed approach just uses speech features and word segmentation information for predicting spoken utterance-level target labels. we show that our model achieves competitive results to other state-of-the-art approaches which use transcribed text for the task of predicting psychotherapy-relevant behavior codes."], "speech and multimodality"], [["adaptive attention span in transformers", "sainbayar sukhbaatar | edouard grave | piotr bojanowski | armand joulin", "we propose a novel self-attention mechanism that can learn its optimal attention span. this allows us to extend significantly the maximum context size used in transformer, while maintaining control over their memory footprint and computational time. we show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters."], "machine learning for nlp"], [["we need to talk about standard splits", "kyle gorman | steven bedrick", "it is standard practice in speech & language technology to rank systems according to their performance on a test set held out for evaluation. however, few researchers apply statistical tests to determine whether differences in performance are likely to arise by chance, and few examine the stability of system ranking across multiple training-testing splits. we conduct replication and reproduction experiments with nine part-of-speech taggers published between 2000 and 2018, each of which claimed state-of-the-art performance on a widely-used \u201cstandard split\u201d. while we replicate results on the standard split, we fail to reliably reproduce some rankings when we repeat this analysis with randomly generated training-testing splits. we argue that randomly generated splits should be used in system evaluation."], "resources and evaluation"], [["like a baby: visually situated neural language acquisition", "alexander ororbia | ankur mali | matthew kelly | david reitter", "we examine the benefits of visual context in training neural language models to perform next-word prediction. a multi-modal neural architecture is introduced that outperform its equivalent trained on language alone with a 2% decrease in perplexity, even when no visual context is available at test. fine-tuning the embeddings of a pre-trained state-of-the-art bidirectional language model (bert) in the language modeling framework yields a 3.5% improvement. the advantage for training with visual context when testing without is robust across different languages (english, german and spanish) and different models (gru, lstm, delta-rnn, as well as those that use bert embeddings). thus, language models perform better when they learn like a baby, i.e, in a multi-modal environment. this finding is compatible with the theory of situated cognition: language is inseparable from its physical context."], "linguistic theories, cognitive modeling and psycholinguistics"], [["nat: noise-aware training for robust neural sequence labeling", "marcin namysl | sven behnke | joachim k\u00f6hler", "sequence labeling systems should perform reliably not only under ideal conditions but also with corrupted inputs\u2014as these systems often process user-generated text or follow an error-prone upstream component. to this end, we formulate the noisy sequence labeling problem, where the input may undergo an unknown noising process and propose two noise-aware training (nat) objectives that improve robustness of sequence labeling performed on perturbed input: our data augmentation method trains a neural model using a mixture of clean and noisy samples, whereas our stability training algorithm encourages the model to create a noise-invariant latent representation. we employ a vanilla noise model at training time. for evaluation, we use both the original data and its variants perturbed with real ocr errors and misspellings. extensive experiments on english and german named entity recognition benchmarks confirmed that nat consistently improved robustness of popular sequence labeling models, preserving accuracy on the original input. we make our code and data publicly available for the research community."], "information extraction, retrieval and text mining"], [["keyphrase generation for scientific document retrieval", "florian boudin | ygor gallina | akiko aizawa", "sequence-to-sequence models have lead to significant progress in keyphrase generation, but it remains unknown whether they are reliable enough to be beneficial for document retrieval. this study provides empirical evidence that such models can significantly improve retrieval performance, and introduces a new extrinsic evaluation framework that allows for a better understanding of the limitations of keyphrase generation models. using this framework, we point out and discuss the difficulties encountered with supplementing documents with -not present in text- keyphrases, and generalizing models across domains. our code is available at https://github.com/boudinfl/ir-using-kg"], "information extraction, retrieval and text mining"], [["optimizing the factual correctness of a summary: a study of summarizing radiology reports", "yuhao zhang | derek merck | emily tsai | christopher d. manning | curtis langlotz", "neural abstractive summarization models are able to generate summaries which have high overlap with human references. however, existing models are not optimized for factual correctness, a critical metric in real-world applications. in this work, we develop a general framework where we evaluate the factual correctness of a generated summary by fact-checking it automatically against its reference using an information extraction module. we further propose a training strategy which optimizes a neural summarization model with a factual correctness reward via reinforcement learning. we apply the proposed method to the summarization of radiology reports, where factual correctness is a key requirement. on two separate datasets collected from hospitals, we show via both automatic and human evaluation that the proposed approach substantially improves the factual correctness and overall quality of outputs over a competitive neural summarization system, producing radiology summaries that approach the quality of human-authored ones."], "summarization"], [["a span-based linearization for constituent trees", "yang wei | yuanbin wu | man lan", "we propose a novel linearization of a constituent tree, together with a new locally normalized model. for each split point in a sentence, our model computes the normalizer on all spans ending with that split point, and then predicts a tree span from them. compared with global models, our model is fast and parallelizable. different from previous local models, our linearization method is tied on the spans directly and considers more local features when performing span prediction, which is more interpretable and effective. experiments on ptb (95.8 f1) and ctb (92.4 f1) show that our model significantly outperforms existing local models and efficiently achieves competitive results with global models."], "tagging, chunking, syntax and parsing"], [["extracting symptoms and their status from clinical conversations", "nan du | kai chen | anjuli kannan | linh tran | yuhui chen | izhak shafran", "this paper describes novel models tailored for a new application, that of extracting the symptoms mentioned in clinical conversations along with their status. lack of any publicly available corpus in this privacy-sensitive domain led us to develop our own corpus, consisting of about 3k conversations annotated by professional medical scribes. we propose two novel deep learning approaches to infer the symptom names and their status: (1) a new hierarchical span-attribute tagging (sa-t) model, trained using curriculum learning, and (2) a variant of sequence-to-sequence model which decodes the symptoms and their status from a few speaker turns within a sliding window over the conversation. this task stems from a realistic application of assisting medical providers in capturing symptoms mentioned by patients from their clinical conversations. to reflect this application, we define multiple metrics. from inter-rater agreement, we find that the task is inherently difficult. we conduct comprehensive evaluations on several contrasting conditions and observe that the performance of the models range from an f-score of 0.5 to 0.8 depending on the condition. our analysis not only reveals the inherent challenges of the task, but also provides useful directions to improve the models."], "nlp applications"], [["opiniondigest: a simple framework for opinion summarization", "yoshihiko suhara | xiaolan wang | stefanos angelidis | wang-chiew tan", "we present opiniondigest, an abstractive opinion summarization framework, which does not rely on gold-standard summaries for training. the framework uses an aspect-based sentiment analysis model to extract opinion phrases from reviews, and trains a transformer model to reconstruct the original reviews from these extractions. at summarization time, we merge extractions from multiple reviews and select the most popular ones. the selected opinions are used as input to the trained transformer model, which verbalizes them into an opinion summary. opiniondigest can also generate customized summaries, tailored to specific user needs, by filtering the selected opinions according to their aspect and/or sentiment. automatic evaluation on yelp data shows that our framework outperforms competitive baselines. human studies on two corpora verify that opiniondigest produces informative summaries and shows promising customization capabilities."], "sentiment analysis, stylistic analysis, and argument mining"], [["is word segmentation necessary for deep learning of chinese representations?", "xiaoya li | yuxian meng | xiaofei sun | qinghong han | arianna yuan | jiwei li", "segmenting a chunk of text into words is usually the first step of processing chinese text, but its necessity has rarely been explored. in this paper, we ask the fundamental question of whether chinese word segmentation (cws) is necessary for deep learning-based chinese natural language processing. we benchmark neural word-based models which rely on word segmentation against neural char-based models which do not involve word segmentation in four end-to-end nlp benchmark tasks: language modeling, machine translation, sentence matching/paraphrase and text classification. through direct comparisons between these two types of models, we find that char-based models consistently outperform word-based models. based on these observations, we conduct comprehensive experiments to study why word-based models underperform char-based models in these deep learning-based nlp tasks. we show that it is because word-based models are more vulnerable to data sparsity and the presence of out-of-vocabulary (oov) words, and thus more prone to overfitting. we hope this paper could encourage researchers in the community to rethink the necessity of word segmentation in deep learning-based chinese natural language processing."], "semantics"], [["clinical reading comprehension: a thorough analysis of the emrqa dataset", "xiang yue | bernal jimenez gutierrez | huan sun", "machine reading comprehension has made great progress in recent years owing to large-scale annotated datasets. in the clinical domain, however, creating such datasets is quite difficult due to the domain expertise required for annotation. recently, pampari et al. (emnlp\u201918) tackled this issue by using expert-annotated question templates and existing i2b2 annotations to create emrqa, the first large-scale dataset for question answering (qa) based on clinical notes. in this paper, we provide an in-depth analysis of this dataset and the clinical reading comprehension (clinirc) task. from our qualitative analysis, we find that (i) emrqa answers are often incomplete, and (ii) emrqa questions are often answerable without using domain knowledge. from our quantitative experiments, surprising results include that (iii) using a small sampled subset (5%-20%), we can obtain roughly equal performance compared to the model trained on the entire dataset, (iv) this performance is close to human expert\u2019s performance, and (v) bert models do not beat the best performing base model. following our analysis of the emrqa, we further explore two desired aspects of clinirc systems: the ability to utilize clinical domain knowledge and to generalize to unseen questions and contexts. we argue that both should be considered when creating future datasets."], "question answering"], [["episodic memory reader: learning what to remember for question answering from streaming data", "moonsu han | minki kang | hyunwoo jung | sung ju hwang", "we consider a novel question answering (qa) task where the machine needs to read from large streaming data (long documents or videos) without knowing when the questions will be given, which is difficult to solve with existing qa methods due to their lack of scalability. to tackle this problem, we propose a novel end-to-end deep network model for reading comprehension, which we refer to as episodic memory reader (emr) that sequentially reads the input contexts into an external memory, while replacing memories that are less important for answering unseen questions. specifically, we train an rl agent to replace a memory entry when the memory is full, in order to maximize its qa accuracy at a future timepoint, while encoding the external memory using either the gru or the transformer architecture to learn representations that considers relative importance between the memory entries. we validate our model on a synthetic dataset (babi) as well as real-world large-scale textual qa (triviaqa) and video qa (tvqa) datasets, on which it achieves significant improvements over rule based memory scheduling policies or an rl based baseline that independently learns the query-specific importance of each memory."], "machine learning for nlp"], [["exclusive hierarchical decoding for deep keyphrase generation", "wang chen | hou pong chan | piji li | irwin king", "keyphrase generation (kg) aims to summarize the main ideas of a document into a set of keyphrases. a new setting is recently introduced into this problem, in which, given a document, the model needs to predict a set of keyphrases and simultaneously determine the appropriate number of keyphrases to produce. previous work in this setting employs a sequential decoding process to generate keyphrases. however, such a decoding method ignores the intrinsic hierarchical compositionality existing in the keyphrase set of a document. moreover, previous work tends to generate duplicated keyphrases, which wastes time and computing resources. to overcome these limitations, we propose an exclusive hierarchical decoding framework that includes a hierarchical decoding process and either a soft or a hard exclusion mechanism. the hierarchical decoding process is to explicitly model the hierarchical compositionality of a keyphrase set. both the soft and the hard exclusion mechanisms keep track of previously-predicted keyphrases within a window size to enhance the diversity of the generated keyphrases. extensive experiments on multiple kg benchmark datasets demonstrate the effectiveness of our method to generate less duplicated and more accurate keyphrases."], "information extraction, retrieval and text mining"], [["ordinal and attribute aware response generation in a multimodal dialogue system", "hardik chauhan | mauajama firdaus | asif ekbal | pushpak bhattacharyya", "multimodal dialogue systems have opened new frontiers in the traditional goal-oriented dialogue systems. the state-of-the-art dialogue systems are primarily based on unimodal sources, predominantly the text, and hence cannot capture the information present in the other sources such as videos, audios, images etc. with the availability of large scale multimodal dialogue dataset (mmd) (saha et al., 2018) on the fashion domain, the visual appearance of the products is essential for understanding the intention of the user. without capturing the information from both the text and image, the system will be incapable of generating correct and desirable responses. in this paper, we propose a novel position and attribute aware attention mechanism to learn enhanced image representation conditioned on the user utterance. our evaluation shows that the proposed model can generate appropriate responses while preserving the position and attribute information. experimental results also prove that our proposed approach attains superior performance compared to the baseline models, and outperforms the state-of-the-art approaches on text similarity based evaluation metrics."], "dialogue and interactive systems"], [["putting words in context: lstm language models and lexical ambiguity", "laura aina | kristina gulordava | gemma boleda", "in neural network models of language, words are commonly represented using context-invariant representations (word embeddings) which are then put in context in the hidden layers. since words are often ambiguous, representing the contextually relevant information is not trivial. we investigate how an lstm language model deals with lexical ambiguity in english, designing a method to probe its hidden representations for lexical and contextual information about words. we find that both types of information are represented to a large extent, but also that there is room for improvement for contextual information."], "semantics"], [["graphrel: modeling text as relational graphs for joint entity and relation extraction", "tsu-jui fu | peng-hsuan li | wei-yun ma", "in this paper, we present graphrel, an end-to-end relation extraction model which uses graph convolutional networks (gcns) to jointly learn named entities and relations. in contrast to previous baselines, we consider the interaction between named entities and relations via a 2nd-phase relation-weighted gcn to better extract relations. linear and dependency structures are both used to extract both sequential and regional features of the text, and a complete word graph is further utilized to extract implicit features among all word pairs of the text. with the graph-based approach, the prediction for overlapping relations is substantially improved over previous sequential approaches. we evaluate graphrel on two public datasets: nyt and webnlg. results show that graphrel maintains high precision while increasing recall substantially. also, graphrel outperforms previous work by 3.2% and 5.8% (f1 score), achieving a new state-of-the-art for relation extraction."], "information extraction, retrieval and text mining"], [["self-supervised learning for contextualized extractive summarization", "hong wang | xin wang | wenhan xiong | mo yu | xiaoxiao guo | shiyu chang | william yang wang", "existing models for extractive summarization are usually trained from scratch with a cross-entropy loss, which does not explicitly capture the global context at the document level. in this paper, we aim to improve this task by introducing three auxiliary pre-training tasks that learn to capture the document-level context in a self-supervised fashion. experiments on the widely-used cnn/dm dataset validate the effectiveness of the proposed auxiliary tasks. furthermore, we show that after pre-training, a clean model with simple building blocks is able to outperform previous state-of-the-art that are carefully designed."], "summarization"], [["mie: a medical information extractor towards medical dialogues", "yuanzhe zhang | zhongtao jiang | tao zhang | shiwan liu | jiarun cao | kang liu | shengping liu | jun zhao", "electronic medical records (emrs) have become key components of modern medical care systems. despite the merits of emrs, many doctors suffer from writing them, which is time-consuming and tedious. we believe that automatically converting medical dialogues to emrs can greatly reduce the burdens of doctors, and extracting information from medical dialogues is an essential step. to this end, we annotate online medical consultation dialogues in a window-sliding style, which is much easier than the sequential labeling annotation. we then propose a medical information extractor (mie) towards medical dialogues. mie is able to extract mentioned symptoms, surgeries, tests, other information and their corresponding status. to tackle the particular challenges of the task, mie uses a deep matching architecture, taking dialogue turn-interaction into account. the experimental results demonstrate mie is a promising solution to extract medical information from doctor-patient dialogues."], "information extraction, retrieval and text mining"], [["improving image captioning with better use of caption", "zhan shi | xu zhou | xipeng qiu | xiaodan zhu", "image captioning is a multimodal problem that has drawn extensive attention in both the natural language processing and computer vision community. in this paper, we present a novel image captioning architecture to better explore semantics available in captions and leverage that to enhance both image representation and caption generation. our models first construct caption-guided visual relationship graphs that introduce beneficial inductive bias using weakly supervised multi-instance learning. the representation is then enhanced with neighbouring and contextual nodes with their textual and visual features. during generation, the model further incorporates visual relationships using multi-task learning for jointly predicting word and object/predicate tag sequences. we perform extensive experiments on the mscoco dataset, showing that the proposed framework significantly outperforms the baselines, resulting in the state-of-the-art performance under a wide range of evaluation metrics. the code of our paper has been made publicly available."], "generation"], [["amr parsing via graph-sequence iterative inference", "deng cai | wai lam", "we propose a new end-to-end model that treats amr parsing as a series of dual decisions on the input sequence and the incrementally constructed graph. at each time step, our model performs multiple rounds of attention, reasoning, and composition that aim to answer two critical questions: (1) which part of the input sequence to abstract; and (2) where in the output graph to construct the new concept. we show that the answers to these two questions are mutually causalities. we design a model based on iterative inference that helps achieve better answers in both perspectives, leading to greatly improved parsing accuracy. our experimental results significantly outperform all previously reported smatch scores by large margins. remarkably, without the help of any large-scale pre-trained language model (e.g., bert), our model already surpasses previous state-of-the-art using bert. with the help of bert, we can push the state-of-the-art results to 80.2% on ldc2017t10 (amr 2.0) and 75.4% on ldc2014t12 (amr 1.0)."], "semantics"], [["how accents confound: probing for accent information in end-to-end speech recognition systems", "archiki prasad | preethi jyothi", "in this work, we present a detailed analysis of how accent information is reflected in the internal representation of speech in an end-to-end automatic speech recognition (asr) system. we use a state-of-the-art end-to-end asr system, comprising convolutional and recurrent layers, that is trained on a large amount of us-accented english speech and evaluate the model on speech samples from seven different english accents. we examine the effects of accent on the internal representation using three main probing techniques: a) gradient-based explanation methods, b) information-theoretic measures, and c) outputs of accent and phone classifiers. we find different accents exhibiting similar trends irrespective of the probing technique used. we also find that most accent information is encoded within the first recurrent layer, which is suggestive of how one could adapt such an end-to-end model to learn representations that are invariant to accents."], "speech and multimodality"], [["agreement prediction of arguments in cyber argumentation for detecting stance polarity and intensity", "joseph sirrianni | xiaoqing liu | douglas adams", "in online debates, users express different levels of agreement/disagreement with one another\u2019s arguments and ideas. often levels of agreement/disagreement are implicit in the text, and must be predicted to analyze collective opinions. existing stance detection methods predict the polarity of a post\u2019s stance toward a topic or post, but don\u2019t consider the stance\u2019s degree of intensity. we introduce a new research problem, stance polarity and intensity prediction in response relationships between posts. this problem is challenging because differences in stance intensity are often subtle and require nuanced language understanding. cyber argumentation research has shown that incorporating both stance polarity and intensity data in online debates leads to better discussion analysis. we explore five different learning models: ridge-m regression, ridge-s regression, svr-rf-r, pkudblab-pip, and t-pan-pip for predicting stance polarity and intensity in argumentation. these models are evaluated using a new dataset for stance polarity and intensity prediction collected using a cyber argumentation platform. the svr-rf-r model performs best for prediction of stance polarity with an accuracy of 70.43% and intensity with rmse of 0.596. this work is the first to train models for predicting a post\u2019s stance polarity and intensity in one combined value in cyber argumentation with reasonably good accuracy."], "sentiment analysis, stylistic analysis, and argument mining"], [["integrating multimodal information in large pretrained transformers", "wasifur rahman | md kamrul hasan | sangwu lee | amirali bagher zadeh | chengfeng mao | louis-philippe morency | ehsan hoque", "recent transformer-based contextual word representations, including bert and xlnet, have shown state-of-the-art performance in multiple disciplines within nlp. fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream. while fine-tuning these pre-trained models is straightforward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in nlp focused on modeling face-to-face communication). more specifically, this is due to the fact that pre-trained models don\u2019t have the necessary components to accept two extra modalities of vision and acoustic. in this paper, we proposed an attachment to bert and xlnet called multimodal adaptation gate (mag). mag allows bert and xlnet to accept multimodal nonverbal data during fine-tuning. it does so by generating a shift to internal representation of bert and xlnet; a shift that is conditioned on the visual and acoustic modalities. in our experiments, we study the commonly used cmu-mosi and cmu-mosei datasets for multimodal sentiment analysis. fine-tuning mag-bert and mag-xlnet significantly boosts the sentiment analysis performance over previous baselines as well as language-only fine-tuning of bert and xlnet. on the cmu-mosi dataset, mag-xlnet achieves human-level multimodal sentiment analysis performance for the first time in the nlp community."], "speech and multimodality"], [["neural legal judgment prediction in english", "ilias chalkidis | ion androutsopoulos | nikolaos aletras", "legal judgment prediction is the task of automatically predicting the outcome of a court case, given a text describing the case\u2019s facts. previous work on using neural models for this task has focused on chinese; only feature-based models (e.g., using bags of words and topics) have been considered in english. we release a new english legal judgment prediction dataset, containing cases from the european court of human rights. we evaluate a broad variety of neural models on the new dataset, establishing strong baselines that surpass previous feature-based models in three tasks: (1) binary violation classification; (2) multi-label classification; (3) case importance prediction. we also explore if models are biased towards demographic information via data anonymization. as a side-product, we propose a hierarchical version of bert, which bypasses bert\u2019s length limitation."], "nlp applications"], [["latent retrieval for weakly supervised open domain question answering", "kenton lee | ming-wei chang | kristina toutanova", "recent work on open domain question answering (qa) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (ir) system to retrieve evidence candidates. we argue that both are suboptimal, since gold evidence is not always available, and qa is fundamentally different from ir. we show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any ir system. in this setting, evidence retrieval from all of wikipedia is treated as a latent variable. since this is impractical to learn from scratch, we pre-train the retriever with an inverse cloze task. we evaluate on open versions of five qa datasets. on datasets where the questioner already knows the answer, a traditional ir system such as bm25 is sufficient. on datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming bm25 by up to 19 points in exact match."], "question answering"], [["cost-sensitive regularization for label confusion-aware event detection", "hongyu lin | yaojie lu | xianpei han | le sun", "in supervised event detection, most of the mislabeling occurs between a small number of confusing type pairs, including trigger-nil pairs and sibling sub-types of the same coarse type. to address this label confusion problem, this paper proposes cost-sensitive regularization, which can force the training procedure to concentrate more on optimizing confusing type pairs. specifically, we introduce a cost-weighted term into the training loss, which penalizes more on mislabeling between confusing label pairs. furthermore, we also propose two estimators which can effectively measure such label confusion based on instance-level or population-level statistics. experiments on tac-kbp 2017 datasets demonstrate that the proposed method can significantly improve the performances of different models in both english and chinese event detection."], "information extraction, retrieval and text mining"], [["predicting the topical stance and political leaning of media using tweets", "peter stefanov | kareem darwish | atanas atanasov | preslav nakov", "discovering the stances of media outlets and influential people on current, debatable topics is important for social statisticians and policy makers. many supervised solutions exist for determining viewpoints, but manually annotating training data is costly. in this paper, we propose a cascaded method that uses unsupervised learning to ascertain the stance of twitter users with respect to a polarizing topic by leveraging their retweet behavior; then, it uses supervised learning based on user labels to characterize both the general political leaning of online media and of popular twitter users, as well as their stance with respect to the target polarizing topic. we evaluate the model by comparing its predictions to gold labels from the media bias/fact check website, achieving 82.6% accuracy."], "computational social science, social media and cultural analytics"], [["a frame-based sentence representation for machine reading comprehension", "shaoru guo | ru li | hongye tan | xiaoli li | yong guan | hongyan zhao | yueping zhang", "sentence representation (sr) is the most crucial and challenging task in machine reading comprehension (mrc). mrc systems typically only utilize the information contained in the sentence itself, while human beings can leverage their semantic knowledge. to bridge the gap, we proposed a novel frame-based sentence representation (fsr) method, which employs frame semantic knowledge to facilitate sentence modelling. specifically, different from existing methods that only model lexical units (lus), frame representation models, which utilize both lus in frame and frame-to-frame (f-to-f) relations, are designed to model frames and sentences with attention schema. our proposed fsr method is able to integrate multiple-frame semantic information to get much better sentence representations. our extensive experimental results show that it performs better than state-of-the-art technologies on machine reading comprehension task."], "question answering"], [["sentence mover\u2019s similarity: automatic evaluation for multi-sentence texts", "elizabeth clark | asli celikyilmaz | noah a. smith", "for evaluating machine-generated texts, automatic methods hold the promise of avoiding collection of human judgments, which can be expensive and time-consuming. the most common automatic metrics, like bleu and rouge, depend on exact word matching, an inflexible approach for measuring semantic similarity. we introduce methods based on sentence mover\u2019s similarity; our automatic metrics evaluate text in a continuous space using word and sentence embeddings. we find that sentence-based metrics correlate with human judgments significantly better than rouge, both on machine-generated summaries (average length of 3.4 sentences) and human-authored essays (average length of 7.5). we also show that sentence mover\u2019s similarity can be used as a reward when learning a generation model via reinforcement learning; we present both automatic and human evaluations of summaries learned in this way, finding that our approach outperforms rouge."], "resources and evaluation"], [["curate and generate: a corpus and method for joint control of semantics and style in neural nlg", "shereen oraby | vrindavan harrison | abteen ebrahimi | marilyn walker", "neural natural language generation (nnlg) from structured meaning representations has become increasingly popular in recent years. while we have seen progress with generating syntactically correct utterances that preserve semantics, various shortcomings of nnlg systems are clear: new tasks require new training data which is not available or straightforward to acquire, and model outputs are simple and may be dull and repetitive. this paper addresses these two critical challenges in nnlg by: (1) scalably (and at no cost) creating training datasets of parallel meaning representations and reference texts with rich style markup by using data from freely available and naturally descriptive user reviews, and (2) systematically exploring how the style markup enables joint control of semantic and stylistic aspects of neural model output. we present yelpnlg, a corpus of 300,000 rich, parallel meaning representations and highly stylistically varied reference texts spanning different restaurant attributes, and describe a novel methodology that can be scalably reused to generate nlg datasets for other domains. the experiments show that the models control important aspects, including lexical choice of adjectives, output length, and sentiment, allowing the models to successfully hit multiple style targets without sacrificing semantics."], "generation"], [["improved speech representations with multi-target autoregressive predictive coding", "yu-an chung | james glass", "training objectives based on predictive coding have recently been shown to be very effective at learning meaningful representations from unlabeled speech. one example is autoregressive predictive coding (chung et al., 2019), which trains an autoregressive rnn to generate an unseen future frame given a context such as recent past frames. the basic hypothesis of these approaches is that hidden states that can accurately predict future frames are a useful representation for many downstream tasks. in this paper we extend this hypothesis and aim to enrich the information encoded in the hidden states by training the model to make more accurate future predictions. we propose an auxiliary objective that serves as a regularization to improve generalization of the future frame prediction task. experimental results on phonetic classification, speech recognition, and speech translation not only support the hypothesis, but also demonstrate the effectiveness of our approach in learning representations that contain richer phonetic content."], "speech and multimodality"], [["expertise style transfer: a new task towards better communication between experts and laymen", "yixin cao | ruihao shui | liangming pan | min-yen kan | zhiyuan liu | tat-seng chua", "the curse of knowledge can impede communication between experts and laymen. we propose a new task of expertise style transfer and contribute a manually annotated dataset with the goal of alleviating such cognitive biases. solving this task not only simplifies the professional language, but also improves the accuracy and expertise level of laymen descriptions using simple words. this is a challenging task, unaddressed in previous work, as it requires the models to have expert intelligence in order to modify text with a deep understanding of domain knowledge and structures. we establish the benchmark performance of five state-of-the-art models for style transfer and text simplification. the results demonstrate a significant gap between machine and human performance. we also discuss the challenges of automatic evaluation, to provide insights into future research directions. the dataset is publicly available at https://srhthu.github.io/expertise-style-transfer/."], "generation"], [["right for the wrong reasons: diagnosing syntactic heuristics in natural language inference", "tom mccoy | ellie pavlick | tal linzen", "a machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. we study this issue within natural language inference (nli), the task of determining whether one sentence entails another. we hypothesize that statistical nli models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. to determine whether models have adopted these heuristics, we introduce a controlled evaluation set called hans (heuristic analysis for nli systems), which contains many examples where the heuristics fail. we find that models trained on mnli, including bert, a state-of-the-art model, perform very poorly on hans, suggesting that they have indeed adopted these heuristics. we conclude that there is substantial room for improvement in nli systems, and that the hans dataset can motivate and measure progress in this area."], "semantics"], [["improving textual network embedding with global attention via optimal transport", "liqun chen | guoyin wang | chenyang tao | dinghan shen | pengyu cheng | xinyuan zhang | wenlin wang | yizhe zhang | lawrence carin", "constituting highly informative network embeddings is an essential tool for network analysis. it encodes network topology, along with other useful side information, into low dimensional node-based feature representations that can be exploited by statistical modeling. this work focuses on learning context-aware network embeddings augmented with text data. we reformulate the network embedding problem, and present two novel strategies to improve over traditional attention mechanisms: (i) a content-aware sparse attention module based on optimal transport; and (ii) a high-level attention parsing module. our approach yields naturally sparse and self-normalized relational inference. it can capture long-term interactions between sequences, thus addressing the challenges faced by existing textual network embedding schemes. extensive experiments are conducted to demonstrate our model can consistently outperform alternative state-of-the-art methods."], "information extraction, retrieval and text mining"], [["matching the blanks: distributional similarity for relation learning", "livio baldini soares | nicholas fitzgerald | jeffrey ling | tom kwiatkowski", "general purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction. efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph. however, both of these approaches are limited in their ability to generalize. in this paper, we build on extensions of harris\u2019 distributional hypothesis to relations, as well as recent advances in learning text representations (specifically, bert), to build task agnostic relation representations solely from entity-linked text. we show that these representations significantly outperform previous work on exemplar based relation extraction (fewrel) even without using any of that task\u2019s training data. we also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on semeval 2010 task 8, kbp37, and tacred"], "information extraction, retrieval and text mining"], [["improving chinese word segmentation with wordhood memory networks", "yuanhe tian | yan song | fei xia | tong zhang | yonggang wang", "contextual features always play an important role in chinese word segmentation (cws). wordhood information, being one of the contextual features, is proved to be useful in many conventional character-based segmenters. however, this feature receives less attention in recent neural models and it is also challenging to design a framework that can properly integrate wordhood information from different wordhood measures to existing neural frameworks. in this paper, we therefore propose a neural framework, wmseg, which uses memory networks to incorporate wordhood information with several popular encoder-decoder combinations for cws. experimental results on five benchmark datasets indicate the memory mechanism successfully models wordhood information for neural segmenters and helps wmseg achieve state-of-the-art performance on all those datasets. further experiments and analyses also demonstrate the robustness of our proposed framework with respect to different wordhood measures and the efficiency of wordhood information in cross-domain experiments."], "phonology, morphology and word segmentation"], [["neural topic modeling with bidirectional adversarial training", "rui wang | xuemeng hu | deyu zhou | yulan he | yuxuan xiong | chenchen ye | haiyang xu", "recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text, since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as latent dirichlet allocation (lda). however, these models either typically assume improper prior (e.g. gaussian or logistic normal) over latent topic space or could not infer topic distribution for a given document. to address these limitations, we propose a neural topic modeling approach, called bidirectional adversarial topic (bat) model, which represents the first attempt of applying bidirectional adversarial training for neural topic modeling. the proposed bat builds a two-way projection between the document-topic distribution and the document-word distribution. it uses a generator to capture the semantic patterns from texts and an encoder for topic inference. furthermore, to incorporate word relatedness information, the bidirectional adversarial topic model with gaussian (gaussian-bat) is extended from bat. to verify the effectiveness of bat and gaussian-bat, three benchmark corpora are used in our experiments. the experimental results show that bat and gaussian-bat obtain more coherent topics, outperforming several competitive baselines. moreover, when performing text clustering based on the extracted topics, our models outperform all the baselines, with more significant improvements achieved by gaussian-bat where an increase of near 6% is observed in accuracy."], "information extraction, retrieval and text mining"], [["learning compressed sentence representations for on-device text processing", "dinghan shen | pengyu cheng | dhanasekar sundararaman | xinyuan zhang | qian yang | meng tang | asli celikyilmaz | lawrence carin", "vector representations of sentences, trained on massive text corpora, are widely used as generic sentence embeddings across a variety of nlp problems. the learned representations are generally assumed to be continuous and real-valued, giving rise to a large memory footprint and slow retrieval speed, which hinders their applicability to low-resource (memory and computation) platforms, such as mobile devices. in this paper, we propose four different strategies to transform continuous and generic sentence embeddings into a binarized form, while preserving their rich semantic information. the introduced methods are evaluated across a wide range of downstream tasks, where the binarized sentence embeddings are demonstrated to degrade performance by only about 2% relative to their continuous counterparts, while reducing the storage requirement by over 98%. moreover, with the learned binary representations, the semantic relatedness of two sentences can be evaluated by simply calculating their hamming distance, which is more computational efficient compared with the inner product operation between continuous embeddings. detailed analysis and case study further validate the effectiveness of proposed methods."], "semantics"], [["learning to deceive with attention-based explanations", "danish pruthi | mansi gupta | bhuwan dhingra | graham neubig | zachary c. lipton", "attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. in addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. we call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions. across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy. through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender. consequently, our results cast doubt on attention\u2019s reliability as a tool for auditing algorithms in the context of fairness and accountability."], "interpretability and analysis of models for nlp"], [["posterior control of blackbox generation", "xiang lisa li | alexander rush", "text generation often requires high-precision output that obeys task-specific rules. this fine-grained control is difficult to enforce with off-the-shelf deep learning models. in this work, we consider augmenting neural generation models with discrete control states learned through a structured latent-variable approach. under this formulation, task-specific knowledge can be encoded through a range of rich, posterior constraints that are effectively trained into the model. this approach allows users to ground internal model decisions based on prior knowledge, without sacrificing the representational power of neural generative models. experiments consider applications of this approach for text generation. we find that this method improves over standard benchmarks, while also providing fine-grained control."], "machine learning for nlp"], [["a two-stage masked lm method for term set expansion", "guy kushilevitz | shaul markovitch | yoav goldberg", "we tackle the task of term set expansion (tse): given a small seed set of example terms from a semantic class, finding more members of that class. the task is of great practical utility, and also of theoretical utility as it requires generalization from few examples. previous approaches to the tse task can be characterized as either distributional or pattern-based. we harness the power of neural masked language models (mlm) and propose a novel tse algorithm, which combines the pattern-based and distributional approaches. due to the small size of the seed set, fine-tuning methods are not effective, calling for more creative use of the mlm. the gist of the idea is to use the mlm to first mine for informative patterns with respect to the seed set, and then to obtain more members of the seed class by generalizing these patterns. our method outperforms state-of-the-art tse algorithms. implementation is available at: https://github.com/ guykush/termsetexpansion-mpb/"], "information extraction, retrieval and text mining"], [["transformers to learn hierarchical contexts in multiparty dialogue for span-based question answering", "changmao li | jinho d. choi", "we introduce a novel approach to transformers that learns hierarchical representations in multiparty dialogue. first, three language modeling tasks are used to pre-train the transformers, token- and utterance-level language modeling and utterance order prediction, that learn both token and utterance embeddings for better understanding in dialogue contexts. then, multi-task learning between the utterance prediction and the token span prediction is applied to fine-tune for span-based question answering (qa). our approach is evaluated on the friendsqa dataset and shows improvements of 3.8% and 1.4% over the two state-of-the-art transformer models, bert and roberta, respectively."], "question answering"], [["robust zero-shot cross-domain slot filling with example values", "darsh shah | raghav gupta | amir fayazi | dilek hakkani-tur", "task-oriented dialog systems increasingly rely on deep learning-based slot filling models, usually needing extensive labeled training data for target domains. often, however, little to no target domain training data may be available, or the training and target domain schemas may be misaligned, as is common for web forms on similar websites. prior zero-shot slot filling models use slot descriptions to learn concepts, but are not robust to misaligned schemas. we propose utilizing both the slot description and a small number of examples of slot values, which may be easily available, to learn semantic representations of slots which are transferable across domains and robust to misaligned schemas. our approach outperforms state-of-the-art models on two multi-domain datasets, especially in the low-data setting."], "dialogue and interactive systems"], [["inferring concept hierarchies from text corpora via hyperbolic embeddings", "matthew le | stephen roller | laetitia papaxanthos | douwe kiela | maximilian nickel", "we consider the task of inferring \u201cis-a\u201d relationships from large text corpora. for this purpose, we propose a new method combining hyperbolic embeddings and hearst patterns. this approach allows us to set appropriate constraints for inferring concept hierarchies from distributional contexts while also being able to predict missing \u201cis-a\u201d-relationships and to correct wrong extractions. moreover \u2013 and in contrast with other methods \u2013 the hierarchical nature of hyperbolic space allows us to learn highly efficient representations and to improve the taxonomic consistency of the inferred hierarchies. experimentally, we show that our approach achieves state-of-the-art performance on several commonly-used benchmarks."], "semantics"], [["reverse engineering configurations of neural text generation models", "yi tay | dara bahri | che zheng | clifford brunk | donald metzler | andrew tomkins", "recent advances in neural text generation modeling have resulted in a number of societal concerns related to how such approaches might be used in malicious ways. it is therefore desirable to develop a deeper understanding of the fundamental properties of such models. the study of artifacts that emerge in machine generated text as a result of modeling choices is a nascent research area. to this end, the extent and degree to which these artifacts surface in generated text is still unclear. in the spirit of better understanding generative text models and their artifacts, we propose the new task of distinguishing which of several variants of a given model generated some piece of text. specifically, we conduct an extensive suite of diagnostic tests to observe whether modeling choices (e.g., sampling methods, top-k probabilities, model architectures, etc.) leave detectable artifacts in the text they generate. our key finding, which is backed by a rigorous set of experiments, is that such artifacts are present and that different modeling choices can be inferred by looking at generated text alone. this suggests that neural text generators may actually be more sensitive to various modeling choices than previously thought."], "generation"], [["probing for referential information in language models", "ionut-teodor sorodoc | kristina gulordava | gemma boleda", "language models keep track of complex information about the preceding context \u2013 including, e.g., syntactic relations in a sentence. we investigate whether they also capture information beneficial for resolving pronominal anaphora in english. we analyze two state of the art models with lstm and transformer architectures, via probe tasks and analysis on a coreference annotated corpus. the transformer outperforms the lstm in all analyses. our results suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information, in the sense of capturing the fact that we use language to refer to entities in the world. however, we find traces of the latter aspect, too."], "interpretability and analysis of models for nlp"], [["rigid formats controlled text generation", "piji li | haisong zhang | xiaojiang liu | shuming shi", "neural text generation has made tremendous progress in various tasks. one common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating. however, we may confront some special text paradigms such as lyrics (assume the music score is given), sonnet, songci (classical chinese poetry of the song dynasty), etc. the typical characteristics of these texts are in three folds: (1) they must comply fully with the rigid predefined formats. (2) they must obey some rhyming schemes. (3) although they are restricted to some formats, the sentence integrity must be guaranteed. to the best of our knowledge, text generation based on the predefined rigid formats has not been well investigated. therefore, we propose a simple and elegant framework named songnet to tackle this problem. the backbone of the framework is a transformer-based auto-regressive language model. sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity. we improve the attention mechanism to impel the model to capture some future information on the format. a pre-training and fine-tuning framework is designed to further improve the generation quality. extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation."], "generation"], [["multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs", "ming tu | guangtao wang | jing huang | yun tang | xiaodong he | bowen zhou", "multi-hop reading comprehension (rc) across documents poses new challenge over single-document rc because it requires reasoning over multiple documents to reach the final answer. in this paper, we propose a new model to tackle the multi-hop rc problem. we introduce a heterogeneous graph with different types of nodes and edges, which is named as heterogeneous document-entity (hde) graph. the advantage of hde graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. our proposed model can do reasoning over the hde graph with nodes representation initialized with co-attention and self-attention based context encoders. we employ graph neural networks (gnn) based message passing algorithms to accumulate evidences on the proposed hde graph. evaluated on the blind test set of the qangaroo wikihop data set, our hde graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance."], "question answering"], [["coupling retrieval and meta-learning for context-dependent semantic parsing", "daya guo | duyu tang | nan duan | ming zhou | jian yin", "in this paper, we present an approach to incorporate retrieved datapoints as supporting evidence for context-dependent semantic parsing, such as generating source code conditioned on the class environment. our approach naturally combines a retrieval model and a meta-learner, where the former learns to find similar datapoints from the training data, and the latter considers retrieved datapoints as a pseudo task for fast adaptation. specifically, our retriever is a context-aware encoder-decoder model with a latent variable which takes context environment into consideration, and our meta-learner learns to utilize retrieved datapoints in a model-agnostic meta-learning paradigm for fast adaptation. we conduct experiments on concode and csqa datasets, where the context refers to class environment in java codes and conversational history, respectively. we use sequence-to-action model as the base semantic parser, which performs the state-of-the-art accuracy on both datasets. results show that both the context-aware retriever and the meta-learning strategy improve accuracy, and our approach performs better than retrieve-and-edit baselines."], "semantics"], [["curriculum learning for natural language understanding", "benfeng xu | licheng zhang | zhendong mao | quan wang | hongtao xie | yongdong zhang", "with the great success of pre-trained language models, the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding (nlu) tasks. at the fine-tune stage, target task data is usually introduced in a completely random order and treated equally. however, examples in nlu tasks can vary greatly in difficulty, and similar to human learning procedure, language models can benefit from an easy-to-difficult curriculum. based on this idea, we propose our curriculum learning approach. by reviewing the trainset in a crossed way, we are able to distinguish easy examples from difficult ones, and arrange a curriculum for language models. without any manual model architecture design or use of external data, our curriculum learning approach obtains significant and universal performance improvements on a wide range of nlu tasks."], "semantics"], [["argument generation with retrieval, planning, and realization", "xinyu hua | zhe hu | lu wang", "automatic argument generation is an appealing but challenging task. in this paper, we study the specific problem of counter-argument generation, and present a novel framework, candela. it consists of a powerful retrieval system and a novel two-step generation model, where a text planning decoder first decides on the main talking points and a proper language style for each sentence, then a content realization decoder reflects the decisions and constructs an informative paragraph-level argument. furthermore, our generation model is empowered by a retrieval system indexed with 12 million articles collected from wikipedia and popular english news media, which provides access to high-quality content with diversity. automatic evaluation on a large-scale dataset collected from reddit shows that our model yields significantly higher bleu, rouge, and meteor scores than the state-of-the-art and non-trivial comparisons. human evaluation further indicates that our system arguments are more appropriate for refutation and richer in content."], "generation"], [["in layman\u2019s terms: semi-open relation extraction from scientific texts", "ruben kruiper | julian vincent | jessica chen-burger | marc desmulliez | ioannis konstas", "information extraction (ie) from scientific texts can be used to guide readers to the central information in scientific documents. but narrow ie systems extract only a fraction of the information captured, and open ie systems do not perform well on the long and complex sentences encountered in scientific texts. in this work we combine the output of both types of systems to achieve semi-open relation extraction, a new task that we explore in the biology domain. first, we present the focused open biological information extraction (fobie) dataset and use fobie to train a state-of-the-art narrow scientific ie system to extract trade-off relations and arguments that are central to biology texts. we then run both the narrow ie system and a state-of-the-art open ie system on a corpus of 10k open-access scientific biological texts. we show that a significant amount (65%) of erroneous and uninformative open ie extractions can be filtered using narrow ie extractions. furthermore, we show that the retained extractions are significantly more often informative to a reader."], "information extraction, retrieval and text mining"], [["exploring author context for detecting intended vs perceived sarcasm", "silviu oprea | walid magdy", "we investigate the impact of using author context on textual sarcasm detection. we define author context as the embedded representation of their historical posts on twitter and suggest neural models that extract these representations. we experiment with two tweet datasets, one labelled manually for sarcasm, and the other via tag-based distant supervision. we achieve state-of-the-art performance on the second dataset, but not on the one labelled manually, indicating a difference between intended sarcasm, captured by distant supervision, and perceived sarcasm, captured by manual labelling."], "computational social science, social media and cultural analytics"], [["dtca: decision tree-based co-attention networks for explainable claim verification", "lianwei wu | yuan rao | yongqiang zhao | hao liang | ambreen nazir", "recently, many methods discover effective evidence from reliable sources by appropriate neural networks for explainable claim verification, which has been widely recognized. however, in these methods, the discovery process of evidence is nontransparent and unexplained. simultaneously, the discovered evidence is aimed at the interpretability of the whole sequence of claims but insufficient to focus on the false parts of claims. in this paper, we propose a decision tree-based co-attention model (dtca) to discover evidence for explainable claim verification. specifically, we first construct decision tree-based evidence model (dte) to select comments with high credibility as evidence in a transparent and interpretable way. then we design co-attention self-attention networks (casa) to make the selected evidence interact with claims, which is for 1) training dte to determine the optimal decision thresholds and obtain more powerful evidence; and 2) utilizing the evidence to find the false parts in the claim. experiments on two public datasets, rumoureval and pheme, demonstrate that dtca not only provides explanations for the results of claim verification but also achieves the state-of-the-art performance, boosting the f1-score by more than 3.11%, 2.41%, respectively."], "computational social science, social media and cultural analytics"], [["slot-consistent nlg for task-oriented dialogue systems with iterative rectification network", "yangming li | kaisheng yao | libo qin | wanxiang che | xiaolong li | ting liu", "data-driven approaches using neural networks have achieved promising performances in natural language generation (nlg). however, neural generators are prone to make mistakes, e.g., neglecting an input slot value and generating a redundant slot value. prior works refer this to hallucination phenomenon. in this paper, we study slot consistency for building reliable nlg systems with all slot values of input dialogue act (da) properly generated in output sentences. we propose iterative rectification network (irn) for improving general nlg systems to produce both correct and fluent responses. it applies a bootstrapping algorithm to sample training candidates and uses reinforcement learning to incorporate discrete reward related to slot inconsistency into training. comprehensive studies have been conducted on multiple benchmark datasets, showing that the proposed methods have significantly reduced the slot error rate (err) for all strong baselines. human evaluations also have confirmed its effectiveness."], "dialogue and interactive systems"], [["cross-domain generalization of neural constituency parsers", "daniel fried | nikita kitaev | dan klein", "neural parsers obtain state-of-the-art results on benchmark treebanks for constituency parsing\u2014but to what degree do they generalize to other domains? we present three results about the generalization of neural parsers in a zero-shot setting: training on trees from one corpus and evaluating on out-of-domain corpora. first, neural and non-neural parsers generalize comparably to new domains. second, incorporating pre-trained encoder representations into neural parsers substantially improves their performance across all domains, but does not give a larger relative improvement for out-of-domain treebanks. finally, despite the rich input representations they learn, neural parsers still benefit from structured output prediction of output trees, yielding higher exact match accuracy and stronger generalization both to larger text spans and to out-of-domain corpora. we analyze generalization on english and chinese corpora, and in the process obtain state-of-the-art parsing results for the brown, genia, and english web treebanks."], "machine learning for nlp"], [["learning low-resource end-to-end goal-oriented dialog for fast and reliable system deployment", "yinpei dai | hangyu li | chengguang tang | yongbin li | jian sun | xiaodan zhu", "existing end-to-end dialog systems perform less effectively when data is scarce. to obtain an acceptable success in real-life online services with only a handful of training examples, both fast adaptability and reliable performance are highly desirable for dialog systems. in this paper, we propose the meta-dialog system (mds), which combines the advantages of both meta-learning approaches and human-machine collaboration. we evaluate our methods on a new extended-babi dataset and a transformed multiwoz dataset for low-resource goal-oriented dialog learning. experimental results show that mds significantly outperforms non-meta-learning baselines and can achieve more than 90% per-turn accuracies with only 10 dialogs on the extended-babi dataset."], "dialogue and interactive systems"], [["a mixture of h - 1 heads is better than h heads", "hao peng | roy schwartz | dianqi li | noah a. smith", "multi-head attentive neural architectures have achieved state-of-the-art results on a variety of natural language processing tasks. evidence has shown that they are overparameterized; attention heads can be pruned without significant performance loss. in this work, we instead \u201creallocate\u201d them\u2014the model learns to activate different heads on different inputs. drawing connections between multi-head attention and mixture of experts, we propose the mixture of attentive experts model (mae). mae is trained using a block coordinate descent algorithm that alternates between updating (1) the responsibilities of the experts and (2) their parameters. experiments on machine translation and language modeling show that mae outperforms strong baselines on both tasks. particularly, on the wmt14 english to german translation dataset, mae improves over \u201ctransformer-base\u201d by 0.8 bleu, with a comparable number of parameters. our analysis shows that our model learns to specialize different experts to different inputs."], "machine learning for nlp"], [["evaluating dialogue generation systems via response selection", "shiki sato | reina akama | hiroki ouchi | jun suzuki | kentaro inui", "existing automatic evaluation metrics for open-domain dialogue response generation systems correlate poorly with human evaluation. we focus on evaluating response generation systems via response selection. to evaluate systems properly via response selection, we propose a method to construct response selection test sets with well-chosen false candidates. specifically, we propose to construct test sets filtering out some types of false candidates: (i) those unrelated to the ground-truth response and (ii) those acceptable as appropriate responses. through experiments, we demonstrate that evaluating systems via response selection with the test set developed by our method correlates more strongly with human evaluation, compared with widely used automatic evaluation metrics such as bleu."], "dialogue and interactive systems"], [["stay on the path: instruction fidelity in vision-and-language navigation", "vihan jain | gabriel magalhaes | alexander ku | ashish vaswani | eugene ie | jason baldridge", "advances in learning and representations have reinvigorated work that connects language to other modalities. a particularly exciting direction is vision-and-language navigation(vln), in which agents interpret natural language instructions and visual scenes to move through environments and reach goals. despite recent progress, current research leaves unclear how much of a role language under-standing plays in this task, especially because dominant evaluation metrics have focused on goal completion rather than the sequence of actions corresponding to the instructions. here, we highlight shortcomings of current metrics for the room-to-room dataset (anderson et al.,2018b) and propose a new metric, coverage weighted by length score (cls). we also show that the existing paths in the dataset are not ideal for evaluating instruction following because they are direct-to-goal shortest paths. we join existing short paths to form more challenging extended paths to create a new data set, room-for-room (r4r). using r4r and cls, we show that agents that receive rewards for instruction fidelity outperform agents that focus on goal completion."], "language grounding to vision, robotics and beyond"], [["knowledge-aware pronoun coreference resolution", "hongming zhang | yan song | yangqiu song | dong yu", "resolving pronoun coreference requires knowledge support, especially for particular domains (e.g., medicine). in this paper, we explore how to leverage different types of knowledge to better resolve pronoun coreference with a neural model. to ensure the generalization ability of our model, we directly incorporate knowledge in the format of triplets, which is the most common format of modern knowledge graphs, instead of encoding it with features or rules as that in conventional approaches. moreover, since not all knowledge is helpful in certain contexts, to selectively use them, we propose a knowledge attention module, which learns to select and use informative knowledge based on contexts, to enhance our model. experimental results on two datasets from different domains prove the validity and effectiveness of our model, where it outperforms state-of-the-art baselines by a large margin. moreover, since our model learns to use external knowledge rather than only fitting the training data, it also demonstrates superior performance to baselines in the cross-domain setting."], "semantics"], [["that is a known lie: detecting previously fact-checked claims", "shaden shaar | nikolay babulkov | giovanni da san martino | preslav nakov", "the recent proliferation of \u201dfake news\u201d has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives. as a result and over time, a large number of fact-checked claims have been accumulated, which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact-checked by some trusted fact-checking organization, as viral claims often come back after a while in social media, and politicians like to repeat their favorite statements, true or false, over and over again. as manual fact-checking is very time-consuming (and fully automatic fact-checking has credibility issues), it is important to try to save this effort and to avoid wasting time on claims that have already been fact-checked. interestingly, despite the importance of the task, it has been largely ignored by the research community so far. here, we aim to bridge this gap. in particular, we formulate the task and we discuss how it relates to, but also differs from, previous work. we further create a specialized dataset, which we release to the research community. finally, we present learning-to-rank experiments that demonstrate sizable improvements over state-of-the-art retrieval and textual similarity approaches."], "resources and evaluation"], [["crafting adversarial examples for neural machine translation", "xinze zhang | junzhe zhang | zhenhua chen | kun he", "effective adversary generation for neural machine translation (nmt) is a crucial prerequisite for building robust machine translation systems. in this work, we investigate veritable evaluations of nmt adversarial attacks, and propose a novel method to craft nmt adversarial examples. we first show the current nmt adversarial attacks may be improperly estimated by the commonly used mono-directional translation, and we propose to leverage the round-trip translation technique to build valid metrics for evaluating nmt adversarial attacks. our intuition is that an effective nmt adversarial example, which imposes minor shifting on the source and degrades the translation dramatically, would naturally lead to a semantic-destroyed round-trip translation result. we then propose a promising black-box attack method called word saliency speedup local search (wsls) that could effectively attack the mainstream nmt architectures. comprehensive experiments demonstrate that the proposed metrics could accurately evaluate the attack effectiveness, and the proposed wsls could significantly break the state-of-art nmt models with small perturbation. besides, wsls exhibits strong transferability on attacking baidu and bing online translators."], "machine translation and multilinguality"], [["imojie: iterative memory-based joint open information extraction", "keshav kolluru | samarth aggarwal | vipul rathore | mausam | soumen chakrabarti", "while traditional systems for open information extraction were statistical and rule-based, recently neural models have been introduced for the task. our work builds upon copyattention, a sequence generation openie model (cui et. al. 18). our analysis reveals that copyattention produces a constant number of extractions per sentence, and its extracted tuples often express redundant information. we present imojie, an extension to copyattention, which produces the next extraction conditioned on all previously extracted tuples. this approach overcomes both shortcomings of copyattention, resulting in a variable number of diverse extractions per sentence. we train imojie on training data bootstrapped from extractions of several non-neural systems, which have been automatically filtered to reduce redundancy and noise. imojie outperforms copyattention by about 18 f1 pts, and a bert-based strong baseline by 2 f1 pts, establishing a new state of the art for the task."], "information extraction, retrieval and text mining"], [["hypercore: hyperbolic and co-graph representation for automatic icd coding", "pengfei cao | yubo chen | kang liu | jun zhao | shengping liu | weifeng chong", "the international classification of diseases (icd) provides a standardized way for classifying diseases, which endows each disease with a unique code. icd coding aims to assign proper icd codes to a medical record. since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic icd coding task. however, most of existing methods independently predict each code, ignoring two important characteristics: code hierarchy and code co-occurrence. in this paper, we propose a hyperbolic and co-graph representation method (hypercore) to address the above problem. specifically, we propose a hyperbolic representation method to leverage the code hierarchy. moreover, we propose a graph convolutional network to utilize the code co-occurrence. experimental results on two widely used datasets demonstrate that our proposed model outperforms previous state-of-the-art methods."], "nlp applications"], [["evaluating and enhancing the robustness of neural network-based dependency parsing models with adversarial examples", "xiaoqing zheng | jiehang zeng | yi zhou | cho-jui hsieh | minhao cheng | xuanjing huang", "despite achieving prominent performance on many important tasks, it has been reported that neural networks are vulnerable to adversarial examples. previously studies along this line mainly focused on semantic tasks such as sentiment analysis, question answering and reading comprehension. in this study, we show that adversarial examples also exist in dependency parsing: we propose two approaches to study where and how parsers make mistakes by searching over perturbations to existing texts at sentence and phrase levels, and design algorithms to construct such examples in both of the black-box and white-box settings. our experiments with one of state-of-the-art parsers on the english penn treebank (ptb) show that up to 77% of input examples admit adversarial perturbations, and we also show that the robustness of parsing models can be improved by crafting high-quality adversaries and including them in the training stage, while suffering little to no performance drop on the clean input data."], "machine learning for nlp"], [["online infix probability computation for probabilistic finite automata", "marco cognetta | yo-sub han | soon chan kwon", "probabilistic finite automata (pfas) are com- mon statistical language model in natural lan- guage and speech processing. a typical task for pfas is to compute the probability of all strings that match a query pattern. an impor- tant special case of this problem is computing the probability of a string appearing as a pre- fix, suffix, or infix. these problems find use in many natural language processing tasks such word prediction and text error correction. recently, we gave the first incremental algorithm to efficiently compute the infix probabilities of each prefix of a string (cognetta et al., 2018). we develop an asymptotic improvement of that algorithm and solve the open problem of computing the infix probabilities of pfas from streaming data, which is crucial when process- ing queries online and is the ultimate goal of the incremental approach."], "tagging, chunking, syntax and parsing"], [["interpretable neural predictions with differentiable binary variables", "jasmijn bastings | wilker aziz | ivan titov", "the success of neural networks comes hand in hand with a desire for more interpretability. we focus on text classifiers and make them more interpretable by having them provide a justification\u2013a rationale\u2013for their predictions. we approach this problem by jointly training two neural network models: a latent model that selects a rationale (i.e. a short and informative part of the input text), and a classifier that learns from the words in the rationale alone. previous work proposed to assign binary latent masks to input positions and to promote short selections via sparsity-inducing penalties such as l0 regularisation. we propose a latent model that mixes discrete and continuous behaviour allowing at the same time for binary selections and gradient-based training without reinforce. in our formulation, we can tractably compute the expected value of penalties such as l0, which allows us to directly optimise the model towards a pre-specified text selection rate. we show that our approach is competitive with previous work on rationale extraction, and explore further uses in attention mechanisms."], "machine learning for nlp"], [["training neural response selection for task-oriented dialogue systems", "matthew henderson | ivan vuli\u0107 | daniela gerz | i\u00f1igo casanueva | pawe\u0142 budzianowski | sam coope | georgios spithourakis | tsung-hsien wen | nikola mrk\u0161i\u0107 | pei-hao su", "despite their popularity in the chatbot literature, retrieval-based models have had modest impact on task-oriented dialogue systems, with the main obstacle to their application being the low-data regime of most task-oriented dialogue tasks. inspired by the recent success of pretraining in language modelling, we propose an effective method for deploying response selection in task-oriented dialogue. to train response selection models for task-oriented dialogue tasks, we propose a novel method which: 1) pretrains the response selection model on large general-domain conversational corpora; and then 2) fine-tunes the pretrained model for the target dialogue domain, relying only on the small in-domain dataset to capture the nuances of the given dialogue domain. our evaluation on five diverse application domains, ranging from e-commerce to banking, demonstrates the effectiveness of the proposed training method."], "dialogue and interactive systems"], [["towards holistic and automatic evaluation of open-domain dialogue generation", "bo pang | erik nijkamp | wenjuan han | linqi zhou | yixian liu | kewei tu", "open-domain dialogue generation has gained increasing attention in natural language processing. its evaluation requires a holistic means. human ratings are deemed as the gold standard. as human evaluation is inefficient and costly, an automated substitute is highly desirable. in this paper, we propose holistic evaluation metrics that capture different aspects of open-domain dialogues. our metrics consist of (1) gpt-2 based context coherence between sentences in a dialogue, (2) gpt-2 based fluency in phrasing, (3) n-gram based diversity in responses to augmented queries, and (4) textual-entailment-inference based logical self-consistency. the empirical validity of our metrics is demonstrated by strong correlations with human judgments. we open source the code and relevant materials."], "resources and evaluation"], [["cross-lingual syntactic transfer through unsupervised adaptation of invertible projections", "junxian he | zhisong zhang | taylor berg-kirkpatrick | graham neubig", "cross-lingual transfer is an effective way to build syntactic analysis tools in low-resource languages. however, transfer is difficult when transferring to typologically distant languages, especially when neither annotated target data nor parallel corpora are available. in this paper, we focus on methods for cross-lingual transfer to distant languages and propose to learn a generative model with a structured prior that utilizes labeled source data and unlabeled target data jointly. the parameters of source model and target model are softly shared through a regularized log likelihood objective. an invertible projection is employed to learn a new interlingual latent embedding space that compensates for imperfect cross-lingual word embedding input. we evaluate our method on two syntactic tasks: part-of-speech (pos) tagging and dependency parsing. on the universal dependency treebanks, we use english as the only source corpus and transfer to a wide range of target languages. on the 10 languages in this dataset that are distant from english, our method yields an average of 5.2% absolute improvement on pos tagging and 8.3% absolute improvement on dependency parsing over a direct transfer method using state-of-the-art discriminative models."], "machine translation and multilinguality"], [["transition-based directed graph construction for emotion-cause pair extraction", "chuang fan | chaofa yuan | jiachen du | lin gui | min yang | ruifeng xu", "emotion-cause pair extraction aims to extract all potential pairs of emotions and corresponding causes from unannotated emotion text. most existing methods are pipelined framework, which identifies emotions and extracts causes separately, leading to a drawback of error propagation. towards this issue, we propose a transition-based model to transform the task into a procedure of parsing-like directed graph construction. the proposed model incrementally generates the directed graph with labeled edges based on a sequence of actions, from which we can recognize emotions with the corresponding causes simultaneously, thereby optimizing separate subtasks jointly and maximizing mutual benefits of tasks interdependently. experimental results show that our approach achieves the best performance, outperforming the state-of-the-art methods by 6.71% (p<0.01) in f1 measure."], "sentiment analysis, stylistic analysis, and argument mining"], [["to pretrain or not to pretrain: examining the benefits of pretrainng on resource rich tasks", "sinong wang | madian khabsa | hao ma", "pretraining nlp models with variants of masked language model (mlm) objectives has recently led to a significant improvements on many tasks. this paper examines the benefits of pretrained models as a function of the number of training samples used in the downstream task. on several text classification tasks, we show that as the number of training examples grow into the millions, the accuracy gap between finetuning bert-based model and training vanilla lstm from scratch narrows to within 1%. our findings indicate that mlm-based models might reach a diminishing return point as the supervised data size increases significantly."], "machine learning for nlp"], [["disentangled representation learning for non-parallel text style transfer", "vineet john | lili mou | hareesh bahuleyan | olga vechtomova", "this paper tackles the problem of disentangling the latent representations of style and content in language models. we propose a simple yet effective approach, which incorporates auxiliary multi-task and adversarial objectives, for style prediction and bag-of-words prediction, respectively. we show, both qualitatively and quantitatively, that the style and content are indeed disentangled in the latent space. this disentangled latent representation learning can be applied to style transfer on non-parallel corpora. we achieve high performance in terms of transfer accuracy, content preservation, and language fluency, in comparison to various previous approaches."], "nlp applications"], [["reasoning with multimodal sarcastic tweets via modeling cross-modality contrast and semantic association", "nan xu | zhixiong zeng | wenji mao", "sarcasm is a sophisticated linguistic phenomenon to express the opposite of what one really means. with the rapid growth of social media, multimodal sarcastic tweets are widely posted on various social platforms. in multimodal context, sarcasm is no longer a pure linguistic phenomenon, and due to the nature of social media short text, the opposite is more often manifested via cross-modality expressions. thus traditional text-based methods are insufficient to detect multimodal sarcasm. to reason with multimodal sarcastic tweets, in this paper, we propose a novel method for modeling cross-modality contrast in the associated context. our method models both cross-modality contrast and semantic association by constructing the decomposition and relation network (namely d&r net). the decomposition network represents the commonality and discrepancy between image and text, and the relation network models the semantic association in cross-modality context. experimental results on a public dataset demonstrate the effectiveness of our model in multimodal sarcasm detection."], "speech and multimodality"], [["rewarding smatch: transition-based amr parsing with reinforcement learning", "tahira naseem | abhishek shah | hui wan | radu florian | salim roukos | miguel ballesteros", "our work involves enriching the stack-lstm transition-based amr parser (ballesteros and al-onaizan, 2017) by augmenting training with policy learning and rewarding the smatch score of sampled graphs. in addition, we also combined several amr-to-text alignments with an attention mechanism and we supplemented the parser with pre-processed concept identification, named entities and contextualized embeddings. we achieve a highly competitive performance that is comparable to the best published results. we show an in-depth study ablating each of the new components of the parser."], "semantics"], [["improving entity linking through semantic reinforced entity embeddings", "feng hou | ruili wang | jun he | yi zhou", "entity embeddings, which represent different aspects of each entity with a single vector like word embeddings, are a key component of neural entity linking models. existing entity embeddings are learned from canonical wikipedia articles and local contexts surrounding target entities. such entity embeddings are effective, but too distinctive for linking models to learn contextual commonality. we propose a simple yet effective method, fgs2ee, to inject fine-grained semantic information into entity embeddings to reduce the distinctiveness and facilitate the learning of contextual commonality. fgs2ee first uses the embeddings of semantic type words to generate semantic embeddings, and then combines them with existing entity embeddings through linear aggregation. extensive experiments show the effectiveness of such embeddings. based on our entity embeddings, we achieved new sate-of-the-art performance on entity linking."], "information extraction, retrieval and text mining"], [["discovering dialogue slots with weak supervision", "vojt\u011bch hude\u010dek | ond\u0159ej du\u0161ek | zhou yu", "task-oriented dialogue systems typically require manual annotation of dialogue slots in training data, which is costly to obtain. we propose a method that eliminates this requirement: we use weak supervision from existing linguistic annotation models to identify potential slot candidates, then automatically identify domain-relevant slots by using clustering algorithms. furthermore, we use the resulting slot annotation to train a neural-network-based tagger that is able to perform slot tagging with no human intervention. this tagger is trained solely on the outputs of our method and thus does not rely on any labeled data. our model demonstrates state-of-the-art performance in slot tagging without labeled training data on four different dialogue domains. moreover, we find that slot annotations discovered by our model significantly improve the performance of an end-to-end dialogue response generation model, compared to using no slot annotation at all."], "dialogue and interactive systems"], [["template-based question generation from retrieved sentences for improved unsupervised question answering", "alexander fabbri | patrick ng | zhiguo wang | ramesh nallapati | bing xiang", "question answering (qa) is in increasing demand as the amount of information available online and the desire for quick access to this content grows. a common approach to qa has been to fine-tune a pretrained language model on a task-specific labeled dataset. this paradigm, however, relies on scarce, and costly to obtain, large-scale human-labeled data. we propose an unsupervised approach to training qa models with generated pseudo-training data. we show that generating questions for qa training by applying a simple template on a related, retrieved sentence rather than the original context sentence improves downstream qa performance by allowing the model to learn more complex context-question relationships. training a qa model on this data gives a relative improvement over a previous unsupervised model in f1 score on the squad dataset by about 14%, and 20% when the answer is a named entity, achieving state-of-the-art performance on squad for unsupervised qa."], "question answering"], [["hellaswag: can a machine really finish your sentence?", "rowan zellers | ari holtzman | yonatan bisk | ali farhadi | yejin choi", "recent work by zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as \u201ca woman sits at a piano,\u201d a machine must select the most likely followup: \u201cshe sets her fingers on the keys.\u201d with the introduction of bert, near human-level performance was reached. does this mean that machines can perform human level commonsense inference? in this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting hellaswag, a new challenge dataset. though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). we achieve this via adversarial filtering (af), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. af proves to be surprisingly robust. the key insight is to scale up the length and complexity of the dataset examples towards a critical \u2018goldilocks\u2019 zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. our construction of hellaswag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. more broadly, it suggests a new path forward for nlp research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges."], "semantics"], [["leveraging local and global patterns for self-attention networks", "mingzhou xu | derek f. wong | baosong yang | yue zhang | lidia s. chao", "self-attention networks have received increasing research attention. by default, the hidden states of each word are hierarchically calculated by attending to all words in the sentence, which assembles global information. however, several studies pointed out that taking all signals into account may lead to overlooking neighboring information (e.g. phrase pattern). to address this argument, we propose a hybrid attention mechanism to dynamically leverage both of the local and global information. specifically, our approach uses a gating scalar for integrating both sources of the information, which is also convenient for quantifying their contributions. experiments on various neural machine translation tasks demonstrate the effectiveness of the proposed method. the extensive analyses verify that the two types of contexts are complementary to each other, and our method gives highly effective improvements in their integration."], "machine translation and multilinguality"], [["finding universal grammatical relations in multilingual bert", "ethan a. chi | john hewitt | christopher d. manning", "recent work has found evidence that multilingual bert (mbert), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually. to better understand this overlap, we extend recent work on finding syntactic trees in neural networks\u2019 internal representations to the multilingual setting. we show that subspaces of mbert representations recover syntactic tree distances in languages other than english, and that these subspaces are approximately shared across languages. motivated by these results, we present an unsupervised analysis method that provides evidence mbert learns representations of syntactic dependency labels, in the form of clusters which largely agree with the universal dependencies taxonomy. this evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals."], "interpretability and analysis of models for nlp"], [["speak to your parser: interactive text-to-sql with natural language feedback", "ahmed elgohary | saghar hosseini | ahmed hassan awadallah", "we study the task of semantic parse correction with natural language feedback. given a natural language utterance, most semantic parsing systems pose the problem as one-shot translation where the utterance is mapped to a corresponding logical form. in this paper, we investigate a more interactive scenario where humans can further interact with the system by providing free-form natural language feedback to correct the system when it generates an inaccurate interpretation of an initial utterance. we focus on natural language to sql systems and construct, splash, a dataset of utterances, incorrect sql interpretations and the corresponding natural language feedback. we compare various reference models for the correction task and show that incorporating such a rich form of feedback can significantly improve the overall semantic parsing accuracy while retaining the flexibility of natural language interaction. while we estimated human correction accuracy is 81.5%, our best model achieves only 25.1%, which leaves a large gap for improvement in future research. splash is publicly available at https://aka.ms/splash_dataset."], "dialogue and interactive systems"], [["information-theoretic probing for linguistic structure", "tiago pimentel | josef valvoda | rowan hall maudslay | ran zmigrod | adina williams | ryan cotterell", "the success of neural networks on a diverse set of nlp tasks has led researchers to question how much these networks actually \u201cknow\u201d about natural language. probes are a natural way of assessing this. when probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network\u2019s learned representations. if the probe does well, the researcher may conclude that the representations encode knowledge related to the task. a commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure, but not learn the task itself. we propose an information-theoretic operationalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inherent in the representation. the experimental portion of our paper focuses on empirically estimating the mutual information between a linguistic property and bert, comparing these estimates to several baselines. we evaluate on a set of ten typologically diverse languages often underrepresented in nlp research\u2014plus english\u2014totalling eleven languages. our implementation is available in https://github.com/rycolab/info-theoretic-probing."], "interpretability and analysis of models for nlp"], [["relational word embeddings", "jose camacho-collados | luis espinosa anke | steven schockaert", "while word embeddings have been shown to implicitly encode various forms of attributional knowledge, the extent to which they capture relational information is far more limited. in previous work, this limitation has been addressed by incorporating relational knowledge from external knowledge bases when learning the word embedding. such strategies may not be optimal, however, as they are limited by the coverage of available resources and conflate similarity with other forms of relatedness. as an alternative, in this paper we propose to encode relational knowledge in a separate word embedding, which is aimed to be complementary to a given standard word embedding. this relational word embedding is still learned from co-occurrence statistics, and can thus be used even when no external knowledge base is available. our analysis shows that relational word vectors do indeed capture information that is complementary to what is encoded in standard word embeddings."], "semantics"], [["do neural dialog systems use the conversation history effectively? an empirical study", "chinnadhurai sankar | sandeep subramanian | chris pal | sarath chandar | yoshua bengio", "neural generative models have been become increasingly popular when building conversational agents. they offer flexibility, can be easily adapted to new domains, and require minimal domain engineering. a common criticism of these systems is that they seldom understand or use the available dialog history effectively. in this paper, we take an empirical approach to understanding how these models use the available dialog history by studying the sensitivity of the models to artificially introduced unnatural changes or perturbations to their context at test time. we experiment with 10 different types of perturbations on 4 multi-turn dialog datasets and find that commonly used neural dialog architectures like recurrent and transformer-based seq2seq models are rarely sensitive to most perturbations such as missing or reordering utterances, shuffling words, etc. also, by open-sourcing our code, we believe that it will serve as a useful diagnostic tool for evaluating dialog systems in the future."], "dialogue and interactive systems"], [["neural aspect and opinion term extraction with mined rules as weak supervision", "hongliang dai | yangqiu song", "lack of labeled training data is a major bottleneck for neural network based aspect and opinion term extraction on product reviews. to alleviate this problem, we first propose an algorithm to automatically mine extraction rules from existing training examples based on dependency parsing results. the mined rules are then applied to label a large amount of auxiliary data. finally, we study training procedures to train a neural model which can learn from both the data automatically labeled by the rules and a small amount of data accurately annotated by human. experimental results show that although the mined rules themselves do not perform well due to their limited flexibility, the combination of human annotated data and rule labeled auxiliary data can improve the neural model and allow it to achieve performance better than or comparable with the current state-of-the-art."], "information extraction, retrieval and text mining"], [["end-to-end neural pipeline for goal-oriented dialogue systems using gpt-2", "donghoon ham | jeong-gwan lee | youngsoo jang | kee-eung kim", "the goal-oriented dialogue system needs to be optimized for tracking the dialogue flow and carrying out an effective conversation under various situations to meet the user goal. the traditional approach to build such a dialogue system is to take a pipelined modular architecture, where its modules are optimized individually. however, such an optimization scheme does not necessarily yield the overall performance improvement of the whole system. on the other hand, end-to-end dialogue systems with monolithic neural architecture are often trained only with input-output utterances, without taking into account the entire annotations available in the corpus. this scheme makes it difficult for goal-oriented dialogues where the system needs to integrate with external systems or to provide interpretable information about why the system generated a particular response. in this paper, we present an end-to-end neural architecture for dialogue systems that addresses both challenges above. in the human evaluation, our dialogue system achieved the success rate of 68.32%, the language understanding score of 4.149, and the response appropriateness score of 4.287, which ranked the system at the top position in the end-to-end multi-domain dialogue system task in the 8th dialogue systems technology challenge (dstc8)."], "dialogue and interactive systems"], [["contextualizing hate speech classifiers with post-hoc explanation", "brendan kennedy | xisen jin | aida mostafazadeh davani | morteza dehghani | xiang ren", "hate speech classifiers trained on imbalanced datasets struggle to determine if group identifiers like \u201cgay\u201d or \u201cblack\u201d are used in offensive or prejudiced ways. such biases manifest in false positives when these identifiers are present, due to models\u2019 inability to learn the contexts which constitute a hateful usage of identifiers. we extract post-hoc explanations from fine-tuned bert classifiers to detect bias towards identity terms. then, we propose a novel regularization technique based on these explanations that encourages models to learn from the context of group identifiers in addition to the identifiers themselves. our approach improved over baselines in limiting false positives on out-of-domain data while maintaining and in cases improving in-domain performance."], "ethics in nlp"], [["a joint named-entity recognizer for heterogeneous tag-sets using a tag hierarchy", "genady beryozkin | yoel drori | oren gilon | tzvika hartman | idan szpektor", "we study a variant of domain adaptation for named-entity recognition where multiple, heterogeneously tagged training sets are available. furthermore, the test tag-set is not identical to any individual training tag-set. yet, the relations between all tags are provided in a tag hierarchy, covering the test tags as a combination of training tags. this setting occurs when various datasets are created using different annotation schemes. this is also the case of extending a tag-set with a new tag by annotating only the new tag in a new dataset. we propose to use the given tag hierarchy to jointly learn a neural network that shares its tagging layer among all tag-sets. we compare this model to combining independent models and to a model based on the multitasking approach. our experiments show the benefit of the tag-hierarchy model, especially when facing non-trivial consolidation of tag-sets."], "tagging, chunking, syntax and parsing"], [["head-driven phrase structure grammar parsing on penn treebank", "junru zhou | hai zhao", "head-driven phrase structure grammar (hpsg) enjoys a uniform formalism representing rich contextual syntactic and even semantic meanings. this paper makes the first attempt to formulate a simplified hpsg by integrating constituent and dependency formal representations into head-driven phrase structure. then two parsing algorithms are respectively proposed for two converted tree representations, division span and joint span. as hpsg encodes both constituent and dependency structure information, the proposed hpsg parsers may be regarded as a sort of joint decoder for both types of structures and thus are evaluated in terms of extracted or converted constituent and dependency parsing trees. our parser achieves new state-of-the-art performance for both parsing tasks on penn treebank (ptb) and chinese penn treebank, verifying the effectiveness of joint learning constituent and dependency structures. in details, we report 95.84 f1 of constituent parsing and 97.00% uas of dependency parsing on ptb."], "tagging, chunking, syntax and parsing"], [["don\u2019t take the premise for granted: mitigating artifacts in natural language inference", "yonatan belinkov | adam poliak | stuart shieber | benjamin van durme | alexander rush", "natural language inference (nli) datasets often contain hypothesis-only biases\u2014artifacts that allow models to achieve non-trivial performance without learning whether a premise entails a hypothesis. we propose two probabilistic methods to build models that are more robust to such biases and better transfer across datasets. in contrast to standard approaches to nli, our methods predict the probability of a premise given a hypothesis and nli label, discouraging models from ignoring the premise. we evaluate our methods on synthetic and existing nli datasets by training on datasets containing biases and testing on datasets containing no (or different) hypothesis-only biases. our results indicate that these methods can make nli models more robust to dataset-specific artifacts, transferring better than a baseline architecture in 9 out of 12 nli datasets. additionally, we provide an extensive analysis of the interplay of our methods with known biases in nli datasets, as well as the effects of encouraging models to ignore biases and fine-tuning on target datasets."], "semantics"], [["revisiting joint modeling of cross-document entity and event coreference resolution", "shany barhom | vered shwartz | alon eirew | michael bugert | nils reimers | ido dagan", "recognizing coreferring events and entities across multiple texts is crucial for many nlp applications. despite the task\u2019s importance, research focus was given mostly to within-document entity coreference, with rather little attention to the other variants. we propose a neural architecture for cross-document coreference resolution. inspired by lee et al. (2012), we jointly model entity and event coreference. we represent an event (entity) mention using its lexical span, surrounding context, and relation to entity (event) mentions via predicate-arguments structures. our model outperforms the previous state-of-the-art event coreference model on ecb+, while providing the first entity coreference results on this corpus. our analysis confirms that all our representation elements, including the mention span itself, its context, and the relation to other mentions contribute to the model\u2019s success."], "discourse and pragmatics"], [["mixtext: linguistically-informed interpolation of hidden space for semi-supervised text classification", "jiaao chen | zichao yang | diyi yang", "this paper presents mixtext, a semi-supervised learning method for text classification, which uses our newly designed data augmentation method called tmix. tmix creates a large amount of augmented training samples by interpolating text in hidden space. moreover, we leverage recent advances in data augmentation to guess low-entropy labels for unlabeled data, hence making them as easy to use as labeled data. by mixing labeled, unlabeled and augmented data, mixtext significantly outperformed current pre-trained and fined-tuned models and other state-of-the-art semi-supervised learning methods on several text classification benchmarks. the improvement is especially prominent when supervision is extremely limited. we have publicly released our code at https://github.com/gt-salt/mixtext."], "machine learning for nlp"], [["asking the crowd: question analysis, evaluation and generation for open discussion on online forums", "zi chai | xinyu xing | xiaojun wan | bo huang", "teaching machines to ask questions is an important yet challenging task. most prior work focused on generating questions with fixed answers. as contents are highly limited by given answers, these questions are often not worth discussing. in this paper, we take the first step on teaching machines to ask open-answered questions from real-world news for open discussion (openqg). to generate high-qualified questions, effective ways for question evaluation are required. we take the perspective that the more answers a question receives, the better it is for open discussion, and analyze how language use affects the number of answers. compared with other factors, e.g. topic and post time, linguistic factors keep our evaluation from being domain-specific. we carefully perform variable control on 11.5m questions from online forums to get a dataset, oqrand, and further perform question analysis. based on these conclusions, several models are built for question evaluation. for openqg task, we construct oqgend, the first dataset as far as we know, and propose a model based on conditional generative adversarial networks and our question evaluation model. experiments show that our model can generate questions with higher quality compared with commonly-used text generation methods."], "computational social science, social media and cultural analytics"], [["gated embeddings in end-to-end speech recognition for conversational-context fusion", "suyoun kim | siddharth dalmia | florian metze", "we present a novel conversational-context aware end-to-end speech recognizer based on a gated neural network that incorporates conversational-context/word/speech embeddings. unlike conventional speech recognition models, our model learns longer conversational-context information that spans across sentences and is consequently better at recognizing long conversations. specifically, we propose to use text-based external word and/or sentence embeddings (i.e., fasttext, bert) within an end-to-end framework, yielding significant improvement in word error rate with better conversational-context representation. we evaluated the models on the switchboard conversational speech corpus and show that our model outperforms standard end-to-end speech recognition models."], "nlp applications"], [["detecting concealed information in text and speech", "shengli hu", "motivated by infamous cheating scandals in the media industry, the wine industry, and political campaigns, we address the problem of detecting concealed information in technical settings. in this work, we explore acoustic-prosodic and linguistic indicators of information concealment by collecting a unique corpus of professionals practicing for oral exams while concealing information. we reveal subtle signs of concealing information in speech and text, compare and contrast them with those in deception detection literature, uncovering the link between concealing information and deception. we then present a series of experiments that automatically detect concealed information from text and speech. we compare the use of acoustic-prosodic, linguistic, and individual feature sets, using different machine learning models. finally, we present a multi-task learning framework with acoustic, linguistic, and individual features, that outperforms human performance by over 15%."], "nlp applications"], [["fast and accurate non-projective dependency tree linearization", "xiang yu | simon tannert | ngoc thang vu | jonas kuhn", "we propose a graph-based method to tackle the dependency tree linearization task. we formulate the task as a traveling salesman problem (tsp), and use a biaffine attention model to calculate the edge costs. we facilitate the decoding by solving the tsp for each subtree and combining the solution into a projective tree. we then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences. our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding."], "generation"], [["learning to execute instructions in a minecraft dialogue", "prashant jayannavar | anjali narayan-chen | julia hockenmaier", "the minecraft collaborative building task is a two-player game in which an architect (a) instructs a builder (b) to construct a target structure in a simulated blocks world environment. we define the subtask of predicting correct action sequences (block placements and removals) in a given game context, and show that capturing b\u2019s past actions as well as b\u2019s perspective leads to a significant improvement in performance on this challenging language understanding problem."], "language grounding to vision, robotics and beyond"], [["exploiting syntactic structure for better language modeling: a syntactic distance approach", "wenyu du | zhouhan lin | yikang shen | timothy j. o\u2019donnell | yoshua bengio | yue zhang", "it is commonly believed that knowledge of syntactic structure should improve language modeling. however, effectively and computationally efficiently incorporating syntactic structure into neural language models has been a challenging topic. in this paper, we make use of a multi-task objective, i.e., the models simultaneously predict words as well as ground truth parse trees in a form called \u201csyntactic distances\u201d, where information between these two separate objectives shares the same intermediate representation. experimental results on the penn treebank and chinese treebank datasets show that when ground truth parse trees are provided as additional training signals, the model is able to achieve lower perplexity and induce trees with better quality."], "machine learning for nlp"], [["explicit semantic decomposition for definition generation", "jiahuan li | yu bao | shujian huang | xinyu dai | jiajun chen", "definition generation, which aims to automatically generate dictionary definitions for words, has recently been proposed to assist the construction of dictionaries and help people understand unfamiliar texts. however, previous works hardly consider explicitly modeling the \u201ccomponents\u201d of definitions, leading to under-specific generation results. in this paper, we propose esd, namely explicit semantic decomposition for definition generation, which explicitly decomposes the meaning of words into semantic components, and models them with discrete latent variables for definition generation. experimental results show that achieves top results on wordnet and oxford benchmarks, outperforming strong previous baselines."], "generation"], [["learning a matching model with co-teaching for multi-turn response selection in retrieval-based dialogue systems", "jiazhan feng | chongyang tao | wei wu | yansong feng | dongyan zhao | rui yan", "we study learning of a matching model for response selection in retrieval-based dialogue systems. the problem is equally important with designing the architecture of a model, but is less explored in existing literature. to learn a robust matching model from noisy training data, we propose a general co-teaching framework with three specific teaching strategies that cover both teaching with loss functions and teaching with data curriculum. under the framework, we simultaneously learn two matching models with independent training sets. in each iteration, one model transfers the knowledge learned from its training set to the other model, and at the same time receives the guide from the other model on how to overcome noise in training. through being both a teacher and a student, the two models learn from each other and get improved together. evaluation results on two public data sets indicate that the proposed learning approach can generally and significantly improve the performance of existing matching models."], "dialogue and interactive systems"], [["word-level textual adversarial attacking as combinatorial optimization", "yuan zang | fanchao qi | chenghao yang | zhiyuan liu | meng zhang | qun liu | maosong sun", "adversarial attacks are carried out to reveal the vulnerability of deep neural networks. textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods. however, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed. in this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately. we conduct exhaustive experiments to evaluate our attack model by attacking bilstm and bert on three benchmark datasets. experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods. also, further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training. all the code and data of this paper can be obtained on https://github.com/thunlp/sememepso-attack."], "semantics"], [["neural decipherment via minimum-cost flow: from ugaritic to linear b", "jiaming luo | yuan cao | regina barzilay", "in this paper we propose a novel neural approach for automatic decipherment of lost languages. to compensate for the lack of strong supervision signal, our model design is informed by patterns in language change documented in historical linguistics. the model utilizes an expressive sequence-to-sequence model to capture character-level correspondences between cognates. to effectively train the model in unsupervised manner, we innovate the training procedure by formalizing it as a minimum-cost flow problem. when applied to decipherment of ugaritic, we achieve 5% absolute improvement over state-of-the-art results. we also report first automatic results in deciphering linear b, a syllabic language related to ancient greek, where our model correctly translates 67.3% of cognates."], "machine translation and multilinguality"], [["a semi-markov structured support vector machine model for high-precision named entity recognition", "ravneet arora | chen-tse tsai | ketevan tsereteli | prabhanjan kambadur | yi yang", "named entity recognition (ner) is the backbone of many nlp solutions. f1 score, the harmonic mean of precision and recall, is often used to select/evaluate the best models. however, when precision needs to be prioritized over recall, a state-of-the-art model might not be the best choice. there is little in literature that directly addresses training-time modifications to achieve higher precision information extraction. in this paper, we propose a neural semi-markov structured support vector machine model that controls the precision-recall trade-off by assigning weights to different types of errors in the loss-augmented inference during training. the semi-markov property provides more accurate phrase-level predictions, thereby improving performance. we empirically demonstrate the advantage of our model when high precision is required by comparing against strong baselines based on crf. in our experiments with the conll 2003 dataset, our model achieves a better precision-recall trade-off at various precision levels."], "information extraction, retrieval and text mining"], [["identification of tasks, datasets, evaluation metrics, and numeric scores for scientific leaderboards construction", "yufang hou | charles jochim | martin gleize | francesca bonin | debasis ganguly", "while the fast-paced inception of novel tasks and new datasets helps foster active research in a community towards interesting directions, keeping track of the abundance of research activity in different areas on different datasets is likely to become increasingly difficult. the community could greatly benefit from an automatic system able to summarize scientific results, e.g., in the form of a leaderboard. in this paper we build two datasets and develop a framework (tdms-ie) aimed at automatically extracting task, dataset, metric and score from nlp papers, towards the automatic construction of leaderboards. experiments show that our model outperforms several baselines by a large margin. our model is a first step towards automatic leaderboard construction, e.g., in the nlp domain."], "information extraction, retrieval and text mining"], [["entity-centric contextual affective analysis", "anjalie field | yulia tsvetkov", "while contextualized word representations have improved state-of-the-art benchmarks in many nlp tasks, their potential usefulness for social-oriented tasks remains largely unexplored. we show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people. we evaluate our methodology quantitatively, on held-out affect lexicons, and qualitatively, through case examples. we find that contextualized word representations do encode meaningful affect information, but they are heavily biased towards their training data, which limits their usefulness to in-domain analyses. we ultimately use our method to examine differences in portrayals of men and women."], "computational social science, social media and cultural analytics"], [["geometry-aware domain adaptation for unsupervised alignment of word embeddings", "pratik jawanpuria | mayank meghwanshi | bamdev mishra", "we propose a novel manifold based geometric approach for learning unsupervised alignment of word embeddings between the source and the target languages. our approach formulates the alignment learning problem as a domain adaptation problem over the manifold of doubly stochastic matrices. this viewpoint arises from the aim to align the second order information of the two language spaces. the rich geometry of the doubly stochastic manifold allows to employ efficient riemannian conjugate gradient algorithm for the proposed formulation. empirically, the proposed approach outperforms state-of-the-art optimal transport based approach on the bilingual lexicon induction task across several language pairs. the performance improvement is more significant for distant language pairs."], "machine translation and multilinguality"], [["inter-sentence relation extraction with document-level graph convolutional neural network", "sunil kumar sahu | fenia christopoulou | makoto miwa | sophia ananiadou", "inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. existing methods do not fully exploit such dependencies. we present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. the graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. in order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. our analysis shows that all the types in the graph are effective for inter-sentence relation extraction."], "nlp applications"], [["gated convolutional bidirectional attention-based model for off-topic spoken response detection", "yefei zha | ruobing li | hui lin", "off-topic spoken response detection, the task aiming at predicting whether a response is off-topic for the corresponding prompt, is important for an automated speaking assessment system. in many real-world educational applications, off-topic spoken response detectors are required to achieve high recall for off-topic responses not only on seen prompts but also on prompts that are unseen during training. in this paper, we propose a novel approach for off-topic spoken response detection with high off-topic recall on both seen and unseen prompts. we introduce a new model, gated convolutional bidirectional attention-based model (gcbia), which applies bi-attention mechanism and convolutions to extract topic words of prompts and key-phrases of responses, and introduces gated unit and residual connections between major layers to better represent the relevance of responses and prompts. moreover, a new negative sampling method is proposed to augment training data. experiment results demonstrate that our novel approach can achieve significant improvements in detecting off-topic responses with extremely high on-topic recall, for both seen and unseen prompts."], "dialogue and interactive systems"], [["predicting humorousness and metaphor novelty with gaussian process preference learning", "edwin simpson | erik-l\u00e2n do dinh | tristan miller | iryna gurevych", "the inability to quantify key aspects of creative language is a frequent obstacle to natural language understanding. to address this, we introduce novel tasks for evaluating the creativeness of language\u2014namely, scoring and ranking text by humorousness and metaphor novelty. to sidestep the difficulty of assigning discrete labels or numeric scores, we learn from pairwise comparisons between texts. we introduce a bayesian approach for predicting humorousness and metaphor novelty using gaussian process preference learning (gppl), which achieves a spearman\u2019s \u03c1 of 0.56 against gold using word embeddings and linguistic features. our experiments show that given sparse, crowdsourced annotation data, ranking using gppl outperforms best\u2013worst scaling. we release a new dataset for evaluating humour containing 28,210 pairwise comparisons of 4,030 texts, and make our software freely available."], "semantics"], [["automated chess commentator powered by neural chess engine", "hongyu zang | zhiwei yu | xiaojun wan", "in this paper, we explore a new approach for automated chess commentary generation, which aims to generate chess commentary texts in different categories (e.g., description, comparison, planning, etc.). we introduce a neural chess engine into text generation models to help with encoding boards, predicting moves, and analyzing situations. by jointly training the neural chess engine and the generation models for different categories, the models become more effective. we conduct experiments on 5 categories in a benchmark chess commentary dataset and achieve inspiring results in both automatic and human evaluations."], "generation"], [["unsupervised morphological paradigm completion", "huiming jin | liwei cai | yihui peng | chen xia | arya mccarthy | katharina kann", "we propose the task of unsupervised morphological paradigm completion. given only raw text and a lemma list, the task consists of generating the morphological paradigms, i.e., all inflected forms, of the lemmas. from a natural language processing (nlp) perspective, this is a challenging unsupervised task, and high-performing systems have the potential to improve tools for low-resource languages or to assist linguistic annotators. from a cognitive science perspective, this can shed light on how children acquire morphological knowledge. we further introduce a system for the task, which generates morphological paradigms via the following steps: (i) edit tree retrieval, (ii) additional lemma retrieval, (iii) paradigm size discovery, and (iv) inflection generation. we perform an evaluation on 14 typologically diverse languages. our system outperforms trivial baselines with ease and, for some languages, even obtains a higher accuracy than minimally supervised systems."], "phonology, morphology and word segmentation"], [["codraw: collaborative drawing as a testbed for grounded goal-driven communication", "jin-hwa kim | nikita kitaev | xinlei chen | marcus rohrbach | byoung-tak zhang | yuandong tian | dhruv batra | devi parikh", "in this work, we propose a goal-driven collaborative task that combines language, perception, and action. specifically, we develop a collaborative image-drawing game between two agents, called codraw. our game is grounded in a virtual world that contains movable clip art objects. the game involves two players: a teller and a drawer. the teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the drawer tries to reconstruct the scene on an empty canvas using available clip art pieces. the two players communicate with each other using natural language. we collect the codraw dataset of ~10k dialogs consisting of ~138k messages exchanged between human players. we define protocols and metrics to evaluate learned agents in this testbed, highlighting the need for a novel \u201ccrosstalk\u201d evaluation condition which pairs agents trained independently on disjoint subsets of the training data. we present models for our task and benchmark them using both fully automated evaluation and by having them play the game live with humans."], "language grounding to vision, robotics and beyond"], [["unsupervised neural machine translation for low-resource domains via meta-learning", "cheonbok park | yunwon tae | taehee kim | soyoung yang | mohammad azam khan | lucy park | jaegul choo", "unsupervised machine translation, which utilizes unpaired monolingual corpora as training data, has achieved comparable performance against supervised machine translation. however, it still suffers from data-scarce domains. to address this issue, this paper presents a novel meta-learning algorithm for unsupervised neural machine translation (unmt) that trains the model to adapt to another domain by utilizing only a small amount of training data. we assume that domain-general knowledge is a significant factor in handling data-scarce domains. hence, we extend the meta-learning algorithm, which utilizes knowledge learned from high-resource domains, to boost the performance of low-resource unmt. our model surpasses a transfer learning-based approach by up to 2-3 bleu scores. extensive experimental results show that our proposed algorithm is pertinent for fast adaptation and consistently outperforms other baselines."], "machine translation and multilinguality"], [["xtremedistil: multi-stage distillation for massive multilingual models", "subhabrata mukherjee | ahmed hassan awadallah", "deep and large pre-trained language models are the state-of-the-art for various natural language processing tasks. however, the huge size of these models could be a deterrent to using them in practice. some recent works use knowledge distillation to compress these huge models into shallow ones. in this work we study knowledge distillation with a focus on multilingual named entity recognition (ner). in particular, we study several distillation strategies and propose a stage-wise optimization scheme leveraging teacher internal representations, that is agnostic of teacher architecture, and show that it outperforms strategies employed in prior works. additionally, we investigate the role of several factors like the amount of unlabeled data, annotation resources, model architecture and inference latency to name a few. we show that our approach leads to massive compression of teacher models like mbert by upto 35x in terms of parameters and 51x in terms of latency for batch inference while retaining 95% of its f1-score for ner over 41 languages."], "machine learning for nlp"], [["don\u2019t stop pretraining: adapt language models to domains and tasks", "suchin gururangan | ana marasovi\u0107 | swabha swayamdipta | kyle lo | iz beltagy | doug downey | noah a. smith", "language models pretrained on text from a wide variety of sources form the foundation of today\u2019s nlp. in light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. we present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. moreover, adapting to the task\u2019s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance."], "semantics"], [["a simple recipe towards reducing hallucination in neural surface realisation", "feng nie | jin-ge yao | jinpeng wang | rong pan | chin-yew lin", "recent neural language generation systems often hallucinate contents (i.e., producing irrelevant or contradicted facts), especially when trained on loosely corresponding pairs of the input structure and text. to mitigate this issue, we propose to integrate a language understanding module for data refinement with self-training iterations to effectively induce strong equivalence between the input data and the paired text. experiments on the e2e challenge dataset show that our proposed framework can reduce more than 50% relative unaligned noise from the original data-text pairs. a vanilla sequence-to-sequence neural nlg model trained on the refined data has improved on content correctness compared with the current state-of-the-art ensemble generator."], "generation"], [["encoder-decoder models can benefit from pre-trained masked language models in grammatical error correction", "masahiro kaneko | masato mita | shun kiyono | jun suzuki | kentaro inui", "this paper investigates how to effectively incorporate a pre-trained masked language model (mlm), such as bert, into an encoder-decoder (encdec) model for grammatical error correction (gec). the answer to this question is not as straightforward as one might expect because the previous common methods for incorporating a mlm into an encdec model have potential drawbacks when applied to gec. for example, the distribution of the inputs to a gec model can be considerably different (erroneous, clumsy, etc.) from that of the corpora used for pre-training mlms; however, this issue is not addressed in the previous methods. our experiments show that our proposed method, where we first fine-tune a mlm with a given gec corpus and then use the output of the fine-tuned mlm as additional features in the gec model, maximizes the benefit of the mlm. the best-performing model achieves state-of-the-art performances on the bea-2019 and conll-2014 benchmarks. our code is publicly available at: https://github.com/kanekomasahiro/bert-gec."], "nlp applications"], [["neural relation extraction for knowledge base enrichment", "bayu distiawan trisedya | gerhard weikum | jianzhong qi | rui zhang", "we study relation extraction for knowledge base (kb) enrichment. specifically, we aim to extract entities and their relationships from sentences in the form of triples and map the elements of the extracted triples to an existing kb in an end-to-end manner. previous studies focus on the extraction itself and rely on named entity disambiguation (ned) to map triples into the kb space. this way, ned errors may cause extraction errors that affect the overall precision and recall.to address this problem, we propose an end-to-end relation extraction model for kb enrichment based on a neural encoder-decoder model. we collect high-quality training data by distant supervision with co-reference resolution and paraphrase detection. we propose an n-gram based attention model that captures multi-word entity names in a sentence. our model employs jointly learned word and entity embeddings to support named entity disambiguation. finally, our model uses a modified beam search and a triple classifier to help generate high-quality triples. our model outperforms state-of-the-art baselines by 15.51% and 8.38% in terms of f1 score on two real-world datasets."], "information extraction, retrieval and text mining"], [["topological sort for sentence ordering", "shrimai prabhumoye | ruslan salakhutdinov | alan w black", "sentence ordering is the task of arranging the sentences of a given text in the correct order. recent work using deep neural networks for this task has framed it as a sequence prediction problem. in this paper, we propose a new framing of this task as a constraint solving problem and introduce a new technique to solve it. additionally, we propose a human evaluation for this task. the results on both automatic and human metrics across four different datasets show that this new technique is better at capturing coherence in documents."], "machine learning for nlp"], [["constrained decoding for neural nlg from compositional representations in task-oriented dialogue", "anusha balakrishnan | jinfeng rao | kartikeya upasani | michael white | rajen subba", "generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems. avenues like the e2e nlg challenge have encouraged the development of neural approaches, particularly sequence-to-sequence (seq2seq) models for this problem. the semantic representations used, however, are often underspecified, which places a higher burden on the generation model for sentence planning, and also limits the extent to which generated responses can be controlled in a live system. in this paper, we (1) propose using tree-structured semantic representations, like those used in traditional rule-based nlg systems, for better discourse-level structuring and sentence-level planning; (2) introduce a challenging dataset using this representation for the weather domain; (3) introduce a constrained decoding approach for seq2seq models that leverages this representation to improve semantic correctness; and (4) demonstrate promising results on our dataset and the e2e dataset."], "dialogue and interactive systems"], [["towards conversational recommendation over multi-type dialogs", "zeming liu | haifeng wang | zheng-yu niu | hua wu | wanxiang che | ting liu", "we focus on the study of conversational recommendation in the context of multi-type dialogs, where the bots can proactively and naturally lead a conversation from a non-recommendation dialog (e.g., qa) to a recommendation dialog, taking into account user\u2019s interests and feedback. to facilitate the study of this task, we create a human-to-human chinese dialog dataset durecdial (about 10k dialogs, 156k utterances), where there are multiple sequential dialogs for a pair of a recommendation seeker (user) and a recommender (bot). in each dialog, the recommender proactively leads a multi-type dialog to approach recommendation targets and then makes multiple recommendations with rich interaction behavior. this dataset allows us to systematically investigate different parts of the overall problem, e.g., how to naturally lead a dialog, how to interact with users for recommendation. finally we establish baseline results on durecdial for future studies."], "dialogue and interactive systems"], [["selecting backtranslated data from multiple sources for improved neural machine translation", "xabier soto | dimitar shterionov | alberto poncelas | andy way", "machine translation (mt) has benefited from using synthetic training data originating from translating monolingual corpora, a technique known as backtranslation. combining backtranslated data from different sources has led to better results than when using such data in isolation. in this work we analyse the impact that data translated with rule-based, phrase-based statistical and neural mt systems has on new mt systems. we use a real-world low-resource use-case (basque-to-spanish in the clinical domain) as well as a high-resource language pair (german-to-english) to test different scenarios with backtranslation and employ data selection to optimise the synthetic corpora. we exploit different data selection strategies in order to reduce the amount of data used, while at the same time maintaining high-quality mt systems. we further tune the data selection method by taking into account the quality of the mt systems used for backtranslation and lexical diversity of the resulting corpora. our experiments show that incorporating backtranslated data from different sources can be beneficial, and that availing of data selection can yield improved performance."], "machine translation and multilinguality"], [["the language of legal and illegal activity on the darknet", "leshem choshen | dan eldad | daniel hershcovich | elior sulem | omri abend", "the non-indexed parts of the internet (the darknet) have become a haven for both legal and illegal anonymous activity. given the magnitude of these networks, scalably monitoring their activity necessarily relies on automated tools, and notably on nlp tools. however, little is known about what characteristics texts communicated through the darknet have, and how well do off-the-shelf nlp tools do on this domain. this paper tackles this gap and performs an in-depth investigation of the characteristics of legal and illegal text in the darknet, comparing it to a clear net website with similar content as a control condition. taking drugs-related websites as a test case, we find that texts for selling legal and illegal drugs have several linguistic characteristics that distinguish them from one another, as well as from the control condition, among them the distribution of pos tags, and the coverage of their named entities in wikipedia."], "nlp applications"], [["semantic parsing with dual learning", "ruisheng cao | su zhu | chen liu | jieyu li | kai yu", "semantic parsing converts natural language queries into structured logical forms. the lack of training data is still one of the most serious problems in this area. in this work, we develop a semantic parsing framework with the dual learning algorithm, which enables a semantic parser to make full use of data (labeled and even unlabeled) through a dual-learning game. this game between a primal model (semantic parsing) and a dual model (logical form to query) forces them to regularize each other, and can achieve feedback signals from some prior-knowledge. by utilizing the prior-knowledge of logical form structures, we propose a novel reward signal at the surface and semantic levels which tends to generate complete and reasonable logical forms. experimental results show that our approach achieves new state-of-the-art performance on atis dataset and gets competitive performance on overnight dataset."], "semantics"], [["exploring contextual word-level style relevance for unsupervised style transfer", "chulun zhou | liangyu chen | jiachen liu | xinyan xiao | jinsong su | sheng guo | hua wu", "unsupervised style transfer aims to change the style of an input sentence while preserving its original content without using parallel training data. in current dominant approaches, owing to the lack of fine-grained control on the influence from the target style, they are unable to yield desirable output sentences. in this paper, we propose a novel attentional sequence-to-sequence (seq2seq) model that dynamically exploits the relevance of each output word to the target style for unsupervised style transfer. specifically, we first pretrain a style classifier, where the relevance of each input word to the original style can be quantified via layer-wise relevance propagation. in a denoising auto-encoding manner, we train an attentional seq2seq model to reconstruct input sentences and repredict word-level previously-quantified style relevance simultaneously. in this way, this model is endowed with the ability to automatically predict the style relevance of each output word. then, we equip the decoder of this model with a neural style component to exploit the predicted wordlevel style relevance for better style transfer. particularly, we fine-tune this model using a carefully-designed objective function involving style transfer, style relevance consistency, content preservation and fluency modeling loss terms. experimental results show that our proposed model achieves state-of-the-art performance in terms of both transfer accuracy and content preservation."], "generation"], [["plome: pre-training with misspelled knowledge for chinese spelling correction", "shulin liu | tao yang | tianchi yue | feng zhang | di wang", "chinese spelling correction (csc) is a task to detect and correct spelling errors in texts. csc is essentially a linguistic problem, thus the ability of language understanding is crucial to this task. in this paper, we propose a pre-trained masked language model with misspelled knowledge (plome) for csc, which jointly learns how to understand language and correct spelling errors. to this end, plome masks the chosen tokens with similar characters according to a confusion set rather than the fixed token \u201c[mask]\u201d as in bert. besides character prediction, plome also introduces pronunciation prediction to learn the misspelled knowledge on phonic level. moreover, phonological and visual similarity knowledge is important to this task. plome utilizes gru networks to model such knowledge based on characters\u2019 phonics and strokes. experiments are conducted on widely used benchmarks. our method achieves superior performance against state-of-the-art approaches by a remarkable margin. we release the source code and pre-trained model for further use by the community (https://github.com/liushulinle/plome)."], "nlp applications"], [["exploration and exploitation: two ways to improve chinese spelling correction models", "chong li | cenyuan zhang | xiaoqing zheng | xuanjing huang", "a sequence-to-sequence learning with neural networks has empirically proven to be an effective framework for chinese spelling correction (csc), which takes a sentence with some spelling errors as input and outputs the corrected one. however, csc models may fail to correct spelling errors covered by the confusion sets, and also will encounter unseen ones. we propose a method, which continually identifies the weak spots of a model to generate more valuable training instances, and apply a task-specific pre-training strategy to enhance the model. the generated adversarial examples are gradually added to the training set. experimental results show that such an adversarial training method combined with the pre-training strategy can improve both the generalization and robustness of multiple csc models across three different datasets, achieving state-of-the-art performance for csc task."], "nlp applications"], [["it takes two to lie: one to lie, and one to listen", "denis peskov | benny cheng | ahmed elgohary | joe barrow | cristian danescu-niculescu-mizil | jordan boyd-graber", "trust is implicit in many online text conversations\u2014striking up new friendships, or asking for tech support. but trust can be betrayed through deception. we study the language and dynamics of deception in the negotiation-based game diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. our study with players from the diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. a model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players."], "computational social science, social media and cultural analytics"], [["designing precise and robust dialogue response evaluators", "tianyu zhao | divesh lala | tatsuya kawahara", "automatic dialogue response evaluator has been proposed as an alternative to automated metrics and human evaluation. however, existing automatic evaluators achieve only moderate correlation with human judgement and they are not robust. in this work, we propose to build a reference-free evaluator and exploit the power of semi-supervised training and pretrained (masked) language models. experimental results demonstrate that the proposed evaluator achieves a strong correlation (> 0.6) with human judgement and generalizes robustly to diverse responses and corpora. we open-source the code and data in https://github.com/zhaoting/dialog-processing."], "dialogue and interactive systems"], [["multi-news: a large-scale multi-document summarization dataset and abstractive hierarchical model", "alexander fabbri | irene li | tianwei she | suyi li | dragomir radev", "automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. single document summarization (sds) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. however, multi-document summarization (mds) of news articles has been limited to datasets of a couple of hundred examples. in this paper, we introduce multi-news, the first large-scale mds news dataset. additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard sds model and achieves competitive results on mds datasets. we benchmark several methods on multi-news and hope that this work will promote advances in summarization in the multi-document setting."], "summarization"], [["dialogue natural language inference", "sean welleck | jason weston | arthur szlam | kyunghyun cho", "consistency is a long standing issue faced by dialogue models. in this paper, we frame the consistency of dialogue agents as natural language inference (nli) and create a new natural language inference dataset called dialogue nli. we propose a method which demonstrates that a model trained on dialogue nli can be used to improve the consistency of a dialogue model, and evaluate the method with human evaluation and with automatic metrics on a suite of evaluation sets designed to measure a dialogue model\u2019s consistency."], "dialogue and interactive systems"], [["generating logical forms from graph representations of text and entities", "peter shaw | philip massey | angelica chen | francesco piccinno | yasemin altun", "structured information about entities is critical for many semantic parsing tasks. we present an approach that uses a graph neural network (gnn) architecture to incorporate information about relevant entities and their relations during parsing. combined with a decoder copy mechanism, this approach provides a conceptually simple mechanism to generate logical forms with entities. we demonstrate that this approach is competitive with the state-of-the-art across several tasks without pre-training, and outperforms existing approaches when combined with bert pre-training."], "semantics"], [["multi-relational script learning for discourse relations", "i-ta lee | dan goldwasser", "modeling script knowledge can be useful for a wide range of nlp tasks. current statistical script learning approaches embed the events, such that their relationships are indicated by their similarity in the embedding. while intuitive, these approaches fall short of representing nuanced relations, needed for downstream tasks. in this paper, we suggest to view learning event embedding as a multi-relational problem, which allows us to capture different aspects of event pairs. we model a rich set of event relations, such as cause and contrast, derived from the penn discourse tree bank. we evaluate our model on three types of tasks, the popular mutli-choice narrative cloze and its variants, several multi-relational prediction tasks, and a related downstream task\u2014implicit discourse sense classification."], "discourse and pragmatics"], [["bridging anaphora resolution as question answering", "yufang hou", "most previous studies on bridging anaphora resolution (poesio et al., 2004; hou et al., 2013b; hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given. in this paper, we cast bridging anaphora resolution as question answering based on context. this allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). we present a question answering framework (barqa) for this task, which leverages the power of transfer learning. furthermore, we propose a novel method to generate a large amount of \u201cquasi-bridging\u201d training data. we show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (isnotes (markert et al., 2012) and bashi (ro \u0308siger, 2018))."], "discourse and pragmatics"], [["learning from dialogue after deployment: feed yourself, chatbot!", "braden hancock | antoine bordes | pierre-emmanuel mazare | jason weston", "the majority of conversations a dialogue agent sees over its lifetime occur after it has already been trained and deployed, leaving a vast store of potential training signal untapped. in this work, we propose the self-feeding chatbot, a dialogue agent with the ability to extract new training examples from the conversations it participates in. as our agent engages in conversation, it also estimates user satisfaction in its responses. when the conversation appears to be going well, the user\u2019s responses become new training examples to imitate. when the agent believes it has made a mistake, it asks for feedback; learning to predict the feedback that will be given improves the chatbot\u2019s dialogue abilities further. on the personachat chit-chat dataset with over 131k training examples, we find that learning from dialogue with a self-feeding chatbot significantly improves performance, regardless of the amount of traditional supervision."], "dialogue and interactive systems"], [["spellgcn: incorporating phonological and visual similarities into language models for chinese spelling check", "xingyi cheng | weidi xu | kunlong chen | shaohua jiang | feng wang | taifeng wang | wei chu | yuan qi", "chinese spelling check (csc) is a task to detect and correct spelling errors in chinese natural language. existing methods have made attempts to incorporate the similarity knowledge between chinese characters. however, they take the similarity knowledge as either an external input resource or just heuristic rules. this paper proposes to incorporate phonological and visual similarity knowledge into language models for csc via a specialized graph convolutional network (spellgcn). the model builds a graph over the characters, and spellgcn is learned to map this graph into a set of inter-dependent character classifiers. these classifiers are applied to the representations extracted by another network, such as bert, enabling the whole network to be end-to-end trainable. experiments are conducted on three human-annotated datasets. our method achieves superior performance against previous models by a large margin."], "nlp applications"], [["towards propagation uncertainty: edge-enhanced bayesian graph convolutional networks for rumor detection", "lingwei wei | dou hu | wei zhou | zhaojuan yue | songlin hu", "detecting rumors on social media is a very critical task with significant implications to the economy, public health, etc. previous works generally capture effective features from texts and the propagation structure. however, the uncertainty caused by unreliable relations in the propagation structure is common and inevitable due to wily rumor producers and the limited collection of spread data. most approaches neglect it and may seriously limit the learning of features. towards this issue, this paper makes the first attempt to explore propagation uncertainty for rumor detection. specifically, we propose a novel edge-enhanced bayesian graph convolutional network (ebgcn) to capture robust structural features. the model adaptively rethinks the reliability of latent relations by adopting a bayesian approach. besides, we design a new edge-wise consistency training framework to optimize the model by enforcing consistency on relations. experiments on three public benchmark datasets demonstrate that the proposed model achieves better performance than baseline methods on both rumor detection and early rumor detection tasks."], "information extraction, retrieval and text mining"], [["discrete optimization for unsupervised sentence summarization with word-level extraction", "raphael schumann | lili mou | yao lu | olga vechtomova | katja markert", "automatic sentence summarization produces a shorter version of a sentence, while preserving its most important information. a good summary is characterized by language fluency and high information overlap with the source sentence. we model these two aspects in an unsupervised objective function, consisting of language modeling and semantic similarity metrics. we search for a high-scoring summary by discrete optimization. our proposed method achieves a new state-of-the art for unsupervised sentence summarization according to rouge scores. additionally, we demonstrate that the commonly reported rouge f1 metric is sensitive to summary length. since this is unwillingly exploited in recent work, we emphasize that future evaluation should explicitly group summarization systems by output length brackets."], "summarization"], [["bilingual lexicon induction via unsupervised bitext construction and word alignment", "haoyue shi | luke zettlemoyer | sida i. wang", "bilingual lexicons map words in one language to their translations in another, and are typically induced by learning linear projections to align monolingual word embedding spaces. in this paper, we show it is possible to produce much higher quality lexicons with methods that combine (1) unsupervised bitext mining and (2) unsupervised word alignment. directly applying a pipeline that uses recent algorithms for both subproblems significantly improves induced lexicon quality and further gains are possible by learning to filter the resulting lexical entries, with both unsupervised and semi-supervised schemes. our final model outperforms the state of the art on the bucc 2020 shared task by 14 f1 points averaged over 12 language pairs, while also providing a more interpretable approach that allows for rich reasoning of word meaning in context. further analysis of our output and the standard reference lexicons suggests they are of comparable quality, and new benchmarks may be needed to measure further progress on this task."], "machine translation and multilinguality"], [["leveraging meta information in short text aggregation", "he zhao | lan du | guanfeng liu | wray buntine", "short texts such as tweets often contain insufficient word co-occurrence information for training conventional topic models. to deal with the insufficiency, we propose a generative model that aggregates short texts into clusters by leveraging the associated meta information. our model can generate more interpretable topics as well as document clusters. we develop an effective gibbs sampling algorithm favoured by the fully local conjugacy in the model. extensive experiments demonstrate that our model achieves better performance in terms of document clustering and topic coherence."], "machine learning for nlp"], [["lightweight and efficient neural natural language processing with quaternion networks", "yi tay | aston zhang | anh tuan luu | jinfeng rao | shuai zhang | shuohang wang | jie fu | siu cheung hui", "many state-of-the-art neural models for nlp are heavily parameterized and thus memory inefficient. this paper proposes a series of lightweight and memory efficient neural architectures for a potpourri of natural language processing (nlp) tasks. to this end, our models exploit computation using quaternion algebra and hypercomplex spaces, enabling not only expressive inter-component interactions but also significantly (75%) reduced parameter size due to lesser degrees of freedom in the hamilton product. we propose quaternion variants of models, giving rise to new architectures such as the quaternion attention model and quaternion transformer. extensive experiments on a battery of nlp tasks demonstrates the utility of proposed quaternion-inspired models, enabling up to 75% reduction in parameter size without significant loss in performance."], "machine learning for nlp"], [["revisiting unsupervised relation extraction", "thy thy tran | phong le | sophia ananiadou", "unsupervised relation extraction (ure) extracts relations between named entities from raw text without manually-labelled data and existing knowledge bases (kbs). ure methods can be categorised into generative and discriminative approaches, which rely either on hand-crafted features or surface form. however, we demonstrate that by using only named entities to induce relation types, we can outperform existing methods on two popular datasets. we conduct a comparison and evaluation of our findings with other ure techniques, to ascertain the important features in ure. we conclude that entity types provide a strong inductive bias for ure."], "information extraction, retrieval and text mining"], [["transformer-xl: attentive language models beyond a fixed-length context", "zihang dai | zhilin yang | yiming yang | jaime carbonell | quoc le | ruslan salakhutdinov", "transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. we propose a novel neural architecture transformer-xl that enables learning dependency beyond a fixed length without disrupting temporal coherence. it consists of a segment-level recurrence mechanism and a novel positional encoding scheme. our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. as a result, transformer-xl learns dependency that is 80% longer than rnns and 450% longer than vanilla transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla transformers during evaluation. notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on wikitext-103, 21.8 on one billion word, and 54.5 on penn treebank (without finetuning). when trained only on wikitext-103, transformer-xl manages to generate reasonably coherent, novel text articles with thousands of tokens. our code, pretrained models, and hyperparameters are available in both tensorflow and pytorch."], "machine learning for nlp"], [["hard-coded gaussian attention for neural machine translation", "weiqiu you | simeng sun | mohit iyyer", "recent work has questioned the importance of the transformer\u2019s multi-headed attention for achieving high translation quality. we push further in this direction by developing a \u201chard-coded\u201d attention variant without any learned parameters. surprisingly, replacing all learned self-attention heads in the encoder and decoder with fixed, input-agnostic gaussian distributions minimally impacts bleu scores across four different language pairs. however, additionally, hard-coding cross attention (which connects the decoder to the encoder) significantly lowers bleu, suggesting that it is more important than self-attention. much of this bleu drop can be recovered by adding just a single learned cross attention head to an otherwise hard-coded transformer. taken as a whole, our results offer insight into which components of the transformer are actually important, which we hope will guide future work into the development of simpler and more efficient attention-based models."], "machine translation and multilinguality"], [["self-attention architectures for answer-agnostic neural question generation", "thomas scialom | benjamin piwowarski | jacopo staiano", "neural architectures based on self-attention, such as transformers, recently attracted interest from the research community, and obtained significant improvements over the state of the art in several tasks. we explore how transformers can be adapted to the task of neural question generation without constraining the model to focus on a specific answer passage. we study the effect of several strategies to deal with out-of-vocabulary words such as copy mechanisms, placeholders, and contextual word embeddings. we report improvements obtained over the state-of-the-art on the squad dataset according to automated metrics (bleu, rouge), as well as qualitative human assessments of the system outputs."], "generation"], [["inset: sentence infilling with inter-sentential transformer", "yichen huang | yizhe zhang | oussama elachqar | yu cheng", "missing sentence generation (or sentence in-filling) fosters a wide range of applications in natural language generation, such as document auto-completion and meeting note expansion. this task asks the model to generate intermediate missing sentences that can syntactically and semantically bridge the surrounding context. solving the sentence infilling task requires techniques in natural language processing ranging from understanding to discourse-level planning to generation. in this paper, we propose a framework to decouple the challenge and address these three aspects respectively, leveraging the power of existing large-scale pre-trained models such as bert and gpt-2. we empirically demonstrate the effectiveness of our model in learning a sentence representation for generation and further generating a missing sentence that fits the context."], "generation"], [["dual adversarial neural transfer for low-resource named entity recognition", "joey tianyi zhou | hao zhang | di jin | hongyuan zhu | meng fang | rick siow mong goh | kenneth kwok", "we propose a new neural transfer method termed dual adversarial transfer network (datnet) for addressing low-resource named entity recognition (ner). specifically, two variants of datnet, i.e., datnet-f and datnet-p, are investigated to explore effective feature fusion between high and low resource. to address the noisy and imbalanced training data, we propose a novel generalized resource-adversarial discriminator (grad). additionally, adversarial training is adopted to boost model generalization. in experiments, we examine the effects of different components in datnet across domains and languages and show that significant improvement can be obtained especially for low-resource data, without augmenting any additional hand-crafted features and pre-trained language model."], "tagging, chunking, syntax and parsing"], [["semantic parsing for english as a second language", "yuanyuan zhao | weiwei sun | junjie cao | xiaojun wan", "this paper is concerned with semantic parsing for english as a second language (esl). motivated by the theoretical emphasis on the learning challenges that occur at the syntax-semantics interface during second language acquisition, we formulate the task based on the divergence between literal and intended meanings. we combine the complementary strengths of english resource grammar, a linguistically-precise hand-crafted deep grammar, and tle, an existing manually annotated esl ud-treebank with a novel reranking model. experiments demonstrate that in comparison to human annotations, our method can obtain a very promising sembanking quality. by means of the newly created corpus, we evaluate state-of-the-art semantic parsing as well as grammatical error correction models. the evaluation profiles the performance of neural nlp techniques for handling esl data and suggests some research directions."], "semantics"], [["towards better non-tree argument mining: proposition-level biaffine parsing with task-specific parameterization", "gaku morio | hiroaki ozaki | terufumi morishita | yuta koreeda | kohsuke yanai", "state-of-the-art argument mining studies have advanced the techniques for predicting argument structures. however, the technology for capturing non-tree-structured arguments is still in its infancy. in this paper, we focus on non-tree argument mining with a neural model. we jointly predict proposition types and edges between propositions. our proposed model incorporates (i) task-specific parameterization (tsp) that effectively encodes a sequence of propositions and (ii) a proposition-level biaffine attention (plba) that can predict a non-tree argument consisting of edges. experimental results show that both tsp and plba boost edge prediction performance compared to baselines."], "sentiment analysis, stylistic analysis, and argument mining"], [["revisiting the context window for cross-lingual word embeddings", "ryokan ri | yoshimasa tsuruoka", "existing approaches to mapping-based cross-lingual word embeddings are based on the assumption that the source and target embedding spaces are structurally similar. the structures of embedding spaces largely depend on the co-occurrence statistics of each word, which the choice of context window determines. despite this obvious connection between the context window and mapping-based cross-lingual embeddings, their relationship has been underexplored in prior work. in this work, we provide a thorough evaluation, in various languages, domains, and tasks, of bilingual embeddings trained with different context windows. the highlight of our findings is that increasing the size of both the source and target window sizes improves the performance of bilingual lexicon induction, especially the performance on frequent nouns."], "resources and evaluation"], [["improved zero-shot neural machine translation via ignoring spurious correlations", "jiatao gu | yong wang | kyunghyun cho | victor o.k. li", "zero-shot translation, translating between language pairs on which a neural machine translation (nmt) system has never been trained, is an emergent property when training the system in multilingual settings. however, naive training for zero-shot nmt easily fails, and is sensitive to hyper-parameter setting. the performance typically lags far behind the more conventional pivot-based approach which translates twice using a third language as a pivot. in this work, we address the degeneracy problem due to capturing spurious correlations by quantitatively analyzing the mutual information between language ids of the source and decoded sentences. inspired by this analysis, we propose to use two simple but effective approaches: (1) decoder pre-training; (2) back-translation. these methods show significant improvement (4 22 bleu points) over the vanilla zero-shot translation on three challenging multilingual datasets, and achieve similar or better results than the pivot-based approach."], "machine translation and multilinguality"], [["sembleu: a robust metric for amr parsing evaluation", "linfeng song | daniel gildea", "evaluating amr parsing accuracy involves comparing pairs of amr graphs. the major evaluation metric, smatch (cai and knight, 2013), searches for one-to-one mappings between the nodes of two amrs with a greedy hill-climbing algorithm, which leads to search errors. we propose sembleu, a robust metric that extends bleu (papineni et al., 2002) to amrs. it does not suffer from search errors and considers non-local correspondences in addition to local ones. sembleu is fully content-driven and punishes situations where a system\u2019s output does not preserve most information from the input. preliminary experiments on both sentence and corpus levels show that sembleu has slightly higher consistency with human judgments than smatch. our code is available at http://github.com/ freesunshine0316/sembleu."], "semantics"], [["interactive machine comprehension with information seeking agents", "xingdi yuan | jie fu | marc-alexandre c\u00f4t\u00e9 | yi tay | chris pal | adam trischler", "existing machine reading comprehension (mrc) models do not scale effectively to real-world applications like web-level information retrieval and question answering (qa). we argue that this stems from the nature of mrc datasets: most of these are static environments wherein the supporting documents and all necessary information are fully observed. in this paper, we propose a simple method that reframes existing mrc datasets as interactive, partially observable environments. specifically, we \u201cocclude\u201d the majority of a document\u2019s text and add context-sensitive commands that reveal \u201cglimpses\u201d of the hidden text to a model. we repurpose squad and newsqa as an initial case study, and then show how the interactive corpora can be used to train a model that seeks relevant information through sequential decision making. we believe that this setting can contribute in scaling models to web-level qa scenarios."], "semantics"], [["multimodal transformer networks for end-to-end video-grounded dialogue systems", "hung le | doyen sahoo | nancy chen | steven hoi", "developing video-grounded dialogue systems (vgds), where a dialogue is conducted based on visual and audio aspects of a given video, is significantly more challenging than traditional image or text-grounded dialogue systems because (1) feature space of videos span across multiple picture frames, making it difficult to obtain semantic information; and (2) a dialogue agent must perceive and process information from different modalities (audio, video, caption, etc.) to obtain a comprehensive understanding. most existing work is based on rnns and sequence-to-sequence architectures, which are not very effective for capturing complex long-term dependencies (like in videos). to overcome this, we propose multimodal transformer networks (mtn) to encode videos and incorporate information from different modalities. we also propose query-aware attention through an auto-encoder to extract query-aware features from non-text modalities. we develop a training procedure to simulate token-level decoding to improve the quality of generated responses during inference. we get state of the art performance on dialogue system technology challenge 7 (dstc7). our model also generalizes to another multimodal visual-grounded dialogue task, and obtains promising performance."], "dialogue and interactive systems"], [["making fast graph-based algorithms with graph metric embeddings", "andrey kutuzov | mohammad dorgham | oleksiy oliynyk | chris biemann | alexander panchenko", "graph measures, such as node distances, are inefficient to compute. we explore dense vector representations as an effective way to approximate the same information. we introduce a simple yet efficient and effective approach for learning graph embeddings. instead of directly operating on the graph structure, our method takes structural measures of pairwise node similarities into account and learns dense node representations reflecting user-defined graph distance measures, such as e.g. the shortest path distance or distance measures that take information beyond the graph structure into account. we demonstrate a speed-up of several orders of magnitude when predicting word similarity by vector operations on our embeddings as opposed to directly computing the respective path-based measures, while outperforming various other graph embeddings on semantic similarity and word sense disambiguation tasks."], "semantics"], [["adversarial attention modeling for multi-dimensional emotion regression", "suyang zhu | shoushan li | guodong zhou", "in this paper, we propose a neural network-based approach, namely adversarial attention network, to the task of multi-dimensional emotion regression, which automatically rates multiple emotion dimension scores for an input text. especially, to determine which words are valuable for a particular emotion dimension, an attention layer is trained to weight the words in an input sequence. furthermore, adversarial training is employed between two attention layers to learn better word weights via a discriminator. in particular, a shared attention layer is incorporated to learn public word weights between two emotion dimensions. empirical evaluation on the emobank corpus shows that our approach achieves notable improvements in r-values on both emobank reader\u2019s and writer\u2019s multi-dimensional emotion regression tasks in all domains over the state-of-the-art baselines."], "sentiment analysis, stylistic analysis, and argument mining"], [["a deep reinforced sequence-to-set model for multi-label classification", "pengcheng yang | fuli luo | shuming ma | junyang lin | xu sun", "multi-label classification (mlc) aims to predict a set of labels for a given instance. based on a pre-defined label order, the sequence-to-sequence (seq2seq) model trained via maximum likelihood estimation method has been successfully applied to the mlc task and shows powerful ability to capture high-order correlations between labels. however, the output labels are essentially an unordered set rather than an ordered sequence. this inconsistency tends to result in some intractable problems, e.g., sensitivity to the label order. to remedy this, we propose a simple but effective sequence-to-set model. the proposed model is trained via reinforcement learning, where reward feedback is designed to be independent of the label order. in this way, we can reduce the dependence of the model on the label order, as well as capture high-order correlations between labels. extensive experiments show that our approach can substantially outperform competitive baselines, as well as effectively reduce the sensitivity to the label order."], "information extraction, retrieval and text mining"], [["estimating the influence of auxiliary tasks for multi-task learning of sequence tagging tasks", "fynn schr\u00f6der | chris biemann", "multi-task learning (mtl) and transfer learning (tl) are techniques to overcome the issue of data scarcity when training state-of-the-art neural networks. however, finding beneficial auxiliary datasets for mtl or tl is a time- and resource-consuming trial-and-error approach. we propose new methods to automatically assess the similarity of sequence tagging datasets to identify beneficial auxiliary data for mtl or tl setups. our methods can compute the similarity between any two sequence tagging datasets, they do not need to be annotated with the same tagset or multiple labels in parallel. additionally, our methods take tokens and their labels into account, which is more robust than only using either of them as an information source, as conducted in prior work. we empirically show that our similarity measures correlate with the change in test score of neural networks that use the auxiliary dataset for mtl to increase the main task performance. we provide an efficient, open-source implementation."], "machine learning for nlp"], [["multi-source cross-lingual model transfer: learning what to share", "xilun chen | ahmed hassan awadallah | hany hassan | wei wang | claire cardie", "modern nlp applications have enjoyed a great boost utilizing neural networks models. such deep neural models, however, are not applicable to most human languages due to the lack of annotated training data for various nlp tasks. cross-lingual transfer learning (cltl) is a viable method for building nlp models for a low-resource target language by leveraging labeled data from other (source) languages. in this work, we focus on the multilingual transfer setting where training data in multiple source languages is leveraged to further boost target language performance. unlike most existing methods that rely only on language-invariant features for cltl, our approach coherently utilizes both language-invariant and language-specific features at instance level. our model leverages adversarial networks to learn language-invariant features, and mixture-of-experts models to dynamically exploit the similarity between the target language and each individual source language. this enables our model to learn effectively what to share between various languages in the multilingual setup. moreover, when coupled with unsupervised multilingual embeddings, our model can operate in a zero-resource setting where neither target language training data nor cross-lingual resources are available. our model achieves significant performance gains over prior art, as shown in an extensive set of experiments over multiple text classification and sequence tagging tasks including a large-scale industry dataset."], "machine translation and multilinguality"], [["learning to recover from multi-modality errors for non-autoregressive neural machine translation", "qiu ran | yankai lin | peng li | jie zhou", "non-autoregressive neural machine translation (nat) predicts the entire target sequence simultaneously and significantly accelerates inference process. however, nat discards the dependency information in a sentence, and thus inevitably suffers from the multi-modality problem: the target tokens may be provided by different possible translations, often causing token repetitions or missing. to alleviate this problem, we propose a novel semi-autoregressive model recoversat in this work, which generates a translation as a sequence of segments. the segments are generated simultaneously while each segment is predicted token-by-token. by dynamically determining segment length and deleting repetitive segments, recoversat is capable of recovering from repetitive and missing token errors. experimental results on three widely-used benchmark datasets show that our proposed model achieves more than 4 times speedup while maintaining comparable performance compared with the corresponding autoregressive model."], "machine translation and multilinguality"], [["negative training for neural dialogue response generation", "tianxing he | james glass", "although deep learning models have brought tremendous advancements to the field of open-domain dialogue response generation, recent research results have revealed that the trained models have undesirable generation behaviors, such as malicious responses and generic (boring) responses. in this work, we propose a framework named \u201cnegative training\u201d to minimize such behaviors. given a trained model, the framework will first find generated samples that exhibit the undesirable behavior, and then use them to feed negative training signals for fine-tuning the model. our experiments show that negative training can significantly reduce the hit rate of malicious responses, or discourage frequent responses and improve response diversity."], "dialogue and interactive systems"], [["mooccube: a large-scale data repository for nlp applications in moocs", "jifan yu | gan luo | tong xiao | qingyang zhong | yuquan wang | wenzheng feng | junyi luo | chenyu wang | lei hou | juanzi li | zhiyuan liu | jie tang", "the prosperity of massive open online courses (moocs) provides fodder for many nlp and ai research for education applications, e.g., course concept extraction, prerequisite relation discovery, etc. however, the publicly available datasets of mooc are limited in size with few types of data, which hinders advanced models and novel attempts in related topics. therefore, we present mooccube, a large-scale data repository of over 700 mooc courses, 100k concepts, 8 million student behaviors with an external resource. moreover, we conduct a prerequisite discovery task as an example application to show the potential of mooccube in facilitating relevant research. the data repository is now available at http://moocdata.cn/data/mooccube."], "nlp applications"], [["causal analysis of syntactic agreement mechanisms in neural language models", "matthew finlayson | aaron mueller | sebastian gehrmann | stuart shieber | tal linzen | yonatan belinkov", "targeted syntactic evaluations have demonstrated the ability of language models to perform subject-verb agreement given difficult contexts. to elucidate the mechanisms by which the models accomplish this behavior, this study applies causal mediation analysis to pre-trained neural language models. we investigate the magnitude of models\u2019 preferences for grammatical inflections, as well as whether neurons process subject-verb agreement similarly across sentences with different syntactic structures. we uncover similarities and differences across architectures and model sizes\u2014notably, that larger models do not necessarily learn stronger preferences. we also observe two distinct mechanisms for producing subject-verb agreement depending on the syntactic structure of the input sentence. finally, we find that language models rely on similar sets of neurons when given sentences with similar syntactic structure."], "interpretability and analysis of models for nlp"], [["cross-modality relevance for reasoning on language and vision", "chen zheng | quan guo | parisa kordjamshidi", "this work deals with the challenge of learning and reasoning over language and vision data for the related downstream tasks such as visual question answering (vqa) and natural language for visual reasoning (nlvr). we design a novel cross-modality relevance module that is used in an end-to-end framework to learn the relevance representation between components of various input modalities under the supervision of a target task, which is more generalizable to unobserved data compared to merely reshaping the original representation space. in addition to modeling the relevance between the textual entities and visual entities, we model the higher-order relevance between entity relations in the text and object relations in the image. our proposed approach shows competitive performance on two different language and vision tasks using public benchmarks and improves the state-of-the-art published results. the learned alignments of input spaces and their relevance representations by nlvr task boost the training efficiency of vqa task."], "language grounding to vision, robotics and beyond"], [["a simple theoretical model of importance for summarization", "maxime peyrard", "research on summarization has mainly been driven by empirical approaches, crafting systems to perform well on standard datasets with the notion of information importance remaining latent. we argue that establishing theoretical models of importance will advance our understanding of the task and help to further improve summarization systems. to this end, we propose simple but rigorous definitions of several concepts that were previously used only intuitively in summarization: redundancy, relevance, and informativeness. importance arises as a single quantity naturally unifying these concepts. additionally, we provide intuitions to interpret the proposed quantities and experiments to demonstrate the potential of the framework to inform and guide subsequent works."], "summarization"], [["cross-lingual knowledge graph alignment via graph matching neural network", "kun xu | liwei wang | mo yu | yansong feng | yan song | zhiguo wang | dong yu", "previous cross-lingual knowledge graph (kg) alignment studies rely on entity embeddings derived only from monolingual kg structural information, which may fail at matching entities that have different facts in two kgs. in this paper, we introduce the topic entity graph, a local sub-graph of an entity, to represent entities with their contextual information in kg. from this view, the kb-alignment task can be formulated as a graph matching problem; and we further propose a graph-attention based solution, which first matches all entities in two topic entity graphs, and then jointly model the local matching information to derive a graph-level matching vector. experiments show that our model outperforms previous state-of-the-art methods by a large margin."], "machine translation and multilinguality"], [["attention guided graph convolutional networks for relation extraction", "zhijiang guo | yan zhang | wei lu", "dependency trees convey rich structural information that is proven useful for extracting relations among entities in text. however, how to effectively make use of relevant information while ignoring irrelevant information from the dependency trees remains a challenging research question. existing approaches employing rule based hard-pruning strategies for selecting relevant partial dependency structures may not always yield optimal results. in this work, we propose attention guided graph convolutional networks (aggcns), a novel model which directly takes full dependency trees as inputs. our model can be understood as a soft-pruning approach that automatically learns how to selectively attend to the relevant sub-structures useful for the relation extraction task. extensive results on various tasks including cross-sentence n-ary relation extraction and large-scale sentence-level relation extraction show that our model is able to better leverage the structural information of the full dependency trees, giving significantly better results than previous approaches."], "information extraction, retrieval and text mining"], [["ranking generated summaries by correctness: an interesting but challenging application for natural language inference", "tobias falke | leonardo f. r. ribeiro | prasetya ajie utama | ido dagan | iryna gurevych", "while recent progress on abstractive summarization has led to remarkably fluent summaries, factual errors in generated summaries still severely limit their use in practice. in this paper, we evaluate summaries produced by state-of-the-art models via crowdsourcing and show that such errors occur frequently, in particular with more abstractive models. we study whether textual entailment predictions can be used to detect such errors and if they can be reduced by reranking alternative predicted summaries. that leads to an interesting downstream application for entailment models. in our experiments, we find that out-of-the-box entailment models trained on nli datasets do not yet offer the desired performance for the downstream task and we therefore release our annotations as additional test data for future extrinsic evaluations of nli."], "summarization"], [["smart: robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization", "haoming jiang | pengcheng he | weizhu chen | xiaodong liu | jianfeng gao | tuo zhao", "transfer learning has fundamentally changed the landscape of natural language processing (nlp). many state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks. however, due to limited data resources from downstream tasks and the extremely high complexity of pre-trained models, aggressive fine-tuning often causes the fine-tuned model to overfit the training data of downstream tasks and fail to generalize to unseen data. to address such an issue in a principled manner, we propose a new learning framework for robust and efficient fine-tuning for pre-trained models to attain better generalization performance. the proposed framework contains two important ingredients: 1. smoothness-inducing regularization, which effectively manages the complexity of the model; 2. bregman proximal point optimization, which is an instance of trust-region methods and can prevent aggressive updating. our experiments show that the proposed framework achieves new state-of-the-art performance on a number of nlp tasks including glue, snli, scitail and anli. moreover, it also outperforms the state-of-the-art t5 model, which is the largest pre-trained model containing 11 billion parameters, on glue."], "machine learning for nlp"], [["retrieval-enhanced adversarial training for neural response generation", "qingfu zhu | lei cui | wei-nan zhang | furu wei | ting liu", "dialogue systems are usually built on either generation-based or retrieval-based approaches, yet they do not benefit from the advantages of different models. in this paper, we propose a retrieval-enhanced adversarial training (reat) method for neural response generation. distinct from existing approaches, the reat method leverages an encoder-decoder framework in terms of an adversarial training paradigm, while taking advantage of n-best response candidates from a retrieval-based system to construct the discriminator. an empirical study on a large scale public available benchmark dataset shows that the reat method significantly outperforms the vanilla seq2seq model as well as the conventional adversarial training approach."], "dialogue and interactive systems"], [["automatic evaluation of local topic quality", "jeffrey lund | piper armstrong | wilson fearn | stephen cowley | courtni byun | jordan boyd-graber | kevin seppi", "topic models are typically evaluated with respect to the global topic distributions that they generate, using metrics such as coherence, but without regard to local (token-level) topic assignments. token-level assignments are important for downstream tasks such as classification. even recent models, which aim to improve the quality of these token-level topic assignments, have been evaluated only with respect to global metrics. we propose a task designed to elicit human judgments of token-level topic assignments. we use a variety of topic model types and parameters and discover that global metrics agree poorly with human assignments. since human evaluation is expensive we propose a variety of automated metrics to evaluate topic models at a local level. finally, we correlate our proposed metrics with human judgments from the task on several datasets. we show that an evaluation based on the percent of topic switches correlates most strongly with human judgment of local topic quality. we suggest that this new metric, which we call consistency, be adopted alongside global metrics such as topic coherence when evaluating new topic models."], "resources and evaluation"], [["combining knowledge hunting and neural language models to solve the winograd schema challenge", "ashok prakash | arpit sharma | arindam mitra | chitta baral", "winograd schema challenge (wsc) is a pronoun resolution task which seems to require reasoning with commonsense knowledge. the needed knowledge is not present in the given text. automatic extraction of the needed knowledge is a bottleneck in solving the challenge. the existing state-of-the-art approach uses the knowledge embedded in their pre-trained language model. however, the language models only embed part of the knowledge, the ones related to frequently co-existing concepts. this limits the performance of such models on the wsc problems. in this work, we build-up on the language model based methods and augment them with a commonsense knowledge hunting (using automatic extraction from text) module and an explicit reasoning module. our end-to-end system built in such a manner improves on the accuracy of two of the available language model based approaches by 5.53% and 7.7% respectively. overall our system achieves the state-of-the-art accuracy of 71.06% on the wsc dataset, an improvement of 7.36% over the previous best."], "question answering"], [["learning and evaluating emotion lexicons for 91 languages", "sven buechel | susanna r\u00fccker | udo hahn", "emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis. yet, manually curated lexicons are only available for a handful of languages, leaving most languages of the world without such a precious resource for downstream applications. even worse, their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature. in order to break this bottleneck, we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language. our approach requires nothing but a source language emotion lexicon, a bilingual word translation model, and a target language embedding model. fulfilling these requirements for 91 languages, we are able to generate representationally rich high-coverage lexicons comprising eight emotional variables with more than 100k lexical entries each. we evaluated the automatically generated lexicons against human judgment from 26 datasets, spanning 12 typologically diverse languages, and found that our approach produces results in line with state-of-the-art monolingual approaches to lexicon creation and even surpasses human reliability for some languages and variables. code and data are available at https://github.com/julielab/memolon archived under doi 10.5281/zenodo.3779901."], "resources and evaluation"], [["multi-hop paragraph retrieval for open-domain question answering", "yair feldman | ran el-yaniv", "this paper is concerned with the task of multi-hop open-domain question answering (qa). this task is particularly challenging since it requires the simultaneous performance of textual reasoning and efficient searching. we present a method for retrieving multiple supporting paragraphs, nested amidst a large knowledge base, which contain the necessary evidence to answer a given question. our method iteratively retrieves supporting paragraphs by forming a joint vector representation of both a question and a paragraph. the retrieval is performed by considering contextualized sentence-level representations of the paragraphs in the knowledge source. our method achieves state-of-the-art performance over two well-known datasets, squad-open and hotpotqa, which serve as our single- and multi-hop open-domain qa benchmarks, respectively."], "question answering"], [["memory consolidation for contextual spoken language understanding with dialogue logistic inference", "he bai | yu zhou | jiajun zhang | chengqing zong", "dialogue contexts are proven helpful in the spoken language understanding (slu) system and they are typically encoded with explicit memory representations. however, most of the previous models learn the context memory with only one objective to maximizing the slu performance, leaving the context memory under-exploited. in this paper, we propose a new dialogue logistic inference (dli) task to consolidate the context memory jointly with slu in the multi-task framework. dli is defined as sorting a shuffled dialogue session into its original logical order and shares the same memory encoder and retrieval mechanism as the slu model. our experimental results show that various popular contextual slu models can benefit from our approach, and improvements are quite impressive, especially in slot filling."], "dialogue and interactive systems"], [["towards open domain event trigger identification using adversarial domain adaptation", "aakanksha naik | carolyn rose", "we tackle the task of building supervised event trigger identification models which can generalize better across domains. our work leverages the adversarial domain adaptation (ada) framework to introduce domain-invariance. ada uses adversarial training to construct representations that are predictive for trigger identification, but not predictive of the example\u2019s domain. it requires no labeled data from the target domain, making it completely unsupervised. experiments with two domains (english literature and news) show that ada leads to an average f1 score improvement of 3.9 on out-of-domain data. our best performing model (bert-a) reaches 44-49 f1 across both domains, using no labeled target data. preliminary experiments reveal that finetuning on 1% labeled data, followed by self-training leads to substantial improvement, reaching 51.5 and 67.2 f1 on literature and news respectively."], "information extraction, retrieval and text mining"], [["concept-based label embedding via dynamic routing for hierarchical text classification", "xuepeng wang | li zhao | bing liu | tao chen | feng zhang | di wang", "hierarchical text classification (htc) is a challenging task that categorizes a textual description within a taxonomic hierarchy. most of the existing methods focus on modeling the text. recently, researchers attempt to model the class representations with some resources (e.g., external dictionaries). however, the concept shared among classes which is a kind of domain-specific and fine-grained information has been ignored in previous work. in this paper, we propose a novel concept-based label embedding method that can explicitly represent the concept and model the sharing mechanism among classes for the hierarchical text classification. experimental results on two widely used datasets prove that the proposed model outperforms several state-of-the-art methods. we release our complementary resources (concepts and definitions of classes) for these two datasets to benefit the research on htc."], "information extraction, retrieval and text mining"], [["unsupervised opinion summarization as copycat-review generation", "arthur bra\u017einskas | mirella lapata | ivan titov", "opinion summarization is the task of automatically creating summaries that reflect subjective information expressed in multiple documents, such as product reviews. while the majority of previous work has focused on the extractive setting, i.e., selecting fragments from input reviews to produce a summary, we let the model generate novel sentences and hence produce abstractive summaries. recent progress in summarization has seen the development of supervised models which rely on large quantities of document-summary pairs. since such training data is expensive to acquire, we instead consider the unsupervised setting, in other words, we do not use any summaries in training. we define a generative model for a review collection which capitalizes on the intuition that when generating a new review given a set of other reviews of a product, we should be able to control the \u201camount of novelty\u201d going into the new review or, equivalently, vary the extent to which it deviates from the input. at test time, when generating summaries, we force the novelty to be minimal, and produce a text reflecting consensus opinions. we capture this intuition by defining a hierarchical variational autoencoder model. both individual reviews and the products they correspond to are associated with stochastic latent codes, and the review generator (\u201cdecoder\u201d) has direct access to the text of input reviews through the pointer-generator mechanism. experiments on amazon and yelp datasets, show that setting at test time the review\u2019s latent code to its mean, allows the model to produce fluent and coherent summaries reflecting common opinions."], "summarization"], [["sensebert: driving some sense into bert", "yoav levine | barak lenz | or dagan | ori ram | dan padnos | or sharir | shai shalev-shwartz | amnon shashua | yoav shoham", "the ability to learn from large unlabeled corpora has allowed neural language models to advance the frontier in natural language understanding. however, existing self-supervision techniques operate at the word form level, which serves as a surrogate for the underlying semantic content. this paper proposes a method to employ weak-supervision directly at the word sense level. our model, named sensebert, is pre-trained to predict not only the masked words but also their wordnet supersenses. accordingly, we attain a lexical-semantic level language model, without the use of human annotation. sensebert achieves significantly improved lexical understanding, as we demonstrate by experimenting on semeval word sense disambiguation, and by attaining a state of the art result on the \u2018word in context\u2019 task."], "machine learning for nlp"], [["multi-style generative reading comprehension", "kyosuke nishida | itsumi saito | kosuke nishida | kazutoshi shinoda | atsushi otsuka | hisako asano | junji tomita", "this study tackles generative reading comprehension (rc), which consists of answering questions based on textual evidence and natural language generation (nlg). we propose a multi-style abstractive summarization model for question answering, called masque. the proposed model has two key characteristics. first, unlike most studies on rc that have focused on extracting an answer span from the provided passages, our model instead focuses on generating a summary from the question and multiple passages. this serves to cover various answer styles required for real-world applications. second, whereas previous studies built a specific model for each answer style because of the difficulty of acquiring one general model, our approach learns multi-style answers within a model to improve the nlg capability for all styles involved. this also enables our model to give an answer in the target style. experiments show that our model achieves state-of-the-art performance on the q&a task and the q&a + nlg task of ms marco 2.1 and the summary task of narrativeqa. we observe that the transfer of the style-independent nlg capability to the target style is the key to its success."], "question answering"], [["end-to-end deep reinforcement learning based coreference resolution", "hongliang fei | xu li | dingcheng li | ping li", "recent neural network models have significantly advanced the task of coreference resolution. however, current neural coreference models are usually trained with heuristic loss functions that are computed over a sequence of local decisions. in this paper, we introduce an end-to-end reinforcement learning based coreference resolution model to directly optimize coreference evaluation metrics. specifically, we modify the state-of-the-art higher-order mention ranking approach in lee et al. (2018) to a reinforced policy gradient model by incorporating the reward associated with a sequence of coreference linking actions. furthermore, we introduce maximum entropy regularization for adequate exploration to prevent the model from prematurely converging to a bad local optimum. our proposed model achieves new state-of-the-art performance on the english ontonotes v5.0 benchmark."], "discourse and pragmatics"], [["semantic graphs for generating deep questions", "liangming pan | yuxi xie | yansong feng | tat-seng chua | min-yen kan", "this paper proposes the problem of deep question generation (dqg), which aims to generate complex questions that require reasoning over multiple pieces of information about the input passage. in order to capture the global structure of the document and facilitate reasoning, we propose a novel framework that first constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based ggnn (att-ggnn). afterward, we fuse the document-level and graph-level representations to perform joint training of content selection and question decoding. on the hotpotqa deep-question centric dataset, our model greatly improves performance over questions requiring reasoning over multiple facts, leading to state-of-the-art performance. the code is publicly available at https://github.com/wing-nus/sg-deep-question-generation."], "generation"], [["sas: dialogue state tracking via slot attention and slot information sharing", "jiaying hu | yan yang | chencai chen | liang he | zhou yu", "dialogue state tracker is responsible for inferring user intentions through dialogue history. previous methods have difficulties in handling dialogues with long interaction context, due to the excessive information. we propose a dialogue state tracker with slot attention and slot information sharing (sas) to reduce redundant information\u2019s interference and improve long dialogue context tracking. specially, we first apply a slot attention to learn a set of slot-specific features from the original dialogue and then integrate them using a slot information sharing module. our model yields a significantly improved performance compared to previous state-of the-art models on the multiwoz dataset."], "dialogue and interactive systems"], [["a graph-based coarse-to-fine method for unsupervised bilingual lexicon induction", "shuo ren | shujie liu | ming zhou | shuai ma", "unsupervised bilingual lexicon induction is the task of inducing word translations from monolingual corpora of two languages. recent methods are mostly based on unsupervised cross-lingual word embeddings, the key to which is to find initial solutions of word translations, followed by the learning and refinement of mappings between the embedding spaces of two languages. however, previous methods find initial solutions just based on word-level information, which may be (1) limited and inaccurate, and (2) prone to contain some noise introduced by the insufficiently pre-trained embeddings of some words. to deal with those issues, in this paper, we propose a novel graph-based paradigm to induce bilingual lexicons in a coarse-to-fine way. we first build a graph for each language with its vertices representing different words. then we extract word cliques from the graphs and map the cliques of two languages. based on that, we induce the initial word translation solution with the central words of the aligned cliques. this coarse-to-fine approach not only leverages clique-level information, which is richer and more accurate, but also effectively reduces the bad effect of the noise in the pre-trained embeddings. finally, we take the initial solution as the seed to learn cross-lingual embeddings, from which we induce bilingual lexicons. experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods."], "machine translation and multilinguality"], [["generalized data augmentation for low-resource translation", "mengzhou xia | xiang kong | antonios anastasopoulos | graham neubig", "low-resource language pairs with a paucity of parallel data pose challenges for machine translation in terms of both adequacy and fluency. data augmentation utilizing a large amount of monolingual data is regarded as an effective way to alleviate the problem. in this paper, we propose a general framework of data augmentation for low-resource machine translation not only using target-side monolingual data, but also by pivoting through a related high-resource language. specifically, we experiment with a two-step pivoting method to convert high-resource data to the low-resource language, making best use of available resources to better approximate the true distribution of the low-resource language. first, we inject low-resource words into high-resource sentences through an induced bilingual dictionary. second, we further edit the high-resource data injected with low-resource words using a modified unsupervised machine translation framework. extensive experiments on four low-resource datasets show that under extreme low-resource settings, our data augmentation techniques improve translation quality by up to 1.5 to 8 bleu points compared to supervised back-translation baselines."], "machine translation and multilinguality"], [["the sensitivity of language models and humans to winograd schema perturbations", "mostafa abdou | vinit ravishankar | maria barrett | yonatan belinkov | desmond elliott | anders s\u00f8gaard", "large-scale pretrained language models are the major driving force behind recent improvements in perfromance on the winograd schema challenge, a widely employed test of commonsense reasoning ability. we show, however, with a new diagnostic dataset, that these models are sensitive to linguistic perturbations of the winograd examples that minimally affect human understanding. our results highlight interesting differences between humans and language models: language models are more sensitive to number or gender alternations and synonym replacements than humans, and humans are more stable and consistent in their predictions, maintain a much higher absolute performance, and perform better on non-associative instances than associative ones."], "semantics"], [["semi-supervised stochastic multi-domain learning using variational inference", "yitong li | timothy baldwin | trevor cohn", "supervised models of nlp rely on large collections of text which closely resemble the intended testing setting. unfortunately matching text is often not available in sufficient quantity, and moreover, within any domain of text, data is often highly heterogenous. in this paper we propose a method to distill the important domain signal as part of a multi-domain learning system, using a latent variable model in which parts of a neural model are stochastically gated based on the inferred domain. we compare the use of discrete versus continuous latent variables, operating in a domain-supervised or a domain semi-supervised setting, where the domain is known only for a subset of training inputs. we show that our model leads to substantial performance improvements over competitive benchmark domain adaptation methods, including methods using adversarial learning."], "machine learning for nlp"], [["parallel sentence mining by constrained decoding", "pinzhen chen | nikolay bogoychev | kenneth heafield | faheem kirefu", "we present a novel method to extract parallel sentences from two monolingual corpora, using neural machine translation. our method relies on translating sentences in one corpus, but constraining the decoding by a prefix tree built on the other corpus. we argue that a neural machine translation system by itself can be a sentence similarity scorer and it efficiently approximates pairwise comparison with a modified beam search. when benchmarked on the bucc shared task, our method achieves results comparable to other submissions."], "machine translation and multilinguality"], [["span-based localizing network for natural language video localization", "hao zhang | aixin sun | wei jing | joey tianyi zhou", "given an untrimmed video and a text query, natural language video localization (nlvl) is to locate a matching span from the video that semantically corresponds to the query. existing solutions formulate nlvl either as a ranking task and apply multimodal matching architecture, or as a regression task to directly regress the target video span. in this work, we address nlvl task with a span-based qa approach by treating the input video as text passage. we propose a video span localizing network (vslnet), on top of the standard span-based qa framework, to address nlvl. the proposed vslnet tackles the differences between nlvl and span-based qa through a simple and yet effective query-guided highlighting (qgh) strategy. the qgh guides vslnet to search for matching video span within a highlighted region. through extensive experiments on three benchmark datasets, we show that the proposed vslnet outperforms the state-of-the-art methods; and adopting span-based qa framework is a promising direction to solve nlvl."], "language grounding to vision, robotics and beyond"], [["towards explainable nlp: a generative explanation framework for text classification", "hui liu | qingyu yin | william yang wang", "building explainable systems is a critical problem in the field of natural language processing (nlp), since most machine learning models provide no explanations for the predictions. existing approaches for explainable machine learning systems tend to focus on interpreting the outputs or the connections between inputs and outputs. however, the fine-grained information (e.g. textual explanations for the labels) is often ignored, and the systems do not explicitly generate the human-readable explanations. to solve this problem, we propose a novel generative explanation framework that learns to make classification decisions and generate fine-grained explanations at the same time. more specifically, we introduce the explainable factor and the minimum risk training approach that learn to generate more reasonable explanations. we construct two new datasets that contain summaries, rating scores, and fine-grained reasons. we conduct experiments on both datasets, comparing with several strong neural network baseline systems. experimental results show that our method surpasses all baselines on both datasets, and is able to generate concise explanations at the same time."], "machine learning for nlp"], [["what was written vs. who read it: news media profiling using text analysis and social media context", "ramy baly | georgi karadzhov | jisun an | haewoon kwak | yoan dinkov | ahmed ali | james glass | preslav nakov", "predicting the political bias and the factuality of reporting of entire news outlets are critical elements of media profiling, which is an understudied but an increasingly important research direction. the present level of proliferation of fake, biased, and propagandistic content online has made it impossible to fact-check every single suspicious claim, either manually or automatically. thus, it has been proposed to profile entire news outlets and to look for those that are likely to publish fake or biased content. this makes it possible to detect likely \u201cfake news\u201d the moment they are published, by simply checking the reliability of their source. from a practical perspective, political bias and factuality of reporting have a linguistic aspect but also a social context. here, we study the impact of both, namely (i) what was written (i.e., what was published by the target medium, and how it describes itself in twitter) vs. (ii) who reads it (i.e., analyzing the target medium\u2019s audience on social media). we further study (iii) what was written about the target medium (in wikipedia). the evaluation results show that what was written matters most, and we further show that putting all information sources together yields huge improvements over the current state-of-the-art."], "computational social science, social media and cultural analytics"], [["style transformer: unpaired text style transfer without disentangled latent representation", "ning dai | jianze liang | xipeng qiu | xuanjing huang", "disentangling the content and style in the latent space is prevalent in unpaired text style transfer. however, two major issues exist in most of the current neural models. 1) it is difficult to completely strip the style information from the semantics for a sentence. 2) the recurrent neural network (rnn) based encoder and decoder, mediated by the latent representation, cannot well deal with the issue of the long-term dependency, resulting in poor preservation of non-stylistic semantic content. in this paper, we propose the style transformer, which makes no assumption about the latent representation of source sentence and equips the power of attention mechanism in transformer to achieve better style transfer and better content preservation."], "generation"], [["enhancing answer boundary detection for multilingual machine reading comprehension", "fei yuan | linjun shou | xuanyu bai | ming gong | yaobo liang | nan duan | yan fu | daxin jiang", "multilingual pre-trained models could leverage the training data from a rich source language (such as english) to improve performance on low resource languages. however, the transfer quality for multilingual machine reading comprehension (mrc) is significantly worse than sentence classification tasks mainly due to the requirement of mrc to detect the word level answer boundary. in this paper, we propose two auxiliary tasks in the fine-tuning stage to create additional phrase boundary supervision: (1) a mixed mrc task, which translates the question or passage to other languages and builds cross-lingual question-passage pairs; (2) a language-agnostic knowledge masking task by leveraging knowledge phrases mined from web. besides, extensive experiments on two cross-lingual mrc datasets show the effectiveness of our proposed approach."], "question answering"], [["automl strategy based on grammatical evolution: a case study about knowledge discovery from text", "suilan estevez-velarde | yoan guti\u00e9rrez | andr\u00e9s montoyo | yudivi\u00e1n almeida-cruz", "the process of extracting knowledge from natural language text poses a complex problem that requires both a combination of machine learning techniques and proper feature selection. recent advances in automatic machine learning (automl) provide effective tools to explore large sets of algorithms, hyper-parameters and features to find out the most suitable combination of them. this paper proposes a novel automl strategy based on probabilistic grammatical evolution, which is evaluated on the health domain by facing the knowledge discovery challenge in spanish text documents. our approach achieves state-of-the-art results and provides interesting insights into the best combination of parameters and algorithms to use when dealing with this challenge. source code is provided for the research community."], "information extraction, retrieval and text mining"], [["tree communication models for sentiment analysis", "yuan zhang | yue zhang", "tree-lstms have been used for tree-based sentiment analysis over stanford sentiment treebank, which allows the sentiment signals over hierarchical phrase structures to be calculated simultaneously. however, traditional tree-lstms capture only the bottom-up dependencies between constituents. in this paper, we propose a tree communication model using graph convolutional neural network and graph recurrent neural network, which allows rich information exchange between phrases constituent tree. experiments show that our model outperforms existing work on bidirectional tree-lstms in both accuracy and efficiency, providing more consistent predictions on phrase-level sentiments."], "sentiment analysis, stylistic analysis, and argument mining"], [["selective question answering under domain shift", "amita kamath | robin jia | percy liang", "to avoid giving wrong answers, question answering (qa) models need to know when to abstain from answering. moreover, users often ask questions that diverge from the model\u2019s training data, making errors more likely and thus abstention more critical. in this work, we propose the setting of selective question answering under domain shift, in which a qa model is tested on a mixture of in-domain and out-of-domain data, and must answer (i.e., not abstain on) as many questions as possible while maintaining high accuracy. abstention policies based solely on the model\u2019s softmax probabilities fare poorly, since models are overconfident on out-of-domain inputs. instead, we train a calibrator to identify inputs on which the qa model errs, and abstain when it predicts an error is likely. crucially, the calibrator benefits from observing the model\u2019s behavior on out-of-domain data, even if from a different domain than the test data. we combine this method with a squad-trained qa model and evaluate on mixtures of squad and five other qa datasets. our method answers 56% of questions while maintaining 80% accuracy; in contrast, directly using the model\u2019s probabilities only answers 48% at 80% accuracy."], "question answering"], [["recosa: detecting the relevant contexts with self-attention for multi-turn dialogue generation", "hainan zhang | yanyan lan | liang pang | jiafeng guo | xueqi cheng", "in multi-turn dialogue generation, response is usually related with only a few contexts. therefore, an ideal model should be able to detect these relevant contexts and produce a suitable response accordingly. however, the widely used hierarchical recurrent encoder-decoder models just treat all the contexts indiscriminately, which may hurt the following response generation process. some researchers try to use the cosine similarity or the traditional attention mechanism to find the relevant contexts, but they suffer from either insufficient relevance assumption or position bias problem. in this paper, we propose a new model, named recosa, to tackle this problem. firstly, a word level lstm encoder is conducted to obtain the initial representation of each context. then, the self-attention mechanism is utilized to update both the context and masked response representation. finally, the attention weights between each context and response representations are computed and used in the further decoding process. experimental results on both chinese customer services dataset and english ubuntu dialogue dataset show that recosa significantly outperforms baseline models, in terms of both metric-based and human evaluations. further analysis on attention shows that the detected relevant contexts by recosa are highly coherent with human\u2019s understanding, validating the correctness and interpretability of recosa."], "dialogue and interactive systems"], [["towards integration of statistical hypothesis tests into deep neural networks", "ahmad aghaebrahimian | mark cieliebak", "we report our ongoing work about a new deep architecture working in tandem with a statistical test procedure for jointly training texts and their label descriptions for multi-label and multi-class classification tasks. a statistical hypothesis testing method is used to extract the most informative words for each given class. these words are used as a class description for more label-aware text classification. intuition is to help the model to concentrate on more informative words rather than more frequent ones. the model leverages the use of label descriptions in addition to the input text to enhance text classification performance. our method is entirely data-driven, has no dependency on other sources of information than the training data, and is adaptable to different classification problems by providing appropriate training data without major hyper-parameter tuning. we trained and tested our system on several publicly available datasets, where we managed to improve the state-of-the-art on one set with a high margin and to obtain competitive results on all other ones."], "machine learning for nlp"], [["empower entity set expansion via language model probing", "yunyi zhang | jiaming shen | jingbo shang | jiawei han", "entity set expansion, aiming at expanding a small seed entity set with new entities belonging to the same semantic class, is a critical task that benefits many downstream nlp and ir applications, such as question answering, query understanding, and taxonomy construction. existing set expansion methods bootstrap the seed entity set by adaptively selecting context features and extracting new entities. a key challenge for entity set expansion is to avoid selecting ambiguous context features which will shift the class semantics and lead to accumulative errors in later iterations. in this study, we propose a novel iterative set expansion framework that leverages automatically generated class names to address the semantic drift issue. in each iteration, we select one positive and several negative class names by probing a pre-trained language model, and further score each candidate entity based on selected class names. experiments on two datasets show that our framework generates high-quality class names and outperforms previous state-of-the-art methods significantly."], "information extraction, retrieval and text mining"], [["editnts: an neural programmer-interpreter model for sentence simplification through explicit editing", "yue dong | zichao li | mehdi rezagholizadeh | jackie chi kit cheung", "we present the first sentence simplification model that learns explicit edit operations (add, delete, and keep) via a neural programmer-interpreter approach. most current neural sentence simplification systems are variants of sequence-to-sequence models adopted from machine translation. these methods learn to simplify sentences as a byproduct of the fact that they are trained on complex-simple sentence pairs. by contrast, our neural programmer-interpreter is directly trained to predict explicit edit operations on targeted parts of the input sentence, resembling the way that humans perform simplification and revision. our model outperforms previous state-of-the-art neural sentence simplification models (without external knowledge) by large margins on three benchmark text simplification corpora in terms of sari (+0.95 wikilarge, +1.89 wikismall, +1.41 newsela), and is judged by humans to produce overall better and simpler output sentences."], "generation"], [["nlprolog: reasoning with weak unification for question answering in natural language", "leon weber | pasquale minervini | jannes m\u00fcnchmeyer | ulf leser | tim rockt\u00e4schel", "rule-based models are attractive for various tasks because they inherently lead to interpretable and explainable decisions and can easily incorporate prior knowledge. however, such systems are difficult to apply to problems involving natural language, due to its large linguistic variability. in contrast, neural models can cope very well with ambiguity by learning distributed representations of words and their composition from data, but lead to models that are difficult to interpret. in this paper, we describe a model combining neural networks with logic programming in a novel manner for solving multi-hop reasoning tasks over natural language. specifically, we propose to use an prolog prover which we extend to utilize a similarity function over pretrained sentence encoders. we fine-tune the representations for the similarity function via backpropagation. this leads to a system that can apply rule-based reasoning to natural language, and induce domain-specific natural language rules from training data. we evaluate the proposed system on two different question answering tasks, showing that it outperforms two baselines \u2013 bidaf (seo et al., 2016a) and fastqa( weissenborn et al., 2017) on a subset of the wikihop corpus and achieves competitive results on the medhop data set (welbl et al., 2017)."], "question answering"], [["employing the correspondence of relations and connectives to identify implicit discourse relations via label embeddings", "linh the nguyen | linh van ngo | khoat than | thien huu nguyen", "it has been shown that implicit connectives can be exploited to improve the performance of the models for implicit discourse relation recognition (idrr). an important property of the implicit connectives is that they can be accurately mapped into the discourse relations conveying their functions. in this work, we explore this property in a multi-task learning framework for idrr in which the relations and the connectives are simultaneously predicted, and the mapping is leveraged to transfer knowledge between the two prediction tasks via the embeddings of relations and connectives. we propose several techniques to enable such knowledge transfer that yield the state-of-the-art performance for idrr on several settings of the benchmark dataset (i.e., the penn discourse treebank dataset)."], "discourse and pragmatics"], [["variational pretraining for semi-supervised text classification", "suchin gururangan | tam dang | dallas card | noah a. smith", "we introduce vampire, a lightweight pretraining framework for effective text classification when data and computing resources are limited. we pretrain a unigram document model as a variational autoencoder on in-domain, unlabeled data and use its internal states as features in a downstream classifier. empirically, we show the relative strength of vampire against computationally expensive contextual embeddings and other popular semi-supervised baselines under low resource settings. we also find that fine-tuning to in-domain data is crucial to achieving decent performance from contextual embeddings when working with limited supervision. we accompany this paper with code to pretrain and use vampire embeddings in downstream tasks."], "machine learning for nlp"], [["better character language modeling through morphology", "terra blevins | luke zettlemoyer", "we incorporate morphological supervision into character language models (clms) via multitasking and show that this addition improves bits-per-character (bpc) performance across 24 languages, even when the morphology data and language modeling data are disjoint. analyzing the clms shows that inflected words benefit more from explicitly modeling morphology than uninflected words, and that morphological supervision improves performance even as the amount of language modeling data grows. we then transfer morphological supervision across languages to improve performance in the low-resource setting."], "phonology, morphology and word segmentation"], [["searching for effective neural extractive summarization: what works and what\u2019s next", "ming zhong | pengfei liu | danqing wang | xipeng qiu | xuanjing huang", "the recent years have seen remarkable success in the use of deep neural networks on text summarization. however, there is no clear understanding of why they perform so well, or how they might be improved. in this paper, we seek to better understand how neural extractive summarization systems could benefit from different types of model architectures, transferable knowledge and learning schemas. besides, we find an effective way to improve the current framework and achieve the state-of-the-art result on cnn/dailymail by a large margin based on our observations and analysis. hopefully, our work could provide more hints for future research on extractive summarization."], "summarization"], [["neural graph matching networks for chinese short text matching", "lu chen | yanbin zhao | boer lyu | lesheng jin | zhi chen | su zhu | kai yu", "chinese short text matching usually employs word sequences rather than character sequences to get better performance. however, chinese word segmentation can be erroneous, ambiguous or inconsistent, which consequently hurts the final matching performance. to address this problem, we propose neural graph matching networks, a novel sentence matching framework capable of dealing with multi-granular input information. instead of a character sequence or a single word sequence, paired word lattices formed from multiple word segmentation hypotheses are used as input and the model learns a graph representation according to an attentive graph matching mechanism. experiments on two chinese datasets show that our models outperform the state-of-the-art short text matching models."], "semantics"], [["extracting multiple-relations in one-pass with pre-trained transformers", "haoyu wang | ming tan | mo yu | shiyu chang | dakuo wang | kun xu | xiaoxiao guo | saloni potdar", "many approaches to extract multiple relations from a paragraph require multiple passes over the paragraph. in practice, multiple passes are computationally expensive and this makes difficult to scale to longer paragraphs and larger text corpora. in this work, we focus on the task of multiple relation extractions by encoding the paragraph only once. we build our solution upon the pre-trained self-attentive models (transformer), where we first add a structured prediction layer to handle extraction between multiple entity pairs, then enhance the paragraph embedding to capture multiple relational information associated with each entity with entity-aware attention. we show that our approach is not only scalable but can also perform state-of-the-art on the standard benchmark ace 2005."], "information extraction, retrieval and text mining"], [["autoencoding keyword correlation graph for document clustering", "billy chiu | sunil kumar sahu | derek thomas | neha sengupta | mohammady mahdy", "document clustering requires a deep understanding of the complex structure of long-text; in particular, the intra-sentential (local) and inter-sentential features (global). existing representation learning models do not fully capture these features. to address this, we present a novel graph-based representation for document clustering that builds a graph autoencoder (gae) on a keyword correlation graph. the graph is constructed with topical keywords as nodes and multiple local and global features as edges. a gae is employed to aggregate the two sets of features by learning a latent representation which can jointly reconstruct them. clustering is then performed on the learned representations, using vector dimensions as features for inducing document classes. extensive experiments on two datasets show that the features learned by our approach can achieve better clustering performance than other existing features, including term frequency-inverse document frequency and average embedding."], "semantics"], [["puzzling machines: a challenge on learning from small data", "g\u00f6zde g\u00fcl \u015fahin | yova kementchedjhieva | phillip rust | iryna gurevych", "deep neural models have repeatedly proved excellent at memorizing surface patterns from large datasets for various ml and nlp benchmarks. they struggle to achieve human-like thinking, however, because they lack the skill of iterative reasoning upon knowledge. to expose this problem in a new light, we introduce a challenge on learning from small data, puzzling machines, which consists of rosetta stone puzzles from linguistic olympiads for high school students. these puzzles are carefully designed to contain only the minimal amount of parallel text necessary to deduce the form of unseen expressions. solving them does not require external information (e.g., knowledge bases, visual signals) or linguistic expertise, but meta-linguistic awareness and deductive skills. our challenge contains around 100 puzzles covering a wide range of linguistic phenomena from 81 languages. we show that both simple statistical algorithms and state-of-the-art deep neural models perform inadequately on this challenge, as expected. we hope that this benchmark, available at https://ukplab.github.io/puzzling-machines/, inspires further efforts towards a new paradigm in nlp\u2014one that is grounded in human-like reasoning and understanding."], "resources and evaluation"], [["structured tuning for semantic role labeling", "tao li | parth anand jawale | martha palmer | vivek srikumar", "recent neural network-driven semantic role labeling (srl) systems have shown impressive improvements in f1 scores. these improvements are due to expressive input representations, which, at least at the surface, are orthogonal to knowledge-rich constrained decoding mechanisms that helped linear srl models. introducing the benefits of structure to inform neural models presents a methodological challenge. in this paper, we present a structured tuning framework to improve models using softened constraints only at training time. our framework leverages the expressiveness of neural networks and provides supervision with structured loss components. we start with a strong baseline (roberta) to validate the impact of our approach, and show that our framework outperforms the baseline by learning to comply with declarative constraints. additionally, our experiments with smaller training sizes show that we can achieve consistent improvements under low-resource scenarios."], "semantics"], [["argument invention from first principles", "yonatan bilu | ariel gera | daniel hershcovich | benjamin sznajder | dan lahav | guy moshkowich | anael malet | assaf gavron | noam slonim", "competitive debaters often find themselves facing a challenging task \u2013 how to debate a topic they know very little about, with only minutes to prepare, and without access to books or the internet? what they often do is rely on \u201dfirst principles\u201d, commonplace arguments which are relevant to many topics, and which they have refined in past debates. in this work we aim to explicitly define a taxonomy of such principled recurring arguments, and, given a controversial topic, to automatically identify which of these arguments are relevant to the topic. as far as we know, this is the first time that this approach to argument invention is formalized and made explicit in the context of nlp. the main goal of this work is to show that it is possible to define such a taxonomy. while the taxonomy suggested here should be thought of as a \u201dfirst attempt\u201d it is nonetheless coherent, covers well the relevant topics and coincides with what professional debaters actually argue in their speeches, and facilitates automatic argument invention for new topics."], "sentiment analysis, stylistic analysis, and argument mining"], [["robust neural machine translation with joint textual and phonetic embedding", "hairong liu | mingbo ma | liang huang | hao xiong | zhongjun he", "neural machine translation (nmt) is notoriously sensitive to noises, but noises are almost inevitable in practice. one special kind of noise is the homophone noise, where words are replaced by other words with similar pronunciations. we propose to improve the robustness of nmt to homophone noises by 1) jointly embedding both textual and phonetic information of source sentences, and 2) augmenting the training dataset with homophone noises. interestingly, to achieve better translation quality and more robustness, we found that most (though not all) weights should be put on the phonetic rather than textual information. experiments show that our method not only significantly improves the robustness of nmt to homophone noises, but also surprisingly improves the translation quality on some clean test sets."], "machine translation and multilinguality"], [["eraser: a benchmark to evaluate rationalized nlp models", "jay deyoung | sarthak jain | nazneen fatema rajani | eric lehman | caiming xiong | richard socher | byron c. wallace", "state-of-the-art models in nlp are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. this limitation has increased interest in designing more interpretable deep models for nlp that reveal the \u2018reasoning\u2019 behind model outputs. but work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. we propose the evaluating rationales and simple english reasoning (eraser a benchmark to advance research on interpretable models in nlp. this benchmark comprises multiple datasets and tasks for which human annotations of \u201crationales\u201d (supporting evidence) have been collected. we propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). our hope is that releasing this benchmark facilitates progress on designing more interpretable nlp systems. the benchmark, code, and documentation are available at https://www.eraserbenchmark.com/"], "interpretability and analysis of models for nlp"], [["speakers enhance contextually confusable words", "eric meinhardt | eric bakovic | leon bergen", "recent work has found evidence that natural languages are shaped by pressures for efficient communication \u2014 e.g. the more contextually predictable a word is, the fewer speech sounds or syllables it has (piantadosi et al. 2011). research on the degree to which speech and language are shaped by pressures for effective communication \u2014 robustness in the face of noise and uncertainty \u2014 has been more equivocal. we develop a measure of contextual confusability during word recognition based on psychoacoustic data. applying this measure to naturalistic speech corpora, we find evidence suggesting that speakers alter their productions to make contextually more confusable words easier to understand."], "linguistic theories, cognitive modeling and psycholinguistics"], [["multilingual factor analysis", "francisco vargas | kamen brestnichki | alex papadopoulos korfiatis | nils hammerla", "in this work we approach the task of learning multilingual word representations in an offline manner by fitting a generative latent variable model to a multilingual dictionary. we model equivalent words in different languages as different views of the same word generated by a common latent variable representing their latent lexical meaning. we explore the task of alignment by querying the fitted model for multilingual embeddings achieving competitive results across a variety of tasks. the proposed model is robust to noise in the embedding space making it a suitable method for distributed representations learned from noisy corpora."], "machine translation and multilinguality"], [["image-chat: engaging grounded conversations", "kurt shuster | samuel humeau | antoine bordes | jason weston", "to achieve the long-term goal of machines being able to engage humans in conversation, our models should captivate the interest of their speaking partners. communication grounded in images, whereby a dialogue is conducted based on a given photo, is a setup naturally appealing to humans (hu et al., 2014). in this work we study large-scale architectures and datasets for this goal. we test a set of neural architectures using state-of-the-art image and text representations, considering various ways to fuse the components. to test such models, we collect a dataset of grounded human-human conversations, where speakers are asked to play roles given a provided emotional mood or style, as the use of such traits is also a key factor in engagingness (guo et al., 2019). our dataset, image-chat, consists of 202k dialogues over 202k images using 215 possible style traits. automatic metrics and human evaluations of engagingness show the efficacy of our approach; in particular, we obtain state-of-the-art performance on the existing igc task, and our best performing model is almost on par with humans on the image-chat test set (preferred 47.7% of the time)."], "dialogue and interactive systems"], [["code-switching patterns can be an effective route to improve performance of downstream nlp applications: a case study of humour, sarcasm and hate speech detection", "srijan bansal | vishal garimella | ayush suhane | jasabanta patro | animesh mukherjee", "in this paper, we demonstrate how code-switching patterns can be utilised to improve various downstream nlp applications. in particular, we encode various switching features to improve humour, sarcasm and hate speech detection tasks. we believe that this simple linguistic observation can also be potentially helpful in improving other similar nlp applications."], "computational social science, social media and cultural analytics"], [["coherent comments generation for chinese articles with a graph-to-sequence model", "wei li | jingjing xu | yancheng he | shengli yan | yunfang wu | xu sun", "automatic article commenting is helpful in encouraging user engagement on online news platforms. however, the news documents are usually too long for models under traditional encoder-decoder frameworks, which often results in general and irrelevant comments. in this paper, we propose to generate comments with a graph-to-sequence model that models the input news as a topic interaction graph. by organizing the article into graph structure, our model can better understand the internal structure of the article and the connection between topics, which makes it better able to generate coherent and informative comments. we collect and release a large scale news-comment corpus from a popular chinese online news platform tencent kuaibao. extensive experiment results show that our model can generate much more coherent and informative comments compared with several strong baseline models."], "generation"], [["embarrassingly simple unsupervised aspect extraction", "st\u00e9phan tulkens | andreas van cranenburgh", "we present a simple but effective method for aspect identification in sentiment analysis. our unsupervised method only requires word embeddings and a pos tagger, and is therefore straightforward to apply to new domains and languages. we introduce contrastive attention (cat), a novel single-head attention mechanism based on an rbf kernel, which gives a considerable boost in performance and makes the model interpretable. previous work relied on syntactic features and complex neural models. we show that given the simplicity of current benchmark datasets for aspect extraction, such complex models are not needed. the code to reproduce the experiments reported in this paper is available at https://github.com/clips/cat."], "sentiment analysis, stylistic analysis, and argument mining"], [["multilingual and cross-lingual graded lexical entailment", "ivan vuli\u0107 | simone paolo ponzetto | goran glava\u0161", "grounded in cognitive linguistics, graded lexical entailment (gr-le) is concerned with fine-grained assertions regarding the directional hierarchical relationships between concepts on a continuous scale. in this paper, we present the first work on cross-lingual generalisation of gr-le relation. starting from hyperlex, the only available gr-le dataset in english, we construct new monolingual gr-le datasets for three other languages, and combine those to create a set of six cross-lingual gr-le datasets termed cl-hyperlex. we next present a novel method dubbed clear (cross-lingual lexical entailment attract-repel) for effectively capturing graded (and binary) le, both monolingually in different languages as well as across languages (i.e., on cl-hyperlex). coupled with a bilingual dictionary, clear leverages taxonomic le knowledge in a resource-rich language (e.g., english) and propagates it to other languages. supported by cross-lingual le transfer, clear sets competitive baseline performance on three new monolingual gr-le datasets and six cross-lingual gr-le datasets. in addition, we show that clear outperforms current state-of-the-art on binary cross-lingual le detection by a wide margin for diverse language pairs."], "machine translation and multilinguality"], [["tree lstms with convolution units to predict stance and rumor veracity in social media conversations", "sumeet kumar | kathleen carley", "learning from social-media conversations has gained significant attention recently because of its applications in areas like rumor detection. in this research, we propose a new way to represent social-media conversations as binarized constituency trees that allows comparing features in source-posts and their replies effectively. moreover, we propose to use convolution units in tree lstms that are better at learning patterns in features obtained from the source and reply posts. our tree lstm models employ multi-task (stance + rumor) learning and propagate the useful stance signal up in the tree for rumor classification at the root node. the proposed models achieve state-of-the-art performance, outperforming the current best model by 12% and 15% on f1-macro for rumor-veracity classification and stance classification tasks respectively."], "computational social science, social media and cultural analytics"], [["how does bert\u2019s attention change when you fine-tune? an analysis methodology and a case study in negation scope", "yiyun zhao | steven bethard", "large pretrained language models like bert, after fine-tuning to a downstream task, have achieved high performance on a variety of nlp problems. yet explaining their decisions is difficult despite recent work probing their internal representations. we propose a procedure and analysis methods that take a hypothesis of how a transformer-based model might encode a linguistic phenomenon, and test the validity of that hypothesis based on a comparison between knowledge-related downstream tasks with downstream control tasks, and measurement of cross-dataset consistency. we apply this methodology to test bert and roberta on a hypothesis that some attention heads will consistently attend from a word in negation scope to the negation cue. we find that after fine-tuning bert and roberta on a negation scope task, the average attention head improves its sensitivity to negation and its attention consistency across negation datasets compared to the pre-trained models. however, only the base models (not the large models) improve compared to a control task, indicating there is evidence for a shallow encoding of negation only in the base models."], "interpretability and analysis of models for nlp"], [["is your classifier actually biased? measuring fairness under uncertainty with bernstein bounds", "kawin ethayarajh", "most nlp datasets are not annotated with protected attributes such as gender, making it difficult to measure classification bias using standard measures of fairness (e.g., equal opportunity). however, manually annotating a large dataset with a protected attribute is slow and expensive. instead of annotating all the examples, can we annotate a subset of them and use that sample to estimate the bias? while it is possible to do so, the smaller this annotated sample is, the less certain we are that the estimate is close to the true bias. in this work, we propose using bernstein bounds to represent this uncertainty about the bias estimate as a confidence interval. we provide empirical evidence that a 95% confidence interval derived this way consistently bounds the true bias. in quantifying this uncertainty, our method, which we call bernstein-bounded unfairness, helps prevent classifiers from being deemed biased or unbiased when there is insufficient evidence to make either claim. our findings suggest that the datasets currently used to measure specific biases are too small to conclusively identify bias except in the most egregious cases. for example, consider a co-reference resolution system that is 5% more accurate on gender-stereotypical sentences \u2013 to claim it is biased with 95% confidence, we need a bias-specific dataset that is 3.8 times larger than winobias, the largest available."], "ethics in nlp"], [["multi-source meta transfer for low resource multiple-choice question answering", "ming yan | hao zhang | di jin | joey tianyi zhou", "multiple-choice question answering (mcqa) is one of the most challenging tasks in machine reading comprehension since it requires more advanced reading comprehension skills such as logical reasoning, summarization, and arithmetic operations. unfortunately, most existing mcqa datasets are small in size, which increases the difficulty of model learning and generalization. to address this challenge, we propose a multi-source meta transfer (mmt) for low-resource mcqa. in this framework, we first extend meta learning by incorporating multiple training sources to learn a generalized feature representation across domains. to bridge the distribution gap between training sources and the target, we further introduce the meta transfer that can be integrated into the multi-source meta training. more importantly, the proposed mmt is independent of backbone language models. extensive experiments demonstrate the superiority of mmt over state-of-the-arts, and continuous improvements can be achieved on different backbone networks on both supervised and unsupervised domain adaptation settings."], "question answering"], [["unraveling antonym\u2019s word vectors through a siamese-like network", "mathias etcheverry | dina wonsever", "discriminating antonyms and synonyms is an important nlp task that has the difficulty that both, antonyms and synonyms, contains similar distributional information. consequently, pairs of antonyms and synonyms may have similar word vectors. we present an approach to unravel antonymy and synonymy from word vectors based on a siamese network inspired approach. the model consists of a two-phase training of the same base network: a pre-training phase according to a siamese model supervised by synonyms and a training phase on antonyms through a siamese-like model that supports the antitransitivity present in antonymy. the approach makes use of the claim that the antonyms in common of a word tend to be synonyms. we show that our approach outperforms distributional and pattern-based approaches, relaying on a simple feed forward network as base network of the training phases."], "semantics"], [["multi-grained attention with object-level grounding for visual question answering", "pingping huang | jianhui huang | yuqing guo | min qiao | yong zhu", "attention mechanisms are widely used in visual question answering (vqa) to search for visual clues related to the question. most approaches train attention models from a coarse-grained association between sentences and images, which tends to fail on small objects or uncommon concepts. to address this problem, this paper proposes a multi-grained attention method. it learns explicit word-object correspondence by two types of word-level attention complementary to the sentence-image association. evaluated on the vqa benchmark, the multi-grained attention model achieves competitive performance with state-of-the-art models. and the visualized attention maps demonstrate that addition of object-level groundings leads to a better understanding of the images and locates the attended objects more precisely."], "question answering"], [["doer: dual cross-shared rnn for aspect term-polarity co-extraction", "huaishao luo | tianrui li | bing liu | junbo zhang", "this paper focuses on two related subtasks of aspect-based sentiment analysis, namely aspect term extraction and aspect sentiment classification, which we call aspect term-polarity co-extraction. the former task is to extract aspects of a product or service from an opinion document, and the latter is to identify the polarity expressed in the document about these extracted aspects. most existing algorithms address them as two separate tasks and solve them one by one, or only perform one task, which can be complicated for real applications. in this paper, we treat these two tasks as two sequence labeling problems and propose a novel dual cross-shared rnn framework (doer) to generate all aspect term-polarity pairs of the input sentence simultaneously. specifically, doer involves a dual recurrent neural network to extract the respective representation of each task, and a cross-shared unit to consider the relationship between them. experimental results demonstrate that the proposed framework outperforms state-of-the-art baselines on three benchmark datasets."], "sentiment analysis, stylistic analysis, and argument mining"], [["data manipulation: towards effective instance learning for neural dialogue generation via learning to augment and reweight", "hengyi cai | hongshen chen | yonghao song | cheng zhang | xiaofang zhao | dawei yin", "current state-of-the-art neural dialogue models learn from human conversations following the data-driven paradigm. as such, a reliable training corpus is the crux of building a robust and well-behaved dialogue model. however, due to the open-ended nature of human conversations, the quality of user-generated training data varies greatly, and effective training samples are typically insufficient while noisy samples frequently appear. this impedes the learning of those data-driven neural dialogue models. therefore, effective dialogue learning requires not only more reliable learning samples, but also fewer noisy samples. in this paper, we propose a data manipulation framework to proactively reshape the data distribution towards reliable samples by augmenting and highlighting effective learning samples as well as reducing the effect of inefficient samples simultaneously. in particular, the data manipulation model selectively augments the training samples and assigns an importance weight to each instance to reform the training data. note that, the proposed data manipulation framework is fully data-driven and learnable. it not only manipulates training samples to optimize the dialogue generation model, but also learns to increase its manipulation skills through gradient descent with validation samples. extensive experiments show that our framework can improve the dialogue generation performance with respect to various automatic evaluation metrics and human judgments."], "dialogue and interactive systems"], [["overestimation of syntactic representation in neural language models", "jordan kodner | nitish gupta", "with the advent of powerful neural language models over the last few years, research attention has increasingly focused on what aspects of language they represent that make them so successful. several testing methodologies have been developed to probe models\u2019 syntactic representations. one popular method for determining a model\u2019s ability to induce syntactic structure trains a model on strings generated according to a template then tests the model\u2019s ability to distinguish such strings from superficially similar ones with different syntax. we illustrate a fundamental problem with this approach by reproducing positive results from a recent paper with two non-syntactic baseline language models: an n-gram model and an lstm model trained on scrambled inputs."], "linguistic theories, cognitive modeling and psycholinguistics"], [["modeling word formation in english\u2013german neural machine translation", "marion weller-di marco | alexander fraser", "this paper studies strategies to model word formation in nmt using rich linguistic information, namely a word segmentation approach that goes beyond splitting into substrings by considering fusional morphology. our linguistically sound segmentation is combined with a method for target-side inflection to accommodate modeling word formation. the best system variants employ source-side morphological analysis and model complex target-side words, improving over a standard system."], "machine translation and multilinguality"], [["toxicity detection: does context really matter?", "john pavlopoulos | jeffrey sorensen | lucas dixon | nithum thain | ion androutsopoulos", "moderation is crucial to promoting healthy online discussions. although several \u2018toxicity\u2019 detection datasets and models have been published, most of them ignore the context of the posts, implicitly assuming that comments may be judged independently. we investigate this assumption by focusing on two questions: (a) does context affect the human judgement, and (b) does conditioning on context improve performance of toxicity detection systems? we experiment with wikipedia conversations, limiting the notion of context to the previous post in the thread and the discussion title. we find that context can both amplify or mitigate the perceived toxicity of posts. moreover, a small but significant subset of manually labeled posts (5% in one of our experiments) end up having the opposite toxicity labels if the annotators are not provided with context. surprisingly, we also find no evidence that context actually improves the performance of toxicity classifiers, having tried a range of classifiers and mechanisms to make them context aware. this points to the need for larger datasets of comments annotated in context. we make our code and data publicly available."], "nlp applications"], [["amr parsing with latent structural information", "qiji zhou | yue zhang | donghong ji | hao tang", "abstract meaning representations (amrs) capture sentence-level semantics structural representations to broad-coverage natural sentences. we investigate parsing amr with explicit dependency structures and interpretable latent structures. we generate the latent soft structure without additional annotations, and fuse both dependency and latent structure via an extended graph neural networks. the fused structural information helps our experiments results to achieve the best reported results on both amr 2.0 (77.5% smatch f1 on ldc2017t10) and amr 1.0 ((71.8% smatch f1 on ldc2014t12)."], "semantics"], [["does multi-encoder help? a case study on context-aware neural machine translation", "bei li | hui liu | ziyang wang | yufan jiang | tong xiao | jingbo zhu | tongran liu | changliang li", "in encoder-decoder neural models, multiple encoders are in general used to represent the contextual information in addition to the individual sentence. in this paper, we investigate multi-encoder approaches in document-level neural machine translation (nmt). surprisingly, we find that the context encoder does not only encode the surrounding sentences but also behaves as a noise generator. this makes us rethink the real benefits of multi-encoder in context-aware translation - some of the improvements come from robust training. we compare several methods that introduce noise and/or well-tuned dropout setup into the training of these encoders. experimental results show that noisy training plays an important role in multi-encoder-based nmt, especially when the training data is small. also, we establish a new state-of-the-art on iwslt fr-en task by careful use of noise generation and dropout methods."], "machine translation and multilinguality"], [["speaker sensitive response evaluation model", "jinyeong bak | alice oh", "automatic evaluation of open-domain dialogue response generation is very challenging because there are many appropriate responses for a given context. existing evaluation models merely compare the generated response with the ground truth response and rate many of the appropriate responses as inappropriate if they deviate from the ground truth. one approach to resolve this problem is to consider the similarity of the generated response with the conversational context. in this paper, we propose an automatic evaluation model based on that idea and learn the model parameters from an unlabeled conversation corpus. our approach considers the speakers in defining the different levels of similar context. we use a twitter conversation corpus that contains many speakers and conversations to test our evaluation model. experiments show that our model outperforms the other existing evaluation metrics in terms of high correlation with human annotation scores. we also show that our model trained on twitter can be applied to movie dialogues without any additional training. we provide our code and the learned parameters so that they can be used for automatic evaluation of dialogue response generation models."], "dialogue and interactive systems"], [["improving open information extraction via iterative rank-aware learning", "zhengbao jiang | pengcheng yin | graham neubig", "open information extraction (ie) is the task of extracting open-domain assertions from natural language sentences. a key step in open ie is confidence modeling, ranking the extractions based on their estimated quality to adjust precision and recall of extracted assertions. we found that the extraction likelihood, a confidence measure used by current supervised open ie systems, is not well calibrated when comparing the quality of assertions extracted from different sentences. we propose an additional binary classification loss to calibrate the likelihood to make it more globally comparable, and an iterative learning process, where extractions generated by the open ie model are incrementally included as training samples to help the model learn from trial and error. experiments on oie2016 demonstrate the effectiveness of our method. code and data are available at https://github.com/jzbjyb/oie_rank."], "information extraction, retrieval and text mining"], [["how does selective mechanism improve self-attention networks?", "xinwei geng | longyue wang | xing wang | bing qin | ting liu | zhaopeng tu", "self-attention networks (sans) with selective mechanism has produced substantial improvements in various nlp tasks by concentrating on a subset of input words. however, the underlying reasons for their strong performance have not been well explained. in this paper, we bridge the gap by assessing the strengths of selective sans (ssans), which are implemented with a flexible and universal gumbel-softmax. experimental results on several representative nlp tasks, including natural language inference, semantic role labelling, and machine translation, show that ssans consistently outperform the standard sans. through well-designed probing experiments, we empirically validate that the improvement of ssans can be attributed in part to mitigating two commonly-cited weaknesses of sans: word order encoding and structure modeling. specifically, the selective mechanism improves sans by paying more attention to content words that contribute to the meaning of the sentence."], "machine learning for nlp"], [["improving low-resource cross-lingual document retrieval by reranking with deep bilingual representations", "rui zhang | caitlin westerfield | sungrok shim | garrett bingham | alexander fabbri | william hu | neha verma | dragomir radev", "in this paper, we propose to boost low-resource cross-lingual document retrieval performance with deep bilingual query-document representations. we match queries and documents in both source and target languages with four components, each of which is implemented as a term interaction-based deep neural network with cross-lingual word embeddings as input. by including query likelihood scores as extra features, our model effectively learns to rerank the retrieved documents by using a small number of relevance labels for low-resource language pairs. due to the shared cross-lingual word embedding space, the model can also be directly applied to another language pair without any training label. experimental results on the material dataset show that our model outperforms the competitive translation-based baselines on english-swahili, english-tagalog, and english-somali cross-lingual information retrieval tasks."], "machine translation and multilinguality"], [["diversifying dialog generation via adaptive label smoothing", "yida wang | yinhe zheng | yong jiang | minlie huang", "neural dialogue generation models trained with the one-hot target distribution suffer from the over-confidence issue, which leads to poor generation diversity as widely reported in the literature. although existing approaches such as label smoothing can alleviate this issue, they fail to adapt to diverse dialog contexts. in this paper, we propose an adaptive label smoothing (adalabel) approach that can adaptively estimate a target label distribution at each time step for different contexts. the maximum probability in the predicted distribution is used to modify the soft target distribution produced by a novel light-weight bi-directional decoder module. the resulting target distribution is aware of both previous and future contexts and is adjusted to avoid over-training the dialogue model. our model can be trained in an endto-end manner. extensive experiments on two benchmark datasets show that our approach outperforms various competitive baselines in producing diverse responses."], "dialogue and interactive systems"], [["can generative pre-trained language models serve as knowledge bases for closed-book qa?", "cunxiang wang | pai liu | yue zhang", "recent work has investigated the interesting question using pre-trained language models (plms) as knowledge bases for answering open questions. however, existing work is limited in using small benchmarks with high test-train overlaps. we construct a new dataset of closed-book qa using squad, and investigate the performance of bart. experiments show that it is challenging for bart to remember training facts in high precision, and also challenging to answer closed-book questions even if relevant knowledge is retained. some promising directions are found, including decoupling the knowledge memorizing process and the qa finetune process, forcing the model to recall relevant knowledge when question answering."], "question answering"], [["investigating the effect of auxiliary objectives for the automated grading of learner english speech transcriptions", "hannah craighead | andrew caines | paula buttery | helen yannakoudakis", "we address the task of automatically grading the language proficiency of spontaneous speech based on textual features from automatic speech recognition transcripts. motivated by recent advances in multi-task learning, we develop neural networks trained in a multi-task fashion that learn to predict the proficiency level of non-native english speakers by taking advantage of inductive transfer between the main task (grading) and auxiliary prediction tasks: morpho-syntactic labeling, language modeling, and native language identification (l1). we encode the transcriptions with both bi-directional recurrent neural networks and with bi-directional representations from transformers, compare against a feature-rich baseline, and analyse performance at different proficiency levels and with transcriptions of varying error rates. our best performance comes from a transformer encoder with l1 prediction as an auxiliary task. we discuss areas for improvement and potential applications for text-only speech scoring."], "nlp applications"], [["an effectiveness metric for ordinal classification: formal properties and experimental results", "enrique amigo | julio gonzalo | stefano mizzaro | jorge carrillo-de-albornoz", "in ordinal classification tasks, items have to be assigned to classes that have a relative ordering, such as \u201cpositive\u201d, \u201cneutral\u201d, \u201cnegative\u201d in sentiment analysis. remarkably, the most popular evaluation metrics for ordinal classification tasks either ignore relevant information (for instance, precision/recall on each of the classes ignores their relative ordering) or assume additional information (for instance, mean average error assumes absolute distances between classes). in this paper we propose a new metric for ordinal classification, closeness evaluation measure, that is rooted on measurement theory and information theory. our theoretical analysis and experimental results over both synthetic data and data from nlp shared tasks indicate that the proposed metric captures quality aspects from different traditional tasks simultaneously. in addition, it generalizes some popular classification (nominal scale) and error minimization (interval scale) metrics, depending on the measurement scale in which it is instantiated."], "resources and evaluation"], [["relative importance in sentence processing", "nora hollenstein | lisa beinborn", "determining the relative importance of the elements in a sentence is a key factor for effortless natural language understanding. for human language processing, we can approximate patterns of relative importance by measuring reading fixations using eye-tracking technology. in neural language models, gradient-based saliency methods indicate the relative importance of a token for the target objective. in this work, we compare patterns of relative importance in english language processing by humans and models and analyze the underlying linguistic patterns. we find that human processing patterns in english correlate strongly with saliency-based importance in language models and not with attention-based importance. our results indicate that saliency could be a cognitively more plausible metric for interpreting neural language models. the code is available on github: https://github.com/beinborn/relative_importance."], "interpretability and analysis of models for nlp"], [["on the limitations of cross-lingual encoders as exposed by reference-free machine translation evaluation", "wei zhao | goran glava\u0161 | maxime peyrard | yang gao | robert west | steffen eger", "evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity. in this paper, we concern ourselves with reference-free machine translation (mt) evaluation where we directly compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual encoders. reference-free evaluation holds the promise of web-scale comparison of mt systems. we systematically investigate a range of metrics based on state-of-the-art cross-lingual semantic representations obtained with pretrained m-bert and laser. we find that they perform poorly as semantic encoders for reference-free mt evaluation and identify their two key limitations, namely, (a) a semantic mismatch between representations of mutual translations and, more prominently, (b) the inability to punish \u201ctranslationese\u201d, i.e., low-quality literal translations. we propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling. in segment-level mt evaluation, our best metric surpasses reference-based bleu by 5.7 correlation points."], "machine translation and multilinguality"], [["a retrieve-and-rewrite initialization method for unsupervised machine translation", "shuo ren | yu wu | shujie liu | ming zhou | shuai ma", "the commonly used framework for unsupervised machine translation builds initial translation models of both translation directions, and then performs iterative back-translation to jointly boost their translation performance. the initialization stage is very important since bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance. in this paper, we propose a novel retrieval and rewriting based method to better initialize unsupervised translation models. we first retrieve semantically comparable sentences from monolingual corpora of two languages and then rewrite the target side to minimize the semantic gap between the source and retrieved targets with a designed rewriting model. the rewritten sentence pairs are used to initialize smt models which are used to generate pseudo data for two nmt models, followed by the iterative back-translation. experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 bleu scores. our code is released at https://github.com/imagist-shuo/rrforunmt.git."], "machine translation and multilinguality"], [["large scale multi-actor generative dialog modeling", "alex boyd | raul puri | mohammad shoeybi | mostofa patwary | bryan catanzaro", "non-goal oriented dialog agents (i.e. chatbots) aim to produce varying and engaging conversations with a user; however, they typically exhibit either inconsistent personality across conversations or the average personality of all users. this paper addresses these issues by controlling an agent\u2019s persona upon generation via conditioning on prior conversations of a target actor. in doing so, we are able to utilize more abstract patterns within a person\u2019s speech and better emulate them in generated responses. this work introduces the generative conversation control model, an augmented and fine-tuned gpt-2 language model that conditions on past reference conversations to probabilistically model multi-turn conversations in the actor\u2019s persona. we introduce an accompanying data collection procedure to obtain 10.3m conversations from 6 months worth of reddit comments. we demonstrate that scaling model sizes from 117m to 8.3b parameters yields an improvement from 23.14 to 13.14 perplexity on 1.7m held out reddit conversations. increasing model scale yielded similar improvements in human evaluations that measure preference of model samples to the held out target distribution in terms of realism (31% increased to 37% preference), style matching (37% to 42%), grammar and content quality (29% to 42%), and conversation coherency (32% to 40%). we find that conditionally modeling past conversations improves perplexity by 0.47 in automatic evaluations. through human trials we identify positive trends between conditional modeling and style matching and outline steps to further improve persona control."], "dialogue and interactive systems"], [["unsupervised parallel sentence extraction with parallel segment detection helps machine translation", "viktor hangya | alexander fraser", "mining parallel sentences from comparable corpora is important. most previous work relies on supervised systems, which are trained on parallel data, thus their applicability is problematic in low-resource scenarios. recent developments in building unsupervised bilingual word embeddings made it possible to mine parallel sentences based on cosine similarities of source and target language words. we show that relying only on this information is not enough, since sentences often have similar words but different meanings. we detect continuous parallel segments in sentence pair candidates and rely on them when mining parallel sentences. we show better mining accuracy on three language pairs in a standard shared task on artificial data. we also provide the first experiments showing that parallel sentences mined from real life sources improve unsupervised mt. our code is available, we hope it will be used to support low-resource mt research."], "machine translation and multilinguality"], [["feqa: a question answering evaluation framework for faithfulness assessment in abstractive summarization", "esin durmus | he he | mona diab", "neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful. existing automatic metrics do not capture such mistakes effectively. we tackle the problem of evaluating faithfulness of a generated summary given its source document. we first collected human annotations of faithfulness for outputs from numerous models on two datasets. we find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful. next, we propose an automatic question answering (qa) based metric for faithfulness, feqa, which leverages recent advances in reading comprehension. given question-answer pairs generated from the summary, a qa model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. among metrics based on word overlap, embedding similarity, and learned language understanding models, our qa-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries."], "summarization"], [["sentence meta-embeddings for unsupervised semantic textual similarity", "nina poerner | ulli waltinger | hinrich sch\u00fctze", "we address the task of unsupervised semantic textual similarity (sts) by ensembling diverse pre-trained sentence encoders into sentence meta-embeddings. we apply, extend and evaluate different meta-embedding methods from the word embedding literature at the sentence level, including dimensionality reduction (yin and sch\u00fctze, 2016), generalized canonical correlation analysis (rastogi et al., 2015) and cross-view auto-encoders (bollegala and bao, 2018). our sentence meta-embeddings set a new unsupervised state of the art (sota) on the sts benchmark and on the sts12-sts16 datasets, with gains of between 3.7% and 6.4% pearson\u2019s r over single-source systems."], "semantics"], [["fine-grained spoiler detection from large-scale review corpora", "mengting wan | rishabh misra | ndapa nakashole | julian mcauley", "this paper presents computational approaches for automatically detecting critical plot twists in reviews of media products. first, we created a large-scale book review dataset that includes fine-grained spoiler annotations at the sentence-level, as well as book and (anonymized) user information. second, we carefully analyzed this dataset, and found that: spoiler language tends to be book-specific; spoiler distributions vary greatly across books and review authors; and spoiler sentences tend to jointly appear in the latter part of reviews. third, inspired by these findings, we developed an end-to-end neural network architecture to detect spoiler sentences in review corpora. quantitative and qualitative results demonstrate that the proposed method substantially outperforms existing baselines."], "computational social science, social media and cultural analytics"], [["modeling financial analysts\u2019 decision making via the pragmatics and semantics of earnings calls", "katherine keith | amanda stent", "every fiscal quarter, companies hold earnings calls in which company executives respond to questions from analysts. after these calls, analysts often change their price target recommendations, which are used in equity re- search reports to help investors make deci- sions. in this paper, we examine analysts\u2019 decision making behavior as it pertains to the language content of earnings calls. we identify a set of 20 pragmatic features of analysts\u2019 questions which we correlate with analysts\u2019 pre-call investor recommendations. we also analyze the degree to which semantic and pragmatic features from an earnings call complement market data in predicting analysts\u2019 post-call changes in price targets. our results show that earnings calls are moderately predictive of analysts\u2019 decisions even though these decisions are influenced by a number of other factors including private communication with company executives and market conditions. a breakdown of model errors indicates disparate performance on calls from different market sectors."], "sentiment analysis, stylistic analysis, and argument mining"], [["neural temporal opinion modelling for opinion prediction on twitter", "lixing zhu | yulan he | deyu zhou", "opinion prediction on twitter is challenging due to the transient nature of tweet content and neighbourhood context. in this paper, we model users\u2019 tweet posting behaviour as a temporal point process to jointly predict the posting time and the stance label of the next tweet given a user\u2019s historical tweet sequence and tweets posted by their neighbours. we design a topic-driven attention mechanism to capture the dynamic topic shifts in the neighbourhood context. experimental results show that the proposed model predicts both the posting time and the stance labels of future tweets more accurately compared to a number of competitive baselines."], "computational social science, social media and cultural analytics"], [["multiqa: an empirical investigation of generalization and transfer in reading comprehension", "alon talmor | jonathan berant", "a large number of reading comprehension (rc) datasets has been created recently, but little analysis has been done on whether they generalize to one another, and the extent to which existing datasets can be leveraged for improving performance on new ones. in this paper, we conduct such an investigation over ten rc datasets, training on one or more source rc datasets, and evaluating generalization, as well as transfer to a target rc dataset. we analyze the factors that contribute to generalization, and show that training on a source rc dataset and transferring to a target dataset substantially improves performance, even in the presence of powerful contextual representations from bert (devlin et al., 2019). we also find that training on multiple source rc datasets leads to robust generalization and transfer, and can reduce the cost of example collection for a new rc dataset. following our analysis, we propose multiqa, a bert-based model, trained on multiple rc datasets, which leads to state-of-the-art performance on five rc datasets. we share our infrastructure for the benefit of the research community."], "question answering"], [["textsettr: few-shot text style extraction and tunable targeted restyling", "parker riley | noah constant | mandy guo | girish kumar | david uthus | zarana parekh", "we present a novel approach to the problem of text style transfer. unlike previous approaches requiring style-labeled training data, our method makes use of readily-available unlabeled text by relying on the implicit connection in style between adjacent sentences, and uses labeled data only at inference time. we adapt t5 (raffel et al., 2020), a strong pretrained text-to-text model, to extract a style vector from text and use it to condition the decoder to perform style transfer. as our label-free training results in a style vector space encoding many facets of style, we recast transfers as \u201ctargeted restyling\u201d vector operations that adjust specific attributes of the input while preserving others. we demonstrate that training on unlabeled amazon reviews data results in a model that is competitive on sentiment transfer, even compared to models trained fully on labeled data. furthermore, applying our novel method to a diverse corpus of unlabeled web text results in a single model capable of transferring along multiple dimensions of style (dialect, emotiveness, formality, politeness, sentiment) despite no additional training and using only a handful of exemplars at inference time."], "machine learning for nlp"], [["more diverse dialogue datasets via diversity-informed data collection", "katherine stasaski | grace hui yang | marti a. hearst", "automated generation of conversational dialogue using modern neural architectures has made notable advances. however, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. we introduce a new strategy to address this problem, called diversity-informed data collection. unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpus-level statistics to determine which conversational participants to collect data from. diversity-informed data collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. this method is generalizable and can be used with other corpus-level metrics."], "resources and evaluation"], [["neural news recommendation with long- and short-term user representations", "mingxiao an | fangzhao wu | chuhan wu | kun zhang | zheng liu | xing xie", "personalized news recommendation is important to help users find their interested news and improve reading experience. a key problem in news recommendation is learning accurate user representations to capture their interests. users usually have both long-term preferences and short-term interests. however, existing news recommendation methods usually learn single representations of users, which may be insufficient. in this paper, we propose a neural news recommendation approach which can learn both long- and short-term user representations. the core of our approach is a news encoder and a user encoder. in the news encoder, we learn representations of news from their titles and topic categories, and use attention network to select important words. in the user encoder, we propose to learn long-term user representations from the embeddings of their ids.in addition, we propose to learn short-term user representations from their recently browsed news via gru network. besides, we propose two methods to combine long-term and short-term user representations. the first one is using the long-term user representation to initialize the hidden state of the gru network in short-term user representation. the second one is concatenating both long- and short-term user representations as a unified user vector. extensive experiments on a real-world dataset show our approach can effectively improve the performance of neural news recommendation."], "nlp applications"], [["engine: energy-based inference networks for non-autoregressive machine translation", "lifu tu | richard yuanzhe pang | sam wiseman | kevin gimpel", "we propose to train a non-autoregressive machine translation model to minimize the energy defined by a pretrained autoregressive model. in particular, we view our non-autoregressive translation system as an inference network (tu and gimpel, 2018) trained to minimize the autoregressive teacher energy. this contrasts with the popular approach of training a non-autoregressive model on a distilled corpus consisting of the beam-searched outputs of such a teacher model. our approach, which we call engine (energy-based inference networks), achieves state-of-the-art non-autoregressive results on the iwslt 2014 de-en and wmt 2016 ro-en datasets, approaching the performance of autoregressive models."], "machine translation and multilinguality"], [["latent variable model for multi-modal translation", "iacer calixto | miguel rios | wilker aziz", "in this work, we propose to model the interaction between visual and textual features for multi-modal neural machine translation (mmt) through a latent variable model. this latent variable can be seen as a multi-modal stochastic embedding of an image and its description in a foreign language. it is used in a target-language decoder and also to predict image features. importantly, our model formulation utilises visual and textual inputs during training but does not require that images be available at test time. we show that our latent variable mmt formulation improves considerably over strong baselines, including a multi-task learning approach (elliott and kadar, 2017) and a conditional variational auto-encoder approach (toyama et al., 2016). finally, we show improvements due to (i) predicting image features in addition to only conditioning on them, (ii) imposing a constraint on the kl term to promote models with non-negligible mutual information between inputs and latent variable, and (iii) by training on additional target-language image descriptions (i.e. synthetic data)."], "language grounding to vision, robotics and beyond"], [["a corpus for reasoning about natural language grounded in photographs", "alane suhr | stephanie zhou | ally zhang | iris zhang | huajun bai | yoav artzi", "we introduce a new dataset for joint reasoning about natural language and images, with a focus on semantic diversity, compositionality, and visual reasoning challenges. the data contains 107,292 examples of english sentences paired with web photographs. the task is to determine whether a natural language caption is true about a pair of photographs. we crowdsource the data using sets of visually rich images and a compare-and-contrast task to elicit linguistically diverse language. qualitative analysis shows the data requires compositional joint reasoning, including about quantities, comparisons, and relations. evaluation using state-of-the-art visual reasoning methods shows the data presents a strong challenge."], "language grounding to vision, robotics and beyond"], [["a hierarchical reinforced sequence operation method for unsupervised text style transfer", "chen wu | xuancheng ren | fuli luo | xu sun", "unsupervised text style transfer aims to alter text styles while preserving the content, without aligned data for supervision. existing seq2seq methods face three challenges: 1) the transfer is weakly interpretable, 2) generated outputs struggle in content preservation, and 3) the trade-off between content and style is intractable. to address these challenges, we propose a hierarchical reinforced sequence operation method, named point-then-operate (pto), which consists of a high-level agent that proposes operation positions and a low-level agent that alters the sentence. we provide comprehensive training objectives to control the fluency, style, and content of the outputs and a mask-based inference algorithm that allows for multi-step revision based on the single-step trained agents. experimental results on two text style transfer datasets show that our method significantly outperforms recent methods and effectively addresses the aforementioned challenges."], "generation"], [["the photobook dataset: building common ground through visually-grounded dialogue", "janosch haber | tim baumg\u00e4rtner | ece takmaz | lieke gelderloos | elia bruni | raquel fern\u00e1ndez", "this paper introduces the photobook dataset, a large-scale collection of visually-grounded, task-oriented dialogues in english designed to investigate shared dialogue history accumulating during conversation. taking inspiration from seminal work on dialogue analysis, we propose a data-collection task formulated as a collaborative game prompting two online participants to refer to images utilising both their visual context as well as previously established referring expressions. we provide a detailed description of the task setup and a thorough analysis of the 2,500 dialogues collected. to further illustrate the novel features of the dataset, we propose a baseline model for reference resolution which uses a simple method to take into account shared information accumulated in a reference chain. our results show that this information is particularly important to resolve later descriptions and underline the need to develop more sophisticated models of common ground in dialogue interaction."], "language grounding to vision, robotics and beyond"], [["multilingual unsupervised nmt using shared encoder and language-specific decoders", "sukanta sen | kamal kumar gupta | asif ekbal | pushpak bhattacharyya", "in this paper, we propose a multilingual unsupervised nmt scheme which jointly trains multiple languages with a shared encoder and multiple decoders. our approach is based on denoising autoencoding of each language and back-translating between english and multiple non-english languages. this results in a universal encoder which can encode any language participating in training into an inter-lingual representation, and language-specific decoders. our experiments using only monolingual corpora show that multilingual unsupervised model performs better than the separately trained bilingual models achieving improvement of up to 1.48 bleu points on wmt test sets. we also observe that even if we do not train the network for all possible translation directions, the network is still able to translate in a many-to-many fashion leveraging encoder\u2019s ability to generate interlingual representation."], "machine translation and multilinguality"], [["reranking for neural semantic parsing", "pengcheng yin | graham neubig", "semantic parsing considers the task of transducing natural language (nl) utterances into machine executable meaning representations (mrs). while neural network-based semantic parsers have achieved impressive improvements over previous methods, results are still far from perfect, and cursory manual inspection can easily identify obvious problems such as lack of adequacy or coherence of the generated mrs. this paper presents a simple approach to quickly iterate and improve the performance of an existing neural semantic parser by reranking an n-best list of predicted mrs, using features that are designed to fix observed problems with baseline models. we implement our reranker in a competitive neural semantic parser and test on four semantic parsing (geo, atis) and python code generation (django, conala) tasks, improving the strong baseline parser by up to 5.7% absolute in bleu (conala) and 2.9% in accuracy (django), outperforming the best published neural parser results on all four datasets."], "semantics"], [["a monolingual approach to contextualized word embeddings for mid-resource languages", "pedro javier ortiz su\u00e1rez | laurent romary | beno\u00eet sagot", "we use the multilingual oscar corpus, extracted from common crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (elmo) for five mid-resource languages. we then compare the performance of oscar-based and wikipedia-based elmo embeddings for these languages on the part-of-speech tagging and parsing tasks. we show that, despite the noise in the common-crawl-based oscar data, embeddings trained on oscar perform much better than monolingual embeddings trained on wikipedia. they actually equal or improve the current state of the art in tagging and parsing for all five languages. in particular, they also improve over multilingual wikipedia-based contextual embeddings (multilingual bert), which almost always constitutes the previous state of the art, thereby showing that the benefit of a larger, more diverse corpus surpasses the cross-lingual benefit of multilingual embedding architectures."], "resources and evaluation"], [["confusionset-guided pointer networks for chinese spelling check", "dingmin wang | yi tay | li zhong", "this paper proposes confusionset-guided pointer networks for chinese spell check (csc) task. more concretely, our approach utilizes the off-the-shelf confusionset for guiding the character generation. to this end, our novel seq2seq model jointly learns to copy a correct character from an input sentence through a pointer network, or generate a character from the confusionset rather than the entire vocabulary. we conduct experiments on three human-annotated datasets, and results demonstrate that our proposed generative model outperforms all competitor models by a large margin of up to 20% f1 score, achieving state-of-the-art performance on three datasets."], "resources and evaluation"], [["crossing variational autoencoders for answer retrieval", "wenhao yu | lingfei wu | qingkai zeng | shu tao | yu deng | meng jiang", "answer retrieval is to find the most aligned answer from a large set of candidates given a question. learning vector representations of questions/answers is the key factor. question-answer alignment and question/answer semantics are two important signals for learning the representations. existing methods learned semantic representations with dual encoders or dual variational auto-encoders. the semantic information was learned from language models or question-to-question (answer-to-answer) generative processes. however, the alignment and semantics were too separate to capture the aligned semantics between question and answer. in this work, we propose to cross variational auto-encoders by generating questions with aligned answers and generating answers with aligned questions. experiments show that our method outperforms the state-of-the-art answer retrieval method on squad."], "question answering"], [["zero-shot entity linking by reading entity descriptions", "lajanugen logeswaran | ming-wei chang | kenton lee | kristina toutanova | jacob devlin | honglak lee", "we present the zero-shot entity linking task, where mentions must be linked to unseen entities without in-domain labeled data. the goal is to enable robust transfer to highly specialized domains, and so no metadata or alias tables are assumed. in this setting, entities are only identified by text descriptions, and models must rely strictly on language understanding to resolve the new entities. first, we show that strong reading comprehension models pre-trained on large unlabeled data can be used to generalize to unseen entities. second, we propose a simple and effective adaptive pre-training strategy, which we term domain-adaptive pre-training (dap), to address the domain shift problem associated with linking unseen entities in a new domain. we present experiments on a new dataset that we construct for this task and show that dap improves over strong pre-training baselines, including bert. the data and code are available at https://github.com/lajanugen/zeshel."], "semantics"], [["a unified linear-time framework for sentence-level discourse parsing", "xiang lin | shafiq joty | prathyusha jwalapuram | m saiful bari", "we propose an efficient neural framework for sentence-level discourse analysis in accordance with rhetorical structure theory (rst). our framework comprises a discourse segmenter to identify the elementary discourse units (edu) in a text, and a discourse parser that constructs a discourse tree in a top-down fashion. both the segmenter and the parser are based on pointer networks and operate in linear time. our segmenter yields an f1 score of 95.4%, and our parser achieves an f1 score of 81.7% on the aggregated labeled (relation) metric, surpassing previous approaches by a good margin and approaching human agreement on both tasks (98.3 and 83.0 f1)."], "discourse and pragmatics"], [["modeling intra-relation in math word problems with different functional multi-head attentions", "jierui li | lei wang | jipeng zhang | yan wang | bing tian dai | dongxiang zhang", "several deep learning models have been proposed for solving math word problems (mwps) automatically. although these models have the ability to capture features without manual efforts, their approaches to capturing features are not specifically designed for mwps. to utilize the merits of deep learning models with simultaneous consideration of mwps\u2019 specific features, we propose a group attention mechanism to extract global features, quantity-related features, quantity-pair features and question-related features in mwps respectively. the experimental results show that the proposed approach performs significantly better than previous state-of-the-art methods, and boost performance from 66.9% to 69.5% on math23k with training-test split, from 65.8% to 66.9% on math23k with 5-fold cross-validation and from 69.2% to 76.1% on mawps."], "question answering"], [["graph-to-tree learning for solving math word problems", "jipeng zhang | lei wang | roy ka-wei lee | yi bin | yan wang | jie shao | ee-peng lim", "while the recent tree-based neural models have demonstrated promising results in generating solution expression for the math word problem (mwp), most of these models do not capture the relationships and order information among the quantities well. this results in poor quantity representations and incorrect solution expressions. in this paper, we propose graph2tree, a novel deep learning architecture that combines the merits of the graph-based encoder and tree-based decoder to generate better solution expressions. included in our graph2tree framework are two graphs, namely the quantity cell graph and quantity comparison graph, which are designed to address limitations of existing methods by effectively representing the relationships and order information among the quantities in mwps. we conduct extensive experiments on two available datasets. our experiment results show that graph2tree outperforms the state-of-the-art baselines on two benchmark datasets significantly. we also discuss case studies and empirically examine graph2tree\u2019s effectiveness in translating the mwp text into solution expressions."], "question answering"], [["learning efficient dialogue policy from demonstrations through shaping", "huimin wang | baolin peng | kam-fai wong", "training a task-oriented dialogue agent with reinforcement learning is prohibitively expensive since it requires a large volume of interactions with users. human demonstrations can be used to accelerate learning progress. however, how to effectively leverage demonstrations to learn dialogue policy remains less explored. in this paper, we present s\u02c62agent that efficiently learns dialogue policy from demonstrations through policy shaping and reward shaping. we use an imitation model to distill knowledge from demonstrations, based on which policy shaping estimates feedback on how the agent should act in policy space. reward shaping is then incorporated to bonus state-actions similar to demonstrations explicitly in value space encouraging better exploration. the effectiveness of the proposed s\u02c62agentt is demonstrated in three dialogue domains and a challenging domain adaptation task with both user simulator evaluation and human evaluation."], "dialogue and interactive systems"], [["maam: a morphology-aware alignment model for unsupervised bilingual lexicon induction", "pengcheng yang | fuli luo | peng chen | tianyu liu | xu sun", "the task of unsupervised bilingual lexicon induction (ubli) aims to induce word translations from monolingual corpora in two languages. previous work has shown that morphological variation is an intractable challenge for the ubli task, where the induced translation in failure case is usually morphologically related to the correct translation. to tackle this challenge, we propose a morphology-aware alignment model for the ubli task. the proposed model aims to alleviate the adverse effect of morphological variation by introducing grammatical information learned by the pre-trained denoising language model. results show that our approach can substantially outperform several state-of-the-art unsupervised systems, and even achieves competitive performance compared to supervised methods."], "machine translation and multilinguality"], [["learning emphasis selection for written text in visual media from crowd-sourced label distributions", "amirreza shirani | franck dernoncourt | paul asente | nedim lipka | seokhwan kim | jose echevarria | thamar solorio", "in visual communication, text emphasis is used to increase the comprehension of written text to convey the author\u2019s intent. we study the problem of emphasis selection, i.e. choosing candidates for emphasis in short written text, to enable automated design assistance in authoring. without knowing the author\u2019s intent and only considering the input text, multiple emphasis selections are valid. we propose a model that employs end-to-end label distribution learning (ldl) on crowd-sourced data and predicts a selection distribution, capturing the inter-subjectivity (common-sense) in the audience as well as the ambiguity of the input. we compare the model with several baselines in which the problem is transformed to single-label learning by mapping label distributions to absolute labels via majority voting."], "nlp applications"], [["birre: learning bidirectional residual relation embeddings for supervised hypernymy detection", "chengyu wang | xiaofeng he", "the hypernymy detection task has been addressed under various frameworks. previously, the design of unsupervised hypernymy scores has been extensively studied. in contrast, supervised classifiers, especially distributional models, leverage the global contexts of terms to make predictions, but are more likely to suffer from \u201clexical memorization\u201d. in this work, we revisit supervised distributional models for hypernymy detection. rather than taking embeddings of two terms as classification inputs, we introduce a representation learning framework named bidirectional residual relation embeddings (birre). in this model, a term pair is represented by a birre vector as features for hypernymy classification, which models the possibility of a term being mapped to another in the embedding space by hypernymy relations. a latent projection model with negative regularization (lpmnr) is proposed to simulate how hypernyms and hyponyms are generated by neural language models, and to generate birre vectors based on bidirectional residuals of projections. experiments verify birre outperforms strong baselines over various evaluation frameworks."], "semantics"], [["retrieving sequential information for non-autoregressive neural machine translation", "chenze shao | yang feng | jinchao zhang | fandong meng | xilin chen | jie zhou", "non-autoregressive transformer (nat) aims to accelerate the transformer model through discarding the autoregressive mechanism and generating target words independently, which fails to exploit the target sequential information. over-translation and under-translation errors often occur for the above reason, especially in the long sentence translation scenario. in this paper, we propose two approaches to retrieve the target sequential information for nat to enhance its translation ability while preserving the fast-decoding property. firstly, we propose a sequence-level training method based on a novel reinforcement algorithm for nat (reinforce-nat) to reduce the variance and stabilize the training procedure. secondly, we propose an innovative transformer decoder named fs-decoder to fuse the target sequential information into the top layer of the decoder. experimental results on three translation tasks show that the reinforce-nat surpasses the baseline nat system by a significant margin on bleu without decelerating the decoding speed and the fs-decoder achieves comparable translation performance to the autoregressive transformer with considerable speedup."], "machine translation and multilinguality"], [["adversarial multitask learning for joint multi-feature and multi-dialect morphological modeling", "nasser zalmout | nizar habash", "morphological tagging is challenging for morphologically rich languages due to the large target space and the need for more training data to minimize model sparsity. dialectal variants of morphologically rich languages suffer more as they tend to be more noisy and have less resources. in this paper we explore the use of multitask learning and adversarial training to address morphological richness and dialectal variations in the context of full morphological tagging. we use multitask learning for joint morphological modeling for the features within two dialects, and as a knowledge-transfer scheme for cross-dialectal modeling. we use adversarial training to learn dialect invariant features that can help the knowledge-transfer scheme from the high to low-resource variants. we work with two dialectal variants: modern standard arabic (high-resource \u201cdialect\u2019\u201d) and egyptian arabic (low-resource dialect) as a case study. our models achieve state-of-the-art results for both. furthermore, adversarial training provides more significant improvement when using smaller training datasets in particular."], "machine translation and multilinguality"], [["layoutlmv2: multi-modal pre-training for visually-rich document understanding", "yang xu | yiheng xu | tengchao lv | lei cui | furu wei | guoxin wang | yijuan lu | dinei florencio | cha zhang | wanxiang che | min zhang | lidong zhou", "pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. we propose layoutlmv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. specifically, with a two-stream multi-modal transformer encoder, layoutlmv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. meanwhile, it also integrates a spatial-aware self-attention mechanism into the transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. experiment results show that layoutlmv2 outperforms layoutlm by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including funsd (0.7895 to 0.8420), cord (0.9493 to 0.9601), sroie (0.9524 to 0.9781), kleister-nda (0.8340 to 0.8520), rvl-cdip (0.9443 to 0.9564), and docvqa (0.7295 to 0.8672)."], "speech and multimodality"], [["decompositional argument mining: a general purpose approach for argument graph construction", "debela gemechu | chris reed", "this work presents an approach decomposing propositions into four functional components and identify the patterns linking those components to determine argument structure. the entities addressed by a proposition are target concepts and the features selected to make a point about the target concepts are aspects. a line of reasoning is followed by providing evidence for the points made about the target concepts via aspects. opinions on target concepts and opinions on aspects are used to support or attack the ideas expressed by target concepts and aspects. the relations between aspects, target concepts, opinions on target concepts and aspects are used to infer the argument relations. propositions are connected iteratively to form a graph structure. the approach is generic in that it is not tuned for a specific corpus and evaluated on three different corpora from the literature: aaec, amt, us2016g1tv and achieved an f score of 0.79, 0.77 and 0.64, respectively."], "sentiment analysis, stylistic analysis, and argument mining"], [["demographics should not be the reason of toxicity: mitigating discrimination in text classifications with instance weighting", "guanhua zhang | bing bai | junqi zhang | kun bai | conghui zhu | tiejun zhao", "with the recent proliferation of the use of text classifications, researchers have found that there are certain unintended biases in text classification datasets. for example, texts containing some demographic identity-terms (e.g., \u201cgay\u201d, \u201cblack\u201d) are more likely to be abusive in existing abusive language detection datasets. as a result, models trained with these datasets may consider sentences like \u201cshe makes me happy to be gay\u201d as abusive simply because of the word \u201cgay.\u201d in this paper, we formalize the unintended biases in text classification datasets as a kind of selection bias from the non-discrimination distribution to the discrimination distribution. based on this formalization, we further propose a model-agnostic debiasing training framework by recovering the non-discrimination distribution using instance weighting, which does not require any extra resources or annotations apart from a pre-defined set of demographic identity-terms. experiments demonstrate that our method can effectively alleviate the impacts of the unintended biases without significantly hurting models\u2019 generalization ability."], "ethics in nlp"], [["multi-domain dialogue acts and response co-generation", "kai wang | junfeng tian | rui wang | xiaojun quan | jianxing yu", "generating fluent and informative responses is of critical importance for task-oriented dialogue systems. existing pipeline approaches generally predict multiple dialogue acts first and use them to assist response generation. there are at least two shortcomings with such approaches. first, the inherent structures of multi-domain dialogue acts are neglected. second, the semantic associations between acts and responses are not taken into account for response generation. to address these issues, we propose a neural co-generation model that generates dialogue acts and responses concurrently. unlike those pipeline approaches, our act generation module preserves the semantic structures of multi-domain dialogue acts and our response generation module dynamically attends to different acts as needed. we train the two modules jointly using an uncertainty loss to adjust their task weights adaptively. extensive experiments are conducted on the large-scale multiwoz dataset and the results show that our model achieves very favorable improvement over several state-of-the-art models in both automatic and human evaluations."], "dialogue and interactive systems"], [["plato: pre-trained dialogue generation model with discrete latent variable", "siqi bao | huang he | fan wang | hua wu | haifeng wang", "pre-training models have been proved effective for a wide range of natural language processing tasks. inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering. in this framework, we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. we also introduce discrete latent variables to tackle the inherent one-to-many mapping problem in response generation. two reciprocal tasks of response generation and latent act recognition are designed and carried out simultaneously within a shared network. comprehensive experiments on three publicly available datasets verify the effectiveness and superiority of the proposed framework."], "dialogue and interactive systems"], [["conversational graph grounded policy learning for open-domain conversation generation", "jun xu | haifeng wang | zheng-yu niu | hua wu | wanxiang che | ting liu", "to address the challenge of policy learning in open-domain multi-turn conversation, we propose to represent prior information about dialog transitions as a graph and learn a graph grounded dialog policy, aimed at fostering a more coherent and controllable dialog. to this end, we first construct a conversational graph (cg) from dialog corpora, in which there are vertices to represent \u201cwhat to say\u201d and \u201chow to say\u201d, and edges to represent natural transition between a message (the last utterance in a dialog context) and its response. we then present a novel cg grounded policy learning framework that conducts dialog flow planning by graph traversal, which learns to identify a what-vertex and a how-vertex from the cg at each turn to guide response generation. in this way, we effectively leverage the cg to facilitate policy learning as follows: (1) it enables more effective long-term reward design, (2) it provides high-quality candidate actions, and (3) it gives us more control over the policy. results on two benchmark corpora demonstrate the effectiveness of this framework."], "dialogue and interactive systems"], [["learning to contextually aggregate multi-source supervision for sequence labeling", "ouyu lan | xiao huang | bill yuchen lin | he jiang | liyuan liu | xiang ren", "sequence labeling is a fundamental task for a range of natural language processing problems. when used in practice, its performance is largely influenced by the annotation quality and quantity, and meanwhile, obtaining ground truth labels is often costly. in many cases, ground truth labels do not exist, but noisy annotations or annotations from different domains are accessible. in this paper, we propose a novel framework consensus network (connet) that can be trained on annotations from multiple sources (e.g., crowd annotation, cross-domain data). it learns individual representation for every source and dynamically aggregates source-specific knowledge by a context-aware attention module. finally, it leads to a model reflecting the agreement (consensus) among multiple sources. we evaluate the proposed framework in two practical settings of multi-source learning: learning with crowd annotations and unsupervised cross-domain model adaptation. extensive experimental results show that our model achieves significant improvements over existing methods in both settings. we also demonstrate that the method can apply to various tasks and cope with different encoders."], "machine learning for nlp"], [["generating question relevant captions to aid visual question answering", "jialin wu | zeyuan hu | raymond mooney", "visual question answering (vqa) and image captioning require a shared body of general knowledge connecting language and vision. we present a novel approach to better vqa performance that exploits this connection by jointly generating captions that are targeted to help answer a specific visual question. the model is trained using an existing caption dataset by automatically determining question-relevant captions using an online gradient-based method. experimental results on the vqa v2 challenge demonstrates that our approach obtains state-of-the-art vqa performance (e.g. 68.4% in the test-standard set using a single model) by simultaneously generating question-relevant captions."], "question answering"], [["unsupervised domain clusters in pretrained language models", "roee aharoni | yoav goldberg", "the notion of \u201cin-domain data\u201d in nlp is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality. in addition, domain labels are many times unavailable, making it challenging to build domain-specific systems. we show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision \u2013 suggesting a simple data-driven definition of domains in textual data. we harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data. we evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both bleu and precision and recall with respect to an oracle selection."], "machine translation and multilinguality"], [["zero-shot text classification via reinforced self-training", "zhiquan ye | yuxia geng | jiaoyan chen | jingmin chen | xiaoxiao xu | suhang zheng | feng wang | jun zhang | huajun chen", "zero-shot learning has been a tough problem since no labeled data is available for unseen classes during training, especially for classes with low similarity. in this situation, transferring from seen classes to unseen classes is extremely hard. to tackle this problem, in this paper we propose a self-training based method to efficiently leverage unlabeled data. traditional self-training methods use fixed heuristics to select instances from unlabeled data, whose performance varies among different datasets. we propose a reinforcement learning framework to learn data selection strategy automatically and provide more reliable selection. experimental results on both benchmarks and a real-world e-commerce dataset show that our approach significantly outperforms previous methods in zero-shot text classification"], "machine learning for nlp"], [["\u201cyou sound just like your father\u201d commercial machine translation systems include stylistic biases", "dirk hovy | federico bianchi | tommaso fornaciari", "the main goal of machine translation has been to convey the correct content. stylistic considerations have been at best secondary. we show that as a consequence, the output of three commercial machine translation systems (bing, deepl, google) make demographically diverse samples from five languages \u201csound\u201d older and more male than the original. our findings suggest that translation models reflect demographic bias in the training data. this opens up interesting new research avenues in machine translation to take stylistic considerations into account."], "machine translation and multilinguality"], [["an investigation of transfer learning-based sentiment analysis in japanese", "enkhbold bataa | joshua wu", "text classification approaches have usually required task-specific model architectures and huge labeled datasets. recently, thanks to the rise of text-based transfer learning techniques, it is possible to pre-train a language model in an unsupervised manner and leverage them to perform effective on downstream tasks. in this work we focus on japanese and show the potential use of transfer learning techniques in text classification. specifically, we perform binary and multi-class sentiment classification on the rakuten product review and yahoo movie review datasets. we show that transfer learning-based approaches perform better than task-specific models trained on 3 times as much data. furthermore, these approaches perform just as well for language modeling pre-trained on only 1/30 of the data. we release our pre-trained models and code as open source."], "sentiment analysis, stylistic analysis, and argument mining"], [["incremental transformer with deliberation decoder for document grounded conversations", "zekang li | cheng niu | fandong meng | yang feng | qian li | jie zhou", "document grounded conversations is a task to generate dialogue responses when chatting about the content of a given document. obviously, document knowledge plays a critical role in document grounded conversations, while existing dialogue models do not exploit this kind of knowledge effectively enough. in this paper, we propose a novel transformer-based architecture for multi-turn document grounded conversations. in particular, we devise an incremental transformer to encode multi-turn utterances along with knowledge in related documents. motivated by the human cognitive process, we design a two-pass decoder (deliberation decoder) to improve context coherence and knowledge correctness. our empirical study on a real-world document grounded dataset proves that responses generated by our model significantly outperform competitive baselines on both context coherence and knowledge relevance."], "dialogue and interactive systems"], [["asking and answering questions to evaluate the factual consistency of summaries", "alex wang | kyunghyun cho | mike lewis", "practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input. existing automatic evaluation metrics for summarization are largely insensitive to such errors. we propose qags (pronounced \u201ckags\u201d), an automatic evaluation protocol that is designed to identify factual inconsistencies in a generated summary. qags is based on the intuition that if we ask questions about a summary and its source, we will receive similar answers if the summary is factually consistent with the source. to evaluate qags, we collect human judgments of factual consistency on model-generated summaries for the cnn/dailymail (hermann et al., 2015) and xsum (narayan et al., 2018) summarization datasets. qags has substantially higher correlations with these judgments than other automatic evaluation metrics. also, qags offers a natural form of interpretability: the answers and questions generated while computing qags indicate which tokens of a summary are inconsistent and why. we believe qags is a promising tool in automatically generating usable and factually consistent text. code for qags will be available at https://github.com/w4ngatang/qags."], "summarization"], [["on the inference calibration of neural machine translation", "shuo wang | zhaopeng tu | shuming shi | yang liu", "confidence calibration, which aims to make model predictions equal to the true correctness measures, is important for neural machine translation (nmt) because it is able to offer useful indicators of translation errors in the generated output. while prior studies have shown that nmt models trained with label smoothing are well-calibrated on the ground-truth training data, we find that miscalibration still remains a severe challenge for nmt during inference due to the discrepancy between training and inference. by carefully designing experiments on three language pairs, our work provides in-depth analyses of the correlation between calibration and translation performance as well as linguistic properties of miscalibration and reports a number of interesting findings that might help humans better analyze, understand and improve nmt models. based on these observations, we further propose a new graduated label smoothing method that can improve both inference calibration and translation performance."], "machine translation and multilinguality"], [["bigpatent: a large-scale dataset for abstractive and coherent summarization", "eva sharma | chen li | lu wang", "most existing text summarization datasets are compiled from the news domain, where summaries have a flattened discourse structure. in such datasets, summary-worthy content often appears in the beginning of input articles. moreover, large segments from input articles are present verbatim in their respective summaries. these issues impede the learning and evaluation of systems that can understand an article\u2019s global content structure as well as produce abstractive summaries with high compression ratio. in this work, we present a novel dataset, bigpatent, consisting of 1.3 million records of u.s. patent documents along with human written abstractive summaries. compared to existing summarization datasets, bigpatent has the following properties: i) summaries contain a richer discourse structure with more recurring entities, ii) salient content is evenly distributed in the input, and iii) lesser and shorter extractive fragments are present in the summaries. finally, we train and evaluate baselines and popular learning models on bigpatent to shed light on new challenges and motivate future directions for summarization research."], "summarization"], [["a corpus for large-scale phonetic typology", "elizabeth salesky | eleanor chodroff | tiago pimentel | matthew wiesner | ryan cotterell | alan w black | jason eisner", "a major hurdle in data-driven research on typology is having sufficient data in many languages to draw meaningful conclusions. we present voxclamantis v1.0, the first large-scale corpus for phonetic typology, with aligned segments and estimated phoneme-level labels in 690 readings spanning 635 languages, along with acoustic-phonetic measures of vowels and sibilants. access to such data can greatly facilitate investigation of phonetic typology at a large scale and across many languages. however, it is non-trivial and computationally intensive to obtain such alignments for hundreds of languages, many of which have few to no resources presently available. we describe the methodology to create our corpus, discuss caveats with current methods and their impact on the utility of this data, and illustrate possible research directions through a series of case studies on the 48 highest-quality readings. our corpus and scripts are publicly available for non-commercial use at https://voxclamantisproject.github.io."], "resources and evaluation"], [["sequence-to-nuggets: nested entity mention detection via anchor-region networks", "hongyu lin | yaojie lu | xianpei han | le sun", "sequential labeling-based ner approaches restrict each word belonging to at most one entity mention, which will face a serious problem when recognizing nested entity mentions. in this paper, we propose to resolve this problem by modeling and leveraging the head-driven phrase structures of entity mentions, i.e., although a mention can nest other mentions, they will not share the same head word. specifically, we propose anchor-region networks (arns), a sequence-to-nuggets architecture for nested mention detection. arns first identify anchor words (i.e., possible head words) of all mentions, and then recognize the mention boundaries for each anchor word by exploiting regular phrase structures. furthermore, we also design bag loss, an objective function which can train arns in an end-to-end manner without using any anchor word annotation. experiments show that arns achieve the state-of-the-art performance on three standard nested entity mention detection benchmarks."], "information extraction, retrieval and text mining"], [["n-best asr transformer: enhancing slu performance using multiple asr hypotheses", "karthik ganesan | pakhi bamdev | jaivarsan b | amresh venugopal | abhinav tushar", "spoken language understanding (slu) systems parse speech into semantic structures like dialog acts and slots. this involves the use of an automatic speech recognizer (asr) to transcribe speech into multiple text alternatives (hypotheses). transcription errors, ordinary in asrs, impact downstream slu performance negatively. common approaches to mitigate such errors involve using richer information from the asr, either in form of n-best hypotheses or word-lattices. we hypothesize that transformer models will learn better with a simpler utterance representation using the concatenation of the n-best asr alternatives, where each alternative is separated by a special delimiter [sep]. in our work, we test our hypothesis by using the concatenated n-best asr alternatives as the input to the transformer encoder models, namely bert and xlm-roberta, and achieve equivalent performance to the prior state-of-the-art model on dstc2 dataset. we also show that our approach significantly outperforms the prior state-of-the-art when subjected to the low data regime. additionally, this methodology is accessible to users of third-party asr apis which do not provide word-lattice information."], "dialogue and interactive systems"], [["tapas: weakly supervised table parsing via pre-training", "jonathan herzig | pawel krzysztof nowak | thomas m\u00fcller | francesco piccinno | julian eisenschlos", "answering natural language questions over tables is usually seen as a semantic parsing task. to alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. however, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. in this paper, we present tapas, an approach to question answering over tables without generating logical forms. tapas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. tapas extends bert\u2019s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from wikipedia, and is trained end-to-end. we experiment with three different semantic parsing datasets, and find that tapas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on sqa from 55.1 to 67.2 and performing on par with the state-of-the-art on wikisql and wikitq, but with a simpler model architecture. we additionally find that transfer learning, which is trivial in our setting, from wikisql to wikitq, yields 48.7 accuracy, 4.2 points above the state-of-the-art."], "semantics"], [["combating adversarial misspellings with robust word recognition", "danish pruthi | bhuwan dhingra | zachary c. lipton", "to combat adversarial spelling mistakes, we propose placing a word recognition model in front of the downstream classifier. our word recognition models build upon the rnn semi-character architecture, introducing several new backoff strategies for handling rare and unseen words. trained to recognize words corrupted by random adds, drops, swaps, and keyboard mistakes, our method achieves 32% relative (and 3.3% absolute) error reduction over the vanilla semi-character model. notably, our pipeline confers robustness on the downstream classifier, outperforming both adversarial training and off-the-shelf spell checkers. against a bert model fine-tuned for sentiment analysis, a single adversarially-chosen character attack lowers accuracy from 90.3% to 45.8%. our defense restores accuracy to 75%. surprisingly, better word recognition does not always entail greater robustness. our analysis reveals that robustness also depends upon a quantity that we denote the sensitivity."], "machine learning for nlp"], [["multimodal quality estimation for machine translation", "shu okabe | fr\u00e9d\u00e9ric blain | lucia specia", "we propose approaches to quality estimation (qe) for machine translation that explore both text and visual modalities for multimodal qe. we compare various multimodality integration and fusion strategies. for both sentence-level and document-level predictions, we show that state-of-the-art neural and feature-based qe frameworks obtain better results when using the additional modality."], "resources and evaluation"], [["every document owns its structure: inductive text classification via graph neural networks", "yufeng zhang | xueli yu | zeyu cui | shu wu | zhongzhen wen | liang wang", "text classification is fundamental in natural language processing (nlp) and graph neural networks (gnn) are recently applied in this task. however, the existing graph-based works can neither capture the contextual word relationships within each document nor fulfil the inductive learning of new words. therefore in this work, to overcome such problems, we propose texting for inductive text classification via gnn. we first build individual graphs for each document and then use gnn to learn the fine-grained word representations based on their local structure, which can also effectively produce embeddings for unseen words in the new document. finally, the word nodes are aggregated as the document embedding. extensive experiments on four benchmark datasets show that our method outperforms state-of-the-art text classification methods."], "information extraction, retrieval and text mining"], [["leveraging monolingual data with self-supervision for multilingual neural machine translation", "aditya siddhant | ankur bapna | yuan cao | orhan firat | mia chen | sneha kudugunta | naveen arivazhagan | yonghui wu", "over the last few years two promising research directions in low-resource neural machine translation (nmt) have emerged. the first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual nmt. the second direction employs monolingual data with self-supervision to pre-train translation models, followed by fine-tuning on small amounts of supervised data. in this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision in multilingual nmt. we offer three major results: (i) using monolingual data significantly boosts the translation quality of low-resource languages in multilingual models. (ii) self-supervision improves zero-shot translation quality in multilingual models. (iii) leveraging monolingual data with self-supervision provides a viable path towards adding new languages to multilingual models, getting up to 33 bleu on ro-en translation without any parallel data or back-translation."], "machine translation and multilinguality"], [["tangled up in bleu: reevaluating the evaluation of automatic machine translation evaluation metrics", "nitika mathur | timothy baldwin | trevor cohn", "automatic metrics are fundamental for the development and evaluation of machine translation systems. judging whether, and to what extent, automatic metrics concur with the gold standard of human evaluation is not a straightforward problem. we show that current methods for judging metrics are highly sensitive to the translations used for assessment, particularly the presence of outliers, which often leads to falsely confident conclusions about a metric\u2019s efficacy. finally, we turn to pairwise system ranking, developing a method for thresholding performance improvement under an automatic metric against human judgements, which allows quantification of type i versus type ii errors incurred, i.e., insignificant human differences in system quality that are accepted, and significant human differences that are rejected. together, these findings suggest improvements to the protocols for metric evaluation and system performance evaluation in machine translation."], "resources and evaluation"], [["let me choose: from verbal context to font selection", "amirreza shirani | franck dernoncourt | jose echevarria | paul asente | nedim lipka | thamar solorio", "in this paper, we aim to learn associations between visual attributes of fonts and the verbal context of the texts they are typically applied to. compared to related work leveraging the surrounding visual context, we choose to focus only on the input text, which can enable new applications for which the text is the only visual element in the document. we introduce a new dataset, containing examples of different topics in social media posts and ads, labeled through crowd-sourcing. due to the subjective nature of the task, multiple fonts might be perceived as acceptable for an input text, which makes this problem challenging. to this end, we investigate different end-to-end models to learn label distributions on crowd-sourced data, to capture inter-subjectivity across all annotations."], "nlp applications"], [["recognising agreement and disagreement between stances with reason comparing networks", "chang xu | cecile paris | surya nepal | ross sparks", "we identify agreement and disagreement between utterances that express stances towards a topic of discussion. existing methods focus mainly on conversational settings, where dialogic features are used for (dis)agreement inference. we extend this scope and seek to detect stance (dis)agreement in a broader setting, where independent stance-bearing utterances, which prevail in many stance corpora and real-world scenarios, are compared. to cope with such non-dialogic utterances, we find that the reasons uttered to back up a specific stance can help predict stance (dis)agreements. we propose a reason comparing network (rcn) to leverage reason information for stance comparison. empirical results on a well-known stance corpus show that our method can discover useful reason information, enabling it to outperform several baselines in stance (dis)agreement detection."], "sentiment analysis, stylistic analysis, and argument mining"], [["sherliic: a typed event-focused lexical inference benchmark for evaluating natural language inference", "martin schmitt | hinrich sch\u00fctze", "we present sherliic, a testbed for lexical inference in context (liic), consisting of 3985 manually annotated inference rule candidates (infcands), accompanied by (i) ~960k unlabeled infcands, and (ii) ~190k typed textual relations between freebase entities extracted from the large entity-linked corpus clueweb09. each infcand consists of one of these relations, expressed as a lemmatized dependency path, and two argument placeholders, each linked to one or more freebase types. due to our candidate selection process based on strong distributional evidence, sherliic is much harder than existing testbeds because distributional evidence is of little utility in the classification of infcands. we also show that, due to its construction, many of sherliic\u2019s correct infcands are novel and missing from existing rule bases. we evaluate a large number of strong baselines on sherliic, ranging from semantic vector space models to state of the art neural models of natural language inference (nli). we show that sherliic poses a tough challenge to existing nli systems."], "semantics"], [["showing your work doesn\u2019t always work", "raphael tang | jaejun lee | ji xin | xinyu liu | yaoliang yu | jimmy lin", "in natural language processing, a recently popular line of work explores how to best report the experimental results of neural networks. one exemplar publication, titled \u201cshow your work: improved reporting of experimental results\u201d (dodge et al., 2019), advocates for reporting the expected validation effectiveness of the best-tuned model, with respect to the computational budget. in the present work, we critically examine this paper. as far as statistical generalizability is concerned, we find unspoken pitfalls and caveats with this approach. we analytically show that their estimator is biased and uses error-prone assumptions. we find that the estimator favors negative errors and yields poor bootstrapped confidence intervals. we derive an unbiased alternative and bolster our claims with empirical evidence from statistical simulation. our codebase is at https://github.com/castorini/meanmax."], "machine learning for nlp"], [["you don\u2019t have time to read this: an exploration of document reading time prediction", "orion weller | jordan hildebrandt | ilya reznik | christopher challis | e. shannon tass | quinn snell | kevin seppi", "predicting reading time has been a subject of much previous work, focusing on how different words affect human processing, measured by reading time. however, previous work has dealt with a limited number of participants as well as word level only predictions (i.e. predicting the time to read a single word). we seek to extend these works by examining whether or not document level predictions are effective, given additional information such as subject matter, font characteristics, and readability metrics. we perform a novel experiment to examine how different features of text contribute to the time it takes to read, distributing and collecting data from over a thousand participants. we then employ a large number of machine learning methods to predict a user\u2019s reading time. we find that despite extensive research showing that word level reading time can be most effectively predicted by neural networks, larger scale text can be easily and most accurately predicted by one factor, the number of words."], "linguistic theories, cognitive modeling and psycholinguistics"], [["transferable multi-domain state generator for task-oriented dialogue systems", "chien-sheng wu | andrea madotto | ehsan hosseini-asl | caiming xiong | richard socher | pascale fung", "over-dependence on domain ontology and lack of sharing knowledge across domains are two practical and yet less studied problems of dialogue state tracking. existing approaches generally fall short when tracking unknown slot values during inference and often have difficulties in adapting to new domains. in this paper, we propose a transferable dialogue state generator (trade) that generates dialogue states from utterances using copy mechanism, facilitating transfer when predicting (domain, slot, value) triplets not encountered during training. our model is composed of an utterance encoder, a slot gate, and a state generator, which are shared across domains. empirical results demonstrate that trade achieves state-of-the-art 48.62% joint goal accuracy for the five domains of multiwoz, a human-human dialogue dataset. in addition, we show the transferring ability by simulating zero-shot and few-shot dialogue state tracking for unseen domains. trade achieves 60.58% joint goal accuracy in one of the zero-shot domains, and is able to adapt to few-shot cases without forgetting already trained domains."], "dialogue and interactive systems"], [["fiesta: fast identification of state-of-the-art models using adaptive bandit algorithms", "henry moss | andrew moore | david leslie | paul rayson", "we present fiesta, a model selection approach that significantly reduces the computational resources required to reliably identify state-of-the-art performance from large collections of candidate models. despite being known to produce unreliable comparisons, it is still common practice to compare model evaluations based on single choices of random seeds. we show that reliable model selection also requires evaluations based on multiple train-test splits (contrary to common practice in many shared tasks). using bandit theory from the statistics literature, we are able to adaptively determine appropriate numbers of data splits and random seeds used to evaluate each model, focusing computational resources on the evaluation of promising models whilst avoiding wasting evaluations on models with lower performance. furthermore, our user-friendly python implementation produces confidence guarantees of correctly selecting the optimal model. we evaluate our algorithms by selecting between 8 target-dependent sentiment analysis methods using dramatically fewer model evaluations than current model selection approaches."], "machine learning for nlp"], [["towards complex text-to-sql in cross-domain database with intermediate representation", "jiaqi guo | zecheng zhan | yan gao | yan xiao | jian-guang lou | ting liu | dongmei zhang", "we present a neural approach called irnet for complex and cross-domain text-to-sql. irnet aims to address two challenges: 1) the mismatch between intents expressed in natural language (nl) and the implementation details in sql; 2) the challenge in predicting columns caused by the large number of out-of-domain words. instead of end-to-end synthesizing a sql query, irnet decomposes the synthesis process into three phases. in the first phase, irnet performs a schema linking over a question and a database schema. then, irnet adopts a grammar-based neural model to synthesize a semql query which is an intermediate representation that we design to bridge nl and sql. finally, irnet deterministically infers a sql query from the synthesized semql query with domain knowledge. on the challenging text-to-sql benchmark spider, irnet achieves 46.7% accuracy, obtaining 19.5% absolute improvement over previous state-of-the-art approaches. at the time of writing, irnet achieves the first position on the spider leaderboard."], "semantics"], [["exploring the efficacy of automatically generated counterfactuals for sentiment analysis", "linyi yang | jiazheng li | padraig cunningham | yue zhang | barry smyth | ruihai dong", "while state-of-the-art nlp models have been achieving the excellent performance of a wide range of tasks in recent years, important questions are being raised about their robustness and their underlying sensitivity to systematic biases that may exist in their training and test data. such issues come to be manifest in performance problems when faced with out-of-distribution data in the field. one recent solution has been to use counterfactually augmented datasets in order to reduce any reliance on spurious patterns that may exist in the original data. producing high-quality augmented data can be costly and time-consuming as it usually needs to involve human feedback and crowdsourcing efforts. in this work, we propose an alternative by describing and evaluating an approach to automatically generating counterfactual data for the purpose of data augmentation and explanation. a comprehensive evaluation on several different datasets and using a variety of state-of-the-art benchmarks demonstrate how our approach can achieve significant improvements in model performance when compared to models training on the original data and even when compared to models trained with the benefit of human-generated augmented data."], "sentiment analysis, stylistic analysis, and argument mining"], [["a resource-free evaluation metric for cross-lingual word embeddings based on graph modularity", "yoshinari fujinuma | jordan boyd-graber | michael j. paul", "cross-lingual word embeddings encode the meaning of words from different languages into a shared low-dimensional space. an important requirement for many downstream tasks is that word similarity should be independent of language\u2014i.e., word vectors within one language should not be more similar to each other than to words in another language. we measure this characteristic using modularity, a network measurement that measures the strength of clusters in a graph. modularity has a moderate to strong correlation with three downstream tasks, even though modularity is based only on the structure of embeddings and does not require any external resources. we show through experiments that modularity can serve as an intrinsic validation metric to improve unsupervised cross-lingual word embeddings, particularly on distant language pairs in low-resource settings."], "machine translation and multilinguality"], [["does it make sense? and why? a pilot study for sense making and explanation", "cunxiang wang | shuailong liang | yue zhang | xiaonan li | tian gao", "introducing common sense to natural language understanding systems has received increasing research attention. it remains a fundamental question on how to evaluate whether a system has the sense-making capability. existing benchmarks measure common sense knowledge indirectly or without reasoning. in this paper, we release a benchmark to directly test whether a system can differentiate natural language statements that make sense from those that do not make sense. in addition, a system is asked to identify the most crucial reason why a statement does not make sense. we evaluate models trained over large-scale language modeling tasks as well as human performance, showing that there are different challenges for system sense-making."], "linguistic theories, cognitive modeling and psycholinguistics"], [["a girl has a name: detecting authorship obfuscation", "asad mahmood | zubair shafiq | padmini srinivasan", "authorship attribution aims to identify the author of a text based on the stylometric analysis. authorship obfuscation, on the other hand, aims to protect against authorship attribution by modifying a text\u2019s style. in this paper, we evaluate the stealthiness of state-of-the-art authorship obfuscation methods under an adversarial threat model. an obfuscator is stealthy to the extent an adversary finds it challenging to detect whether or not a text modified by the obfuscator is obfuscated \u2013 a decision that is key to the adversary interested in authorship attribution. we show that the existing authorship obfuscation methods are not stealthy as their obfuscated texts can be identified with an average f1 score of 0.87. the reason for the lack of stealthiness is that these obfuscators degrade text smoothness, as ascertained by neural language models, in a detectable manner. our results highlight the need to develop stealthy authorship obfuscation methods that can better protect the identity of an author seeking anonymity."], "nlp applications"], [["rikinet: reading wikipedia pages for natural question answering", "dayiheng liu | yeyun gong | jie fu | yu yan | jiusheng chen | daxin jiang | jiancheng lv | nan duan", "reading long documents to answer open-domain questions remains challenging in natural language understanding. in this paper, we introduce a new model, called rikinet, which reads wikipedia pages for natural question answering. rikinet contains a dynamic paragraph dual-attention reader and a multi-level cascaded answer predictor. the reader dynamically represents the document and question by utilizing a set of complementary attention mechanisms. the representations are then fed into the predictor to obtain the span of the short answer, the paragraph of the long answer, and the answer type in a cascaded manner. on the natural questions (nq) dataset, a single rikinet achieves 74.3 f1 and 57.9 f1 on long-answer and short-answer tasks. to our best knowledge, it is the first single model that outperforms the single human performance. furthermore, an ensemble rikinet obtains 76.1 f1 and 61.3 f1 on long-answer and short-answer tasks, achieving the best performance on the official nq leaderboard."], "question answering"], [["towards understanding linear word analogies", "kawin ethayarajh | david duvenaud | graeme hirst", "a surprising property of word vectors is that word analogies can often be solved with vector arithmetic. however, it is unclear why arithmetic operators correspond to non-linear embedding models such as skip-gram with negative sampling (sgns). we provide a formal explanation of this phenomenon without making the strong assumptions that past theories have made about the vector space and word distribution. our theory has several implications. past work has conjectured that linear substructures exist in vector spaces because relations can be represented as ratios; we prove that this holds for sgns. we provide novel justification for the addition of sgns word vectors by showing that it automatically down-weights the more frequent word, as weighting schemes do ad hoc. lastly, we offer an information theoretic interpretation of euclidean distance in vector spaces, justifying its use in capturing word dissimilarity."], "semantics"], [["decomposable neural paraphrase generation", "zichao li | xin jiang | lifeng shang | qun liu", "paraphrasing exists at different granularity levels, such as lexical level, phrasal level and sentential level. this paper presents decomposable neural paraphrase generator (dnpg), a transformer-based model that can learn and generate paraphrases of a sentence at different levels of granularity in a disentangled way. specifically, the model is composed of multiple encoders and decoders with different structures, each of which corresponds to a specific granularity. the empirical study shows that the decomposition mechanism of dnpg makes paraphrase generation more interpretable and controllable. based on dnpg, we further develop an unsupervised domain adaptation method for paraphrase generation. experimental results show that the proposed model achieves competitive in-domain performance compared to state-of-the-art neural models, and significantly better performance when adapting to a new domain."], "generation"], [["rhetorically controlled encoder-decoder for modern chinese poetry generation", "zhiqiang liu | zuohui fu | jie cao | gerard de melo | yik-cheung tam | cheng niu | jie zhou", "rhetoric is a vital element in modern poetry, and plays an essential role in improving its aesthetics. however, to date, it has not been considered in research on automatic poetry generation. in this paper, we propose a rhetorically controlled encoder-decoder for modern chinese poetry generation. our model relies on a continuous latent variable as a rhetoric controller to capture various rhetorical patterns in an encoder, and then incorporates rhetoric-based mixtures while generating modern chinese poetry. for metaphor and personification, an automated evaluation shows that our model outperforms state-of-the-art baselines by a substantial margin, while human evaluation shows that our model generates better poems than baseline methods in terms of fluency, coherence, meaningfulness, and rhetorical aesthetics."], "generation"], [["multi-step reasoning via recurrent dual attention for visual dialog", "zhe gan | yu cheng | ahmed kholy | linjie li | jingjing liu | jianfeng gao", "this paper presents a new model for visual dialog, recurrent dual attention network (redan), using multi-step reasoning to answer a series of questions about an image. in each question-answering turn of a dialog, redan infers the answer progressively through multiple reasoning steps. in each step of the reasoning process, the semantic representation of the question is updated based on the image and the previous dialog history, and the recurrently-refined representation is used for further reasoning in the subsequent step. on the visdial v1.0 dataset, the proposed redan model achieves a new state-of-the-art of 64.47% ndcg score. visualization on the reasoning process further demonstrates that redan can locate context-relevant visual and textual clues via iterative refinement, which can lead to the correct answer step-by-step."], "language grounding to vision, robotics and beyond"], [["the knowref coreference corpus: removing gender and number cues for difficult pronominal anaphora resolution", "ali emami | paul trichelair | adam trischler | kaheer suleman | hannes schulz | jackie chi kit cheung", "we introduce a new benchmark for coreference resolution and nli, knowref, that targets common-sense understanding and world knowledge. previous coreference resolution tasks can largely be solved by exploiting the number and gender of the antecedents, or have been handcrafted and do not reflect the diversity of naturally occurring text. we present a corpus of over 8,000 annotated text passages with ambiguous pronominal anaphora. these instances are both challenging and realistic. we show that various coreference systems, whether rule-based, feature-rich, or neural, perform significantly worse on the task than humans, who display high inter-annotator agreement. to explain this performance gap, we show empirically that state-of-the art models often fail to capture context, instead relying on the gender or number of candidate antecedents to make a decision. we then use problem-specific insights to propose a data-augmentation trick called antecedent switching to alleviate this tendency in models. finally, we show that antecedent switching yields promising results on other tasks as well: we use it to achieve state-of-the-art results on the gap coreference task."], "linguistic theories, cognitive modeling and psycholinguistics"], [["pre-training is a hot topic: contextualized document embeddings improve topic coherence", "federico bianchi | silvia terragni | dirk hovy", "topic models extract groups of words from documents, whose interpretation as a topic hopefully allows for a better understanding of the data. however, the resulting word groups are often not coherent, making them harder to interpret. recently, neural topic models have shown improvements in overall coherence. concurrently, contextual embeddings have advanced the state of the art of neural models in general. in this paper, we combine contextualized representations with neural topic models. we find that our approach produces more meaningful and coherent topics than traditional bag-of-words topic models and recent neural models. our results indicate that future improvements in language models will translate into better topic models."], "information extraction, retrieval and text mining"], [["joint diacritization, lemmatization, normalization, and fine-grained morphological tagging", "nasser zalmout | nizar habash", "the written forms of semitic languages are both highly ambiguous and morphologically rich: a word can have multiple interpretations and is one of many inflected forms of the same concept or lemma. this is further exacerbated for dialectal content, which is more prone to noise and lacks a standard orthography. the morphological features can be lexicalized, like lemmas and diacritized forms, or non-lexicalized, like gender, number, and part-of-speech tags, among others. joint modeling of the lexicalized and non-lexicalized features can identify more intricate morphological patterns, which provide better context modeling, and further disambiguate ambiguous lexical choices. however, the different modeling granularity can make joint modeling more difficult. our approach models the different features jointly, whether lexicalized (on the character-level), or non-lexicalized (on the word-level). we use arabic as a test case, and achieve state-of-the-art results for modern standard arabic with 20% relative error reduction, and egyptian arabic with 11% relative error reduction."], "phonology, morphology and word segmentation"], [["data augmentation for text generation without any augmented data", "wei bi | huayang li | jiacheng huang", "data augmentation is an effective way to improve the performance of many neural text generation models. however, current data augmentation methods need to define or choose proper data mapping functions that map the original samples into the augmented samples. in this work, we derive an objective to formulate the problem of data augmentation on text generation tasks without any use of augmented data constructed by specific mapping functions. our proposed objective can be efficiently optimized and applied to popular loss functions on text generation tasks with a convergence rate guarantee. experiments on five datasets of two text generation tasks show that our approach can approximate or even surpass popular data augmentation methods."], "machine learning for nlp"], [["a multitask learning approach for diacritic restoration", "sawsan alqahtani | ajay mishra | mona diab", "in many languages like arabic, diacritics are used to specify pronunciations as well as meanings. such diacritics are often omitted in written text, increasing the number of possible pronunciations and meanings for a word. this results in a more ambiguous text making computational processing on such text more difficult. diacritic restoration is the task of restoring missing diacritics in the written text. most state-of-the-art diacritic restoration models are built on character level information which helps generalize the model to unseen data, but presumably lose useful information at the word level. thus, to compensate for this loss, we investigate the use of multi-task learning to jointly optimize diacritic restoration with related nlp problems namely word segmentation, part-of-speech tagging, and syntactic diacritization. we use arabic as a case study since it has sufficient data resources for tasks that we consider in our joint modeling. our joint models significantly outperform the baselines and are comparable to the state-of-the-art models that are more complex relying on morphological analyzers and/or a lot more data (e.g. dialectal data)."], "phonology, morphology and word segmentation"], [["syntax-aware opinion role labeling with dependency graph convolutional networks", "bo zhang | yue zhang | rui wang | zhenghua li | min zhang", "opinion role labeling (orl) is a fine-grained opinion analysis task and aims to answer \u201cwho expressed what kind of sentiment towards what?\u201d. due to the scarcity of labeled data, orl remains challenging for data-driven methods. in this work, we try to enhance neural orl models with syntactic knowledge by comparing and integrating different representations. we also propose dependency graph convolutional networks (depgcn) to encode parser information at different processing levels. in order to compensate for parser inaccuracy and reduce error propagation, we introduce multi-task learning (mtl) to train the parser and the orl model simultaneously. we verify our methods on the benchmark mpqa corpus. the experimental results show that syntactic information is highly valuable for orl, and our final mtl model effectively boosts the f1 score by 9.29 over the syntax-agnostic baseline. in addition, we find that the contributions from syntactic knowledge do not fully overlap with contextualized word representations (bert). our best model achieves 4.34 higher f1 score than the current state-ofthe-art."], "sentiment analysis, stylistic analysis, and argument mining"], [["multi-sentence argument linking", "seth ebner | patrick xia | ryan culkin | kyle rawlins | benjamin van durme", "we present a novel document-level model for finding argument spans that fill an event\u2019s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, roles across multiple sentences (rams), which contains 9,124 annotated events across 139 types. we demonstrate strong performance of our model on rams and other event-related datasets."], "information extraction, retrieval and text mining"], [["explicit utilization of general knowledge in machine reading comprehension", "chao wang | hui jiang", "to bridge the gap between machine reading comprehension (mrc) models and human beings, which is mainly reflected in the hunger for data and the robustness to noise, in this paper, we explore how to integrate the neural networks of mrc models with the general knowledge of human beings. on the one hand, we propose a data enrichment method, which uses wordnet to extract inter-word semantic connections as general knowledge from each given passage-question pair. on the other hand, we propose an end-to-end mrc model named as knowledge aided reader (kar), which explicitly uses the above extracted general knowledge to assist its attention mechanisms. based on the data enrichment method, kar is comparable in performance with the state-of-the-art mrc models, and significantly more robust to noise than them. when only a subset (20%-80%) of the training examples are available, kar outperforms the state-of-the-art mrc models by a large margin, and is still reasonably robust to noise."], "question answering"], [["tag : type auxiliary guiding for code comment generation", "ruichu cai | zhihao liang | boyan xu | zijian li | yuexing hao | yao chen", "existing leading code comment generation approaches with the structure-to-sequence framework ignores the type information of the interpretation of the code, e.g., operator, string, etc. however, introducing the type information into the existing framework is non-trivial due to the hierarchical dependence among the type information. in order to address the issues above, we propose a type auxiliary guiding encoder-decoder framework for the code comment generation task which considers the source code as an n-ary tree with type information associated with each node. specifically, our framework is featured with a type-associated encoder and a type-restricted decoder which enables adaptive summarization of the source code. we further propose a hierarchical reinforcement learning method to resolve the training difficulties of our proposed framework. extensive evaluations demonstrate the state-of-the-art performance of our framework with both the auto-evaluated metrics and case studies."], "generation"], [["explore, propose, and assemble: an interpretable model for multi-hop reading comprehension", "yichen jiang | nitish joshi | yen-chun chen | mohit bansal", "multi-hop reading comprehension requires the model to explore and connect relevant information from multiple sentences/documents in order to answer the question about the context. to achieve this, we propose an interpretable 3-module system called explore-propose-assemble reader (epar). first, the document explorer iteratively selects relevant documents and represents divergent reasoning chains in a tree structure so as to allow assimilating information from all chains. the answer proposer then proposes an answer from every root-to-leaf path in the reasoning tree. finally, the evidence assembler extracts a key sentence containing the proposed answer from every path and combines them to predict the final answer. intuitively, epar approximates the coarse-to-fine-grained comprehension behavior of human readers when facing multiple long documents. we jointly optimize our 3 modules by minimizing the sum of losses from each stage conditioned on the previous stage\u2019s output. on two multi-hop reading comprehension datasets wikihop and medhop, our epar model achieves significant improvements over the baseline and competitive results compared to the state-of-the-art model. we also present multiple reasoning-chain-recovery tests and ablation studies to demonstrate our system\u2019s ability to perform interpretable and accurate reasoning."], "question answering"], [["towards scalable and reliable capsule networks for challenging nlp applications", "wei zhao | haiyun peng | steffen eger | erik cambria | min yang", "obstacles hindering the development of capsule networks for challenging nlp applications include poor scalability to large output spaces and less reliable routing processes. in this paper, we introduce: (i) an agreement score to evaluate the performance of routing processes at instance-level; (ii) an adaptive optimizer to enhance the reliability of routing; (iii) capsule compression and partial routing to improve the scalability of capsule networks. we validate our approach on two nlp tasks, namely: multi-label text classification and question answering. experimental results show that our approach considerably improves over strong competitors on both tasks. in addition, we gain the best results in low-resource settings with few training instances."], "machine learning for nlp"], [["learning to control the fine-grained sentiment for story ending generation", "fuli luo | damai dai | pengcheng yang | tianyu liu | baobao chang | zhifang sui | xu sun", "automatic story ending generation is an interesting and challenging task in natural language generation. previous studies are mainly limited to generate coherent, reasonable and diversified story endings, and few works focus on controlling the sentiment of story endings. this paper focuses on generating a story ending which meets the given fine-grained sentiment intensity. there are two major challenges to this task. first is the lack of story corpus which has fine-grained sentiment labels. second is the difficulty of explicitly controlling sentiment intensity when generating endings. therefore, we propose a generic and novel framework which consists of a sentiment analyzer and a sentimental generator, respectively addressing the two challenges. the sentiment analyzer adopts a series of methods to acquire sentiment intensities of the story dataset. the sentimental generator introduces the sentiment intensity into decoder via a gaussian kernel layer to control the sentiment of the output. to the best of our knowledge, this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels. experiments show that our proposed framework can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better."], "generation"], [["boosting entity linking performance by leveraging unlabeled documents", "phong le | ivan titov", "modern entity linking systems rely on large collections of documents specifically annotated for the task (e.g., aida conll). in contrast, we propose an approach which exploits only naturally occurring information: unlabeled documents and wikipedia. our approach consists of two stages. first, we construct a high recall list of candidate entities for each mention in an unlabeled document. second, we use the candidate lists as weak supervision to constrain our document-level entity linking model. the model treats entities as latent variables and, when estimated on a collection of unlabelled texts, learns to choose entities relying both on local context of each mention and on coherence with other entities in the document. the resulting approach rivals fully-supervised state-of-the-art systems on standard test sets. it also approaches their performance in the very challenging setting: when tested on a test set sampled from the data used to estimate the supervised systems. by comparing to wikipedia-only training of our model, we demonstrate that modeling unlabeled documents is beneficial."], "machine learning for nlp"], [["uncertainty-aware curriculum learning for neural machine translation", "yikai zhou | baosong yang | derek f. wong | yu wan | lidia s. chao", "neural machine translation (nmt) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. the keys lie in the assessment of data difficulty and model competence. we propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. quantitative analyses reveal that the proposed strategy offers nmt the ability to automatically govern its learning schedule."], "machine translation and multilinguality"], [["assessing the representations of idiomaticity in vector models with a noun compound dataset labeled at type and token levels", "marcos garcia | tiago kramer vieira | carolina scarton | marco idiart | aline villavicencio", "accurate assessment of the ability of embedding models to capture idiomaticity may require evaluation at token rather than type level, to account for degrees of idiomaticity and possible ambiguity between literal and idiomatic usages. however, most existing resources with annotation of idiomaticity include ratings only at type level. this paper presents the noun compound type and token idiomaticity (nctti) dataset, with human annotations for 280 noun compounds in english and 180 in portuguese at both type and token level. we compiled 8,725 and 5,091 token level annotations for english and portuguese, respectively, which are strongly correlated with the corresponding scores obtained at type level. the nctti dataset is used to explore how vector space models reflect the variability of idiomaticity across sentences. several experiments using state-of-the-art contextualised models suggest that their representations are not capturing the noun compounds idiomaticity as human annotators. this new multilingual resource also contains suggestions for paraphrases of the noun compounds both at type and token levels, with uses for lexical substitution or disambiguation in context."], "resources and evaluation"], [["an analysis of the utility of explicit negative examples to improve the syntactic abilities of neural language models", "hiroshi noji | hiroya takamura", "we explore the utilities of explicit negative examples in training neural language models. negative examples here are incorrect words in a sentence, such as barks in *the dogs barks. neural language models are commonly trained only on positive examples, a set of sentences in the training data, but recent studies suggest that the models trained in this way are not capable of robustly handling complex syntactic constructions, such as long-distance agreement. in this paper, we first demonstrate that appropriately using negative examples about particular constructions (e.g., subject-verb agreement) will boost the model\u2019s robustness on them in english, with a negligible loss of perplexity. the key to our success is an additional margin loss between the log-likelihoods of a correct word and an incorrect word. we then provide a detailed analysis of the trained models. one of our findings is the difficulty of object-relative clauses for rnns. we find that even with our direct learning signals the models still suffer from resolving agreement across an object-relative clause. augmentation of training sentences involving the constructions somewhat helps, but the accuracy still does not reach the level of subject-relative clauses. although not directly cognitively appealing, our method can be a tool to analyze the true architectural limitation of neural models on challenging linguistic constructions."], "interpretability and analysis of models for nlp"], [["language to network: conditional parameter adaptation with natural language descriptions", "tian jin | zhun liu | shengjia yan | alexandre eichenberger | louis-philippe morency", "transfer learning using imagenet pre-trained models has been the de facto approach in a wide range of computer vision tasks. however, fine-tuning still requires task-specific training data. in this paper, we propose n3 (neural networks from natural language) - a new paradigm of synthesizing task-specific neural networks from language descriptions and a generic pre-trained model. n3 leverages language descriptions to generate parameter adaptations as well as a new task-specific classification layer for a pre-trained neural network, effectively \u201cfine-tuning\u201d the network for a new task using only language descriptions as input. to the best of our knowledge, n3 is the first method to synthesize entire neural networks from natural language. experimental results show that n3 can out-perform previous natural-language based zero-shot learning methods across 4 different zero-shot image classification benchmarks. we also demonstrate a simple method to help identify keywords in language descriptions leveraged by n3 when synthesizing model parameters."], "nlp applications"], [["neighborhood matching network for entity alignment", "yuting wu | xiao liu | yansong feng | zheng wang | dongyan zhao", "structural heterogeneity between knowledge graphs is an outstanding challenge for entity alignment. this paper presents neighborhood matching network (nmn), a novel entity alignment framework for tackling the structural heterogeneity challenge. nmn estimates the similarities between entities to capture both the topological structure and the neighborhood difference. it provides two innovative components for better learning representations for entity alignment. it first uses a novel graph sampling method to distill a discriminative neighborhood for each entity. it then adopts a cross-graph neighborhood matching module to jointly encode the neighborhood difference for a given entity pair. such strategies allow nmn to effectively construct matching-oriented entity representations while ignoring noisy neighbors that have a negative impact on the alignment task. extensive experiments performed on three entity alignment datasets show that nmn can well estimate the neighborhood similarity in more tough cases and significantly outperforms 12 previous state-of-the-art methods."], "information extraction, retrieval and text mining"], [["deepsentipeer: harnessing sentiment in review texts to recommend peer review decisions", "tirthankar ghosal | rajeev verma | asif ekbal | pushpak bhattacharyya", "automatically validating a research artefact is one of the frontiers in artificial intelligence (ai) that directly brings it close to competing with human intellect and intuition. although criticised sometimes, the existing peer review system still stands as the benchmark of research validation. the present-day peer review process is not straightforward and demands profound domain knowledge, expertise, and intelligence of human reviewer(s), which is somewhat elusive with the current state of ai. however, the peer review texts, which contains rich sentiment information of the reviewer, reflecting his/her overall attitude towards the research in the paper, could be a valuable entity to predict the acceptance or rejection of the manuscript under consideration. here in this work, we investigate the role of reviewer sentiment embedded within peer review texts to predict the peer review outcome. our proposed deep neural architecture takes into account three channels of information: the paper, the corresponding reviews, and review\u2019s polarity to predict the overall recommendation score as well as the final decision. we achieve significant performance improvement over the baselines (\u223c 29% error reduction) proposed in a recently released dataset of peer reviews. an ai of this kind could assist the editors/program chairs as an additional layer of confidence, especially when non-responding/missing reviewers are frequent in present day peer review."], "nlp applications"], [["coreference resolution without span representations", "yuval kirstain | ori ram | omer levy", "the introduction of pretrained language models has reduced many complex task-specific nlp models to simple lightweight layers. an exception to this trend is coreference resolution, where a sophisticated task-specific model is appended to a pretrained transformer encoder. while highly effective, the model has a very large memory footprint \u2013 primarily due to dynamically-constructed span and span-pair representations \u2013 which hinders the processing of complete documents and the ability to train on multiple instances in a single batch. we introduce a lightweight end-to-end coreference model that removes the dependency on span representations, handcrafted features, and heuristics. our model performs competitively with the current standard model, while being simpler and more efficient."], "dialogue and interactive systems"], [["celebrity profiling", "matti wiegmann | benno stein | martin potthast", "celebrities are among the most prolific users of social media, promoting their personas and rallying followers. this activity is closely tied to genuine writing samples, which makes them worthy research subjects in many respects, not least profiling. with this paper we introduce the webis celebrity corpus 2019. for its construction the twitter feeds of 71,706 verified accounts have been carefully linked with their respective wikidata items, crawling both. after cleansing, the resulting profiles contain an average of 29,968 words per profile and up to 239 pieces of personal information. a cross-evaluation that checked the correct association of twitter account and wikidata item revealed an error rate of only 0.6%, rendering the profiles highly reliable. our corpus comprises a wide cross-section of local and global celebrities, forming a unique combination of scale, profile comprehensiveness, and label reliability. we further establish the state of the art\u2019s profiling performance by evaluating the winning approaches submitted to the pan gender prediction tasks in a transfer learning experiment. they are only outperformed by our own deep learning approach, which we also use to exemplify celebrity occupation prediction for the first time."], "computational social science, social media and cultural analytics"], [["arbert & marbert: deep bidirectional transformers for arabic", "muhammad abdul-mageed | abdelrahim elmadany | el moatez billah nagoudi", "pre-trained language models (lms) are currently integral to many natural language processing systems. although multilingual lms were also introduced to serve many languages, these have limitations such as being costly at inference time and the size and diversity of non-english data involved in their pre-training. we remedy these issues for a collection of diverse arabic varieties by introducing two powerful deep bidirectional transformer-based models, arbert and marbert. to evaluate our models, we also introduce arlue, a new benchmark for multi-dialectal arabic language understanding evaluation. arlue is built using 42 datasets targeting six different task clusters, allowing us to offer a series of standardized experiments under rich conditions. when fine-tuned on arlue, our models collectively achieve new state-of-the-art results across the majority of tasks (37 out of 48 classification tasks, on the 42 datasets). our best model acquires the highest arlue score (77.40) across all six task clusters, outperforming all other models including xlm-r large ( 3.4x larger size). our models are publicly available at https://github.com/ubc-nlp/marbert and arlue will be released through the same repository."], "resources and evaluation"], [["neural network alignment for sentential paraphrases", "jessica ouyang | kathy mckeown", "we present a monolingual alignment system for long, sentence- or clause-level alignments, and demonstrate that systems designed for word- or short phrase-based alignment are ill-suited for these longer alignments. our system is capable of aligning semantically similar spans of arbitrary length. we achieve significantly higher recall on aligning phrases of four or more words and outperform state-of-the- art aligners on the long alignments in the msr rte corpus."], "semantics"], [["regularized context gates on transformer for machine translation", "xintong li | lemao liu | rui wang | guoping huang | max meng", "context gates are effective to control the contributions from the source and target contexts in the recurrent neural network (rnn) based neural machine translation (nmt). however, it is challenging to extend them into the advanced transformer architecture, which is more complicated than rnn. this paper first provides a method to identify source and target contexts and then introduce a gate mechanism to control the source and target contributions in transformer. in addition, to further reduce the bias problem in the gate mechanism, this paper proposes a regularization method to guide the learning of the gates with supervision automatically generated using pointwise mutual information. extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 bleu score over a strong transformer baseline."], "machine translation and multilinguality"], [["explaining relationships between scientific documents", "kelvin luu | xinyi wu | rik koncel-kedziorski | kyle lo | isabel cachola | noah a. smith", "we address the task of explaining relationships between two scientific documents using natural language text. this task requires modeling the complex content of long technical documents, deducing a relationship between these documents, and expressing the details of that relationship in text. in addition to the theoretical interest of this task, successful solutions can help improve researcher efficiency in search and review. in this paper we establish a dataset of 622k examples from 154k documents. we pretrain a large language model to serve as the foundation for autoregressive approaches to the task. we explore the impact of taking different views on the two documents, including the use of dense representations extracted with scientific ie systems. we provide extensive automatic and human evaluations which show the promise of such models, but make clear challenges for future work."], "nlp applications"], [["adversarial and domain-aware bert for cross-domain sentiment analysis", "chunning du | haifeng sun | jingyu wang | qi qi | jianxin liao", "cross-domain sentiment classification aims to address the lack of massive amounts of labeled data. it demands to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain. in this paper, we investigate how to efficiently apply the pre-training language model bert on the unsupervised domain adaptation. due to the pre-training task and corpus, bert is task-agnostic, which lacks domain awareness and can not distinguish the characteristic of source and target domain when transferring knowledge. to tackle these problems, we design a post-training procedure, which contains the target domain masked language model task and a novel domain-distinguish pre-training task. the post-training procedure will encourage bert to be domain-aware and distill the domain-specific features in a self-supervised way. based on this, we could then conduct the adversarial training to derive the enhanced domain-invariant features. extensive experiments on amazon dataset show that our model outperforms state-of-the-art methods by a large margin. the ablation study demonstrates that the remarkable improvement is not only from bert but also from our method."], "sentiment analysis, stylistic analysis, and argument mining"], [["unsupervised neural single-document summarization of reviews via learning latent discourse structure and its ranking", "masaru isonuma | junichiro mori | ichiro sakata", "this paper focuses on the end-to-end abstractive summarization of a single product review without supervision. we assume that a review can be described as a discourse tree, in which the summary is the root, and the child sentences explain their parent in detail. by recursively estimating a parent from its children, our model learns the latent discourse tree without an external parser and generates a concise summary. we also introduce an architecture that ranks the importance of each sentence on the tree to support summary generation focusing on the main review point. the experimental results demonstrate that our model is competitive with or outperforms other unsupervised approaches. in particular, for relatively long reviews, it achieves a competitive or better performance than supervised models. the induced tree shows that the child sentences provide additional information about their parent, and the generated summary abstracts the entire review."], "summarization"], [["language embeddings for typology and cross-lingual transfer learning", "dian yu | taiqi he | kenji sagae", "cross-lingual language tasks typically require a substantial amount of annotated data or parallel translation data. we explore whether language representations that capture relationships among languages can be learned and subsequently leveraged in cross-lingual tasks without the use of parallel data. we generate dense embeddings for 29 languages using a denoising autoencoder, and evaluate the embeddings using the world atlas of language structures (wals) and two extrinsic tasks in a zero-shot setting: cross-lingual dependency parsing and cross-lingual natural language inference."], "machine translation and multilinguality"], [["boosting dialog response generation", "wenchao du | alan w black", "neural models have become one of the most important approaches to dialog response generation. however, they still tend to generate the most common and generic responses in the corpus all the time. to address this problem, we designed an iterative training process and ensemble method based on boosting. we combined our method with different training and decoding paradigms as the base model, including mutual-information-based decoding and reward-augmented maximum likelihood learning. empirical results show that our approach can significantly improve the diversity and relevance of the responses generated by all base models, backed by objective measurements and human evaluation."], "dialogue and interactive systems"], [["analysing lexical semantic change with contextualised word representations", "mario giulianelli | marco del tredici | raquel fern\u00e1ndez", "this paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations. we propose a novel method that exploits the bert neural language model to obtain representations of word usages, clusters these representations into usage types, and measures change along time with three proposed metrics. we create a new evaluation dataset and show that the model representations and the detected semantic shifts are positively correlated with human judgements. our extensive qualitative analysis demonstrates that our method captures a variety of synchronic and diachronic linguistic phenomena. we expect our work to inspire further research in this direction."], "semantics"], [["scriptwriter: narrative-guided script generation", "yutao zhu | ruihua song | zhicheng dou | jian-yun nie | jin zhou", "it is appealing to have a system that generates a story or scripts automatically from a storyline, even though this is still out of our reach. in dialogue systems, it would also be useful to drive dialogues by a dialogue plan. in this paper, we address a key problem involved in these applications - guiding a dialogue by a narrative. the proposed model scriptwriter selects the best response among the candidates that fit the context as well as the given narrative. it keeps track of what in the narrative has been said and what is to be said. a narrative plays a different role than the context (i.e., previous utterances), which is generally used in current dialogue systems. due to the unavailability of data for this new application, we construct a new large-scale data collection graphmovie from a movie website where end- users can upload their narratives freely when watching a movie. experimental results on the dataset show that our proposed approach based on narratives significantly outperforms the baselines that simply use the narrative as a kind of context."], "nlp applications"], [["an effective transition-based model for discontinuous ner", "xiang dai | sarvnaz karimi | ben hachey | cecile paris", "unlike widely used named entity recognition (ner) data sets in generic domains, biomedical ner data sets often contain mentions consisting of discontinuous spans. conventional sequence tagging techniques encode markov assumptions that are efficient but preclude recovery of these mentions. we propose a simple, effective transition-based model with generic neural encoding for discontinuous ner. through extensive experiments on three biomedical data sets, we show that our model can effectively recognize discontinuous mentions without sacrificing the accuracy on continuous mentions."], "information extraction, retrieval and text mining"], [["how good is your tokenizer? on the monolingual performance of multilingual language models", "phillip rust | jonas pfeiffer | ivan vuli\u0107 | sebastian ruder | iryna gurevych", "in this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. we study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. we first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for any performance difference. to disentangle conflating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers. we find that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance. our results show that languages that are adequately represented in the multilingual model\u2019s vocabulary exhibit negligible performance decreases over their monolingual counterparts. we further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language."], "machine translation and multilinguality"], [["klej: comprehensive benchmark for polish language understanding", "piotr rybak | robert mroczkowski | janusz tracz | ireneusz gawlik", "in recent years, a series of transformer-based models unlocked major improvements in general natural language understanding (nlu) tasks. such a fast pace of research would not be possible without general nlu benchmarks, which allow for a fair comparison of the proposed methods. however, such benchmarks are available only for a handful of languages. to alleviate this issue, we introduce a comprehensive multi-task benchmark for the polish language understanding, accompanied by an online leaderboard. it consists of a diverse set of tasks, adopted from existing datasets for named entity recognition, question-answering, textual entailment, and others. we also introduce a new sentiment analysis task for the e-commerce domain, named allegro reviews (ar). to ensure a common evaluation scheme and promote models that generalize to different nlu tasks, the benchmark includes datasets from varying domains and applications. additionally, we release herbert, a transformer-based model trained specifically for the polish language, which has the best average performance and obtains the best results for three out of nine tasks. finally, we provide an extensive evaluation, including several standard baselines and recently proposed, multilingual transformer-based models."], "resources and evaluation"], [["visual story post-editing", "ting-yao hsu | chieh-yang huang | yen-chia hsu | ting-hao huang", "we introduce the first dataset for human edits of machine-generated visual stories and explore how these collected edits may be used for the visual story post-editing task. the dataset ,vist-edit, includes 14,905 human-edited versions of 2,981 machine-generated visual stories. the stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions. we establish baselines for the task, showing how a relatively small set of human edits can be leveraged to boost the performance of large visual storytelling models. we also discuss the weak correlation between automatic evaluation scores and human ratings, motivating the need for new automatic metrics."], "language grounding to vision, robotics and beyond"], [["keep meeting summaries on topic: abstractive multi-modal meeting summarization", "manling li | lingyu zhang | heng ji | richard j. radke", "transcripts of natural, multi-person meetings differ significantly from documents like news articles, which can make natural language generation models for generating summaries unfocused. we develop an abstractive meeting summarizer from both videos and audios of meeting recordings. specifically, we propose a multi-modal hierarchical attention across three levels: segment, utterance and word. to narrow down the focus into topically-relevant segments, we jointly model topic segmentation and summarization. in addition to traditional text features, we introduce new multi-modal features derived from visual focus of attention, based on the assumption that the utterance is more important if the speaker receives more attention. experiments show that our model significantly outperforms the state-of-the-art with both bleu and rouge measures."], "summarization"], [["unsupervised question answering by cloze translation", "patrick lewis | ludovic denoyer | sebastian riedel", "obtaining training data for question answering (qa) is time-consuming and resource-intensive, and existing qa datasets are only available for limited domains and languages. in this work, we explore to what extent high quality training data is actually required for extractive qa, and investigate the possibility of unsupervised extractive qa. we approach this problem by first learning to generate context, question and answer triples in an unsupervised manner, which we then use to synthesize extractive qa training data automatically. to generate such triples, we first sample random context paragraphs from a large corpus of documents and then random noun phrases or named entity mentions from these paragraphs as answers. next we convert answers in context to \u201cfill-in-the-blank\u201d cloze questions and finally translate them into natural questions. we propose and compare various unsupervised ways to perform cloze-to-natural question translation, including training an unsupervised nmt model using non-aligned corpora of natural questions and cloze questions as well as a rule-based approach. we find that modern qa models can learn to answer human questions surprisingly well using only synthetic training data. we demonstrate that, without using the squad training data at all, our approach achieves 56.4 f1 on squad v1 (64.5 f1 when the answer is a named entity mention), outperforming early supervised models."], "question answering"], [["a working memory model for task-oriented dialog response generation", "xiuyi chen | jiaming xu | bo xu", "recently, to incorporate external knowledge base (kb) information, one form of world knowledge, several end-to-end task-oriented dialog systems have been proposed. these models, however, tend to confound the dialog history with kb tuples and simply store them into one memory. inspired by the psychological studies on working memory, we propose a working memory model (wmm2seq) for dialog response generation. our wmm2seq adopts a working memory to interact with two separated long-term memories, which are the episodic memory for memorizing dialog history and the semantic memory for storing kb tuples. the working memory consists of a central executive to attend to the aforementioned memories, and a short-term storage system to store the \u201cactivated\u201d contents from the long-term memories. furthermore, we introduce a context-sensitive perceptual process for the token representations of dialog history, and then feed them into the episodic memory. extensive experiments on two task-oriented dialog datasets demonstrate that our wmm2seq significantly outperforms the state-of-the-art results in several evaluation metrics."], "generation"], [["augmenting neural networks with first-order logic", "tao li | vivek srikumar", "today, the dominant paradigm for training neural networks involves minimizing task loss on a large dataset. using world knowledge to inform a model, and yet retain the ability to perform end-to-end training remains an open question. in this paper, we present a novel framework for introducing declarative knowledge to neural network architectures in order to guide training and prediction. our framework systematically compiles logical statements into computation graphs that augment a neural network without extra learnable parameters or manual redesign. we evaluate our modeling strategy on three tasks: machine comprehension, natural language inference, and text chunking. our experiments show that knowledge-augmented networks can strongly improve over baselines, especially in low-data regimes."], "machine learning for nlp"], [["tabert: pretraining for joint understanding of textual and tabular data", "pengcheng yin | graham neubig | wen-tau yih | sebastian riedel", "recent years have witnessed the burgeoning of pretrained language models (lms) for text-based natural language (nl) understanding tasks. such models are typically trained on free-form nl text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both free-form nl questions and structured tabular data (e.g., database tables). in this paper we present tabert, a pretrained lm that jointly learns representations for nl sentences and (semi-)structured tables. tabert is trained on a large corpus of 26 million tables and their english contexts. in experiments, neural semantic parsers using tabert as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark wikitablequestions, while performing competitively on the text-to-sql dataset spider."], "semantics"], [["towards user-driven neural machine translation", "huan lin | liang yao | baosong yang | dayiheng liu | haibo zhang | weihua luo | degen huang | jinsong su", "a good translation should not only translate the original content semantically, but also incarnate personal traits of the original text. for a real-world neural machine translation (nmt) system, these user traits (e.g., topic preference, stylistic characteristics and expression habits) can be preserved in user behavior (e.g., historical inputs). however, current nmt systems marginally consider the user behavior due to: 1) the difficulty of modeling user portraits in zero-shot scenarios, and 2) the lack of user-behavior annotated parallel dataset. to fill this gap, we introduce a novel framework called user-driven nmt. specifically, a cache-based module and a user-driven contrastive learning method are proposed to offer nmt the ability to capture potential user traits from their historical inputs under a zero-shot learning fashion. furthermore, we contribute the first chinese-english parallel corpus annotated with user behavior called udt-corpus. experimental results confirm that the proposed user-driven nmt can generate user-specific translations."], "machine translation and multilinguality"], [["talksumm: a dataset and scalable annotation method for scientific paper summarization based on conference talks", "guy lev | michal shmueli-scheuer | jonathan herzig | achiya jerbi | david konopnicki", "currently, no large-scale training data is available for the task of scientific paper summarization. in this paper, we propose a novel method that automatically generates summaries for scientific papers, by utilizing videos of talks at scientific conferences. we hypothesize that such talks constitute a coherent and concise description of the papers\u2019 content, and can form the basis for good summaries. we collected 1716 papers and their corresponding videos, and created a dataset of paper summaries. a model trained on this dataset achieves similar performance as models trained on a dataset of summaries created manually. in addition, we validated the quality of our summaries by human experts."], "summarization"], [["boosting neural machine translation with similar translations", "jitao xu | josep crego | jean senellart", "this paper explores data augmentation methods for training neural machine translation to make use of similar translations, in a comparable way a human translator employs fuzzy matches. in particular, we show how we can simply present the neural model with information of both source and target sides of the fuzzy matches, we also extend the similarity to include semantically related translations retrieved using sentence distributed representations. we show that translations based on fuzzy matching provide the model with \u201ccopy\u201d information while translations based on embedding similarities tend to extend the translation \u201ccontext\u201d. results indicate that the effect from both similar sentences are adding up to further boost accuracy, combine naturally with model fine-tuning and are providing dynamic adaptation for unseen translation pairs. tests on multiple data sets and domains show consistent accuracy improvements. to foster research around these techniques, we also release an open-source toolkit with efficient and flexible fuzzy-match implementation."], "machine translation and multilinguality"], [["word-order biases in deep-agent emergent communication", "rahma chaabouni | eugene kharitonov | alessandro lazaric | emmanuel dupoux | marco baroni", "sequence-processing neural networks led to remarkable progress on many nlp tasks. as a consequence, there has been increasing interest in understanding to what extent they process language as humans do. we aim here to uncover which biases such models display with respect to \u201cnatural\u201d word-order constraints. we train models to communicate about paths in a simple gridworld, using miniature languages that reflect or violate various natural language trends, such as the tendency to avoid redundancy or to minimize long-distance dependencies. we study how the controlled characteristics of our miniature languages affect individual learning and their stability across multiple network generations. the results draw a mixed picture. on the one hand, neural networks show a strong tendency to avoid long-distance dependencies. on the other hand, there is no clear preference for the efficient, non-redundant encoding of information that is widely attested in natural language. we thus suggest inoculating a notion of \u201ceffort\u201d into neural networks, as a possible way to make their linguistic behavior more human-like."], "linguistic theories, cognitive modeling and psycholinguistics"], [["bpe-dropout: simple and effective subword regularization", "ivan provilkov | dmitrii emelianenko | elena voita", "subword segmentation is widely used to address the open vocabulary problem in machine translation. the dominant approach to subword segmentation is byte pair encoding (bpe), which keeps the most frequent words intact while splitting the rare ones into multiple tokens. while multiple segmentations are possible even with the same vocabulary, bpe splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors. so far, the only way to overcome this bpe imperfection, its deterministic nature, was to create another subword segmentation algorithm (kudo, 2018). in contrast, we show that bpe itself incorporates the ability to produce multiple segmentations of the same word. we introduce bpe-dropout - simple and effective subword regularization method based on and compatible with conventional bpe. it stochastically corrupts the segmentation procedure of bpe, which leads to producing multiple segmentations within the same fixed bpe framework. using bpe-dropout during training and the standard bpe during inference improves translation quality up to 2.3 bleu compared to bpe and up to 0.9 bleu compared to the previous subword regularization."], "machine translation and multilinguality"], [["on positivity bias in negative reviews", "madhusudhan aithal | chenhao tan", "prior work has revealed that positive words occur more frequently than negative words in human expressions, which is typically attributed to positivity bias, a tendency for people to report positive views of reality. but what about the language used in negative reviews? consistent with prior work, we show that english negative reviews tend to contain more positive words than negative words, using a variety of datasets. we reconcile this observation with prior findings on the pragmatics of negation, and show that negations are commonly associated with positive words in negative reviews. furthermore, in negative reviews, the majority of sentences with positive words express negative opinions based on sentiment classifiers, indicating some form of negation."], "computational social science, social media and cultural analytics"], [["learning source phrase representations for neural machine translation", "hongfei xu | josef van genabith | deyi xiong | qiuhui liu | jingyi zhang", "the transformer translation model (vaswani et al., 2017) based on a multi-head attention mechanism can be computed effectively in parallel and has significantly pushed forward the performance of neural machine translation (nmt). though intuitively the attentional network can connect distant words via shorter network paths than rnns, empirical analysis demonstrates that it still has difficulty in fully capturing long-distance dependencies (tang et al., 2018). considering that modeling phrases instead of words has significantly improved the statistical machine translation (smt) approach through the use of larger translation blocks (\u201cphrases\u201d) and its reordering ability, modeling nmt at phrase level is an intuitive proposal to help the model capture long-distance relationships. in this paper, we first propose an attentive phrase representation generation mechanism which is able to generate phrase representations from corresponding token representations. in addition, we incorporate the generated phrase representations into the transformer translation model to enhance its ability to capture long-distance relationships. in our experiments, we obtain significant improvements on the wmt 14 english-german and english-french tasks on top of the strong transformer baseline, which shows the effectiveness of our approach. our approach helps transformer base models perform at the level of transformer big models, and even significantly better for long sentences, but with substantially fewer parameters and training steps. the fact that phrase representations help even in the big setting further supports our conjecture that they make a valuable contribution to long-distance relations."], "machine translation and multilinguality"], [["sentibert: a transferable transformer-based architecture for compositional sentiment semantics", "da yin | tao meng | kai-wei chang", "we propose sentibert, a variant of bert that effectively captures compositional sentiment semantics. the model incorporates contextualized representation with binary constituency parse tree to capture semantic composition. comprehensive experiments demonstrate that sentibert achieves competitive performance on phrase-level sentiment classification. we further demonstrate that the sentiment composition learned from the phrase-level annotations on sst can be transferred to other sentiment analysis tasks as well as related tasks, such as emotion classification tasks. moreover, we conduct ablation studies and design visualization methods to understand sentibert. we show that sentibert is better than baseline approaches in capturing negation and the contrastive relation and model the compositional sentiment semantics."], "sentiment analysis, stylistic analysis, and argument mining"], [["analyzing analytical methods: the case of phonology in neural models of spoken language", "grzegorz chrupa\u0142a | bertrand higy | afra alishahi", "given the fast development of analysis techniques for nlp and speech processing systems, few systematic studies have been conducted to compare the strengths and weaknesses of each method. as a step in this direction we study the case of representations of phonology in neural network models of spoken language. we use two commonly applied analytical techniques, diagnostic classifiers and representational similarity analysis, to quantify to what extent neural activation patterns encode phonemes and phoneme sequences. we manipulate two factors that can affect the outcome of analysis. first, we investigate the role of learning by comparing neural activations extracted from trained versus randomly-initialized models. second, we examine the temporal scope of the activations by probing both local activations corresponding to a few milliseconds of the speech signal, and global activations pooled over the whole utterance. we conclude that reporting analysis results with randomly initialized models is crucial, and that global-scope methods tend to yield more consistent and interpretable results and we recommend their use as a complement to local-scope diagnostic methods."], "interpretability and analysis of models for nlp"], [["unsupervised pivot translation for distant languages", "yichong leng | xu tan | tao qin | xiang-yang li | tie-yan liu", "unsupervised neural machine translation (nmt) has attracted a lot of attention recently. while state-of-the-art methods for unsupervised translation usually perform well between similar languages (e.g., english-german translation), they perform poorly between distant languages, because unsupervised alignment does not work well for distant languages. in this work, we introduce unsupervised pivot translation for distant languages, which translates a language to a distant language through multiple hops, and the unsupervised translation on each hop is relatively easier than the original direct translation. we propose a learning to route (ltr) method to choose the translation path between the source and target languages. ltr is trained on language pairs whose best translation path is available and is applied on the unseen language pairs for path selection. experiments on 20 languages and 294 distant language pairs demonstrate the advantages of the unsupervised pivot translation for distant languages, as well as the effectiveness of the proposed ltr for path selection. specifically, in the best case, ltr achieves an improvement of 5.58 bleu points over the conventional direct unsupervised method."], "machine translation and multilinguality"], [["transforming complex sentences into a semantic hierarchy", "christina niklaus | matthias cetto | andr\u00e9 freitas | siegfried handschuh", "we present an approach for recursively splitting and rephrasing complex english sentences into a novel semantic hierarchy of simplified sentences, with each of them presenting a more regular structure that may facilitate a wide variety of artificial intelligence tasks, such as machine translation (mt) or information extraction (ie). using a set of hand-crafted transformation rules, input sentences are recursively transformed into a two-layered hierarchical representation in the form of core sentences and accompanying contexts that are linked via rhetorical relations. in this way, the semantic relationship of the decomposed constituents is preserved in the output, maintaining its interpretability for downstream applications. both a thorough manual analysis and automatic evaluation across three datasets from two different domains demonstrate that the proposed syntactic simplification approach outperforms the state of the art in structural text simplification. moreover, an extrinsic evaluation shows that when applying our framework as a preprocessing step the performance of state-of-the-art open ie systems can be improved by up to 346% in precision and 52% in recall. to enable reproducible research, all code is provided online."], "generation"], [["analyzing the limitations of cross-lingual word embedding mappings", "aitor ormazabal | mikel artetxe | gorka labaka | aitor soroa | eneko agirre", "recent research in cross-lingual word embeddings has almost exclusively focused on offline methods, which independently train word embeddings in different languages and map them to a shared space through linear transformations. while several authors have questioned the underlying isomorphism assumption, which states that word embeddings in different languages have approximately the same structure, it is not clear whether this is an inherent limitation of mapping approaches or a more general issue when learning cross-lingual embeddings. so as to answer this question, we experiment with parallel corpora, which allows us to compare offline mapping to an extension of skip-gram that jointly learns both embedding spaces. we observe that, under these ideal conditions, joint learning yields to more isomorphic embeddings, is less sensitive to hubness, and obtains stronger results in bilingual lexicon induction. we thus conclude that current mapping methods do have strong limitations, calling for further research to jointly learn cross-lingual embeddings with a weaker cross-lingual signal."], "machine translation and multilinguality"], [["improving neural language models by segmenting, attending, and predicting the future", "hongyin luo | lan jiang | yonatan belinkov | james glass", "common language models typically predict the next word given the context. in this work, we propose a method that improves language modeling by learning to align the given context and the following phrase. the model does not require any linguistic annotation of phrase segmentation. instead, we define syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsupervised learning manner. our method can easily be applied to language models with different network architectures since an independent module is used for phrase induction and context-phrase alignment, and no change is required in the underlying language modeling network. experiments have shown that our model outperformed several strong baseline models on different data sets. we achieved a new state-of-the-art performance of 17.4 perplexity on the wikitext-103 dataset. additionally, visualizing the outputs of the phrase induction module showed that our model is able to learn approximate phrase-level structural knowledge without any annotation."], "machine learning for nlp"], [["premise selection in natural language mathematical texts", "deborah ferreira | andr\u00e9 freitas", "the discovery of supporting evidence for addressing complex mathematical problems is a semantically challenging task, which is still unexplored in the field of natural language processing for mathematical text. the natural language premise selection task consists in using conjectures written in both natural language and mathematical formulae to recommend premises that most likely will be useful to prove a particular statement. we propose an approach to solve this task as a link prediction problem, using deep convolutional graph neural networks. this paper also analyses how different baselines perform in this task and shows that a graph structure can provide higher f1-score, especially when considering multi-hop premise selection."], "semantics"], [["probing for semantic classes: diagnosing the meaning content of word embeddings", "yadollah yaghoobzadeh | katharina kann | t. j. hazen | eneko agirre | hinrich sch\u00fctze", "word embeddings typically represent different meanings of a word in a single conflated vector. empirical analysis of embeddings of ambiguous words is currently limited by the small size of manually annotated resources and by the fact that word senses are treated as unrelated individual concepts. we present a large dataset based on manual wikipedia annotations and word senses, where word senses from different words are related by semantic classes. this is the basis for novel diagnostic tests for an embedding\u2019s content: we probe word embeddings for semantic classes and analyze the embedding space by classifying embeddings into semantic classes. our main findings are: (i) information about a sense is generally represented well in a single-vector embedding \u2013 if the sense is frequent. (ii) a classifier can accurately predict whether a word is single-sense or multi-sense, based only on its embedding. (iii) although rare senses are not well represented in single-vector embeddings, this does not have negative impact on an nlp application whose performance depends on frequent senses."], "resources and evaluation"], [["can you put it all together: evaluating conversational agents\u2019 ability to blend skills", "eric michael smith | mary williamson | kurt shuster | jason weston | y-lan boureau", "being engaging, knowledgeable, and empathetic are all desirable general qualities in a conversational agent. previous work has introduced tasks and datasets that aim to help agents to learn those qualities in isolation and gauge how well they can express them. but rather than being specialized in one single quality, a good open-domain conversational agent should be able to seamlessly blend them all into one cohesive conversational flow. in this work, we investigate several ways to combine models trained towards isolated capabilities, ranging from simple model aggregation schemes that require minimal additional training, to various forms of multi-task training that encompass several skills at all training stages. we further propose a new dataset, blendedskilltalk, to analyze how these capabilities would mesh together in a natural conversation, and compare the performance of different architectures and training schemes. our experiments show that multi-tasking over several tasks that focus on particular capabilities results in better blended conversation performance compared to models trained on a single skill, and that both unified or two-stage approaches perform well if they are constructed to avoid unwanted bias in skill selection or are fine-tuned on our new task."], "dialogue and interactive systems"], [["cross-lingual semantic role labeling with high-quality translated training corpus", "hao fei | meishan zhang | donghong ji", "many efforts of research are devoted to semantic role labeling (srl) which is crucial for natural language understanding. supervised approaches have achieved impressing performances when large-scale corpora are available for resource-rich languages such as english. while for the low-resource languages with no annotated srl dataset, it is still challenging to obtain competitive performances. cross-lingual srl is one promising way to address the problem, which has achieved great advances with the help of model transferring and annotation projection. in this paper, we propose a novel alternative based on corpus translation, constructing high-quality training datasets for the target languages from the source gold-standard srl annotations. experimental results on universal proposition bank show that the translation-based method is highly effective, and the automatic pseudo datasets can improve the target-language srl performances significantly."], "semantics"], [["training hybrid language models by marginalizing over segmentations", "edouard grave | sainbayar sukhbaatar | piotr bojanowski | armand joulin", "in this paper, we study the problem of hybrid language modeling, that is using models which can predict both characters and larger units such as character ngrams or words. using such models, multiple potential segmentations usually exist for a given string, for example one using words and one using characters only. thus, the probability of a string is the sum of the probabilities of all the possible segmentations. here, we show how it is possible to marginalize over the segmentations efficiently, in order to compute the true probability of a sequence. we apply our technique on three datasets, comprising seven languages, showing improvements over a strong character level language model."], "machine learning for nlp"], [["cluhtm - semantic hierarchical topic modeling based on cluwords", "felipe viegas | washington cunha | christian gomes | ant\u00f4nio pereira | leonardo rocha | marcos goncalves", "hierarchical topic modeling (htm) exploits latent topics and relationships among them as a powerful tool for data analysis and exploration. despite advantages over traditional topic modeling, htm poses its own challenges, such as (1) topic incoherence, (2) unreasonable (hierarchical) structure, and (3) issues related to the definition of the \u201cideal\u201d number of topics and depth of the hierarchy. in this paper, we advance the state-of-the-art on htm by means of the design and evaluation of cluhtm, a novel non-probabilistic hierarchical matrix factorization aimed at solving the specific issues of htm. cluhtm\u2019s novel contributions include: (i) the exploration of richer text representation that encapsulates both, global (dataset level) and local semantic information \u2013 when combined, these pieces of information help to solve the topic incoherence problem as well as issues related to the unreasonable structure; (ii) the exploitation of a stability analysis metric for defining the number of topics and the \u201cshape\u201d the hierarchical structure. in our evaluation, considering twelve datasets and seven state-of-the-art baselines, cluhtm outperformed the baselines in the vast majority of the cases, with gains of around 500% over the strongest state-of-the-art baselines. we also provide qualitative and quantitative statistical analyses of why our solution works so well."], "information extraction, retrieval and text mining"], [["storytelling with dialogue: a critical role dungeons and dragons dataset", "revanth rameshkumar | peter bailey", "this paper describes the critical role dungeons and dragons dataset (crd3) and related analyses. critical role is an unscripted, live-streamed show where a fixed group of people play dungeons and dragons, an open-ended role-playing game. the dataset is collected from 159 critical role episodes transcribed to text dialogues, consisting of 398,682 turns. it also includes corresponding abstractive summaries collected from the fandom wiki. the dataset is linguistically unique in that the narratives are generated entirely through player collaboration and spoken interaction. for each dialogue, there are a large number of turns, multiple abstractive summaries with varying levels of detail, and semantic ties to the previous dialogues. in addition, we provide a data augmentation method that produces 34,243 summary-dialogue chunk pairs to support current neural ml approaches, and we provide an abstractive summarization benchmark and evaluation."], "summarization"], [["soft representation learning for sparse transfer", "haeju park | jinyoung yeo | gengyu wang | seung-won hwang", "transfer learning is effective for improving the performance of tasks that are related, and multi-task learning (mtl) and cross-lingual learning (cll) are important instances. this paper argues that hard-parameter sharing, of hard-coding layers shared across different tasks or languages, cannot generalize well, when sharing with a loosely related task. such case, which we call sparse transfer, might actually hurt performance, a phenomenon known as negative transfer. our contribution is using adversarial training across tasks, to \u201csoft-code\u201d shared and private spaces, to avoid the shared space gets too sparse. in cll, our proposed architecture considers another challenge of dealing with low-quality input."], "machine learning for nlp"], [["moving down the long tail of word sense disambiguation with gloss informed bi-encoders", "terra blevins | luke zettlemoyer", "a major obstacle in word sense disambiguation (wsd) is that word senses are not uniformly distributed, causing existing models to generally perform poorly on senses that are either rare or unseen during training. we propose a bi-encoder model that independently embeds (1) the target word with its surrounding context and (2) the dictionary definition, or gloss, of each sense. the encoders are jointly optimized in the same representation space, so that sense disambiguation can be performed by finding the nearest sense embedding for each target word embedding. our system outperforms previous state-of-the-art models on english all-words wsd; these gains predominantly come from improved performance on rare senses, leading to a 31.1% error reduction on less frequent senses over prior work. this demonstrates that rare senses can be more effectively disambiguated by modeling their definitions."], "semantics"], [["unknown intent detection using gaussian mixture model with an application to zero-shot intent classification", "guangfeng yan | lu fan | qimai li | han liu | xiaotong zhang | xiao-ming wu | albert y.s. lam", "user intent classification plays a vital role in dialogue systems. since user intent may frequently change over time in many realistic scenarios, unknown (new) intent detection has become an essential problem, where the study has just begun. this paper proposes a semantic-enhanced gaussian mixture model (seg) for unknown intent detection. in particular, we model utterance embeddings with a gaussian mixture distribution and inject dynamic class semantic information into gaussian means, which enables learning more class-concentrated embeddings that help to facilitate downstream outlier detection. coupled with a density-based outlier detection algorithm, seg achieves competitive results on three real task-oriented dialogue datasets in two languages for unknown intent detection. on top of that, we propose to integrate seg as an unknown intent identifier into existing generalized zero-shot intent classification models to improve their performance. a case study on a state-of-the-art method, recapsnet, shows that seg can push the classification performance to a significantly higher level."], "dialogue and interactive systems"], [["key fact as pivot: a two-stage model for low resource table-to-text generation", "shuming ma | pengcheng yang | tianyu liu | peng li | jie zhou | xu sun", "table-to-text generation aims to translate the structured data into the unstructured text. most existing methods adopt the encoder-decoder framework to learn the transformation, which requires large-scale training samples. however, the lack of large parallel data is a major practical problem for many domains. in this work, we consider the scenario of low resource table-to-text generation, where only limited parallel data is available. we propose a novel model to separate the generation into two stages: key fact prediction and surface realization. it first predicts the key facts from the tables, and then generates the text with the key facts. the training of key fact prediction needs much fewer annotated data, while surface realization can be trained with pseudo parallel corpus. we evaluate our model on a biography generation dataset. our model can achieve 27.34 bleu score with only 1,000 parallel data, while the baseline model only obtain the performance of 9.71 bleu score."], "generation"], [["factoring statutory reasoning as language understanding challenges", "nils holzenberger | benjamin van durme", "statutory reasoning is the task of determining whether a legal statute, stated in natural language, applies to the text description of a case. prior work introduced a resource that approached statutory reasoning as a monolithic textual entailment problem, with neural baselines performing nearly at-chance. to address this challenge, we decompose statutory reasoning into four types of language-understanding challenge problems, through the introduction of concepts and structure found in prolog programs. augmenting an existing benchmark, we provide annotations for the four tasks, and baselines for three of them. models for statutory reasoning are shown to benefit from the additional structure, improving on prior baselines. further, the decomposition into subtasks facilitates finer-grained model diagnostics and clearer incremental progress."], "resources and evaluation"], [["categorizing and inferring the relationship between the text and image of twitter posts", "alakananda vempala | daniel preo\u0163iuc-pietro", "text in social media posts is frequently accompanied by images in order to provide content, supply context, or to express feelings. this paper studies how the meaning of the entire tweet is composed through the relationship between its textual content and its image. we build and release a data set of image tweets annotated with four classes which express whether the text or the image provides additional information to the other modality. we show that by combining the text and image information, we can build a machine learning approach that accurately distinguishes between the relationship types. further, we derive insights into how these relationships are materialized through text and image content analysis and how they are impacted by user demographic traits. these methods can be used in several downstream applications including pre-training image tagging models, collecting distantly supervised data for image captioning, and can be directly used in end-user applications to optimize screen estate."], "computational social science, social media and cultural analytics"], [["character-level translation with self-attention", "yingqiang gao | nikola i. nikolov | yuhuang hu | richard h.r. hahnloser", "we explore the suitability of self-attention models for character-level neural machine translation. we test the standard transformer model, as well as a novel variant in which the encoder block combines information from nearby characters using convolutions. we perform extensive experiments on wmt and un datasets, testing both bilingual and multilingual translation to english using up to three input languages (french, spanish, and chinese). our transformer variant consistently outperforms the standard transformer at the character-level and converges faster while learning more robust character-level alignments."], "machine translation and multilinguality"], [["zero-shot transfer learning with synthesized data for multi-domain dialogue state tracking", "giovanni campagna | agata foryciarz | mehrad moradshahi | monica lam", "zero-shot transfer learning for multi-domain dialogue state tracking can allow us to handle new domains without incurring the high cost of data acquisition. this paper proposes new zero-short transfer learning technique for dialogue state tracking where the in-domain training data are all synthesized from an abstract dialogue model and the ontology of the domain. we show that data augmentation through synthesized data can improve the accuracy of zero-shot learning for both the trade model and the bert-based sumbt model on the multiwoz 2.1 dataset. we show training with only synthesized in-domain data on the sumbt model can reach about 2/3 of the accuracy obtained with the full training dataset. we improve the zero-shot learning state of the art on average across domains by 21%."], "dialogue and interactive systems"], [["pretraining with contrastive sentence objectives improves discourse performance of language models", "dan iter | kelvin guu | larry lansing | dan jurafsky", "recent models for unsupervised representation learning of text have employed a number of techniques to improve contextual word representations but have put little focus on discourse-level representations. we propose conpono, an inter-sentence objective for pretraining language models that models discourse coherence and the distance between sentences. given an anchor sentence, our model is trained to predict the text k sentences away using a sampled-softmax objective where the candidates consist of neighboring sentences and sentences randomly sampled from the corpus. on the discourse representation benchmark discoeval, our model improves over the previous state-of-the-art by up to 13% and on average 4% absolute across 7 tasks. our model is the same size as bert-base, but outperforms the much larger bert-large model and other more recent approaches that incorporate discourse. we also show that conpono yields gains of 2%-6% absolute even for tasks that do not explicitly evaluate discourse: textual entailment (rte), common sense reasoning (copa) and reading comprehension (record)."], "machine learning for nlp"], [["multi-level matching and aggregation network for few-shot relation classification", "zhi-xiu ye | zhen-hua ling", "this paper presents a multi-level matching and aggregation network (mlman) for few-shot relation classification. previous studies on this topic adopt prototypical networks, which calculate the embedding vector of a query instance and the prototype vector of the support set for each relation candidate independently. on the contrary, our proposed mlman model encodes the query instance and each support set in an interactive way by considering their matching information at both local and instance levels. the final class prototype for each support set is obtained by attentive aggregation over the representations of support instances, where the weights are calculated using the query instance. experimental results demonstrate the effectiveness of our proposed methods, which achieve a new state-of-the-art performance on the fewrel dataset."], "information extraction, retrieval and text mining"], [["exploring phoneme-level speech representations for end-to-end speech translation", "elizabeth salesky | matthias sperber | alan w black", "previous work on end-to-end translation from speech has primarily used frame-level features as speech representations, which creates longer, sparser sequences than text. we show that a naive method to create compressed phoneme-like speech representations is far more effective and efficient for translation than traditional frame-level speech features. specifically, we generate phoneme labels for speech frames and average consecutive frames with the same label to create shorter, higher-level source sequences for translation. we see improvements of up to 5 bleu on both our high and low resource language pairs, with a reduction in training time of 60%. our improvements hold across multiple data sizes and two language pairs."], "machine translation and multilinguality"], [["eli5: long form question answering", "angela fan | yacine jernite | ethan perez | david grangier | jason weston | michael auli", "we introduce the first large-scale corpus for long form question answering, a task requiring elaborate and in-depth answers to open-ended questions. the dataset comprises 270k threads from the reddit forum \u201cexplain like i\u2019m five\u201d (eli5) where an online community provides answers to questions which are comprehensible by five year olds. compared to existing datasets, eli5 comprises diverse questions requiring multi-sentence answers. we provide a large set of web documents to help answer the question. automatic and human evaluations show that an abstractive model trained with a multi-task objective outperforms conventional seq2seq, language modeling, as well as a strong extractive baseline.however, our best model is still far from human performance since raters prefer gold responses in over 86% of cases, leaving ample opportunity for future improvement."], "question answering"], [["cognitive graph for multi-hop reading comprehension at scale", "ming ding | chang zhou | qibin chen | hongxia yang | jie tang", "we propose a new cogqa framework for multi-hop reading comprehension question answering in web-scale documents. founded on the dual process theory in cognitive science, the framework gradually builds a cognitive graph in an iterative process by coordinating an implicit extraction module (system 1) and an explicit reasoning module (system 2). while giving accurate answers, our framework further provides explainable reasoning paths. specifically, our implementation based on bert and graph neural network efficiently handles millions of documents for multi-hop reasoning questions in the hotpotqa fullwiki dataset, achieving a winning joint f1 score of 34.9 on the leaderboard, compared to 23.1 of the best competitor."], "question answering"], [["camembert: a tasty french language model", "louis martin | benjamin muller | pedro javier ortiz su\u00e1rez | yoann dupont | laurent romary | \u00e9ric de la clergerie | djam\u00e9 seddah | beno\u00eet sagot", "pretrained language models are now ubiquitous in natural language processing. despite their success, most available models have either been trained on english data or on the concatenation of data in multiple languages. this makes practical use of such models \u2013in all languages except english\u2013 very limited. in this paper, we investigate the feasibility of training monolingual transformer-based language models for other languages, taking french as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. we show that the use of web crawled data is preferable to the use of wikipedia data. more surprisingly, we show that a relatively small web crawled dataset (4gb) leads to results that are as good as those obtained using larger datasets (130+gb). our best performing model camembert reaches or improves the state of the art in all four downstream tasks."], "machine learning for nlp"], [["simple unsupervised summarization by contextual matching", "jiawei zhou | alexander rush", "we propose an unsupervised method for sentence summarization using only language modeling. the approach employs two language models, one that is generic (i.e. pretrained), and the other that is specific to the target domain. we show that by using a product-of-experts criteria these are enough for maintaining continuous contextual matching while maintaining output fluency. experiments on both abstractive and extractive sentence summarization data sets show promising results of our method without being exposed to any paired data."], "summarization"], [["babywalk: going farther in vision-and-language navigation by taking baby steps", "wang zhu | hexiang hu | jiacheng chen | zhiwei deng | vihan jain | eugene ie | fei sha", "learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (vln). in this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. we show that existing state-of-the-art agents do not generalize well. to this end, we propose babywalk, a new vln agent that is learned to navigate by decomposing long instructions into shorter ones (babysteps) and completing them sequentially. a special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. the learning process is composed of two phases. in the first phase, the agent uses imitation learning from demonstration to accomplish babysteps. in the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. we create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine babywalk\u2019s generalization ability. empirical results show that babywalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. the codes and the datasets are released on our project page: https://github.com/sha-lab/babywalk."], "language grounding to vision, robotics and beyond"], [["rationalizing medical relation prediction from corpus-level statistics", "zhen wang | jennifer lee | simon lin | huan sun", "nowadays, the interpretability of machine learning models is becoming increasingly important, especially in the medical domain. aiming to shed some light on how to rationalize medical relation prediction, we present a new interpretable framework inspired by existing theories on how human memory works, e.g., theories of recall and recognition. given the corpus-level statistics, i.e., a global co-occurrence graph of a clinical text corpus, to predict the relations between two entities, we first recall rich contexts associated with the target entities, and then recognize relational interactions between these contexts to form model rationales, which will contribute to the final prediction. we conduct experiments on a real-world public clinical dataset and show that our framework can not only achieve competitive predictive performance against a comprehensive list of neural baseline models, but also present rationales to justify its prediction. we further collaborate with medical experts deeply to verify the usefulness of our model rationales for clinical decision making."], "information extraction, retrieval and text mining"], [["a sweet rabbit hole by darcy: using honeypots to detect universal trigger\u2019s adversarial attacks", "thai le | noseong park | dongwon lee", "the universal trigger (unitrigger) is a recently-proposed powerful adversarial textual attack method. utilizing a learning-based mechanism, unitrigger generates a fixed phrase that, when added to any benign inputs, can drop the prediction accuracy of a textual neural network (nn) model to near zero on a target class. to defend against this attack that can cause significant harm, in this paper, we borrow the \u201choneypot\u201d concept from the cybersecurity community and propose darcy, a honeypot-based defense framework against unitrigger. darcy greedily searches and injects multiple trapdoors into an nn model to \u201cbait and catch\u201d potential attacks. through comprehensive experiments across four public datasets, we show that darcy detects unitrigger\u2019s adversarial attacks with up to 99% tpr and less than 2% fpr in most cases, while maintaining the prediction accuracy (in f1) for clean inputs within a 1% margin. we also demonstrate that darcy with multiple trapdoors is also robust to a diverse set of attack scenarios with attackers\u2019 varying levels of knowledge and skills. we release the source code of darcy at: https://github.com/lethaiq/acl2021-darcy-honeypotdefensenlp."], "machine learning for nlp"], [["what should i ask? using conversationally informative rewards for goal-oriented visual dialog.", "pushkar shukla | carlos elmadjian | richika sharan | vivek kulkarni | matthew turk | william yang wang", "the ability to engage in goal-oriented conversations has allowed humans to gain knowledge, reduce uncertainty, and perform tasks more efficiently. artificial agents, however, are still far behind humans in having goal-driven conversations. in this work, we focus on the task of goal-oriented visual dialogue, aiming to automatically generate a series of questions about an image with a single objective. this task is challenging since these questions must not only be consistent with a strategy to achieve a goal, but also consider the contextual information in the image. we propose an end-to-end goal-oriented visual dialogue system, that combines reinforcement learning with regularized information gain. unlike previous approaches that have been proposed for the task, our work is motivated by the rational speech act framework, which models the process of human inquiry to reach a goal. we test the two versions of our model on the guesswhat?! dataset, obtaining significant results that outperform the current state-of-the-art models in the task of generating questions to find an undisclosed object in an image."], "language grounding to vision, robotics and beyond"], [["learning to update natural language comments based on code changes", "sheena panthaplackel | pengyu nie | milos gligoric | junyi jessy li | raymond mooney", "we formulate the novel task of automatically updating an existing natural language comment based on changes in the body of code it accompanies. we propose an approach that learns to correlate changes across two distinct language representations, to generate a sequence of edits that are applied to the existing comment to reflect the source code modifications. we train and evaluate our model using a dataset that we collected from commit histories of open-source software projects, with each example consisting of a concurrent update to a method and its corresponding comment. we compare our approach against multiple baselines using both automatic metrics and human evaluation. results reflect the challenge of this task and that our model outperforms baselines with respect to making edits."], "generation"], [["a transformer-based approach for source code summarization", "wasi ahmad | saikat chakraborty | baishakhi ray | kai-wei chang", "generating a readable summary that describes the functionality of a program is known as source code summarization. in this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial. to learn code representation for summarization, we explore the transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. in this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin. we perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens\u2019 position hinders, while relative encoding significantly improves the summarization performance. we have made our code publicly available to facilitate future research."], "summarization"], [["compguesswhat?!: a multi-task evaluation framework for grounded language learning", "alessandro suglia | ioannis konstas | andrea vanzo | emanuele bastianelli | desmond elliott | stella frank | oliver lemon", "approaches to grounded language learning are commonly focused on a single task-based final performance measure which may not depend on desirable properties of the learned hidden representations, such as their ability to predict object attributes or generalize to unseen situations. to remedy this, we present grolla, an evaluation framework for grounded language learning with attributes based on three sub-tasks: 1) goal-oriented evaluation; 2) object attribute prediction evaluation; and 3) zero-shot evaluation. we also propose a new dataset compguesswhat?! as an instance of this framework for evaluating the quality of learned neural representations, in particular with respect to attribute grounding. to this end, we extend the original guesswhat?! dataset by including a semantic layer on top of the perceptual one. specifically, we enrich the visualgenome scene graphs associated with the guesswhat?! images with several attributes from resources such as visa and imsitu. we then compare several hidden state representations from current state-of-the-art approaches to grounded language learning. by using diagnostic classifiers, we show that current models\u2019 learned representations are not expressive enough to encode object attributes (average f1 of 44.27). in addition, they do not learn strategies nor representations that are robust enough to perform well when novel scenes or objects are involved in gameplay (zero-shot best accuracy 50.06%)."], "language grounding to vision, robotics and beyond"], [["enriched in-order linearization for faster sequence-to-sequence constituent parsing", "daniel fern\u00e1ndez-gonz\u00e1lez | carlos g\u00f3mez-rodr\u00edguez", "sequence-to-sequence constituent parsing requires a linearization to represent trees as sequences. top-down tree linearizations, which can be based on brackets or shift-reduce actions, have achieved the best accuracy to date. in this paper, we show that these results can be improved by using an in-order linearization instead. based on this observation, we implement an enriched in-order shift-reduce linearization inspired by vinyals et al. (2015)\u2019s approach, achieving the best accuracy to date on the english ptb dataset among fully-supervised single-model sequence-to-sequence constituent parsers. finally, we apply deterministic attention mechanisms to match the speed of state-of-the-art transition-based parsers, thus showing that sequence-to-sequence models can match them, not only in accuracy, but also in speed."], "tagging, chunking, syntax and parsing"], [["few-shot nlg with pre-trained language model", "zhiyu chen | harini eavani | wenhu chen | yinyin liu | william yang wang", "neural-based end-to-end approaches to natural language generation (nlg) from structured data or knowledge are data-hungry, making their adoption for real-world applications difficult with limited data. in this work, we propose the new task of few-shot natural language generation. motivated by how humans tend to summarize tabular data, we propose a simple yet effective approach and show that it not only demonstrates strong performance but also provides good generalization across domains. the design of the model architecture is based on two aspects: content selection from input data and language modeling to compose coherent sentences, which can be acquired from prior knowledge. with just 200 training examples, across multiple domains, we show that our approach achieves very reasonable performances and outperforms the strongest baseline by an average of over 8.0 bleu points improvement. our code and data can be found at https://github.com/czyssrs/few-shot-nlg"], "generation"], [["unsupervised multimodal neural machine translation with pseudo visual pivoting", "po-yao huang | junjie hu | xiaojun chang | alexander hauptmann", "unsupervised machine translation (mt) has recently achieved impressive results with monolingual corpora only. however, it is still challenging to associate source-target sentences in the latent space. as people speak different languages biologically share similar visual systems, the potential of achieving better alignment through visual content is promising yet under-explored in unsupervised multimodal mt (mmt). in this paper, we investigate how to utilize visual content for disambiguation and promoting latent space alignment in unsupervised mmt. our model employs multimodal back-translation and features pseudo visual pivoting in which we learn a shared multilingual visual-semantic embedding space and incorporate visually-pivoted captioning as additional weak supervision. the experimental results on the widely used multi30k dataset show that the proposed model significantly improves over the state-of-the-art methods and generalizes well when images are not available at the testing time."], "language grounding to vision, robotics and beyond"], [["can we predict new facts with open knowledge graph embeddings? a benchmark for open link prediction", "samuel broscheit | kiril gashteovski | yanjie wang | rainer gemulla", "open information extraction systems extract (\u201csubject text\u201d, \u201crelation text\u201d, \u201cobject text\u201d) triples from raw text. some triples are textual versions of facts, i.e., non-canonicalized mentions of entities and relations. in this paper, we investigate whether it is possible to infer new facts directly from the open knowledge graph without any canonicalization or any supervision from curated knowledge. for this purpose, we propose the open link prediction task,i.e., predicting test facts by completing (\u201csubject text\u201d, \u201crelation text\u201d, ?) questions. an evaluation in such a setup raises the question if a correct prediction is actually a new fact that was induced by reasoning over the open knowledge graph or if it can be trivially explained. for example, facts can appear in different paraphrased textual variants, which can lead to test leakage. to this end, we propose an evaluation protocol and a methodology for creating the open link prediction benchmark olpbench. we performed experiments with a prototypical knowledge graph embedding model for openlink prediction. while the task is very challenging, our results suggests that it is possible to predict genuinely new facts, which can not be trivially explained."], "semantics"], [["neural machine translation with reordering embeddings", "kehai chen | rui wang | masao utiyama | eiichiro sumita", "the reordering model plays an important role in phrase-based statistical machine translation. however, there are few works that exploit the reordering information in neural machine translation. in this paper, we propose a reordering mechanism to learn the reordering embedding of a word based on its contextual information. these learned reordering embeddings are stacked together with self-attention networks to learn sentence representation for machine translation. the reordering mechanism can be easily integrated into both the encoder and the decoder in the transformer translation system. experimental results on wmt\u201914 english-to-german, nist chinese-to-english, and wat japanese-to-english translation tasks demonstrate that the proposed methods can significantly improve the performance of the transformer."], "machine translation and multilinguality"], [["exploring unexplored generalization challenges for cross-database semantic parsing", "alane suhr | ming-wei chang | peter shaw | kenton lee", "we study the task of cross-database semantic parsing (xsp), where a system that maps natural language utterances to executable sql queries is evaluated on databases unseen during training. recently, several datasets, including spider, were proposed to support development of xsp systems. we propose a challenging evaluation setup for cross-database semantic parsing, focusing on variation across database schemas and in-domain language use. we re-purpose eight semantic parsing datasets that have been well-studied in the setting where in-domain training data is available, and instead use them as additional evaluation data for xsp systems instead. we build a system that performs well on spider, and find that it struggles to generalize to our re-purposed set. our setup uncovers several generalization challenges for cross-database semantic parsing, demonstrating the need to use and develop diverse training and evaluation datasets."], "semantics"], [["course concept expansion in moocs with external knowledge and interactive game", "jifan yu | chenyu wang | gan luo | lei hou | juanzi li | zhiyuan liu | jie tang", "as massive open online courses (moocs) become increasingly popular, it is promising to automatically provide extracurricular knowledge for mooc users. suffering from semantic drifts and lack of knowledge guidance, existing methods can not effectively expand course concepts in complex mooc environments. in this paper, we first build a novel boundary during searching for new concepts via external knowledge base and then utilize heterogeneous features to verify the high-quality results. in addition, to involve human efforts in our model, we design an interactive optimization mechanism based on a game. our experiments on the four datasets from coursera and xuetangx show that the proposed method achieves significant improvements(+0.19 by map) over existing methods."], "nlp applications"], [["semi-supervised dialogue policy learning via stochastic reward estimation", "xinting huang | jianzhong qi | yu sun | rui zhang", "dialogue policy optimization often obtains feedback until task completion in task-oriented dialogue systems. this is insufficient for training intermediate dialogue turns since supervision signals (or rewards) are only provided at the end of dialogues. to address this issue, reward learning has been introduced to learn from state-action pairs of an optimal policy to provide turn-by-turn rewards. this approach requires complete state-action annotations of human-to-human dialogues (i.e., expert demonstrations), which is labor intensive. to overcome this limitation, we propose a novel reward learning approach for semi-supervised policy learning. the proposed approach learns a dynamics model as the reward function which models dialogue progress (i.e., state-action sequences) based on expert demonstrations, either with or without annotations. the dynamics model computes rewards by predicting whether the dialogue progress is consistent with expert demonstrations. we further propose to learn action embeddings for a better generalization of the reward function. the proposed approach outperforms competitive policy learning baselines on multiwoz, a benchmark multi-domain dataset."], "dialogue and interactive systems"], [["representing schema structure with graph neural networks for text-to-sql parsing", "ben bogin | jonathan berant | matt gardner", "research on parsing language to sql has largely ignored the structure of the database (db) schema, either because the db was very simple, or because it was observed at both training and test time. in spider, a recently-released text-to-sql dataset, new and complex dbs are given at test time, and so the structure of the db schema can inform the predicted sql query. in this paper, we present an encoder-decoder semantic parser, where the structure of the db schema is encoded with a graph neural network, and this representation is later used at both encoding and decoding time. evaluation shows that encoding the schema structure improves our parser accuracy from 33.8% to 39.4%, dramatically above the current state of the art, which is at 19.7%."], "semantics"], [["document modeling with graph attention networks for multi-grained machine reading comprehension", "bo zheng | haoyang wen | yaobo liang | nan duan | wanxiang che | daxin jiang | ming zhou | ting liu", "natural questions is a new challenging machine reading comprehension benchmark with two-grained answers, which are a long answer (typically a paragraph) and a short answer (one or more entities inside the long answer). despite the effectiveness of existing methods on this benchmark, they treat these two sub-tasks individually during training while ignoring their dependencies. to address this issue, we present a novel multi-grained machine reading comprehension framework that focuses on modeling documents at their hierarchical nature, which are different levels of granularity: documents, paragraphs, sentences, and tokens. we utilize graph attention networks to obtain different levels of representations so that they can be learned simultaneously. the long and short answers can be extracted from paragraph-level representation and token-level representation, respectively. in this way, we can model the dependencies between the two-grained answers to provide evidence for each other. we jointly train the two sub-tasks, and our experiments show that our approach significantly outperforms previous systems at both long and short answer criteria."], "question answering"], [["exploring content selection in summarization of novel chapters", "faisal ladhak | bryan li | yaser al-onaizan | kathleen mckeown", "we present a new summarization task, generating summaries of novel chapters using summary/chapter pairs from online study guides. this is a harder task than the news summarization task, given the chapter length as well as the extreme paraphrasing and generalization found in the summaries. we focus on extractive summarization, which requires the creation of a gold-standard set of extractive summaries. we present a new metric for aligning reference summary sentences with chapter sentences to create gold extracts and also experiment with different alignment methods. our experiments demonstrate significant improvement over prior alignment approaches for our task as shown through automatic metrics and a crowd-sourced pyramid analysis."], "summarization"], [["learning to abstract for memory-augmented conversational response generation", "zhiliang tian | wei bi | xiaopeng li | nevin l. zhang", "neural generative models for open-domain chit-chat conversations have become an active area of research in recent years. a critical issue with most existing generative models is that the generated responses lack informativeness and diversity. a few researchers attempt to leverage the results of retrieval models to strengthen the generative models, but these models are limited by the quality of the retrieval results. in this work, we propose a memory-augmented generative model, which learns to abstract from the training corpus and saves the useful information to the memory to assist the response generation. our model clusters query-response samples, extracts characteristics of each cluster, and learns to utilize these characteristics for response generation. experimental results show that our model outperforms other competitive baselines."], "dialogue and interactive systems"], [["efficient dialogue state tracking by selectively overwriting memory", "sungdong kim | sohee yang | gyuwan kim | sang-woo lee", "recent works in dialogue state tracking (dst) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches. however, they are inefficient in that they predict the dialogue state at every turn from scratch. here, we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting mechanism for more efficient dst. this mechanism consists of two steps: (1) predicting state operation on each of the memory slots, and (2) overwriting the memory with new values, of which only a few are generated according to the predicted state operations. our method decomposes dst into two sub-tasks and guides the decoder to focus only on one of the tasks, thus reducing the burden of the decoder. this enhances the effectiveness of training and dst performance. our som-dst (selectively overwriting memory for dialogue state tracking) model achieves state-of-the-art joint goal accuracy with 51.72% in multiwoz 2.0 and 53.01% in multiwoz 2.1 in an open vocabulary-based dst setting. in addition, we analyze the accuracy gaps between the current and the ground truth-given situations and suggest that it is a promising direction to improve state operation prediction to boost the dst performance."], "dialogue and interactive systems"], [["ecpe-2d: emotion-cause pair extraction based on joint two-dimensional representation, interaction and prediction", "zixiang ding | rui xia | jianfei yu", "in recent years, a new interesting task, called emotion-cause pair extraction (ecpe), has emerged in the area of text emotion analysis. it aims at extracting the potential pairs of emotions and their corresponding causes in a document. to solve this task, the existing research employed a two-step framework, which first extracts individual emotion set and cause set, and then pair the corresponding emotions and causes. however, such a pipeline of two steps contains some inherent flaws: 1) the modeling does not aim at extracting the final emotion-cause pair directly; 2) the errors from the first step will affect the performance of the second step. to address these shortcomings, in this paper we propose a new end-to-end approach, called ecpe-two-dimensional (ecpe-2d), to represent the emotion-cause pairs by a 2d representation scheme. a 2d transformer module and two variants, window-constrained and cross-road 2d transformers, are further proposed to model the interactions of different emotion-cause pairs. the 2d representation, interaction, and prediction are integrated into a joint framework. in addition to the advantages of joint modeling, the experimental results on the benchmark emotion cause corpus show that our approach improves the f1 score of the state-of-the-art from 61.28% to 68.89%."], "sentiment analysis, stylistic analysis, and argument mining"], [["transition-based semantic dependency parsing with pointer networks", "daniel fern\u00e1ndez-gonz\u00e1lez | carlos g\u00f3mez-rodr\u00edguez", "transition-based parsers implemented with pointer networks have become the new state of the art in dependency parsing, excelling in producing labelled syntactic trees and outperforming graph-based models in this task. in order to further test the capabilities of these powerful neural networks on a harder nlp problem, we propose a transition system that, thanks to pointer networks, can straightforwardly produce labelled directed acyclic graphs and perform semantic dependency parsing. in addition, we enhance our approach with deep contextualized word embeddings extracted from bert. the resulting system not only outperforms all existing transition-based models, but also matches the best fully-supervised accuracy to date on the semeval 2015 task 18 datasets among previous state-of-the-art graph-based parsers."], "semantics"], [["leveraging graph to improve abstractive multi-document summarization", "wei li | xinyan xiao | jiachen liu | hua wu | haifeng wang | junping du", "graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. in this paper, we develop a neural abstractive multi-document summarization (mds) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. empirical results on the wikisum and multinews dataset show that the proposed architecture brings substantial improvements over several strong baselines."], "summarization"], [["building a user-generated content north-african arabizi treebank: tackling hell", "djam\u00e9 seddah | farah essaidi | amal fethi | matthieu futeral | benjamin muller | pedro javier ortiz su\u00e1rez | beno\u00eet sagot | abhishek srivastava", "we introduce the first treebank for a romanized user-generated content variety of algerian, a north-african arabic dialect known for its frequent usage of code-switching. made of 1500 sentences, fully annotated in morpho-syntax and universal dependency syntax, with full translation at both the word and the sentence levels, this treebank is made freely available. it is supplemented with 50k unlabeled sentences collected from common crawl and web-crawled data using intensive data-mining techniques. preliminary experiments demonstrate its usefulness for pos tagging and dependency parsing. we believe that what we present in this paper is useful beyond the low-resource language community. this is the first time that enough unlabeled and annotated data is provided for an emerging user-generated content dialectal language with rich morphology and code switching, making it an challenging test-bed for most recent nlp approaches."], "resources and evaluation"], [["how multilingual is multilingual bert?", "telmo pires | eva schlinger | dan garrette", "in this paper, we show that multilingual bert (m-bert), released by devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. to understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. from these results, we can conclude that m-bert does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs."], "machine translation and multilinguality"], [["learning to faithfully rationalize by construction", "sarthak jain | sarah wiegreffe | yuval pinter | byron c. wallace", "in many settings it is important for one to be able to understand why a model made a particular prediction. in nlp this often entails extracting snippets of an input text \u2018responsible for\u2019 corresponding model output; when such a snippet comprises tokens that indeed informed the model\u2019s prediction, it is a faithful explanation. in some settings, faithfulness may be critical to ensure transparency. lei et al. (2016) proposed a model to produce faithful rationales for neural text classification by defining independent snippet extraction and prediction modules. however, the discrete selection over input tokens performed by this method complicates training, leading to high variance and requiring careful hyperparameter tuning. we propose a simpler variant of this approach that provides faithful explanations by construction. in our scheme, named fresh, arbitrary feature importance scores (e.g., gradients from a trained model) are used to induce binary labels over token inputs, which an extractor can be trained to predict. an independent classifier module is then trained exclusively on snippets provided by the extractor; these snippets thus constitute faithful explanations, even if the classifier is arbitrarily complex. in both automatic and manual evaluations we find that variants of this simple framework yield predictive performance superior to \u2018end-to-end\u2019 approaches, while being more general and easier to train. code is available at https://github.com/successar/fresh."], "interpretability and analysis of models for nlp"], [["exploring sequence-to-sequence learning in aspect term extraction", "dehong ma | sujian li | fangzhao wu | xing xie | houfeng wang", "aspect term extraction (ate) aims at identifying all aspect terms in a sentence and is usually modeled as a sequence labeling problem. however, sequence labeling based methods cannot make full use of the overall meaning of the whole sentence and have the limitation in processing dependencies between labels. to tackle these problems, we first explore to formalize ate as a sequence-to-sequence (seq2seq) learning task where the source sequence and target sequence are composed of words and labels respectively. at the same time, to make seq2seq learning suit to ate where labels correspond to words one by one, we design the gated unit networks to incorporate corresponding word representation into the decoder, and position-aware attention to pay more attention to the adjacent words of a target word. the experimental results on two datasets show that seq2seq learning is effective in ate accompanied with our proposed gated unit networks and position-aware attention mechanism."], "sentiment analysis, stylistic analysis, and argument mining"], [["explicit memory tracker with coarse-to-fine reasoning for conversational machine reading", "yifan gao | chien-sheng wu | shafiq joty | caiming xiong | richard socher | irwin king | michael lyu | steven c.h. hoi", "the goal of conversational machine reading is to answer user questions given a knowledge base text which may require asking clarification questions. existing approaches are limited in their decision making due to struggles in extracting question-related rules and reasoning about them. in this paper, we present a new framework of conversational machine reading that comprises a novel explicit memory tracker (emt) to track whether conditions listed in the rule text have already been satisfied to make a decision. moreover, our framework generates clarification questions by adopting a coarse-to-fine reasoning strategy, utilizing sentence-level entailment scores to weight token-level distributions. on the sharc benchmark (blind, held-out) testset, emt achieves new state-of-the-art results of 74.6% micro-averaged decision accuracy and 49.5 bleu4. we also show that emt is more interpretable by visualizing the entailment-oriented reasoning process as the conversation flows. code and models are released at https://github.com/yifan-gao/explicit_memory_tracker."], "question answering"], [["what makes a good counselor? learning to distinguish between high-quality and low-quality counseling conversations", "ver\u00f3nica p\u00e9rez-rosas | xinyi wu | kenneth resnicow | rada mihalcea", "the quality of a counseling intervention relies highly on the active collaboration between clients and counselors. in this paper, we explore several linguistic aspects of the collaboration process occurring during counseling conversations. specifically, we address the differences between high-quality and low-quality counseling. our approach examines participants\u2019 turn-by-turn interaction, their linguistic alignment, the sentiment expressed by speakers during the conversation, as well as the different topics being discussed. our results suggest important language differences in low- and high-quality counseling, which we further use to derive linguistic features able to capture the differences between the two groups. these features are then used to build automatic classifiers that can predict counseling quality with accuracies of up to 88%."], "nlp applications"], [["gender bias in multilingual embeddings and cross-lingual transfer", "jieyu zhao | subhabrata mukherjee | saghar hosseini | kai-wei chang | ahmed hassan awadallah", "multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language. these embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (nlp) model trained on one language is deployed to another language. while the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages. in this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for nlp applications. we create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives. experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning. we further provide recommendations for using the multilingual word representations for downstream tasks."], "ethics in nlp"], [["learning to identify follow-up questions in conversational question answering", "souvik kundu | qian lin | hwee tou ng", "despite recent progress in conversational question answering, most prior work does not focus on follow-up questions. practical conversational question answering systems often receive follow-up questions in an ongoing conversation, and it is crucial for a system to be able to determine whether a question is a follow-up question of the current conversation, for more effective answer finding subsequently. in this paper, we introduce a new follow-up question identification task. we propose a three-way attentive pooling network that determines the suitability of a follow-up question by capturing pair-wise interactions between the associated passage, the conversation history, and a candidate follow-up question. it enables the model to capture topic continuity and topic shift while scoring a particular candidate follow-up question. experiments show that our proposed three-way attentive pooling network outperforms all baseline systems by significant margins."], "question answering"], [["improving the similarity measure of determinantal point processes for extractive multi-document summarization", "sangwoo cho | logan lebanoff | hassan foroosh | fei liu", "the most important obstacles facing multi-document summarization include excessive redundancy in source descriptions and the looming shortage of training data. these obstacles prevent encoder-decoder models from being used directly, but optimization-based methods such as determinantal point processes (dpps) are known to handle them well. in this paper we seek to strengthen a dpp-based method for extractive multi-document summarization by presenting a novel similarity measure inspired by capsule networks. the approach measures redundancy between a pair of sentences based on surface form and semantic information. we show that our dpp system with improved similarity measure performs competitively, outperforming strong summarization baselines on benchmark datasets. our findings are particularly meaningful for summarizing documents created by multiple authors containing redundant yet lexically diverse expressions."], "summarization"], [["neural reranking for dependency parsing: an evaluation", "bich-ngoc do | ines rehbein", "recent work has shown that neural rerankers can improve results for dependency parsing over the top k trees produced by a base parser. however, all neural rerankers so far have been evaluated on english and chinese only, both languages with a configurational word order and poor morphology. in the paper, we re-assess the potential of successful neural reranking models from the literature on english and on two morphologically rich(er) languages, german and czech. in addition, we introduce a new variation of a discriminative reranker based on graph convolutional networks (gcns). we show that the gcn not only outperforms previous models on english but is the only model that is able to improve results over the baselines on german and czech. we explain the differences in reranking performance based on an analysis of a) the gold tree ratio and b) the variety in the k-best lists."], "tagging, chunking, syntax and parsing"], [["kingdom: knowledge-guided domain adaptation for sentiment analysis", "deepanway ghosal | devamanyu hazarika | abhinaba roy | navonil majumder | rada mihalcea | soujanya poria", "cross-domain sentiment analysis has received significant attention in recent years, prompted by the need to combat the domain gap between different applications that make use of sentiment analysis. in this paper, we take a novel perspective on this task by exploring the role of external commonsense knowledge. we introduce a new framework, kingdom, which utilizes the conceptnet knowledge graph to enrich the semantics of a document by providing both domain-specific and domain-general background concepts. these concepts are learned by training a graph convolutional autoencoder that leverages inter-domain concepts in a domain-invariant manner. conditioning a popular domain-adversarial baseline method with these learned concepts helps improve its performance over state-of-the-art approaches, demonstrating the efficacy of our proposed framework."], "sentiment analysis, stylistic analysis, and argument mining"], [["multimodal neural graph memory networks for visual question answering", "mahmoud khademi", "we introduce a new neural network architecture, multimodal neural graph memory networks (mn-gmn), for visual question answering. the mn-gmn uses graph structure with different region features as node attributes and applies a recently proposed powerful graph neural network model, graph network (gn), to reason about objects and their interactions in an image. the input module of the mn-gmn generates a set of visual features plus a set of encoded region-grounded captions (rgcs) for the image. the rgcs capture object attributes and their relationships. two gns are constructed from the input module using the visual features and encoded rgcs. each node of the gns iteratively computes a question-guided contextualized representation of the visual/textual information assigned to it. then, to combine the information from both gns, the nodes write the updated representations to an external spatial memory. the final states of the memory cells are fed into an answer module to predict an answer. experiments show mn-gmn rivals the state-of-the-art models on visual7w, vqa-v2.0, and clevr datasets."], "language grounding to vision, robotics and beyond"], [["two birds, one stone: a simple, unified model for text generation from structured and unstructured data", "hamidreza shahidi | ming li | jimmy lin", "a number of researchers have recently questioned the necessity of increasingly complex neural network (nn) architectures. in particular, several recent papers have shown that simpler, properly tuned models are at least competitive across several nlp tasks. in this work, we show that this is also the case for text generation from structured and unstructured data. we consider neural table-to-text generation and neural question generation (nqg) tasks for text generation from structured and unstructured data, respectively. table-to-text generation aims to generate a description based on a given table, and nqg is the task of generating a question from a given passage where the generated question can be answered by a certain sub-span of the passage using nn models. experimental results demonstrate that a basic attention-based seq2seq model trained with the exponential moving average technique achieves the state of the art in both tasks. code is available at https://github.com/h-shahidi/2birds-gen."], "generation"], [["modeling morphological typology for unsupervised learning of language morphology", "hongzhi xu | jordan kodner | mitchell marcus | charles yang", "this paper describes a language-independent model for fully unsupervised morphological analysis that exploits a universal framework leveraging morphological typology. by modeling morphological processes including suffixation, prefixation, infixation, and full and partial reduplication with constrained stem change rules, our system effectively constrains the search space and offers a wide coverage in terms of morphological typology. the system is tested on nine typologically and genetically diverse languages, and shows superior performance over leading systems. we also investigate the effect of an oracle that provides only a handful of bits per language to signal morphological type."], "phonology, morphology and word segmentation"], [["simple and effective paraphrastic similarity from parallel translations", "john wieting | kevin gimpel | graham neubig | taylor berg-kirkpatrick", "we present a model and methodology for learning paraphrastic sentence embeddings directly from bitext, removing the time-consuming intermediate step of creating para-phrase corpora. further, we show that the resulting model can be applied to cross lingual tasks where it both outperforms and is orders of magnitude faster than more complex state-of-the-art baselines."], "semantics"], [["generating diverse and consistent qa pairs from contexts with information-maximizing hierarchical conditional vaes", "dong bok lee | seanie lee | woo tae jeong | donghwan kim | sung ju hwang", "one of the most crucial challenges in question answering (qa) is the scarcity of labeled data, since it is costly to obtain question-answer (qa) pairs for a target text domain with human annotation. an alternative approach to tackle the problem is to use automatically generated qa pairs from either the problem context or from large amount of unstructured texts (e.g. wikipedia). in this work, we propose a hierarchical conditional variational autoencoder (hcvae) for generating qa pairs given unstructured texts as contexts, while maximizing the mutual information between generated qa pairs to ensure their consistency. we validate our information maximizing hierarchical conditional variational autoencoder (info-hcvae) on several benchmark datasets by evaluating the performance of the qa model (bert-base) using only the generated qa pairs (qa-based evaluation) or by using both the generated and human-labeled pairs (semi-supervised learning) for training, against state-of-the-art baseline models. the results show that our model obtains impressive performance gains over all baselines on both tasks, using only a fraction of data for training."], "generation"], [["improving multimodal named entity recognition via entity span detection with unified multimodal transformer", "jianfei yu | jing jiang | li yang | rui xia", "in this paper, we study multimodal named entity recognition (mner) for social media posts. existing approaches for mner mainly suffer from two drawbacks: (1) despite generating word-aware visual representations, their word representations are insensitive to the visual context; (2) most of them ignore the bias brought by the visual context. to tackle the first issue, we propose a multimodal interaction module to obtain both image-aware word representations and word-aware visual representations. to alleviate the visual bias, we further propose to leverage purely text-based entity span detection as an auxiliary module, and design a unified multimodal transformer to guide the final predictions with the entity span predictions. experiments show that our unified approach achieves the new state-of-the-art performance on two benchmark datasets."], "computational social science, social media and cultural analytics"], [["introducing orthogonal constraint in structural probes", "tomasz limisiewicz | david mare\u010dek", "with the recent success of pre-trained models in nlp, a significant focus was put on interpreting their representations. one of the most prominent approaches is structural probing (hewitt and manning, 2019), where a linear projection of word embeddings is performed in order to approximate the topology of dependency structures. in this work, we introduce a new type of structural probing, where the linear projection is decomposed into 1. iso-morphic space rotation; 2. linear scaling that identifies and scales the most relevant dimensions. in addition to syntactic dependency, we evaluate our method on two novel tasks (lexical hypernymy and position in a sentence). we jointly train the probes for multiple tasks and experimentally show that lexical and syntactic information is separated in the representations. moreover, the orthogonal constraint makes the structural probes less vulnerable to memorization."], "interpretability and analysis of models for nlp"], [["improving question answering over incomplete kbs with knowledge-aware reader", "wenhan xiong | mo yu | shiyu chang | xiaoxiao guo | william yang wang", "we propose a new end-to-end question answering model, which learns to aggregate answer evidence from an incomplete knowledge base (kb) and a set of retrieved text snippets.under the assumptions that structured data is easier to query and the acquired knowledge can help the understanding of unstructured text, our model first accumulates knowledge ofkb entities from a question-related kb sub-graph; then reformulates the question in the latent space and reads the text with the accumulated entity knowledge at hand. the evidence from kb and text are finally aggregated to predict answers. on the widely-used kbqa benchmark webqsp, our model achieves consistent improvements across settings with different extents of kb incompleteness."], "question answering"], [["flat: chinese ner using flat-lattice transformer", "xiaonan li | hang yan | xipeng qiu | xuanjing huang", "recently, the character-word lattice structure has been proved to be effective for chinese named entity recognition (ner) by incorporating the word information. however, since the lattice structure is complex and dynamic, the lattice-based models are hard to fully utilize the parallel computation of gpus and usually have a low inference speed. in this paper, we propose flat: flat-lattice transformer for chinese ner, which converts the lattice structure into a flat structure consisting of spans. each span corresponds to a character or latent word and its position in the original lattice. with the power of transformer and well-designed position encoding, flat can fully leverage the lattice information and has an excellent parallel ability. experiments on four datasets show flat outperforms other lexicon-based models in performance and efficiency."], "information extraction, retrieval and text mining"], [["psycholinguistics meets continual learning: measuring catastrophic forgetting in visual question answering", "claudio greco | barbara plank | raquel fern\u00e1ndez | raffaella bernardi", "we study the issue of catastrophic forgetting in the context of neural multimodal approaches to visual question answering (vqa). motivated by evidence from psycholinguistics, we devise a set of linguistically-informed vqa tasks, which differ by the types of questions involved (wh-questions and polar questions). we test what impact task difficulty has on continual learning, and whether the order in which a child acquires question types facilitates computational models. our results show that dramatic forgetting is at play and that task difficulty and order matter. two well-known current continual learning methods mitigate the problem only to a limiting degree."], "question answering"], [["document translation vs. query translation for cross-lingual information retrieval in the medical domain", "shadi saleh | pavel pecina", "we present a thorough comparison of two principal approaches to cross-lingual information retrieval: document translation (dt) and query translation (qt). our experiments are conducted using the cross-lingual test collection produced within the clef ehealth information retrieval tasks in 2013\u20132015 containing english documents and queries in several european languages. we exploit the statistical machine translation (smt) and neural machine translation (nmt) paradigms and train several domain-specific and task-specific machine translation systems to translate the non-english queries into english (for the qt approach) and the english documents to all the query languages (for the dt approach). the results show that the quality of qt by smt is sufficient enough to outperform the retrieval results of the dt approach for all the languages. nmt then further boosts translation quality and retrieval quality for both qt and dt for most languages, but still, qt provides generally better retrieval results than dt."], "information extraction, retrieval and text mining"], [["simple and effective retrieve-edit-rerank text generation", "nabil hossain | marjan ghazvininejad | luke zettlemoyer", "retrieve-and-edit seq2seq methods typically retrieve an output from the training set and learn a model to edit it to produce the final output. we propose to extend this framework with a simple and effective post-generation ranking approach. our framework (i) retrieves several potentially relevant outputs for each input, (ii) edits each candidate independently, and (iii) re-ranks the edited candidates to select the final output. we use a standard editing model with simple task-specific re-ranking approaches, and we show empirically that this approach outperforms existing, significantly more complex methodologies. experiments on two machine translation (mt) datasets show new state-of-art results. we also achieve near state-of-art performance on the gigaword summarization dataset, where our analyses show that there is significant room for performance improvement with better candidate output selection in future work."], "generation"], [["multimodal and multi-view models for emotion recognition", "gustavo aguilar | viktor rozgic | weiran wang | chao wang", "studies on emotion recognition (er) show that combining lexical and acoustic information results in more robust and accurate models. the majority of the studies focus on settings where both modalities are available in training and evaluation. however, in practice, this is not always the case; getting asr output may represent a bottleneck in a deployment pipeline due to computational complexity or privacy-related constraints. to address this challenge, we study the problem of efficiently combining acoustic and lexical modalities during training while still providing a deployable acoustic model that does not require lexical inputs. we first experiment with multimodal models and two attention mechanisms to assess the extent of the benefits that lexical information can provide. then, we frame the task as a multi-view learning problem to induce semantic information from a multimodal model into our acoustic-only network using a contrastive loss function. our multimodal model outperforms the previous state of the art on the usc-iemocap dataset reported on lexical and acoustic information. additionally, our multi-view-trained acoustic network significantly surpasses models that have been exclusively trained with acoustic features."], "sentiment analysis, stylistic analysis, and argument mining"], [["a simple and effective approach to automatic post-editing with transfer learning", "gon\u00e7alo m. correia | andr\u00e9 f. t. martins", "automatic post-editing (ape) seeks to automatically refine the output of a black-box machine translation (mt) system through human post-edits. ape systems are usually trained by complementing human post-edited data with large, artificial data generated through back-translations, a time-consuming process often no easier than training a mt system from scratch. in this paper, we propose an alternative where we fine-tune pre-trained bert models on both the encoder and decoder of an ape system, exploring several parameter sharing strategies. by only training on a dataset of 23k sentences for 3 hours on a single gpu we obtain results that are competitive with systems that were trained on 5m artificial sentences. when we add this artificial data our method obtains state-of-the-art results."], "machine translation and multilinguality"], [["explaining black box predictions and unveiling data artifacts through influence functions", "xiaochuang han | byron c. wallace | yulia tsvetkov", "modern deep learning models for nlp are notoriously opaque. this has motivated the development of methods for interpreting such models, e.g., via gradient-based saliency maps or the visualization of attention weights. such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text. while this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input, we suspect that such highlighting is not suitable for tasks where model decisions should be driven by more complex reasoning. in this work, we investigate the use of influence functions for nlp, providing an alternative approach to interpreting neural text classifiers. influence functions explain the decisions of a model by identifying influential training examples. despite the promise of this approach, influence functions have not yet been extensively evaluated in the context of nlp, a gap addressed by this work. we conduct a comparison between influence functions and common word-saliency methods on representative tasks. as suspected, we find that influence functions are particularly useful for natural language inference, a task in which \u2018saliency maps\u2019 may not have clear interpretation. furthermore, we develop a new quantitative measure based on influence functions that can reveal artifacts in training data."], "interpretability and analysis of models for nlp"], [["aspect sentiment classification with document-level sentiment preference modeling", "xiao chen | changlong sun | jingjing wang | shoushan li | luo si | min zhang | guodong zhou", "in the literature, existing studies always consider aspect sentiment classification (asc) as an independent sentence-level classification problem aspect by aspect, which largely ignore the document-level sentiment preference information, though obviously such information is crucial for alleviating the information deficiency problem in asc. in this paper, we explore two kinds of sentiment preference information inside a document, i.e., contextual sentiment consistency w.r.t. the same aspect (namely intra-aspect sentiment consistency) and contextual sentiment tendency w.r.t. all the related aspects (namely inter-aspect sentiment tendency). on the basis, we propose a cooperative graph attention networks (cogan) approach for cooperatively learning the aspect-related sentence representation. specifically, two graph attention networks are leveraged to model above two kinds of document-level sentiment preference information respectively, followed by an interactive mechanism to integrate the two-fold preference. detailed evaluation demonstrates the great advantage of the proposed approach to asc over the state-of-the-art baselines. this justifies the importance of the document-level sentiment preference information to asc and the effectiveness of our approach capturing such information."], "sentiment analysis, stylistic analysis, and argument mining"], [["vifidel: evaluating the visual fidelity of image descriptions", "pranava madhyastha | josiah wang | lucia specia", "we address the task of evaluating image description generation systems. we propose a novel image-aware metric for this task: vifidel. it estimates the faithfulness of a generated caption with respect to the content of the actual image, based on the semantic similarity between labels of objects depicted in images and words in the description. the metric is also able to take into account the relative importance of objects mentioned in human reference descriptions during evaluation. even if these human reference descriptions are not available, vifidel can still reliably evaluate system descriptions. the metric achieves high correlation with human judgments on two well-known datasets and is competitive with metrics that depend on and rely exclusively on human references."], "language grounding to vision, robotics and beyond"], [["connecting embeddings for knowledge graph entity typing", "yu zhao | anxiang zhang | ruobing xie | kang liu | xiaojie wang", "knowledge graph (kg) entity typing aims at inferring possible missing entity type instances in kg, which is a very significant but still under-explored subtask of knowledge graph completion. in this paper, we propose a novel approach for kg entity typing which is trained by jointly utilizing local typing knowledge from existing entity type assertions and global triple knowledge in kgs. specifically, we present two distinct knowledge-driven effective mechanisms of entity type inference. accordingly, we build two novel embedding models to realize the mechanisms. afterward, a joint model via connecting them is used to infer missing entity type instances, which favors inferences that agree with both entity type instances and triple knowledge in kgs. experimental results on two real-world datasets (freebase and yago) demonstrate the effectiveness of our proposed mechanisms and models for improving kg entity typing. the source code and data of this paper can be obtained from: https://github.com/adam1679/connecte ."], "information extraction, retrieval and text mining"], [["delta embedding learning", "xiao zhang | ji wu | dejing dou", "unsupervised word embeddings have become a popular approach of word representation in nlp tasks. however there are limitations to the semantics represented by unsupervised embeddings, and inadequate fine-tuning of embeddings can lead to suboptimal performance. we propose a novel learning technique called delta embedding learning, which can be applied to general nlp tasks to improve performance by optimized tuning of the word embeddings. a structured regularization is applied to the embeddings to ensure they are tuned in an incremental way. as a result, the tuned word embeddings become better word representations by absorbing semantic information from supervision without \u201cforgetting.\u201d we apply the method to various nlp tasks and see a consistent improvement in performance. evaluation also confirms the tuned word embeddings have better semantic properties."], "semantics"], [["interactive construction of user-centric dictionary for text analytics", "ryosuke kohita | issei yoshida | hiroshi kanayama | tetsuya nasukawa", "we propose a methodology to construct a term dictionary for text analytics through an interactive process between a human and a machine, which helps the creation of flexible dictionaries with precise granularity required in typical text analysis. this paper introduces the first formulation of interactive dictionary construction to address this issue. to optimize the interaction, we propose a new algorithm that effectively captures an analyst\u2019s intention starting from only a small number of sample terms. along with the algorithm, we also design an automatic evaluation framework that provides a systematic assessment of any interactive method for the dictionary creation task. experiments using real scenario based corpora and dictionaries show that our algorithm outperforms baseline methods, and works even with a small number of interactions."], "information extraction, retrieval and text mining"], [["seek: segmented embedding of knowledge graphs", "wentao xu | shun zheng | liang he | bin shao | jian yin | tie-yan liu", "in recent years, knowledge graph embedding becomes a pretty hot research topic of artificial intelligence and plays increasingly vital roles in various downstream applications, such as recommendation and question answering. however, existing methods for knowledge graph embedding can not make a proper trade-off between the model complexity and the model expressiveness, which makes them still far from satisfactory. to mitigate this problem, we propose a lightweight modeling framework that can achieve highly competitive relational expressiveness without increasing the model complexity. our framework focuses on the design of scoring functions and highlights two critical characteristics: 1) facilitating sufficient feature interactions; 2) preserving both symmetry and antisymmetry properties of relations. it is noteworthy that owing to the general and elegant design of scoring functions, our framework can incorporate many famous existing methods as special cases. moreover, extensive experiments on public benchmarks demonstrate the efficiency and effectiveness of our framework. source codes and data can be found at https://github.com/wentao-xu/seek."], "machine learning for nlp"], [["posterior calibrated training on sentence classification tasks", "taehee jung | dongyeop kang | hua cheng | lucas mentch | thomas schaaf", "most classification models work by first predicting a posterior probability distribution over all classes and then selecting that class with the largest estimated probability. in many settings however, the quality of posterior probability itself (e.g., 65% chance having diabetes), gives more reliable information than the final predicted class alone. when these methods are shown to be poorly calibrated, most fixes to date have relied on posterior calibration, which rescales the predicted probabilities but often has little impact on final classifications. here we propose an end-to-end training procedure called posterior calibrated (poscal) training that directly optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities.we show that poscal not only helps reduce the calibration error but also improve task performance by penalizing drops in performance of both objectives. our poscal achieves about 2.5% of task performance gain and 16.1% of calibration error reduction on glue (wang et al., 2018) compared to the baseline. we achieved the comparable task performance with 13.2% calibration error reduction on xslue (kang and hovy, 2019), but not outperforming the two-stage calibration baseline. poscal training can be easily extendable to any types of classification tasks as a form of regularization term. also, poscal has the advantage that it incrementally tracks needed statistics for the calibration objective during the training process, making efficient use of large training sets."], "machine learning for nlp"], [["beyond user self-reported likert scale ratings: a comparison model for automatic dialog evaluation", "weixin liang | james zou | zhou yu", "open domain dialog system evaluation is one of the most important challenges in dialog research. existing automatic evaluation metrics, such as bleu are mostly reference-based. they calculate the difference between the generated response and a limited number of available references. likert-score based self-reported user rating is widely adopted by social conversational systems, such as amazon alexa prize chatbots. however, self-reported user rating suffers from bias and variance among different users. to alleviate this problem, we formulate dialog evaluation as a comparison task. we also propose an automatic evaluation model cmade (comparison model for automatic dialog evaluation) that automatically cleans self-reported user ratings as it trains on them. specifically, we first use a self-supervised method to learn better dialog feature representation, and then use knn and shapley to remove confusing samples. our experiments show that cmade achieves 89.2% accuracy in the dialog comparison task."], "dialogue and interactive systems"], [["deep dominance - how to properly compare deep neural models", "rotem dror | segev shlomov | roi reichart", "comparing between deep neural network (dnn) models based on their performance on unseen data is crucial for the progress of the nlp field. however, these models have a large number of hyper-parameters and, being non-convex, their convergence point depends on the random values chosen at initialization and during training. proper dnn comparison hence requires a comparison between their empirical score distributions on unseen data, rather than between single evaluation scores as is standard for more simple, convex models. in this paper, we propose to adapt to this problem a recently proposed test for the almost stochastic dominance relation between two distributions. we define the criteria for a high quality comparison method between dnns, and show, both theoretically and through analysis of extensive experimental results with leading dnn models for sequence tagging tasks, that the proposed test meets all criteria while previously proposed methods fail to do so. we hope the test we propose here will set a new working practice in the nlp community."], "resources and evaluation"], [["constructing interpretive spatio-temporal features for multi-turn responses selection", "junyu lu | chenbin zhang | zeying xie | guang ling | tom chao zhou | zenglin xu", "response selection plays an important role in fully automated dialogue systems. given the dialogue context, the goal of response selection is to identify the best-matched next utterance (i.e., response) from multiple candidates. despite the efforts of many previous useful models, this task remains challenging due to the huge semantic gap and also the large size of candidate set. to address these issues, we propose a spatio-temporal matching network (stm) for response selection. in detail, soft alignment is first used to obtain the local relevance between the context and the response. and then, we construct spatio-temporal features by aggregating attention images in time dimension and make use of 3d convolution and pooling operations to extract matching information. evaluation on two large-scale multi-turn response selection tasks has demonstrated that our proposed model significantly outperforms the state-of-the-art model. particularly, visualization analysis shows that the spatio-temporal features enables matching information in segment pairs and time sequences, and have good interpretability for multi-turn text matching."], "dialogue and interactive systems"], [["chinese relation extraction with multi-grained information and external linguistic knowledge", "ziran li | ning ding | zhiyuan liu | haitao zheng | ying shen", "chinese relation extraction is conducted using neural networks with either character-based or word-based inputs, and most existing methods typically suffer from segmentation errors and ambiguity of polysemy. to address the issues, we propose a multi-grained lattice framework (mg lattice) for chinese relation extraction to take advantage of multi-grained language information and external linguistic knowledge. in this framework, (1) we incorporate word-level information into character sequence inputs so that segmentation errors can be avoided. (2) we also model multiple senses of polysemous words with the help of external linguistic knowledge, so as to alleviate polysemy ambiguity. experiments on three real-world datasets in distinct domains show consistent and significant superiority and robustness of our model, as compared with other baselines. we will release the source code of this paper in the future."], "information extraction, retrieval and text mining"], [["fact-based content weighting for evaluating abstractive summarisation", "xinnuo xu | ond\u0159ej du\u0161ek | jingyi li | verena rieser | ioannis konstas", "abstractive summarisation is notoriously hard to evaluate since standard word-overlap-based metrics are insufficient. we introduce a new evaluation metric which is based on fact-level content weighting, i.e. relating the facts of the document to the facts of the summary. we fol- low the assumption that a good summary will reflect all relevant facts, i.e. the ones present in the ground truth (human-generated refer- ence summary). we confirm this hypothe- sis by showing that our weightings are highly correlated to human perception and compare favourably to the recent manual highlight- based metric of hardy et al. (2019)."], "summarization"], [["rationalizing text matching: learning sparse alignments via optimal transport", "kyle swanson | lili yu | tao lei", "selecting input features of top relevance has become a popular method for building self-explaining models. in this work, we extend this selective rationalization approach to text matching, where the goal is to jointly select and align text pieces, such as tokens or sentences, as a justification for the downstream prediction. our approach employs optimal transport (ot) to find a minimal cost alignment between the inputs. however, directly applying ot often produces dense and therefore uninterpretable alignments. to overcome this limitation, we introduce novel constrained variants of the ot problem that result in highly sparse alignments with controllable sparsity. our model is end-to-end differentiable using the sinkhorn algorithm for ot and can be trained without any alignment annotations. we evaluate our model on the stackexchange, multinews, e-snli, and multirc datasets. our model achieves very sparse rationale selections with high fidelity while preserving prediction accuracy compared to strong attention baseline models."], "interpretability and analysis of models for nlp"], [["this email could save your life: introducing the task of email subject line generation", "rui zhang | joel tetreault", "given the overwhelming number of emails, an effective subject line becomes essential to better inform the recipient of the email\u2019s content. in this paper, we propose and study the task of email subject line generation: automatically generating an email subject line from the email body. we create the first dataset for this task and find that email subject line generation favor extremely abstractive summary which differentiates it from news headline generation or news single document summarization. we then develop a novel deep learning method and compare it to several baselines as well as recent state-of-the-art text summarization systems. we also investigate the efficacy of several automatic metrics based on correlations with human judgments and propose a new automatic evaluation metric. our system outperforms competitive baselines given both automatic and human evaluations. to our knowledge, this is the first work to tackle the problem of effective email subject line generation."], "nlp applications"], [["storyboarding of recipes: grounded contextual generation", "khyathi chandu | eric nyberg | alan w black", "information need of humans is essentially multimodal in nature, enabling maximum exploitation of situated context. we introduce a dataset for sequential procedural (how-to) text generation from images in cooking domain. the dataset consists of 16,441 cooking recipes with 160,479 photos associated with different steps. we setup a baseline motivated by the best performing model in terms of human evaluation for the visual story telling (vist) task. in addition, we introduce two models to incorporate high level structure learnt by a finite state machine (fsm) in neural sequential generation process by: (1) scaffolding structure in decoder (ssid) (2) scaffolding structure in loss (ssil). our best performing model (ssil) achieves a meteor score of 0.31, which is an improvement of 0.6 over the baseline model. we also conducted human evaluation of the generated grounded recipes, which reveal that 61% found that our proposed (ssil) model is better than the baseline model in terms of overall recipes. we also discuss analysis of the output highlighting key important nlp issues for prospective directions."], "generation"], [["a simple and effective unified encoder for document-level machine translation", "shuming ma | dongdong zhang | ming zhou", "most of the existing models for document-level machine translation adopt dual-encoder structures. the representation of the source sentences and the document-level contexts are modeled with two separate encoders. although these models can make use of the document-level contexts, they do not fully model the interaction between the contexts and the source sentences, and can not directly adapt to the recent pre-training models (e.g., bert) which encodes multiple sentences with a single encoder. in this work, we propose a simple and effective unified encoder that can outperform the baseline models of dual-encoder models in terms of bleu and meteor scores. moreover, the pre-training models can further boost the performance of our proposed model."], "machine translation and multilinguality"], [["learning a multi-domain curriculum for neural machine translation", "wei wang | ye tian | jiquan ngiam | yinfei yang | isaac caswell | zarana parekh", "most data selection research in machine translation focuses on improving a single domain. we perform data selection for multiple domains at once. this is achieved by carefully introducing instance-level domain-relevance features and automatically constructing a training curriculum to gradually concentrate on multi-domain relevant and noise-reduced data batches. both the choice of features and the use of curriculum are crucial for balancing and improving all domains, including out-of-domain. in large-scale experiments, the multi-domain curriculum simultaneously reaches or outperforms the individual performance and brings solid gains over no-curriculum training."], "machine translation and multilinguality"], [["bilingual lexicon induction through unsupervised machine translation", "mikel artetxe | gorka labaka | eneko agirre", "a recent research line has obtained strong results on bilingual lexicon induction by aligning independently trained word embeddings in two languages and using the resulting cross-lingual embeddings to induce word translation pairs through nearest neighbor or related retrieval methods. in this paper, we propose an alternative approach to this problem that builds on the recent work on unsupervised machine translation. this way, instead of directly inducing a bilingual lexicon from cross-lingual embeddings, we use them to build a phrase-table, combine it with a language model, and use the resulting machine translation system to generate a synthetic parallel corpus, from which we extract the bilingual lexicon using statistical word alignment techniques. as such, our method can work with any word embedding and cross-lingual mapping technique, and it does not require any additional resource besides the monolingual corpus used to train the embeddings. when evaluated on the exact same cross-lingual embeddings, our proposed method obtains an average improvement of 6 accuracy points over nearest neighbor and 4 points over csls retrieval, establishing a new state-of-the-art in the standard muse dataset."], "machine translation and multilinguality"], [["self-attentive, multi-context one-class classification for unsupervised anomaly detection on text", "lukas ruff | yury zemlyanskiy | robert vandermeulen | thomas schnake | marius kloft", "there exist few text-specific methods for unsupervised anomaly detection, and for those that do exist, none utilize pre-trained models for distributed vector representations of words. in this paper we introduce a new anomaly detection method\u2014context vector data description (cvdd)\u2014which builds upon word embedding models to learn multiple sentence representations that capture multiple semantic contexts via the self-attention mechanism. modeling multiple contexts enables us to perform contextual anomaly detection of sentences and phrases with respect to the multiple themes and concepts present in an unlabeled text corpus. these contexts in combination with the self-attention weights make our method highly interpretable. we demonstrate the effectiveness of cvdd quantitatively as well as qualitatively on the well-known reuters, 20 newsgroups, and imdb movie reviews datasets."], "machine learning for nlp"], [["cross-media structured common space for multimedia event extraction", "manling li | alireza zareian | qi zeng | spencer whitehead | di lu | heng ji | shih-fu chang", "we introduce a new task, multimedia event extraction, which aims to extract events and their arguments from multimedia documents. we develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments. we propose a novel method, weakly aligned structured embedding (wase), that encodes structured representations of semantic information from textual and visual data into a common embedding space. the structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation. compared to uni-modal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute f-score gains on text event argument role labeling and visual event extraction. compared to state-of-the-art multimedia unstructured representations, we achieve 8.3% and 5.0% absolute f-score gains on multimedia event extraction and argument role labeling, respectively. by utilizing images, we extract 21.4% more event mentions than traditional text-only methods."], "language grounding to vision, robotics and beyond"], [["a generate-and-rank framework with semantic type regularization for biomedical concept normalization", "dongfang xu | zeyu zhang | steven bethard", "concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is challenging because ontologies are large. in most cases, annotated datasets cover only a small sample of the concepts, yet concept normalizers are expected to predict all concepts in the ontology. in this paper, we propose an architecture consisting of a candidate generator and a list-wise ranker based on bert. the ranker considers pairings of concept mentions and candidate concepts, allowing it to make predictions for any concept, not just those seen during training. we further enhance this list-wise approach with a semantic type regularizer that allows the model to incorporate semantic type information from the ontology during training. our proposed concept normalization framework achieves state-of-the-art performance on multiple datasets."], "information extraction, retrieval and text mining"], [["autoencoding pixies: amortised variational inference with graph convolutions for functional distributional semantics", "guy emerson", "functional distributional semantics provides a linguistically interpretable framework for distributional semantics, by representing the meaning of a word as a function (a binary classifier), instead of a vector. however, the large number of latent variables means that inference is computationally expensive, and training a model is therefore slow to converge. in this paper, i introduce the pixie autoencoder, which augments the generative model of functional distributional semantics with a graph-convolutional neural network to perform amortised variational inference. this allows the model to be trained more effectively, achieving better results on two tasks (semantic similarity in context and semantic composition), and outperforming bert, a large pre-trained language model."], "semantics"], [["successfully applying the stabilized lottery ticket hypothesis to the transformer architecture", "christopher brix | parnia bahar | hermann ney", "sparse models require less memory for storage and enable a faster inference by reducing the necessary number of flops. this is relevant both for time-critical and on-device computations using neural networks. the stabilized lottery ticket hypothesis states that networks can be pruned after none or few training iterations, using a mask computed based on the unpruned converged model. on the transformer architecture and the wmt 2014 english-to-german and english-to-french tasks, we show that stabilized lottery ticket pruning performs similar to magnitude pruning for sparsity levels of up to 85%, and propose a new combination of pruning techniques that outperforms all other techniques for even higher levels of sparsity. furthermore, we confirm that the parameter\u2019s initial sign and not its specific value is the primary factor for successful training, and show that magnitude pruning cannot be used to find winning lottery tickets."], "machine translation and multilinguality"], [["exploiting explicit paths for multi-hop reading comprehension", "souvik kundu | tushar khot | ashish sabharwal | peter clark", "we propose a novel, path-based reasoning approach for the multi-hop reading comprehension task where a system needs to combine facts from multiple passages to answer a question. although inspired by multi-hop reasoning over knowledge graphs, our proposed approach operates directly over unstructured text. it generates potential paths through passages and scores them without any direct path supervision. the proposed model, named pathnet, attempts to extract implicit relations from text through entity pair representations, and compose them to encode each path. to capture additional context, pathnet also composes the passage representations along each path to compute a passage-based representation. unlike previous approaches, our model is then able to explain its reasoning via these explicit paths through the passages. we show that our approach outperforms prior models on the multi-hop wikihop dataset, and also can be generalized to apply to the openbookqa dataset, matching state-of-the-art performance."], "question answering"], [["modeling affirmative and negated action processing in the brain with lexical and compositional semantic models", "vesna djokic | jean maillard | luana bulat | ekaterina shutova", "recent work shows that distributional semantic models can be used to decode patterns of brain activity associated with individual words and sentence meanings. however, it is yet unclear to what extent such models can be used to study and decode fmri patterns associated with specific aspects of semantic composition such as the negation function. in this paper, we apply lexical and compositional semantic models to decode fmri patterns associated with negated and affirmative sentences containing hand-action verbs. our results show reduced decoding (correlation) of sentences where the verb is in the negated context, as compared to the affirmative one, within brain regions implicated in action-semantic processing. this supports behavioral and brain imaging studies, suggesting that negation involves reduced access to aspects of the affirmative mental representation. the results pave the way for testing alternate semantic models of negation against human semantic processing in the brain."], "linguistic theories, cognitive modeling and psycholinguistics"], [["effective cross-lingual transfer of neural machine translation models without shared vocabularies", "yunsu kim | yingbo gao | hermann ney", "transfer learning or multilingual model is essential for low-resource neural machine translation (nmt), but the applicability is limited to cognate languages by sharing their vocabularies. this paper shows effective techniques to transfer a pretrained nmt model to a new, unrelated language without shared vocabularies. we relieve the vocabulary mismatch by using cross-lingual word embedding, train a more language-agnostic encoder by injecting artificial noises, and generate synthetic data easily from the pretraining data without back-translation. our methods do not require restructuring the vocabulary or retraining the model. we improve plain nmt transfer by up to +5.1% bleu in five low-resource translation tasks, outperforming multilingual joint training by a large margin. we also provide extensive ablation studies on pretrained embedding, synthetic data, vocabulary size, and parameter freezing for a better understanding of nmt transfer."], "machine translation and multilinguality"], [["visualsparta: an embarrassingly simple approach to large-scale text-to-image search with weighted bag-of-words", "xiaopeng lu | tiancheng zhao | kyusong lee", "text-to-image retrieval is an essential task in cross-modal information retrieval, i.e., retrieving relevant images from a large and unlabelled dataset given textual queries. in this paper, we propose visualsparta, a novel (visual-text sparse transformer matching) model that shows significant improvement in terms of both accuracy and efficiency. visualsparta is capable of outperforming previous state-of-the-art scalable methods in mscoco and flickr30k. we also show that it achieves substantial retrieving speed advantages, i.e., for a 1 million image index, visualsparta using cpu gets ~391x speedup compared to cpu vector search and ~5.4x speedup compared to vector search with gpu acceleration. experiments show that this speed advantage even gets bigger for larger datasets because visualsparta can be efficiently implemented as an inverted index. to the best of our knowledge, visualsparta is the first transformer-based text-to-image retrieval model that can achieve real-time searching for large-scale datasets, with significant accuracy improvement compared to previous state-of-the-art methods."], "information extraction, retrieval and text mining"], [["an online semantic-enhanced dirichlet model for short text stream clustering", "jay kumar | junming shao | salah uddin | wazir ali", "clustering short text streams is a challenging task due to its unique properties: infinite length, sparse data representation and cluster evolution. existing approaches often exploit short text streams in a batch way. however, determine the optimal batch size is usually a difficult task since we have no priori knowledge when the topics evolve. in addition, traditional independent word representation in graphical model tends to cause \u201cterm ambiguity\u201d problem in short text clustering. therefore, in this paper, we propose an online semantic-enhanced dirichlet model for short sext stream clustering, called osdm, which integrates the word-occurance semantic information (i.e., context) into a new graphical model and clusters each arriving short text automatically in an online way. extensive results have demonstrated that osdm has better performance compared to many state-of-the-art algorithms on both synthetic and real-world data sets."], "information extraction, retrieval and text mining"], [["symbolic inductive bias for visually grounded learning of spoken language", "grzegorz chrupa\u0142a", "a widespread approach to processing spoken language is to first automatically transcribe it into text. an alternative is to use an end-to-end approach: recent works have proposed to learn semantic embeddings of spoken language from images with spoken captions, without an intermediate transcription step. we propose to use multitask learning to exploit existing transcribed speech within the end-to-end setting. we describe a three-task architecture which combines the objectives of matching spoken captions with corresponding images, speech with text, and text with images. we show that the addition of the speech/text task leads to substantial performance improvements on image retrieval when compared to training the speech/image task in isolation. we conjecture that this is due to a strong inductive bias transcribed speech provides to the model, and offer supporting evidence for this."], "language grounding to vision, robotics and beyond"], [["goemotions: a dataset of fine-grained emotions", "dorottya demszky | dana movshovitz-attias | jeongwoo ko | alan cowen | gaurav nemade | sujith ravi", "understanding emotion expressed in language has a wide range of applications, from building empathetic chatbots to detecting harmful online behavior. advancement in this area can be improved using large-scale datasets with a fine-grained typology, adaptable to multiple downstream tasks. we introduce goemotions, the largest manually annotated dataset of 58k english reddit comments, labeled for 27 emotion categories or neutral. we demonstrate the high quality of the annotations via principal preserved component analysis. we conduct transfer learning experiments with existing emotion benchmarks to show that our dataset generalizes well to other domains and different emotion taxonomies. our bert-based model achieves an average f1-score of .46 across our proposed taxonomy, leaving much room for improvement."], "sentiment analysis, stylistic analysis, and argument mining"], [["improving truthfulness of headline generation", "kazuki matsumaru | sho takase | naoaki okazaki", "most studies on abstractive summarization report rouge scores between system and reference summaries. however, we have a concern about the truthfulness of generated summaries: whether all facts of a generated summary are mentioned in the source text. this paper explores improving the truthfulness in headline generation on two popular datasets. analyzing headlines generated by the state-of-the-art encoder-decoder model, we show that the model sometimes generates untruthful headlines. we conjecture that one of the reasons lies in untruthful supervision data used for training the model. in order to quantify the truthfulness of article-headline pairs, we consider the textual entailment of whether an article entails its headline. after confirming quite a few untruthful instances in the datasets, this study hypothesizes that removing untruthful instances from the supervision data may remedy the problem of the untruthful behaviors of the model. building a binary classifier that predicts an entailment relation between an article and its headline, we filter out untruthful instances from the supervision data. experimental results demonstrate that the headline generation model trained on filtered supervision data shows no clear difference in rouge scores but remarkable improvements in automatic and manual evaluations of the generated headlines."], "summarization"], [["evidence-based trustworthiness", "yi zhang | zachary ives | dan roth", "the information revolution brought with it information pollution. information retrieval and extraction help us cope with abundant information from diverse sources. but some sources are of anonymous authorship, and some are of uncertain accuracy, so how can we determine what we should actually believe? not all information sources are equally trustworthy, and simply accepting the majority view is often wrong. this paper develops a general framework for estimating the trustworthiness of information sources in an environment where multiple sources provide claims and supporting evidence, and each claim can potentially be produced by multiple sources. we consider two settings: one in which information sources directly assert claims, and a more realistic and challenging one, in which claims are inferred from evidence provided by sources, via (possibly noisy) nlp techniques. our key contribution is to develop a family of probabilistic models that jointly estimate the trustworthiness of sources, and the credibility of claims they assert. this is done while accounting for the (possibly noisy) nlp needed to infer claims from evidence supplied by sources. we evaluate our framework on several datasets, showing strong results and significant improvement over baselines."], "nlp applications"], [["it\u2019s easier to translate out of english than into it: measuring neural translation difficulty by cross-mutual information", "emanuele bugliarello | sabrina j. mielke | antonios anastasopoulos | ryan cotterell | naoaki okazaki", "the performance of neural machine translation systems is commonly evaluated in terms of bleu. however, due to its reliance on target language properties and generation, the bleu metric does not allow an assessment of which translation directions are more difficult to model. in this paper, we propose cross-mutual information (xmi): an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models. xmi allows us to better evaluate the difficulty of translating text into the target language while controlling for the difficulty of the target-side generation component independent of the translation task. we then present the first systematic and controlled study of cross-lingual translation difficulties using modern neural translation systems. code for replicating our experiments is available online at https://github.com/e-bug/nmt-difficulty."], "machine translation and multilinguality"], [["margin-based parallel corpus mining with multilingual sentence embeddings", "mikel artetxe | holger schwenk", "machine translation is highly sensitive to the size and quality of the training data, which has led to an increasing interest in collecting and filtering large parallel corpora. in this paper, we propose a new method for this task based on multilingual sentence embeddings. in contrast to previous approaches, which rely on nearest neighbor retrieval with a hard threshold over cosine similarity, our proposed method accounts for the scale inconsistencies of this measure, considering the margin between a given sentence pair and its closest candidates instead. our experiments show large improvements over existing methods. we outperform the best published results on the bucc mining task and the un reconstruction task by more than 10 f1 and 30 precision points, respectively. filtering the english-german paracrawl corpus with our approach, we obtain 31.2 bleu points on newstest2014, an improvement of more than one point over the best official filtered version."], "machine translation and multilinguality"], [["distilling translations with visual awareness", "julia ive | pranava madhyastha | lucia specia", "previous work on multimodal machine translation has shown that visual information is only needed in very specific cases, for example in the presence of ambiguous words where the textual context is not sufficient. as a consequence, models tend to learn to ignore this information. we propose a translate-and-refine approach to this problem where images are only used by a second stage decoder. this approach is trained jointly to generate a good first draft translation and to improve over this draft by (i) making better use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context. this approach leads to the state of the art results. additionally, we show that it has the ability to recover from erroneous or missing words in the source language."], "language grounding to vision, robotics and beyond"], [["unsupervised cross-lingual representation learning at scale", "alexis conneau | kartikay khandelwal | naman goyal | vishrav chaudhary | guillaume wenzek | francisco guzm\u00e1n | edouard grave | myle ott | luke zettlemoyer | veselin stoyanov", "this paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. we train a transformer-based masked language model on one hundred languages, using more than two terabytes of filtered commoncrawl data. our model, dubbed xlm-r, significantly outperforms multilingual bert (mbert) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on xnli, +13% average f1 score on mlqa, and +2.4% f1 score on ner. xlm-r performs particularly well on low-resource languages, improving 15.7% in xnli accuracy for swahili and 11.4% for urdu over previous xlm models. we also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; xlm-r is very competitive with strong monolingual models on the glue and xnli benchmarks. we will make our code and models publicly available."], "semantics"], [["are natural language inference models imppressive? learning implicature and presupposition", "paloma jeretic | alex warstadt | suvrat bhooshan | adina williams", "natural language inference (nli) is an increasingly important task for natural language understanding, which requires one to infer whether a sentence entails another. however, the ability of nli models to make pragmatic inferences remains understudied. we create an implicature and presupposition diagnostic dataset (imppres), consisting of 32k semi-automatically generated sentence pairs illustrating well-studied pragmatic inference types. we use imppres to evaluate whether bert, infersent, and bow nli models trained on multinli (williams et al., 2018) learn to make pragmatic inferences. although multinli appears to contain very few pairs illustrating these inference types, we find that bert learns to draw pragmatic inferences. it reliably treats scalar implicatures triggered by \u201csome\u201d as entailments. for some presupposition triggers like \u201conly\u201d, bert reliably recognizes the presupposition as an entailment, even when the trigger is embedded under an entailment canceling operator like negation. bow and infersent show weaker evidence of pragmatic reasoning. we conclude that nli training encourages models to learn some, but not all, pragmatic inferences."], "semantics"], [["towards emotion-aided multi-modal dialogue act classification", "tulika saha | aditya patra | sriparna saha | pushpak bhattacharyya", "the task of dialogue act classification (dac) that purports to capture communicative intent has been studied extensively. but these studies limit themselves to text. non-verbal features (change of tone, facial expressions etc.) can provide cues to identify das, thus stressing the benefit of incorporating multi-modal inputs in the task. also, the emotional state of the speaker has a substantial effect on the choice of the dialogue act, since conversations are often influenced by emotions. hence, the effect of emotion too on automatic identification of das needs to be studied. in this work, we address the role of both multi-modality and emotion recognition (er) in dac. dac and er help each other by way of multi-task learning. one of the major contributions of this work is a new dataset- multimodal emotion aware dialogue act dataset called emotyda, collected from open-sourced dialogue datasets. to demonstrate the utility of emotyda, we build an attention based (self, inter-modal, inter-task) multi-modal, multi-task deep neural network (dnn) for joint learning of das and emotions. we show empirically that multi-modality and multi-tasking achieve better performance of dac compared to uni-modal and single task dac variants."], "speech and multimodality"], [["dynamic online conversation recommendation", "xingshan zeng | jing li | lu wang | zhiming mao | kam-fai wong", "trending topics in social media content evolve over time, and it is therefore crucial to understand social media users and their interpersonal communications in a dynamic manner. here we study dynamic online conversation recommendation, to help users engage in conversations that satisfy their evolving interests. while most prior work assumes static user interests, our model is able to capture the temporal aspects of user interests, and further handle future conversations that are unseen during training time. concretely, we propose a neural architecture to exploit changes of user interactions and interests over time, to predict which discussions they are likely to enter. we conduct experiments on large-scale collections of reddit conversations, and results on three subreddits show that our model significantly outperforms state-of-the-art models that make a static assumption of user interests. we further evaluate on handling \u201ccold start\u201d, and observe consistently better performance by our model when considering various degrees of sparsity of user\u2019s chatting history and conversation contexts. lastly, analyses on our model outputs indicate user interest change, explaining the advantage and efficacy of our approach."], "computational social science, social media and cultural analytics"], [["semantic scaffolds for pseudocode-to-code generation", "ruiqi zhong | mitchell stern | dan klein", "we propose a method for program generation based on semantic scaffolds, lightweight structures representing the high-level semantic and syntactic composition of a program. by first searching over plausible scaffolds then using these as constraints for a beam search over programs, we achieve better coverage of the search space when compared with existing techniques. we apply our hierarchical search method to the spoc dataset for pseudocode-to-code generation, in which we are given line-level natural language pseudocode annotations and aim to produce a program satisfying execution-based test cases. by using semantic scaffolds during inference, we achieve a 10% absolute improvement in top-100 accuracy over the previous state-of-the-art. additionally, we require only 11 candidates to reach the top-3000 performance of the previous best approach when tested against unseen problems, demonstrating a substantial improvement in efficiency."], "nlp applications"], [["code and named entity recognition in stackoverflow", "jeniya tabassum | mounica maddela | wei xu | alan ritter", "there is an increasing interest in studying natural language and computer code together, as large corpora of programming texts become readily available on the internet. for example, stackoverflow currently has over 15 million programming related questions written by 8.5 million users. meanwhile, there is still a lack of fundamental nlp techniques for identifying code tokens or software-related named entities that appear within natural language sentences. in this paper, we introduce a new named entity recognition (ner) corpus for the computer programming domain, consisting of 15,372 sentences annotated with 20 fine-grained entity types. we trained in-domain bert representations (bertoverflow) on 152 million sentences from stackoverflow, which lead to an absolute increase of +10 f1 score over off-the-shelf bert. we also present the softner model which achieves an overall 79.10 f-1 score for code and named entity recognition on stackoverflow data. our softner model incorporates a context-independent code token classifier with corpus-level features to improve the bert-based tagging model. our code and data are available at: https://github.com/jeniyat/stackoverflowner/"], "resources and evaluation"], [["writing by memorizing: hierarchical retrieval-based medical report generation", "xingyi yang | muchao ye | quanzeng you | fenglong ma", "medical report generation is one of the most challenging tasks in medical image analysis. although existing approaches have achieved promising results, they either require a predefined template database in order to retrieve sentences or ignore the hierarchical nature of medical report generation. to address these issues, we propose medwriter that incorporates a novel hierarchical retrieval mechanism to automatically extract both report and sentence-level templates for clinically accurate report generation. medwriter first employs the visual-language retrieval (vlr) module to retrieve the most relevant reports for the given images. to guarantee the logical coherence between generated sentences, the language-language retrieval (llr) module is introduced to retrieve relevant sentences based on the previous generated description. at last, a language decoder fuses image features and features from retrieved reports and sentences to generate meaningful medical reports. we verified the effectiveness of our model by automatic evaluation and human evaluation on two datasets, i.e., open-i and mimic-cxr."], "nlp applications"], [["training neural machine translation to apply terminology constraints", "georgiana dinu | prashant mathur | marcello federico | yaser al-onaizan", "this paper proposes a novel method to inject custom terminology into neural machine translation at run time. previous works have mainly proposed modifications to the decoding algorithm in order to constrain the output to include run-time-provided target terms. while being effective, these constrained decoding methods add, however, significant computational overhead to the inference step, and, as we show in this paper, can be brittle when tested in realistic conditions. in this paper we approach the problem by training a neural mt system to learn how to use custom terminology when provided with the input. comparative experiments show that our method is not only more effective than a state-of-the-art implementation of constrained decoding, but is also as fast as constraint-free decoding."], "machine translation and multilinguality"], [["bert-based lexical substitution", "wangchunshu zhou | tao ge | ke xu | furu wei | ming zhou", "previous studies on lexical substitution tend to obtain substitute candidates by finding the target word\u2019s synonyms from lexical resources (e.g., wordnet) and then rank the candidates based on its contexts. these approaches have two limitations: (1) they are likely to overlook good substitute candidates that are not the synonyms of the target words in the lexical resources; (2) they fail to take into account the substitution\u2019s influence on the global context of the sentence. to address these issues, we propose an end-to-end bert-based lexical substitution approach which can propose and validate substitute candidates without using any annotated data or manually curated resources. our approach first applies dropout to the target word\u2019s embedding for partially masking the word, allowing bert to take balanced consideration of the target word\u2019s semantics and contexts for proposing substitute candidates, and then validates the candidates based on their substitution\u2019s influence on the global contextualized representation of the sentence. experiments show our approach performs well in both proposing and ranking substitute candidates, achieving the state-of-the-art results in both ls07 and ls14 benchmarks."], "semantics"], [["a methodology for creating question answering corpora using inverse data annotation", "jan deriu | katsiaryna mlynchyk | philippe schl\u00e4pfer | alvaro rodrigo | dirk von gr\u00fcnigen | nicolas kaiser | kurt stockinger | eneko agirre | mark cieliebak", "in this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data. for this, we introduce an intermediate representation that is based on the logical query plan in a database, called operation trees (ot). this representation allows us to invert the annotation process without loosing flexibility in the types of queries that we generate. furthermore, it allows for fine-grained alignment of the tokens to the operations. thus, we randomly generate ots from a context free grammar and annotators just have to write the appropriate question and assign the tokens. we compare our corpus otta (operation trees and token assignment), a large semantic parsing corpus for evaluating natural language interfaces to databases, to spider and lc-quad 2.0 and show that our methodology more than triples the annotation speed while maintaining the complexity of the queries. finally, we train a state-of-the-art semantic parsing model on our data and show that our dataset is a challenging dataset and that the token alignment can be leveraged to significantly increase the performance."], "question answering"], [["simple and effective text matching with richer alignment features", "runqi yang | jianhai zhang | xing gao | feng ji | haiqing chen", "in this paper, we present a fast and strong neural approach for general purpose text matching applications. we explore what is sufficient to build a fast and well-performed text matching model and propose to keep three key features available for inter-sequence alignment: original point-wise features, previous aligned features, and contextual features while simplifying all the remaining components. we conduct experiments on four well-studied benchmark datasets across tasks of natural language inference, paraphrase identification and answer selection. the performance of our model is on par with the state-of-the-art on all datasets with much fewer parameters and the inference speed is at least 6 times faster compared with similarly performed ones."], "semantics"], [["learning from omission", "bill mcdowell | noah goodman", "pragmatic reasoning allows humans to go beyond the literal meaning when interpret- ing language in context. previous work has shown that such reasoning can improve the performance of already-trained language understanding systems. here, we explore whether pragmatic reasoning during training can improve the quality of learned meanings. our experiments on reference game data show that end-to-end pragmatic training produces more accurate utterance interpretation models, especially when data is sparse and language is complex."], "discourse and pragmatics"], [["on the robustness of self-attentive models", "yu-lun hsieh | minhao cheng | da-cheng juan | wei wei | wen-lian hsu | cho-jui hsieh", "this work examines the robustness of self-attentive neural networks against adversarial input perturbations. specifically, we investigate the attention and feature extraction mechanisms of state-of-the-art recurrent neural networks and self-attentive architectures for sentiment analysis, entailment and machine translation under adversarial attacks. we also propose a novel attack algorithm for generating more natural adversarial examples that could mislead neural models but not humans. experimental results show that, compared to recurrent neural models, self-attentive models are more robust against adversarial perturbation. in addition, we provide theoretical explanations for their superior robustness to support our claims."], "machine learning for nlp"], [["distilling knowledge learned in bert for text generation", "yen-chun chen | zhe gan | yu cheng | jingzhou liu | jingjing liu", "large-scale pre-trained language model such as bert has achieved great success in language understanding tasks. however, it remains an open question how to utilize bert for language generation. in this paper, we present a novel approach, conditional masked language modeling (c-mlm), to enable the finetuning of bert on target generation tasks. the finetuned bert (teacher) is exploited as extra supervision to improve conventional seq2seq models (student) for better text generation performance. by leveraging bert\u2019s idiosyncratic bidirectional nature, distilling knowledge learned in bert can encourage auto-regressive seq2seq models to plan ahead, imposing global sequence-level supervision for coherent text generation. experiments show that the proposed approach significantly outperforms strong transformer baselines on multiple language generation tasks such as machine translation and text summarization. our proposed model also achieves new state of the art on iwslt german-english and english-vietnamese mt datasets."], "generation"], [["learning dialog policies from weak demonstrations", "gabriel gordon-hall | philip john gorinski | shay b. cohen", "deep reinforcement learning is a promising approach to training a dialog manager, but current methods struggle with the large state and action spaces of multi-domain dialog systems. building upon deep q-learning from demonstrations (dqfd), an algorithm that scores highly in difficult atari games, we leverage dialog data to guide the agent to successfully respond to a user\u2019s requests. we make progressively fewer assumptions about the data needed, using labeled, reduced-labeled, and even unlabeled data to train expert demonstrators. we introduce reinforced fine-tune learning, an extension to dqfd, enabling us to overcome the domain gap between the datasets and the environment. experiments in a challenging multi-domain dialog system framework validate our approaches, and get high success rates even when trained on out-of-domain data."], "dialogue and interactive systems"], [["guiding variational response generator to exploit persona", "bowen wu | mengyuan li | zongsheng wang | yifu chen | derek f. wong | qihang feng | junhong huang | baoxun wang", "leveraging persona information of users in neural response generators (nrg) to perform personalized conversations has been considered as an attractive and important topic in the research of conversational agents over the past few years. despite of the promising progress achieved by recent studies in this field, persona information tends to be incorporated into neural networks in the form of user embeddings, with the expectation that the persona can be involved via end-to-end learning. this paper proposes to adopt the personality-related characteristics of human conversations into variational response generators, by designing a specific conditional variational autoencoder based deep model with two new regularization terms employed to the loss function, so as to guide the optimization towards the direction of generating both persona-aware and relevant responses. besides, to reasonably evaluate the performances of various persona modeling approaches, this paper further presents three direct persona-oriented metrics from different perspectives. the experimental results have shown that our proposed methodology can notably improve the performance of persona-aware response generation, and the metrics are reasonable to evaluate the results."], "dialogue and interactive systems"], [["a prioritization model for suicidality risk assessment", "han-chin shing | philip resnik | douglas oard", "we reframe suicide risk assessment from social media as a ranking problem whose goal is maximizing detection of severely at-risk individuals given the time available. building on measures developed for resource-bounded document retrieval, we introduce a well founded evaluation paradigm, and demonstrate using an expert-annotated test collection that meaningful improvements over plausible cascade model baselines can be achieved using an approach that jointly ranks individuals and their social media posts."], "information extraction, retrieval and text mining"], [["from arguments to key points: towards automatic argument summarization", "roy bar-haim | lilach eden | roni friedman | yoav kantor | dan lahav | noam slonim", "generating a concise summary from a large collection of arguments on a given topic is an intriguing yet understudied problem. we propose to represent such summaries as a small set of talking points, termed key points, each scored according to its salience. we show, by analyzing a large dataset of crowd-contributed arguments, that a small number of key points per topic is typically sufficient for covering the vast majority of the arguments. furthermore, we found that a domain expert can often predict these key points in advance. we study the task of argument-to-key point mapping, and introduce a novel large-scale dataset for this task. we report empirical results for an extensive set of experiments with this dataset, showing promising performance."], "sentiment analysis, stylistic analysis, and argument mining"], [["model-agnostic meta-learning for relation classification with limited supervision", "abiola obamuyide | andreas vlachos", "in this paper we frame the task of supervised relation classification as an instance of meta-learning. we propose a model-agnostic meta-learning protocol for training relation classifiers to achieve enhanced predictive performance in limited supervision settings. during training, we aim to not only learn good parameters for classifying relations with sufficient supervision, but also learn model parameters that can be fine-tuned to enhance predictive performance for relations with limited supervision. in experiments conducted on two relation classification datasets, we demonstrate that the proposed meta-learning approach improves the predictive performance of two state-of-the-art supervised relation classification models."], "information extraction, retrieval and text mining"], [["self-attentional models for lattice inputs", "matthias sperber | graham neubig | ngoc-quan pham | alex waibel", "lattices are an efficient and effective method to encode ambiguity of upstream systems in natural language processing tasks, for example to compactly capture multiple speech recognition hypotheses, or to represent multiple linguistic analyses. previous work has extended recurrent neural networks to model lattice inputs and achieved improvements in various tasks, but these models suffer from very slow computation speeds. this paper extends the recently proposed paradigm of self-attention to handle lattice inputs. self-attention is a sequence modeling technique that relates inputs to one another by computing pairwise similarities and has gained popularity for both its strong results and its computational efficiency. to extend such models to handle lattices, we introduce probabilistic reachability masks that incorporate lattice structure into the model and support lattice scores if available. we also propose a method for adapting positional embeddings to lattice structures. we apply the proposed model to a speech translation task and find that it outperforms all examined baselines while being much faster to compute than previous neural lattice models during both training and inference."], "machine translation and multilinguality"], [["heterogeneous graph transformer for graph-to-sequence learning", "shaowei yao | tianming wang | xiaojun wan", "the graph-to-sequence (graph2seq) learning aims to transduce graph-structured representations to word sequences for text generation. recent studies propose various models to encode graph structure. however, most previous works ignore the indirect relations between distance nodes, or treat indirect relations and direct relations in the same way. in this paper, we propose the heterogeneous graph transformer to independently model the different relations in the individual subgraphs of the original graph, including direct relations, indirect relations and multiple possible relations between nodes. experimental results show that our model strongly outperforms the state of the art on all four standard benchmarks of amr-to-text generation and syntax-based neural machine translation."], "generation"], [["do transformers need deep long-range memory?", "jack rae | ali razavi", "deep attention models have advanced the modelling of sequential data across many domains. for language modelling in particular, the transformer-xl \u2014 a transformer augmented with a long-range memory of past activations \u2014 has been shown to be state-of-the-art across a variety of well-studied benchmarks. the transformer-xl incorporates a long-range memory at every layer of the network, which renders its state to be thousands of times larger than rnn predecessors. however it is unclear whether this is necessary. we perform a set of interventions to show that comparable performance can be obtained with 6x fewer long range memories and better performance can be obtained by limiting the range of attention in lower layers of the network."], "machine learning for nlp"], [["you write like you eat: stylistic variation as a predictor of social stratification", "angelo basile | albert gatt | malvina nissim", "inspired by labov\u2019s seminal work on stylisticvariation as a function of social stratification,we develop and compare neural models thatpredict a person\u2019s presumed socio-economicstatus, obtained through distant supervision,from their writing style on social media. thefocus of our work is on identifying the mostimportant stylistic parameters to predict socio-economic group. in particular, we show theeffectiveness of morpho-syntactic features aspredictors of style, in contrast to lexical fea-tures, which are good predictors of topic"], "computational social science, social media and cultural analytics"], [["sparc: cross-domain semantic parsing in context", "tao yu | rui zhang | michihiro yasunaga | yi chern tan | xi victoria lin | suyi li | heyang er | irene li | bo pang | tao chen | emily ji | shreya dixit | david proctor | sungrok shim | jonathan kraft | vincent zhang | caiming xiong | richard socher | dragomir radev", "we present sparc, a dataset for cross-domainsemanticparsing incontext that consists of 4,298 coherent question sequences (12k+ individual questions annotated with sql queries). it is obtained from controlled user interactions with 200 complex databases over 138 domains. we provide an in-depth analysis of sparc and show that it introduces new challenges compared to existing datasets. sparc demonstrates complex contextual dependencies, (2) has greater semantic diversity, and (3) requires generalization to unseen domains due to its cross-domain nature and the unseen databases at test time. we experiment with two state-of-the-art text-to-sql models adapted to the context-dependent, cross-domain setup. the best model obtains an exact match accuracy of 20.2% over all questions and less than10% over all interaction sequences, indicating that the cross-domain setting and the con-textual phenomena of the dataset present significant challenges for future research. the dataset, baselines, and leaderboard are released at https://yale-lily.github.io/sparc."], "semantics"], [["unsupervised paraphrasing without translation", "aurko roy | david grangier", "paraphrasing is an important task demonstrating the ability to abstract semantic content from its surface form. recent literature on automatic paraphrasing is dominated by methods leveraging machine translation as an intermediate step. this contrasts with humans, who can paraphrase without necessarily being bilingual. this work proposes to learn paraphrasing models only from a monolingual corpus. to that end, we propose a residual variant of vector-quantized variational auto-encoder. our experiments consider paraphrase identification, and paraphrasing for training set augmentation, comparing to supervised and unsupervised translation-based approaches. monolingual paraphrasing is shown to outperform unsupervised translation in all contexts. the comparison with supervised mt is more mixed: monolingual paraphrasing is interesting for identification and augmentation but supervised mt is superior for generation."], "generation"], [["self-attention guided copy mechanism for abstractive summarization", "song xu | haoran li | peng yuan | youzheng wu | xiaodong he | bowen zhou", "copy module has been widely equipped in the recent abstractive summarization models, which facilitates the decoder to extract words from the source into the summary. generally, the encoder-decoder attention is served as the copy distribution, while how to guarantee that important words in the source are copied remains a challenge. in this work, we propose a transformer-based model to enhance the copy mechanism. specifically, we identify the importance of each source word based on the degree centrality with a directed graph built by the self-attention layer in the transformer. we use the centrality of each source word to guide the copy process explicitly. experimental results show that the self-attention graph provides useful guidance for the copy distribution. our proposed models significantly outperform the baseline methods on the cnn/daily mail dataset and the gigaword dataset."], "summarization"], [["weakly-supervised spatio-temporally grounding natural sentence in video", "zhenfang chen | lin ma | wenhan luo | kwan-yee kenneth wong", "in this paper, we address a novel task, namely weakly-supervised spatio-temporally grounding natural sentence in video. specifically, given a natural sentence and a video, we localize a spatio-temporal tube in the video that semantically corresponds to the given sentence, with no reliance on any spatio-temporal annotations during training. first, a set of spatio-temporal tubes, referred to as instances, are extracted from the video. we then encode these instances and the sentence using our newly proposed attentive interactor which can exploit their fine-grained relationships to characterize their matching behaviors. besides a ranking loss, a novel diversity loss is introduced to train our attentive interactor to strengthen the matching behaviors of reliable instance-sentence pairs and penalize the unreliable ones. we also contribute a dataset, called vid-sentence, based on the imagenet video object detection dataset, to serve as a benchmark for our task. results from extensive experiments demonstrate the superiority of our model over the baseline approaches."], "language grounding to vision, robotics and beyond"], [["moroco: the moldavian and romanian dialectal corpus", "andrei butnaru | radu tudor ionescu", "in this work, we introduce the moldavian and romanian dialectal corpus (moroco), which is freely available for download at https://github.com/butnaruandrei/moroco. the corpus contains 33564 samples of text (with over 10 million tokens) collected from the news domain. the samples belong to one of the following six topics: culture, finance, politics, science, sports and tech. the data set is divided into 21719 samples for training, 5921 samples for validation and another 5924 samples for testing. for each sample, we provide corresponding dialectal and category labels. this allows us to perform empirical studies on several classification tasks such as (i) binary discrimination of moldavian versus romanian text samples, (ii) intra-dialect multi-class categorization by topic and (iii) cross-dialect multi-class categorization by topic. we perform experiments using a shallow approach based on string kernels, as well as a novel deep approach based on character-level convolutional neural networks containing squeeze-and-excitation blocks. we also present and analyze the most discriminative features of our best performing model, before and after named entity removal."], "resources and evaluation"], [["calibrating structured output predictors for natural language processing", "abhyuday jagannatha | hong yu", "we address the problem of calibrating prediction confidence for output entities of interest in natural language processing (nlp) applications. it is important that nlp applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the applications are to be deployed in a safety-critical domain such as healthcare. however the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration methods. in this study, we propose a general calibration scheme for output entities of interest in neural network based structured prediction models. our proposed method can be used with any binary class calibration scheme and a neural network model. additionally, we show that our calibration method can also be used as an uncertainty-aware, entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements. we show that our method outperforms current calibration techniques for named entity recognition, part-of-speech tagging and question answering systems. we also observe an improvement in model performance from our decoding step across several tasks and benchmark datasets. our method improves the calibration and model performance on out-of-domain test scenarios as well."], "machine learning for nlp"], [["unsupervised faq retrieval with question generation and bert", "yosi mass | boaz carmeli | haggai roitman | david konopnicki", "we focus on the task of frequently asked questions (faq) retrieval. a given user query can be matched against the questions and/or the answers in the faq. we present a fully unsupervised method that exploits the faq pairs to train two bert models. the two models match user queries to faq answers and questions, respectively. we alleviate the missing labeled data of the latter by automatically generating high-quality question paraphrases. we show that our model is on par and even outperforms supervised models on existing datasets."], "information extraction, retrieval and text mining"], [["should all cross-lingual embeddings speak english?", "antonios anastasopoulos | graham neubig", "most of recent work in cross-lingual word embeddings is severely anglocentric. the vast majority of lexicon induction evaluation dictionaries are between english and another language, and the english embedding space is selected by default as the hub when learning in a multilingual setting. with this work, however, we challenge these practices. first, we show that the choice of hub language can significantly impact downstream lexicon induction zero-shot pos tagging performance. second, we both expand a standard english-centered evaluation dictionary collection to include all language pairs using triangulation, and create new dictionaries for under-represented languages. evaluating established methods over all these language pairs sheds light into their suitability for aligning embeddings from distant languages and presents new challenges for the field. finally, in our analysis we identify general guidelines for strong cross-lingual embedding baselines, that extend to language pairs that do not include english."], "nlp applications"], [["logic-guided data augmentation and regularization for consistent question answering", "akari asai | hannaneh hajishirzi", "many natural language questions require qualitative, quantitative or logical comparisons between two entities or events. this paper addresses the problem of improving the accuracy and consistency of responses to comparison questions by integrating logic rules and neural models. our method leverages logical and linguistic knowledge to augment labeled training data and then uses a consistency-based regularizer to train the model. improving the global consistency of predictions, our approach achieves large improvements over previous methods in a variety of question answering (qa) tasks, including multiple-choice qualitative reasoning, cause-effect reasoning, and extractive machine reading comprehension. in particular, our method significantly improves the performance of roberta-based models by 1-5% across datasets. we advance state of the art by around 5-8% on wiqa and quarel and reduce consistency violations by 58% on hotpotqa. we further demonstrate that our approach can learn effectively from limited data."], "question answering"], [["perturbed masking: parameter-free probing for analyzing and interpreting bert", "zhiyong wu | yun chen | ben kao | qun liu", "by introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings). the effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge. however, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself. complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., bert). our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process. our experiments on bert show that syntactic trees recovered from bert using our method are significantly better than linguistically-uninformed baselines. we further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema."], "interpretability and analysis of models for nlp"], [["variational neural machine translation with normalizing flows", "hendra setiawan | matthias sperber | udhyakumar nallasamy | matthias paulik", "variational neural machine translation (vnmt) is an attractive framework for modeling the generation of target translations, conditioned not only on the source sentence but also on some latent random variables. the latent variable modeling may introduce useful statistical dependencies that can improve translation accuracy. unfortunately, learning informative latent variables is non-trivial, as the latent space can be prohibitively large, and the latent codes are prone to be ignored by many translation models at training time. previous works impose strong assumptions on the distribution of the latent code and limit the choice of the nmt architecture. in this paper, we propose to apply the vnmt framework to the state-of-the-art transformer and introduce a more flexible approximate posterior based on normalizing flows. we demonstrate the efficacy of our proposal under both in-domain and out-of-domain conditions, significantly outperforming strong baselines."], "machine translation and multilinguality"], [["variance of average surprisal: a better predictor for quality of grammar from unsupervised pcfg induction", "lifeng jin | william schuler", "in unsupervised grammar induction, data likelihood is known to be only weakly correlated with parsing accuracy, especially at convergence after multiple runs. in order to find a better indicator for quality of induced grammars, this paper correlates several linguistically- and psycholinguistically-motivated predictors to parsing accuracy on a large multilingual grammar induction evaluation data set. results show that variance of average surprisal (vas) better correlates with parsing accuracy than data likelihood and that using vas instead of data likelihood for model selection provides a significant accuracy boost. further evidence shows vas to be a better candidate than data likelihood for predicting word order typology classification. analyses show that vas seems to separate content words from function words in natural language grammars, and to better arrange words with different frequencies into separate classes that are more consistent with linguistic theory."], "tagging, chunking, syntax and parsing"], [["multi-grained named entity recognition", "congying xia | chenwei zhang | tao yang | yaliang li | nan du | xian wu | wei fan | fenglong ma | philip yu", "this paper presents a novel framework, mgner, for multi-grained named entity recognition where multiple entities or entity mentions in a sentence could be non-overlapping or totally nested. different from traditional approaches regarding ner as a sequential labeling task and annotate entities consecutively, mgner detects and recognizes entities on multiple granularities: it is able to recognize named entities without explicitly assuming non-overlapping or totally nested structures. mgner consists of a detector that examines all possible word segments and a classifier that categorizes entities. in addition, contextual information and a self-attention mechanism are utilized throughout the framework to improve the ner performance. experimental results show that mgner outperforms current state-of-the-art baselines up to 4.4% in terms of the f1 score among nested/non-overlapping ner tasks."], "information extraction, retrieval and text mining"], [["opendialkg: explainable conversational reasoning with attention-based walks over knowledge graphs", "seungwhan moon | pararth shah | anuj kumar | rajen subba", "we study a conversational reasoning model that strategically traverses through a large-scale common fact knowledge graph (kg) to introduce engaging and contextually diverse entities and attributes. for this study, we collect a new open-ended dialog <-> kg parallel corpus called opendialkg, where each utterance from 15k human-to-human role-playing dialogs is manually annotated with ground-truth reference to corresponding entities and paths from a large-scale kg with 1m+ facts. we then propose the dialkg walker model that learns the symbolic transitions of dialog contexts as structured traversals over kg, and predicts natural entities to introduce given previous dialog contexts via a novel domain-agnostic, attention-based graph path decoder. automatic and human evaluations show that our model can retrieve more natural and human-like responses than the state-of-the-art baselines or rule-based models, in both in-domain and cross-domain tasks. the proposed model also generates a kg walk path for each entity retrieved, providing a natural way to explain conversational reasoning."], "dialogue and interactive systems"], [["#youtoo? detection of personal recollections of sexual harassment on social media", "arijit ghosh chowdhury | ramit sawhney | rajiv ratn shah | debanjan mahata", "the availability of large-scale online social data, coupled with computational methods can help us answer fundamental questions relat- ing to our social lives, particularly our health and well-being. the #metoo trend has led to people talking about personal experiences of harassment more openly. this work at- tempts to aggregate such experiences of sex- ual abuse to facilitate a better understanding of social media constructs and to bring about social change. it has been found that disclo- sure of abuse has positive psychological im- pacts. hence, we contend that such informa- tion can leveraged to create better campaigns for social change by analyzing how users react to these stories and to obtain a better insight into the consequences of sexual abuse. we use a three part twitter-specific social media lan- guage model to segregate personal recollec- tions of sexual harassment from twitter posts. an extensive comparison with state-of-the-art generic and specific models along with a de- tailed error analysis explores the merit of our proposed model."], "computational social science, social media and cultural analytics"], [["dynamic fusion network for multi-domain end-to-end task-oriented dialog", "libo qin | xiao xu | wanxiang che | yue zhang | ting liu", "recent studies have shown remarkable success in end-to-end task-oriented dialog system. however, most neural models rely on large training data, which are only available for a certain number of task domains, such as navigation and scheduling. this makes it difficult to scalable for a new domain with limited labeled data. however, there has been relatively little research on how to effectively use data from all domains to improve the performance of each domain and also unseen domains. to this end, we investigate methods that can make explicit use of domain knowledge and introduce a shared-private network to learn shared and specific knowledge. in addition, we propose a novel dynamic fusion network (df-net) which automatically exploit the relevance between the target domain and each domain. results show that our models outperforms existing methods on multi-domain dialogue, giving the state-of-the-art in the literature. besides, with little training data, we show its transferability by outperforming prior best model by 13.9% on average."], "dialogue and interactive systems"], [["empowering active learning to jointly optimize system and user demands", "ji-ung lee | christian m. meyer | iryna gurevych", "existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training. however, when active learning is integrated with an end-user application, this can lead to frustration for participating users, as they spend time labeling instances that they would not otherwise be interested in reading. in this paper, we propose a new active learning approach that jointly optimizes the seemingly counteracting objectives of the active learning system (training efficiently) and the user (receiving useful instances). we study our approach in an educational application, which particularly benefits from this technique as the system needs to rapidly learn to predict the appropriateness of an exercise to a particular user, while the users should receive only exercises that match their skills. we evaluate multiple learning strategies and user types with data from real users and find that our joint approach better satisfies both objectives when alternative methods lead to many unsuitable exercises for end users."], "nlp applications"], [["discourse as a function of event: profiling discourse structure in news articles around the main event", "prafulla kumar choubey | aaron lee | ruihong huang | lu wang", "understanding discourse structures of news articles is vital to effectively contextualize the occurrence of a news event. to enable computational modeling of news structures, we apply an existing theory of functional discourse structure for news articles that revolves around the main event and create a human-annotated corpus of 802 documents spanning over four domains and three media sources. next, we propose several document-level neural-network models to automatically construct news content structures. finally, we demonstrate that incorporating system predicted news structures yields new state-of-the-art performance for event coreference resolution. the news documents we annotated are openly available and the annotations are publicly released for future research."], "discourse and pragmatics"], [["discourse-aware neural extractive text summarization", "jiacheng xu | zhe gan | yu cheng | jingjing liu", "recently bert has been adopted for document encoding in state-of-the-art text summarization models. however, sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries. also, long-range dependencies throughout a document are not well captured by bert, which is pre-trained on sentence pairs instead of documents. to address these issues, we present a discourse-aware neural summarization model - discobert. discobert extracts sub-sentential discourse units (instead of sentences) as candidates for extractive selection on a finer granularity. to capture the long-range dependencies among discourse units, structural discourse graphs are constructed based on rst trees and coreference mentions, encoded with graph convolutional networks. experiments show that the proposed model outperforms state-of-the-art methods by a significant margin on popular summarization benchmarks compared to other bert-base models."], "summarization"], [["text classification with negative supervision", "sora ohashi | junya takayama | tomoyuki kajiwara | chenhui chu | yuki arase", "advanced pre-trained models for text representation have achieved state-of-the-art performance on various text classification tasks. however, the discrepancy between the semantic similarity of texts and labelling standards affects classifiers, i.e. leading to lower performance in cases where classifiers should assign different labels to semantically similar texts. to address this problem, we propose a simple multitask learning model that uses negative supervision. specifically, our model encourages texts with different labels to have distinct representations. comprehensive experiments show that our model outperforms the state-of-the-art pre-trained model on both single- and multi-label classifications, sentence and document classifications, and classifications in three different languages."], "information extraction, retrieval and text mining"], [["similarity analysis of contextual word representation models", "john wu | yonatan belinkov | hassan sajjad | nadir durrani | fahim dalvi | james glass", "this paper investigates contextual word representation models from the lens of similarity analysis. given a collection of trained models, we measure the similarity of their internal representations and attention. critically, these models come from vastly different architectures. we use existing and novel similarity measures that aim to gauge the level of localization of information in the deep models, and facilitate the investigation of which design factors affect model similarity, without requiring any external linguistic annotation. the analysis reveals that models within the same family are more similar to one another, as may be expected. surprisingly, different architectures have rather similar representations, but different individual neurons. we also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks."], "interpretability and analysis of models for nlp"], [["graph-based dependency parsing with graph neural networks", "tao ji | yuanbin wu | man lan", "we investigate the problem of efficiently incorporating high-order features into neural graph-based dependency parsing. instead of explicitly extracting high-order features from intermediate parse trees, we develop a more powerful dependency tree node representation which captures high-order information concisely and efficiently. we use graph neural networks (gnns) to learn the representations and discuss several new configurations of gnn\u2019s updating and aggregation functions. experiments on ptb show that our parser achieves the best uas and las on ptb (96.0%, 94.3%) among systems without using any external resources."], "tagging, chunking, syntax and parsing"], [["mmpe: a multi-modal interface for post-editing machine translation", "nico herbig | tim d\u00fcwel | santanu pal | kalliopi meladaki | mahsa monshizadeh | antonio kr\u00fcger | josef van genabith", "current advances in machine translation (mt) increase the need for translators to switch from traditional translation to post-editing (pe) of machine-translated text, a process that saves time and reduces errors. this affects the design of translation interfaces, as the task changes from mainly generating text to correcting errors within otherwise helpful translation proposals. since this paradigm shift offers potential for modalities other than mouse and keyboard, we present mmpe, the first prototype to combine traditional input modes with pen, touch, and speech modalities for pe of mt. the results of an evaluation with professional translators suggest that pen and touch interaction are suitable for deletion and reordering tasks, while they are of limited use for longer insertions. on the other hand, speech and multi-modal combinations of select & speech are considered suitable for replacements and insertions but offer less potential for deletion and reordering. overall, participants were enthusiastic about the new modalities and saw them as good extensions to mouse & keyboard, but not as a complete substitute."], "nlp applications"], [["reversing gradients in adversarial domain adaptation for question deduplication and textual entailment tasks", "anush kamath | sparsh gupta | vitor carvalho", "adversarial domain adaptation has been recently proposed as an effective technique for textual matching tasks, such as question deduplication. here we investigate the use of gradient reversal on adversarial domain adaptation to explicitly learn both shared and unshared (domain specific) representations between two textual domains. in doing so, gradient reversal learns features that explicitly compensate for domain mismatch, while still distilling domain specific knowledge that can improve target domain accuracy. we evaluate reversing gradients for adversarial adaptation on multiple domains, and demonstrate that it significantly outperforms other methods on question deduplication as well as on recognizing textual entailment (rte) tasks, achieving up to 7% absolute boost in base model accuracy on some datasets."], "machine learning for nlp"], [["balancing training for multilingual neural machine translation", "xinyi wang | yulia tsvetkov | graham neubig", "when training multilingual machine translation (mt) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others. standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance. in this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages. experiments on two sets of languages under both one-to-many and many-to-one mt settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized."], "machine translation and multilinguality"], [["history for visual dialog: do we really need it?", "shubham agarwal | trung bui | joon-young lee | ioannis konstas | verena rieser", "visual dialogue involves \u201cunderstanding\u201d the dialogue history (what has been discussed previously) and the current question (what is asked), in addition to grounding information in the image, to accurately generate the correct response. in this paper, we show that co-attention models which explicitly encode dialoh history outperform models that don\u2019t, achieving state-of-the-art performance (72 % ndcg on val set). however, we also expose shortcomings of the crowdsourcing dataset collection procedure, by showing that dialogue history is indeed only required for a small amount of the data, and that the current evaluation metric encourages generic replies. to that end, we propose a challenging subset (visdialconv) of the visdialval set and the benchmark ndcg of 63%."], "language grounding to vision, robotics and beyond"], [["handling rare entities for neural sequence labeling", "yangming li | han li | kaisheng yao | xiaolong li", "one great challenge in neural sequence labeling is the data sparsity problem for rare entity words and phrases. most of test set entities appear only few times and are even unseen in training corpus, yielding large number of out-of-vocabulary (oov) and low-frequency (lf) entities during evaluation. in this work, we propose approaches to address this problem. for oov entities, we introduce local context reconstruction to implicitly incorporate contextual information into their representations. for lf entities, we present delexicalized entity identification to explicitly extract their frequency-agnostic and entity-type-specific representations. extensive experiments on multiple benchmark datasets show that our model has significantly outperformed all previous methods and achieved new start-of-the-art results. notably, our methods surpass the model fine-tuned on pre-trained language models without external resource."], "information extraction, retrieval and text mining"], [["language modeling with shared grammar", "yuyu zhang | le song", "sequential recurrent neural networks have achieved superior performance on language modeling, but overlook the structure information in natural language. recent works on structure-aware models have shown promising results on language modeling. however, how to incorporate structure knowledge on corpus without syntactic annotations remains an open problem. in this work, we propose neural variational language model (nvlm), which enables the sharing of grammar knowledge among different corpora. experimental results demonstrate the effectiveness of our framework on two popular benchmark datasets. with the help of shared grammar, our language model converges significantly faster to a lower perplexity on new training corpus."], "machine learning for nlp"], [["studying summarization evaluation metrics in the appropriate scoring range", "maxime peyrard", "in summarization, automatic evaluation metrics are usually compared based on their ability to correlate with human judgments. unfortunately, the few existing human judgment datasets have been created as by-products of the manual evaluations performed during the duc/tac shared tasks. however, modern systems are typically better than the best systems submitted at the time of these shared tasks. we show that, surprisingly, evaluation metrics which behave similarly on these datasets (average-scoring range) strongly disagree in the higher-scoring range in which current systems now operate. it is problematic because metrics disagree yet we can\u2019t decide which one to trust. this is a call for collecting human judgments for high-scoring summaries as this would resolve the debate over which metrics to trust. this would also be greatly beneficial to further improve summarization systems and metrics alike."], "summarization"], [["improving segmentation for technical support problems", "kushal chauhan | abhirut gupta", "technical support problems are often long and complex. they typically contain user descriptions of the problem, the setup, and steps for attempted resolution. often they also contain various non-natural language text elements like outputs of commands, snippets of code, error messages or stack traces. these elements contain potentially crucial information for problem resolution. however, they cannot be correctly parsed by tools designed for natural language. in this paper, we address the problem of segmentation for technical support questions. we formulate the problem as a sequence labelling task, and study the performance of state of the art approaches. we compare this against an intuitive contextual sentence-level classification baseline, and a state of the art supervised text-segmentation approach. we also introduce a novel component of combining contextual embeddings from multiple language models pre-trained on different data sources, which achieves a marked improvement over using embeddings from a single pre-trained language model. finally, we also demonstrate the usefulness of such segmentation with improvements on the downstream task of answer retrieval."], "nlp applications"], [["head-qa: a healthcare dataset for complex reasoning", "david vilares | carlos g\u00f3mez-rodr\u00edguez", "we present head-qa, a multi-choice question answering testbed to encourage research on complex reasoning. the questions come from exams to access a specialized position in the spanish healthcare system, and are challenging even for highly specialized humans. we then consider monolingual (spanish) and cross-lingual (to english) experiments with information retrieval and neural techniques. we show that: (i) head-qa challenges current methods, and (ii) the results lag well behind human performance, demonstrating its usefulness as a benchmark for future work."], "nlp applications"], [["measuring conversational uptake: a case study on student-teacher interactions", "dorottya demszky | jing liu | zid mancenido | julie cohen | heather hill | dan jurafsky | tatsunori hashimoto", "in conversation, uptake happens when a speaker builds on the contribution of their interlocutor by, for example, acknowledging, repeating or reformulating what they have said. in education, teachers\u2019 uptake of student contributions has been linked to higher student achievement. yet measuring and improving teachers\u2019 uptake at scale is challenging, as existing methods require expensive annotation by experts. we propose a framework for computationally measuring uptake, by (1) releasing a dataset of student-teacher exchanges extracted from us math classroom transcripts annotated for uptake by experts; (2) formalizing uptake as pointwise jensen-shannon divergence (pjsd), estimated via next utterance classification; (3) conducting a linguistically-motivated comparison of different unsupervised measures and (4) correlating these measures with educational outcomes. we find that although repetition captures a significant part of uptake, pjsd outperforms repetition-based baselines, as it is capable of identifying a wider range of uptake phenomena like question answering and reformulation. we apply our uptake measure to three different educational datasets with outcome indicators. unlike baseline measures, pjsd correlates significantly with instruction quality in all three, providing evidence for its generalizability and for its potential to serve as an automated professional development tool for teachers."], "computational social science, social media and cultural analytics"], [["seqvat: virtual adversarial training for semi-supervised sequence labeling", "luoxin chen | weitong ruan | xinyue liu | jianhua lu", "virtual adversarial training (vat) is a powerful technique to improve model robustness in both supervised and semi-supervised settings. it is effective and can be easily adopted on lots of image classification and text classification tasks. however, its benefits to sequence labeling tasks such as named entity recognition (ner) have not been shown as significant, mostly, because the previous approach can not combine vat with the conditional random field (crf). crf can significantly boost accuracy for sequence models by putting constraints on label transitions, which makes it an essential component in most state-of-the-art sequence labeling model architectures. in this paper, we propose seqvat, a method which naturally applies vat to sequence labeling models with crf. empirical studies show that seqvat not only significantly improves the sequence labeling performance over baselines under supervised settings, but also outperforms state-of-the-art approaches under semi-supervised settings."], "tagging, chunking, syntax and parsing"], [["biset: bi-directional selective encoding with template for abstractive summarization", "kai wang | xiaojun quan | rui wang", "the success of neural summarization models stems from the meticulous encodings of source articles. to overcome the impediments of limited and sometimes noisy training data, one promising direction is to make better use of the available training data by applying filters during summarization. in this paper, we propose a novel bi-directional selective encoding with template (biset) model, which leverages template discovered from training data to softly select key information from each source article to guide its summarization process. extensive experiments on a standard summarization dataset are conducted and the results show that the template-equipped biset model manages to improve the summarization performance significantly with a new state of the art."], "summarization"], [["diversifying dialogue generation with non-conversational text", "hui su | xiaoyu shen | sanqiang zhao | zhou xiao | pengwei hu | randy zhong | cheng niu | jie zhou", "neural network-based sequence-to-sequence (seq2seq) models strongly suffer from the low-diversity problem when it comes to open-domain dialogue generation. as bland and generic utterances usually dominate the frequency distribution in our daily chitchat, avoiding them to generate more interesting responses requires complex data filtering, sampling techniques or modifying the training objective. in this paper, we propose a new perspective to diversify dialogue generation by leveraging non-conversational text. compared with bilateral conversations, non-conversational text are easier to obtain, more diverse and cover a much broader range of topics. we collect a large-scale non-conversational corpus from multi sources including forum comments, idioms and book snippets. we further present a training paradigm to effectively incorporate these text via iterative back translation. the resulting model is tested on two conversational datasets from different domains and is shown to produce significantly more diverse responses without sacrificing the relevance with context."], "dialogue and interactive systems"], [["paraphrase generation by learning how to edit from samples", "amirhossein kazemnejad | mohammadreza salehi | mahdieh soleymani baghshah", "neural sequence to sequence text generation has been proved to be a viable approach to paraphrase generation. despite promising results, paraphrases generated by these models mostly suffer from lack of quality and diversity. to address these problems, we propose a novel retrieval-based method for paraphrase generation. our model first retrieves a paraphrase pair similar to the input sentence from a pre-defined index. with its novel editor module, the model then paraphrases the input sequence by editing it using the extracted relations between the retrieved pair of sentences. in order to have fine-grained control over the editing process, our model uses the newly introduced concept of micro edit vectors. it both extracts and exploits these vectors using the attention mechanism in the transformer architecture. experimental results show the superiority of our paraphrase generation method in terms of both automatic metrics, and human evaluation of relevance, grammaticality, and diversity of generated paraphrases."], "nlp applications"], [["reasoning with latent structure refinement for document-level relation extraction", "guoshun nan | zhijiang guo | ivan sekulic | wei lu", "document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. however, effective aggregation of relevant information in the document remains a challenging research question. existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. we further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. specifically, our model achieves an f1 score of 59.05 on a large-scale document-level dataset (docred), significantly improving over the previous results, and also yields new state-of-the-art results on the cdr and gda dataset. furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations."], "information extraction, retrieval and text mining"], [["spherere: distinguishing lexical relations with hyperspherical relation embeddings", "chengyu wang | xiaofeng he | aoying zhou", "lexical relations describe how meanings of terms relate to each other. typical examples include hypernymy, synonymy, meronymy, etc. automatic distinction of lexical relations is vital for nlp applications, and also challenging due to the lack of contextual signals to discriminate between such relations. in this work, we present a neural representation learning model to distinguish lexical relations among term pairs based on hyperspherical relation embeddings (spherere). rather than learning embeddings for individual terms, the model learns representations of relation triples by mapping them to the hyperspherical embedding space, where relation triples of different lexical relations are well separated. experiments over several benchmarks confirm spherere outperforms state-of-the-arts."], "semantics"], [["safer: a structure-free approach for certified robustness to adversarial word substitutions", "mao ye | chengyue gong | qiang liu", "state-of-the-art nlp models can often be fooled by human-unaware transformations such as synonymous word substitution. for security reasons, it is of critical importance to develop models with certified robustness that can provably guarantee that the prediction is can not be altered by any possible synonymous word substitution. in this work, we propose a certified robust method based on a new randomized smoothing technique, which constructs a stochastic ensemble by applying random word substitutions on the input sentences, and leverage the statistical properties of the ensemble to provably certify the robustness. our method is simple and structure-free in that it only requires the black-box queries of the model outputs, and hence can be applied to any pre-trained models (such as bert) and any types of models (world-level or subword-level). our method significantly outperforms recent state-of-the-art methods for certified robustness on both imdb and amazon text classification tasks. to the best of our knowledge, we are the first work to achieve certified robustness on large systems such as bert with practically meaningful certified accuracy."], "machine learning for nlp"], [["shape of synth to come: why we should use synthetic data for english surface realization", "henry elder | robert burke | alexander o\u2019connor | jennifer foster", "the surface realization shared tasks of 2018 and 2019 were natural language generation shared tasks with the goal of exploring approaches to surface realization from universal-dependency-like trees to surface strings for several languages. in the 2018 shared task there was very little difference in the absolute performance of systems trained with and without additional, synthetically created data, and a new rule prohibiting the use of synthetic data was introduced for the 2019 shared task. contrary to the findings of the 2018 shared task, we show, in experiments on the english 2018 dataset, that the use of synthetic data can have a substantial positive effect \u2013 an improvement of almost 8 bleu points for a previously state-of-the-art system. we analyse the effects of synthetic data, and we argue that its use should be encouraged rather than prohibited so that future research efforts continue to explore systems that can take advantage of such data."], "generation"], [["simple and effective curriculum pointer-generator networks for reading comprehension over long narratives", "yi tay | shuohang wang | anh tuan luu | jie fu | minh c. phan | xingdi yuan | jinfeng rao | siu cheung hui | aston zhang", "this paper tackles the problem of reading comprehension over long narratives where documents easily span over thousands of tokens. we propose a curriculum learning (cl) based pointer-generator framework for reading/sampling over large documents, enabling diverse training of the neural model based on the notion of alternating contextual difficulty. this can be interpreted as a form of domain randomization and/or generative pretraining during training. to this end, the usage of the pointer-generator softens the requirement of having the answer within the context, enabling us to construct diverse training samples for learning. additionally, we propose a new introspective alignment layer (ial), which reasons over decomposed alignments using block-based self-attention. we evaluate our proposed method on the narrativeqa reading comprehension benchmark, achieving state-of-the-art performance, improving existing baselines by 51% relative improvement on bleu-4 and 17% relative improvement on rouge-l. extensive ablations confirm the effectiveness of our proposed ial and cl components."], "question answering"], [["don\u2019t say that! making inconsistent dialogue unlikely with unlikelihood training", "margaret li | stephen roller | ilia kulikov | sean welleck | y-lan boureau | kyunghyun cho | jason weston", "generative dialogue models currently suffer from a number of problems which standard maximum likelihood training does not address. they tend to produce generations that (i) rely too much on copying from the context, (ii) contain repetitions within utterances, (iii) overuse frequent words, and (iv) at a deeper level, contain logical flaws.in this work we show how all of these problems can be addressed by extending the recently introduced unlikelihood loss (welleck et al., 2019) to these cases. we show that appropriate loss functions which regularize generated outputs to match human distributions are effective for the first three issues. for the last important general issue, we show applying unlikelihood to collected data of what a model should not do is effective for improving logical consistency, potentially paving the way to generative models with greater reasoning ability. we demonstrate the efficacy of our approach across several dialogue tasks."], "dialogue and interactive systems"], [["target conditioned sampling: optimizing data selection for multilingual neural machine translation", "xinyi wang | graham neubig", "to improve low-resource neural machine translation (nmt) with multilingual corpus, training on the most related high-resource language only is generally more effective than us- ing all data available (neubig and hu, 2018). however, it remains a question whether a smart data selection strategy can further improve low-resource nmt with data from other auxiliary languages. in this paper, we seek to construct a sampling distribution over all multilingual data, so that it minimizes the training loss of the low-resource language. based on this formulation, we propose and efficient algorithm, (tcs), which first samples a target sentence, and then conditionally samples its source sentence. experiments show tcs brings significant gains of up to 2 bleu improvements on three of four languages we test, with minimal training overhead."], "machine translation and multilinguality"], [["towards understanding gender bias in relation extraction", "andrew gaut | tony sun | shirlyn tang | yuxin huang | jing qian | mai elsherief | jieyu zhao | diba mirza | elizabeth belding | kai-wei chang | william yang wang", "recent developments in neural relation extraction (nre) have made significant strides towards automated knowledge base construction. while much attention has been dedicated towards improvements in accuracy, there have been no attempts in the literature to evaluate social biases exhibited in nre systems. in this paper, we create wikigenderbias, a distantly supervised dataset composed of over 45,000 sentences including a 10% human annotated test set for the purpose of analyzing gender bias in relation extraction systems. we find that when extracting spouse-of and hypernym (i.e., occupation) relations, an nre system performs differently when the gender of the target entity is different. however, such disparity does not appear when extracting relations such as birthdate or birthplace. we also analyze how existing bias mitigation techniques, such as name anonymization, word embedding debiasing, and data augmentation affect the nre system in terms of maintaining the test performance and reducing biases. unfortunately, due to nre models rely heavily on surface level cues, we find that existing bias mitigation approaches have a negative effect on nre. our analysis lays groundwork for future quantifying and mitigating bias in nre."], "ethics in nlp"], [["the curse of dense low-dimensional information retrieval for large index sizes", "nils reimers | iryna gurevych", "information retrieval using dense low-dimensional representations recently became popular and showed out-performance to traditional sparse-representations like bm25. however, no previous work investigated how dense representations perform with large index sizes. we show theoretically and empirically that the performance for dense representations decreases quicker than sparse representations for increasing index sizes. in extreme cases, this can even lead to a tipping point where at a certain index size sparse representations outperform dense representations. we show that this behavior is tightly connected to the number of dimensions of the representations: the lower the dimension, the higher the chance for false positives, i.e. returning irrelevant documents"], "information extraction, retrieval and text mining"], [["probabilistically masked language model capable of autoregressive generation in arbitrary word order", "yi liao | xin jiang | qun liu", "masked language model and autoregressive language model are two types of language models. while pretrained masked language models such as bert overwhelm the line of natural language understanding (nlu) tasks, autoregressive language models such as gpt are especially capable in natural language generation (nlg). in this paper, we propose a probabilistic masking scheme for the masked language model, which we call probabilistically masked language model (pmlm). we implement a specific pmlm with a uniform prior distribution on the masking ratio named u-pmlm. we prove that u-pmlm is equivalent to an autoregressive permutated language model. one main advantage of the model is that it supports text generation in arbitrary order with surprisingly good quality, which could potentially enable new applications over traditional unidirectional generation. besides, the pretrained u-pmlm also outperforms bert on a bunch of downstream nlu tasks."], "generation"], [["learning web-based procedures by reasoning over explanations and demonstrations in context", "shashank srivastava | oleksandr polozov | nebojsa jojic | christopher meek", "we explore learning web-based tasks from a human teacher through natural language explanations and a single demonstration. our approach investigates a new direction for semantic parsing that models explaining a demonstration in a context, rather than mapping explanations to demonstrations. by leveraging the idea of inverse semantics from program synthesis to reason backwards from observed demonstrations, we ensure that all considered interpretations are consistent with executable actions in any context, thus simplifying the problem of search over logical forms. we present a dataset of explanations paired with demonstrations for web-based tasks. our methods show better task completion rates than a supervised semantic parsing baseline (40% relative improvement on average), and are competitive with simple exploration-and-demonstration based methods, while requiring no exploration of the environment. in learning to align explanations with demonstrations, basic properties of natural language syntax emerge as learned behavior. this is an interesting example of pragmatic language acquisition without any linguistic annotation."], "language grounding to vision, robotics and beyond"], [["modeling task-aware mimo cardinality for efficient multilingual neural machine translation", "hongfei xu | qiuhui liu | josef van genabith | deyi xiong", "neural machine translation has achieved great success in bilingual settings, as well as in multilingual settings. with the increase of the number of languages, multilingual systems tend to underperform their bilingual counterparts. model capacity has been found crucial for massively multilingual nmt to support language pairs with varying typological characteristics. previous work increases the modeling capacity by deepening or widening the transformer. however, modeling cardinality based on aggregating a set of transformations with the same topology has been proven more effective than going deeper or wider when increasing capacity. in this paper, we propose to efficiently increase the capacity for multilingual nmt by increasing the cardinality. unlike previous work which feeds the same input to several transformations and merges their outputs into one, we present a multi-input-multi-output (mimo) architecture that allows each transformation of the block to have its own input. we also present a task-aware attention mechanism to learn to selectively utilize individual transformations from a set of transformations for different translation directions. our model surpasses previous work and establishes a new state-of-the-art on the large scale opus-100 corpus while being 1.31 times as fast."], "machine translation and multilinguality"], [["conversing by reading: contentful neural conversation with on-demand machine reading", "lianhui qin | michel galley | chris brockett | xiaodong liu | xiang gao | bill dolan | yejin choi | jianfeng gao", "although neural conversational models are effective in learning how to produce fluent responses, their primary challenge lies in knowing what to say to make the conversation contentful and non-vacuous. we present a new end-to-end approach to contentful neural conversation that jointly models response generation and on-demand machine reading. the key idea is to provide the conversation model with relevant long-form text on the fly as a source of external knowledge. the model performs qa-style reading comprehension on this text in response to each conversational turn, thereby allowing for more focused integration of external knowledge than has been possible in prior approaches. to support further research on knowledge-grounded conversation, we introduce a new large-scale conversation dataset grounded in external web pages (2.8m turns, 7.4m sentences of grounding). both human evaluation and automated metrics show that our approach results in more contentful responses compared to a variety of previous methods, improving both the informativeness and diversity of generated output."], "dialogue and interactive systems"], [["evaluating evaluation measures for ordinal classification and ordinal quantification", "tetsuya sakai", "ordinal classification (oc) is an important classification task where the classes are ordinal. for example, an oc task for sentiment analysis could have the following classes: highly positive, positive, neutral, negative, highly negative. clearly, evaluation measures for an oc task should penalise misclassifications by considering the ordinal nature of the classes. ordinal quantification (oq) is a related task where the gold data is a distribution over ordinal classes, and the system is required to estimate this distribution. evaluation measures for an oq task should also take the ordinal nature of the classes into account. however, for both oc and oq, there are only a small number of known evaluation measures that meet this basic requirement. in the present study, we utilise data from the semeval and ntcir communities to clarify the properties of nine evaluation measures in the context of oc tasks, and six measures in the context of oq tasks."], "resources and evaluation"], [["highres: highlight-based reference-less evaluation of summarization", "hardy hardy | shashi narayan | andreas vlachos", "there has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. however, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. to address this issue, we propose a novel approach for manual evaluation, highlight-based reference-less evaluation of summarization (highres), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. to validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. we demonstrate that highres improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches."], "generation"], [["automatic generation of citation texts in scholarly papers: a pilot study", "xinyu xing | xiaosheng fan | xiaojun wan", "in this paper, we study the challenging problem of automatic generation of citation texts in scholarly papers. given the context of a citing paper a and a cited paper b, the task aims to generate a short text to describe b in the given context of a. one big challenge for addressing this task is the lack of training data. usually, explicit citation texts are easy to extract, but it is not easy to extract implicit citation texts from scholarly papers. we thus first train an implicit citation extraction model based on bert and leverage the model to construct a large training dataset for the citation text generation task. then we propose and train a multi-source pointer-generator network with cross attention mechanism for citation text generation. empirical evaluation results on a manually labeled test dataset verify the efficacy of our model. this pilot study confirms the feasibility of automatically generating citation texts in scholarly papers and the technique has the great potential to help researchers prepare their scientific papers."], "summarization"], [["coach: a coarse-to-fine approach for cross-domain slot filling", "zihan liu | genta indra winata | peng xu | pascale fung", "as an essential task in task-oriented dialog systems, slot filling requires extensive training data in a certain domain. however, such data are not always available. hence, cross-domain slot filling has naturally arisen to cope with this data scarcity problem. in this paper, we propose a coarse-to-fine approach (coach) for cross-domain slot filling. our model first learns the general pattern of slot entities by detecting whether the tokens are slot entities or not. it then predicts the specific types for the slot entities. in addition, we propose a template regularization approach to improve the adaptation robustness by regularizing the representation of utterances based on utterance templates. experimental results show that our model significantly outperforms state-of-the-art approaches in slot filling. furthermore, our model can also be applied to the cross-domain named entity recognition task, and it achieves better adaptation performance than other existing baselines. the code is available at https://github.com/zliucr/coach."], "dialogue and interactive systems"], [["adversarial nli: a new benchmark for natural language understanding", "yixin nie | adina williams | emily dinan | mohit bansal | jason weston | douwe kiela", "we introduce a new large-scale nli benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. we show that training models on this new dataset leads to state-of-the-art performance on a variety of popular nli benchmarks, while posing a more difficult challenge with its new test set. our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. the data collection method can be applied in a never-ending learning scenario, becoming a moving target for nlu, rather than a static benchmark that will quickly saturate."], "resources and evaluation"]]
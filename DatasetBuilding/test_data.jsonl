[[["changes in european solidarity before and during covid-19: evidence from a large crowd- and expert-annotated twitter dataset", "alexandra ils | dan liu | daniela grunow | steffen eger", "we introduce the well-established social scientific concept of social solidarity and its contestation, anti-solidarity, as a new problem setting to supervised machine learning in nlp to assess how european solidarity discourses changed before and after the covid-19 outbreak was declared a global pandemic. to this end, we annotate 2.3k english and german tweets for (anti-)solidarity expressions, utilizing multiple human annotators and two annotation approaches (experts vs. crowds). we use these annotations to train a bert model with multiple data augmentation strategies. our augmented bert model that combines both expert and crowd annotations outperforms the baseline bert classifier trained with expert annotations only by over 25 points, from 58% macro-f1 to almost 85%. we use this high-quality model to automatically label over 270k tweets between september 2019 and december 2020. we then assess the automatically labeled data for how statements related to european (anti-)solidarity discourses developed over time and in relation to one another, before and during the covid-19 crisis. our results show that solidarity became increasingly salient and contested during the crisis. while the number of solidarity tweets remained on a higher level and dominated the discourse in the scrutinized time frame, anti-solidarity tweets initially spiked, then decreased to (almost) pre-covid-19 values before rising to a stable higher level until the end of 2020."], "computational social science, social media and cultural analytics"], [["the statistical advantage of automatic nlg metrics at the system level", "johnny wei | robin jia", "estimating the expected output quality of generation systems is central to nlg. this paper qualifies the notion that automatic metrics are not as good as humans in estimating system-level quality. statistically, humans are unbiased, high variance estimators, while metrics are biased, low variance estimators. we compare these estimators by their error in pairwise prediction (which generation system is better?) using the bootstrap. measuring this error is complicated: predictions are evaluated against noisy, human predicted labels instead of the ground truth, and metric predictions fluctuate based on the test sets they were calculated on. by applying a bias-variance-noise decomposition, we adjust this error to a noise-free, infinite test set setting. our analysis compares the adjusted error of metrics to humans and a derived, perfect segment-level annotator, both of which are unbiased estimators dependent on the number of judgments collected. in mt, we identify two settings where metrics outperform humans due to a statistical advantage in variance: when the number of human judgments used is small, and when the quality difference between compared systems is small."], "resources and evaluation"], [["structformer: joint unsupervised induction of dependency and constituency structure from masked language modeling", "yikang shen | yi tay | che zheng | dara bahri | donald metzler | aaron courville", "there are two major classes of natural language grammars \u2014 the dependency grammar that models one-to-one correspondences between words and the constituency grammar that models the assembly of one or several corresponded words. while previous unsupervised parsing methods mostly focus on only inducing one class of grammars, we introduce a novel model, structformer, that can induce dependency and constituency structure at the same time. to achieve this, we propose a new parsing framework that can jointly generate a constituency tree and dependency graph. then we integrate the induced dependency relations into the transformer, in a differentiable manner, through a novel dependency-constrained self-attention mechanism. experimental results show that our model can achieve strong results on unsupervised constituency parsing, unsupervised dependency parsing, and masked language modeling at the same time."], "tagging, chunking, syntax and parsing"], [["ontoed: low-resource event detection with ontology embedding", "shumin deng | ningyu zhang | luoqiu li | chen hui | tou huaixiao | mosha chen | fei huang | huajun chen", "event detection (ed) aims to identify event trigger words from a given text and classify it into an event type. most current methods to ed rely heavily on training instances, and almost ignore the correlation of event types. hence, they tend to suffer from data scarcity and fail to handle new unseen event types. to address these problems, we formulate ed as a process of event ontology population: linking event instances to pre-defined event types in event ontology, and propose a novel ed framework entitled ontoed with ontology embedding. we enrich event ontology with linkages among event types, and further induce more event-event correlations. based on the event ontology, ontoed can leverage and propagate correlation knowledge, particularly from data-rich to data-poor event types. furthermore, ontoed can be applied to new unseen event types, by establishing linkages to existing ones. experiments indicate that ontoed is more predominant and robust than previous approaches to ed, especially in data-scarce scenarios."], "information extraction, retrieval and text mining"], [["beyond noise: mitigating the impact of fine-grained semantic divergences on neural machine translation", "eleftheria briakou | marine carpuat", "while it has been shown that neural machine translation (nmt) is highly sensitive to noisy parallel training samples, prior work treats all types of mismatches between source and target as noise. as a result, it remains unclear how samples that are mostly equivalent but contain a small number of semantically divergent tokens impact nmt training. to close this gap, we analyze the impact of different types of fine-grained semantic divergences on transformer models. we show that models trained on synthetic divergences output degenerated text more frequently and are less confident in their predictions. based on these findings, we introduce a divergent-aware nmt framework that uses factors to help nmt recover from the degradation caused by naturally occurring divergences, improving both translation quality and model calibration on en-fr tasks."], "machine translation and multilinguality"], [["generating landmark navigation instructions from maps as a graph-to-text problem", "raphael schumann | stefan riezler", "car-focused navigation services are based on turns and distances of named streets, whereas navigation instructions naturally used by humans are centered around physical objects called landmarks. we present a neural model that takes openstreetmap representations as input and learns to generate navigation instructions that contain visible and salient landmarks from human natural language instructions. routes on the map are encoded in a location- and rotation-invariant graph representation that is decoded into natural language instructions. our work is based on a novel dataset of 7,672 crowd-sourced instances that have been verified by human navigation in street view. our evaluation shows that the navigation instructions generated by our system have similar properties as human-generated instructions, and lead to successful human navigation in street view."], "language grounding to vision, robotics and beyond"], [["dynamic contextualized word embeddings", "valentin hofmann | janet pierrehumbert | hinrich sch\u00fctze", "static word embeddings that represent words by a single vector cannot capture the variability of word meaning in different linguistic and extralinguistic contexts. building on prior work on contextualized and dynamic word embeddings, we introduce dynamic contextualized word embeddings that represent words as a function of both linguistic and extralinguistic context. based on a pretrained language model (plm), dynamic contextualized word embeddings model time and social space jointly, which makes them attractive for a range of nlp tasks involving semantic variability. we highlight potential application scenarios by means of qualitative and quantitative analyses on four english datasets."], "semantics"], [["a semantic-based method for unsupervised commonsense question answering", "yilin niu | fei huang | jiaming liang | wenkai chen | xiaoyan zhu | minlie huang", "unsupervised commonsense question answering is appealing since it does not rely on any labeled task data. among existing work, a popular solution is to use pre-trained language models to score candidate choices directly conditioned on the question or context. however, such scores from language models can be easily affected by irrelevant factors, such as word frequencies, sentence structures, etc. these distracting factors may not only mislead the model to choose a wrong answer but also make it oversensitive to lexical perturbations in candidate answers. in this paper, we present a novel semantic-based question answering method (seqa) for unsupervised commonsense question answering. instead of directly scoring each answer choice, our method first generates a set of plausible answers with generative models (e.g., gpt-2), and then uses these plausible answers to select the correct choice by considering the semantic similarity between each plausible answer and each choice. we devise a simple, yet sound formalism for this idea and verify its effectiveness and robustness with extensive experiments. we evaluate the proposed method on four benchmark datasets, and our method achieves the best results in unsupervised settings. moreover, when attacked by textfooler with synonym replacement, seqa demonstrates much less performance drops than baselines, thereby indicating stronger robustness."], "question answering"], [["when is char better than subword: a systematic study of segmentation algorithms for neural machine translation", "jiahuan li | yutong shen | shujian huang | xinyu dai | jiajun chen", "subword segmentation algorithms have been a de facto choice when building neural machine translation systems. however, most of them need to learn a segmentation model based on some heuristics, which may produce sub-optimal segmentation. this can be problematic in some scenarios when the target language has rich morphological changes or there is not enough data for learning compact composition rules. translating at fully character level has the potential to alleviate the issue, but empirical performances of character-based models has not been fully explored. in this paper, we present an in-depth comparison between character-based and subword-based nmt systems under three settings: translating to typologically diverse languages, training with low resource, and adapting to unseen domains. experiment results show strong competitiveness of character-based models. further analyses show that compared to subword-based models, character-based models are better at handling morphological phenomena, generating rare and unknown words, and more suitable for transferring to unseen domains."], "phonology, morphology and word segmentation"], [["stereorel: relational triple extraction from a stereoscopic perspective", "xuetao tian | liping jing | lu he | feng liu", "relational triple extraction is critical to understanding massive text corpora and constructing large-scale knowledge graph, which has attracted increasing research interest. however, existing studies still face some challenging issues, including information loss, error propagation and ignoring the interaction between entity and relation. to intuitively explore the above issues and address them, in this paper, we provide a revealing insight into relational triple extraction from a stereoscopic perspective, which rationalizes the occurrence of these issues and exposes the shortcomings of existing methods. further, a novel model is proposed for relational triple extraction, which maps relational triples to a three-dimension (3-d) space and leverages three decoders to extract them, aimed at simultaneously handling the above issues. a series of experiments are conducted on five public datasets, demonstrating that the proposed model outperforms the recent advanced baselines."], "information extraction, retrieval and text mining"], [["cross-lingual abstractive summarization with limited parallel resources", "yu bai | yang gao | heyan huang", "parallel cross-lingual summarization data is scarce, requiring models to better use the limited available cross-lingual resources. existing methods to do so often adopt sequence-to-sequence networks with multi-task frameworks. such approaches apply multiple decoders, each of which is utilized for a specific task. however, these independent decoders share no parameters, hence fail to capture the relationships between the discrete phrases of summaries in different languages, breaking the connections in order to transfer the knowledge of the high-resource languages to low-resource languages. to bridge these connections, we propose a novel multi-task framework for cross-lingual abstractive summarization (mclas) in a low-resource setting. employing one unified decoder to generate the sequential concatenation of monolingual and cross-lingual summaries, mclas makes the monolingual summarization task a prerequisite of the cls task. in this way, the shared decoder learns interactions involving alignments and summary patterns across languages, which encourages attaining knowledge transfer. experiments on two cls datasets demonstrate that our model significantly outperforms three baseline models in both low-resource and full-dataset scenarios. moreover, in-depth analysis on the generated summaries and attention heads verifies that interactions are learned well using mclas, which benefits the cls task under limited parallel resources."], "summarization"], [["stacked acoustic-and-textual encoding: integrating the pre-trained models into speech translation encoders", "chen xu | bojie hu | yanyang li | yuhao zhang | shen huang | qi ju | tong xiao | jingbo zhu", "encoder pre-training is promising in end-to-end speech translation (st), given the fact that speech-to-translation data is scarce. but st encoders are not simple instances of automatic speech recognition (asr) or machine translation (mt) encoders. for example, we find that asr encoders lack the global context representation, which is necessary for translation, whereas mt encoders are not designed to deal with long but locally attentive acoustic sequences. in this work, we propose a stacked acoustic-and-textual encoding (sate) method for speech translation. our encoder begins with processing the acoustic sequence as usual, but later behaves more like an mt encoder for a global representation of the input sequence. in this way, it is straightforward to incorporate the pre-trained models into the system. also, we develop an adaptor module to alleviate the representation inconsistency between the pre-trained asr encoder and mt encoder, and develop a multi-teacher knowledge distillation method to preserve the pre-training knowledge. experimental results on the librispeech en-fr and must-c en-de st tasks show that our method achieves state-of-the-art bleu scores of 18.3 and 25.2. to our knowledge, we are the first to develop an end-to-end st system that achieves comparable or even better bleu performance than the cascaded st counterpart when large-scale asr and mt data is available."], "speech and multimodality"], [["a mixture-of-experts model for antonym-synonym discrimination", "zhipeng xie | nan zeng", "discrimination between antonyms and synonyms is an important and challenging nlp task. antonyms and synonyms often share the same or similar contexts and thus are hard to make a distinction. this paper proposes two underlying hypotheses and employs the mixture-of-experts framework as a solution. it works on the basis of a divide-and-conquer strategy, where a number of localized experts focus on their own domains (or subspaces) to learn their specialties, and a gating mechanism determines the space partitioning and the expert mixture. experimental results have shown that our method achieves the state-of-the-art performance on the task."], "semantics"], [["men are elected, women are married: events gender bias on wikipedia", "jiao sun | nanyun peng", "human activities can be seen as sequences of events, which are crucial to understanding societies. disproportional event distribution for different demographic groups can manifest and amplify social stereotypes, and potentially jeopardize the ability of members in some groups to pursue certain goals. in this paper, we present the first event-centric study of gender biases in a wikipedia corpus. to facilitate the study, we curate a corpus of career and personal life descriptions with demographic information consisting of 7,854 fragments from 10,412 celebrities. then we detect events with a state-of-the-art event detection model, calibrate the results using strategically generated templates, and extract events that have asymmetric associations with genders. our study discovers that the wikipedia pages tend to intermingle personal life events with professional events for females but not for males, which calls for the awareness of the wikipedia community to formalize guidelines and train the editors to mind the implicit biases that contributors carry. our work also lays the foundation for future works on quantifying and discovering event biases at the corpus level."], "ethics in nlp"], [["km-bart: knowledge enhanced multimodal bart for visual commonsense generation", "yiran xing | zai shi | zhao meng | gerhard lakemeyer | yunpu ma | roger wattenhofer", "we present knowledge enhanced multimodal bart (km-bart), which is a transformer-based sequence-to-sequence model capable of reasoning about commonsense knowledge from multimodal inputs of images and texts. we adapt the generative bart architecture (lewis et al., 2020) to a multimodal model with visual and textual inputs. we further develop novel pretraining tasks to improve the model performance on the visual commonsense generation (vcg) task. in particular, our pretraining task of knowledge-based commonsense generation (kcg) boosts model performance on the vcg task by leveraging commonsense knowledge from a large language model pretrained on external commonsense knowledge graphs. to the best of our knowledge, we are the first to propose a dedicated task for improving model performance on the vcg task. experimental results show that our model reaches state-of-the-art performance on the vcg task (park et al., 2020) by applying these novel pretraining tasks."], "language grounding to vision, robotics and beyond"], [["style is not a single variable: case studies for cross-stylistic language understanding", "dongyeop kang | eduard hovy", "every natural text is written in some style. style is formed by a complex combination of different stylistic factors, including formality markers, emotions, metaphors, etc. one cannot form a complete understanding of a text without considering these factors. the factors combine and co-vary in complex ways to form styles. studying the nature of the covarying combinations sheds light on stylistic language in general, sometimes called cross-style language understanding. this paper provides the benchmark corpus (xslue) that combines existing datasets and collects a new one for sentence-level cross-style language understanding and evaluation. the benchmark contains text in 15 different styles under the proposed four theoretical groupings: figurative, personal, affective, and interpersonal groups. for valid evaluation, we collect an additional diagnostic set by annotating all 15 styles on the same text. using xslue, we propose three interesting cross-style applications in classification, correlation, and generation. first, our proposed cross-style classifier trained with multiple styles together helps improve overall classification performance against individually-trained style classifiers. second, our study shows that some styles are highly dependent on each other in human-written text. finally, we find that combinations of some contradictive styles likely generate stylistically less appropriate text. we believe our benchmark and case studies help explore interesting future directions for cross-style research. the preprocessed datasets and code are publicly available."], "sentiment analysis, stylistic analysis, and argument mining"], [["e2e-vlp: end-to-end vision-language pre-training enhanced by visual learning", "haiyang xu | ming yan | chenliang li | bin bi | songfang huang | wenming xiao | fei huang", "vision-language pre-training (vlp) on large-scale image-text pairs has achieved huge success for the cross-modal downstream tasks. the most existing pre-training methods mainly adopt a two-step training procedure, which firstly employs a pre-trained object detector to extract region-based visual features, then concatenates the image representation and text embedding as the input of transformer to train. however, these methods face problems of using task-specific visual representation of the specific object detector for generic cross-modal understanding, and the computation inefficiency of two-stage pipeline. in this paper, we propose the first end-to-end vision-language pre-trained model for both v+l understanding and generation, namely e2e-vlp, where we build a unified transformer framework to jointly learn visual representation, and semantic alignments between image and text. we incorporate the tasks of object detection and image captioning into pre-training with a unified transformer encoder-decoder architecture for enhancing visual learning. an extensive set of experiments have been conducted on well-established vision-language downstream tasks to demonstrate the effectiveness of this novel vlp paradigm."], "language grounding to vision, robotics and beyond"], [["surprisal estimators for human reading times need character models", "byung-doh oh | christian clark | william schuler", "while the use of character models has been popular in nlp applications, it has not been explored much in the context of psycholinguistic modeling. this paper presents a character model that can be applied to a structural parser-based processing model to calculate word generation probabilities. experimental results show that surprisal estimates from a structural processing model using this character model deliver substantially better fits to self-paced reading, eye-tracking, and fmri data than those from large-scale language models trained on much more data. this may suggest that the proposed processing model provides a more humanlike account of sentence processing, which assumes a larger role of morphology, phonotactics, and orthographic complexity than was previously thought."], "linguistic theories, cognitive modeling and psycholinguistics"], [["how helpful is inverse reinforcement learning for table-to-text generation?", "sayan ghosh | zheng qi | snigdha chaturvedi | shashank srivastava", "existing approaches for the table-to-text task suffer from issues such as missing information, hallucination and repetition. many approaches to this problem use reinforcement learning (rl), which maximizes a single manually defined reward, such as bleu. in this work, we instead pose the table-to-text task as inverse reinforcement learning (irl) problem. we explore using multiple interpretable unsupervised reward components that are combined linearly to form a composite reward function. the composite reward function and the description generator are learned jointly. we find that irl outperforms strong rl baselines marginally. we further study the generalization of learned irl rewards in scenarios involving domain adaptation. our experiments reveal significant challenges in using irl for this task."], "machine learning for nlp"], [["online learning meets machine translation evaluation: finding the best systems with the least human effort", "v\u00e2nia mendon\u00e7a | ricardo rei | luisa coheur | alberto sardinha | ana l\u00facia santos", "in machine translation, assessing the quality of a large amount of automatic translations can be challenging. automatic metrics are not reliable when it comes to high performing systems. in addition, resorting to human evaluators can be expensive, especially when evaluating multiple systems. to overcome the latter challenge, we propose a novel application of online learning that, given an ensemble of machine translation systems, dynamically converges to the best systems, by taking advantage of the human feedback available. our experiments on wmt\u201919 datasets show that our online approach quickly converges to the top-3 ranked systems for the language pairs considered, despite the lack of human feedback for many translations."], "machine translation and multilinguality"], [["risk minimization for zero-shot sequence labeling", "zechuan hu | yong jiang | nguyen bach | tao wang | zhongqiang huang | fei huang | kewei tu", "zero-shot sequence labeling aims to build a sequence labeler without human-annotated datasets. one straightforward approach is utilizing existing systems (source models) to generate pseudo-labeled datasets and train a target sequence labeler accordingly. however, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. in this paper, we propose a novel unified framework for zero-shot sequence labeling with minimum risk training and design a new decomposable risk function that models the relations between the predicted labels from the source models and the true labels. by making the risk function trainable, we draw a connection between minimum risk training and latent variable model learning. we propose a unified learning algorithm based on the expectation maximization (em) algorithm. we extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. the results show that our approaches outperform state-of-the-art baseline systems."], "machine learning for nlp"], [["learning from perturbations: diverse and informative dialogue generation with inverse adversarial training", "wangchunshu zhou | qifei li | chenle li", "in this paper, we propose inverse adversarial training (iat) algorithm for training neural dialogue systems to avoid generic responses and model dialogue history better. in contrast to standard adversarial training algorithms, iat encourages the model to be sensitive to the perturbation in the dialogue history and therefore learning from perturbations. by giving higher rewards for responses whose output probability reduces more significantly when dialogue history is perturbed, the model is encouraged to generate more diverse and consistent responses. by penalizing the model when generating the same response given perturbed dialogue history, the model is forced to better capture dialogue history and generate more informative responses. experimental results on two benchmark datasets show that our approach can better model dialogue history and generate more diverse and consistent responses. in addition, we point out a problem of the widely used maximum mutual information (mmi) based methods for improving the diversity of dialogue response generation models and demonstrate it empirically."], "dialogue and interactive systems"], [["towards a more robust evaluation for conversational question answering", "wissam siblini | baris sayil | yacine kessaci", "with the explosion of chatbot applications, conversational question answering (cqa) has generated a lot of interest in recent years. among proposals, reading comprehension models which take advantage of the conversation history (previous qa) seem to answer better than those which only consider the current question. nevertheless, we note that the cqa evaluation protocol has a major limitation. in particular, models are allowed, at each turn of the conversation, to access the ground truth answers of the previous turns. not only does this severely prevent their applications in fully autonomous chatbots, it also leads to unsuspected biases in their behavior. in this paper, we highlight this effect and propose new tools for evaluation and training in order to guard against the noted issues. the new results that we bring come to reinforce methods of the current state of the art."], "question answering"], [["fast and accurate neural machine translation with translation memory", "qiuxiang he | guoping huang | qu cui | li li | lemao liu", "it is generally believed that a translation memory (tm) should be beneficial for machine translation tasks. unfortunately, existing wisdom demonstrates the superiority of tm-based neural machine translation (nmt) only on the tm-specialized translation tasks rather than general tasks, with a non-negligible computational overhead. in this paper, we propose a fast and accurate approach to tm-based nmt within the transformer framework: the model architecture is simple and employs a single bilingual sentence as its tm, leading to efficient training and inference; and its parameters are effectively optimized through a novel training criterion. extensive experiments on six tm-specialized tasks show that the proposed approach substantially surpasses several strong baselines that use multiple tms, in terms of bleu and running time. in particular, the proposed approach also advances the strong baselines on two general tasks (wmt news zh->en and en->de)."], "machine translation and multilinguality"], [["evidence-based factual error correction", "james thorne | andreas vlachos", "this paper introduces the task of factual error correction: performing edits to a claim so that the generated rewrite is better supported by evidence. this extends the well-studied task of fact verification by providing a mechanism to correct written texts that are refuted or only partially supported by evidence. we demonstrate that it is feasible to train factual error correction systems from existing fact checking datasets which only contain labeled claims accompanied by evidence, but not the correction. we achieve this by employing a two-stage distant supervision approach that incorporates evidence into masked claims when generating corrections. our approach, based on the t5 transformer and using retrieved evidence, achieved better results than existing work which used a pointer copy network and gold evidence, producing accurate factual error corrections for 5x more instances in human evaluation and a .125 increase in sari score. the evaluation is conducted on a dataset of 65,000 instances based on a recent fact verification shared task and we release it to enable further work on the task."], "semantics"], [["exploring the representation of word meanings in context: a case study on homonymy and synonymy", "marcos garcia", "this paper presents a multilingual study of word meaning representations in context. we assess the ability of both static and contextualized models to adequately represent different lexical-semantic relations, such as homonymy and synonymy. to do so, we created a new multilingual dataset that allows us to perform a controlled evaluation of several factors such as the impact of the surrounding context or the overlap between words, conveying the same or different senses. a systematic assessment on four scenarios shows that the best monolingual models based on transformers can adequately disambiguate homonyms in context. however, as they rely heavily on context, these models fail at representing words with different senses when occurring in similar sentences. experiments are performed in galician, portuguese, english, and spanish, and both the dataset (with more than 3,000 evaluation items) and new models are freely released with this study."], "semantics"], [["mpc-bert: a pre-trained language model for multi-party conversation understanding", "jia-chen gu | chongyang tao | zhenhua ling | can xu | xiubo geng | daxin jiang", "recently, various neural models for multi-party conversation (mpc) have achieved impressive improvements on a variety of tasks such as addressee recognition, speaker identification and response prediction. however, these existing methods on mpc usually represent interlocutors and utterances individually and ignore the inherent complicated structure in mpc which may provide crucial interlocutor and utterance semantics and would enhance the conversation understanding process. to this end, we present mpc-bert, a pre-trained model for mpc understanding that considers learning who says what to whom in a unified model with several elaborated self-supervised tasks. particularly, these tasks can be generally categorized into (1) interlocutor structure modeling including reply-to utterance recognition, identical speaker searching and pointer consistency distinction, and (2) utterance semantics modeling including masked shared utterance restoration and shared node detection. we evaluate mpc-bert on three downstream tasks including addressee recognition, speaker identification and response selection. experimental results show that mpc-bert outperforms previous methods by large margins and achieves new state-of-the-art performance on all three downstream tasks at two benchmarks."], "dialogue and interactive systems"], [["a systematic investigation of kb-text embedding alignment at scale", "vardaan pahuja | yu gu | wenhu chen | mehdi bahrami | lei liu | wei-peng chen | yu su", "knowledge bases (kbs) and text often contain complementary knowledge: kbs store structured knowledge that can support long range reasoning, while text stores more comprehensive and timely knowledge in an unstructured way. separately embedding the individual knowledge sources into vector spaces has demonstrated tremendous successes in encoding the respective knowledge, but how to jointly embed and reason with both knowledge sources to fully leverage the complementary information is still largely an open problem. we conduct a large-scale, systematic investigation of aligning kb and text embeddings for joint reasoning. we set up a novel evaluation framework with two evaluation tasks, few-shot link prediction and analogical reasoning, and evaluate an array of kb-text embedding alignment methods. we also demonstrate how such alignment can infuse textual information into kb embeddings for more accurate link prediction on emerging entities and events, using covid-19 as a case study."], "information extraction, retrieval and text mining"], [["keep it simple: unsupervised simplification of multi-paragraph text", "philippe laban | tobias schnabel | paul bennett | marti a. hearst", "this work presents keep it simple (kis), a new approach to unsupervised text simplification which learns to balance a reward across three properties: fluency, salience and simplicity. we train the model with a novel algorithm to optimize the reward (k-scst), in which the model proposes several candidate simplifications, computes each candidate\u2019s reward, and encourages candidates that outperform the mean reward. finally, we propose a realistic text comprehension task as an evaluation method for text simplification. when tested on the english news domain, the kis model outperforms strong supervised baselines by more than 4 sari points, and can help people complete a comprehension task an average of 18% faster while retaining accuracy, when compared to the original text."], "generation"], [["structured sentiment analysis as dependency graph parsing", "jeremy barnes | robin kurtz | stephan oepen | lilja \u00f8vrelid | erik velldal", "structured sentiment analysis attempts to extract full opinion tuples from a text, but over time this task has been subdivided into smaller and smaller sub-tasks, e.g., target extraction or targeted polarity classification. we argue that this division has become counterproductive and propose a new unified framework to remedy the situation. we cast the structured sentiment problem as dependency graph parsing, where the nodes are spans of sentiment holders, targets and expressions, and the arcs are the relations between them. we perform experiments on five datasets in four languages (english, norwegian, basque, and catalan) and show that this approach leads to strong improvements over state-of-the-art baselines. our analysis shows that refining the sentiment graphs with syntactic dependency information further improves results."], "sentiment analysis, stylistic analysis, and argument mining"], [["exploiting document structures and cluster consistencies for event coreference resolution", "hieu minh tran | duy phung | thien huu nguyen", "we study the problem of event coreference resolution (ecr) that seeks to group coreferent event mentions into the same clusters. deep learning methods have recently been applied for this task to deliver state-of-the-art performance. however, existing deep learning models for ecr are limited in that they cannot exploit important interactions between relevant objects for ecr, e.g., context words and entity mentions, to support the encoding of document-level context. in addition, consistency constraints between golden and predicted clusters of event mentions have not been considered to improve representation learning in prior deep learning models for ecr. this work addresses such limitations by introducing a novel deep learning model for ecr. at the core of our model are document structures to explicitly capture relevant objects for ecr. our document structures introduce diverse knowledge sources (discourse, syntax, semantics) to compute edges/interactions between structure nodes for document-level representation learning. we also present novel regularization techniques based on consistencies of golden and predicted clusters for event mentions in documents. extensive experiments show that our model achieve state-of-the-art performance on two benchmark datasets."], "information extraction, retrieval and text mining"], [["document-level event extraction via heterogeneous graph-based interaction model with a tracker", "runxin xu | tianyu liu | lei li | baobao chang", "document-level event extraction aims to recognize event information from a whole piece of article. existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. in this paper, we propose heterogeneous graph-based interaction model with a tracker (git) to solve the aforementioned two challenges. for the first challenge, git constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions. for the second, git introduces a tracker module to track the extracted events and hence capture the interdependency among the events. experiments on a large-scale dataset (zheng et al, 2019) show git outperforms the previous methods by 2.8 f1. further analysis reveals is effective in extracting multiple correlated events and event arguments that scatter across the document."], "information extraction, retrieval and text mining"], [["convosumm: conversation summarization benchmark and improved abstractive summarization with argument mining", "alexander fabbri | faiaz rahman | imad rizvi | borui wang | haoran li | yashar mehdad | dragomir radev", "while online conversations can cover a vast amount of information in many different formats, abstractive text summarization has primarily focused on modeling solely news articles. this research gap is due, in part, to the lack of standardized datasets for summarizing online discussions. to address this gap, we design annotation protocols motivated by an issues\u2013viewpoints\u2013assertions framework to crowdsource four new datasets on diverse online conversation forms of news comments, discussion forums, community question answering forums, and email threads. we benchmark state-of-the-art models on our datasets and analyze characteristics associated with the data. to create a comprehensive benchmark, we also evaluate these models on widely-used conversation summarization datasets to establish strong baselines in this domain. furthermore, we incorporate argument mining through graph construction to directly model the issues, viewpoints, and assertions present in a conversation and filter noisy input, showing comparable or improved results according to automatic and human evaluations."], "summarization"], [["beyond laurel/yanny: an autoencoder-enabled search for polyperceivable audio", "kartik chandra | chuma kabaghe | gregory valiant", "the famous \u201claurel/yanny\u201d phenomenon references an audio clip that elicits dramatically different responses from different listeners. for the original clip, roughly half the population hears the word \u201claurel,\u201d while the other half hears \u201cyanny.\u201d how common are such \u201cpolyperceivable\u201d audio clips? in this paper we apply ml techniques to study the prevalence of polyperceivability in spoken language. we devise a metric that correlates with polyperceivability of audio clips, use it to efficiently find new \u201claurel/yanny\u201d-type examples, and validate these results with human experiments. our results suggest that polyperceivable examples are surprisingly prevalent in natural language, existing for >2% of english words."], "linguistic theories, cognitive modeling and psycholinguistics"], [["qasr: qcri aljazeera speech resource a large scale annotated arabic speech corpus", "hamdy mubarak | amir hussein | shammur absar chowdhury | ahmed ali", "we introduce the largest transcribed arabic speech corpus, qasr, collected from the broadcast domain. this multi-dialect speech dataset contains 2,000 hours of speech sampled at 16khz crawled from aljazeera news channel. the dataset is released with lightly supervised transcriptions, aligned with the audio segments. unlike previous datasets, qasr contains linguistically motivated segmentation, punctuation, speaker information among others. qasr is suitable for training and evaluating speech recognition systems, acoustics- and/or linguistics- based arabic dialect identification, punctuation restoration, speaker identification, speaker linking, and potentially other nlp modules for spoken data. in addition to qasr transcription, we release a dataset of 130m words to aid in designing and training a better language model. we show that end-to-end automatic speech recognition trained on qasr reports a competitive word error rate compared to the previous mgb-2 corpus. we report baseline results for downstream natural language processing tasks such as named entity recognition using speech transcript. we also report the first baseline for arabic punctuation restoration. we make the corpus available for the research community."], "resources and evaluation"], [["veco: variable and flexible cross-lingual pre-training for language understanding and generation", "fuli luo | wei wang | jiahao liu | yijia liu | bin bi | songfang huang | fei huang | luo si", "existing work in multilingual pretraining has demonstrated the potential of cross-lingual transferability by training a unified transformer encoder for multiple languages. however, much of this work only relies on the shared vocabulary and bilingual contexts to encourage the correlation across languages, which is loose and implicit for aligning the contextual representations between languages. in this paper, we plug a cross-attention module into the transformer encoder to explicitly build the interdependence between languages. it can effectively avoid the degeneration of predicting masked words only conditioned on the context in its own language. more importantly, when fine-tuning on downstream tasks, the cross-attention module can be plugged in or out on-demand, thus naturally benefiting a wider range of cross-lingual tasks, from language understanding to generation. as a result, the proposed cross-lingual model delivers new state-of-the-art results on various cross-lingual understanding tasks of the xtreme benchmark, covering text classification, sequence labeling, question answering, and sentence retrieval. for cross-lingual generation tasks, it also outperforms all existing cross-lingual models and state-of-the-art transformer variants on wmt14 english-to-german and english-to-french translation datasets, with gains of up to 1 2 bleu."], "machine translation and multilinguality"], [["self-supervised multimodal opinion summarization", "jinbae im | moonki kim | hoyeop lee | hyunsouk cho | sehee chung", "recently, opinion summarization, which is the generation of a summary from multiple reviews, has been conducted in a self-supervised manner by considering a sampled review as a pseudo summary. however, non-text data such as image and metadata related to reviews have been considered less often. to use the abundant information contained in non-text data, we propose a self-supervised multimodal opinion summarization framework called multimodalsum. our framework obtains a representation of each modality using a separate encoder for each modality, and the text decoder generates a summary. to resolve the inherent heterogeneity of multimodal data, we propose a multimodal training pipeline. we first pretrain the text encoder\u2013decoder based solely on text modality data. subsequently, we pretrain the non-text modality encoders by considering the pretrained text decoder as a pivot for the homogeneous representation of multimodal data. finally, to fuse multimodal representations, we train the entire framework in an end-to-end manner. we demonstrate the superiority of multimodalsum by conducting experiments on yelp and amazon datasets."], "summarization"], [["parameter-efficient transfer learning with diff pruning", "demi guo | alexander rush | yoon kim", "the large size of pretrained networks makes them difficult to deploy for multiple tasks in storage-constrained settings. diff pruning enables parameter-efficient transfer learning that scales well with new tasks. the approach learns a task-specific \u201cdiff\u201d vector that extends the original pretrained parameters. this diff vector is adaptively pruned during training with a differentiable approximation to the l0-norm penalty to encourage sparsity. as the number of tasks increases, diff pruning remains parameter-efficient, as it requires storing only a small diff vector for each task. since it does not require access to all tasks during training, it is attractive in on-device deployment settings where tasks arrive in stream or even from different providers. diff pruning can match the performance of finetuned baselines on the glue benchmark while only modifying 0.5% of the pretrained model\u2019s parameters per task and scales favorably in comparison to popular pruning approaches."], "machine learning for nlp"], [["spanner: named entity re-/recognition as span prediction", "jinlan fu | xuanjing huang | pengfei liu", "recent years have seen the paradigm shift of named entity recognition (ner) systems from sequence labeling to span prediction. despite its preliminary effectiveness, the span prediction model\u2019s architectural bias has not been fully understood. in this paper, we first investigate the strengths and weaknesses when the span prediction model is used for named entity recognition compared with the sequence labeling framework and how to further improve it, which motivates us to make complementary advantages of systems based on different paradigms. we then reveal that span prediction, simultaneously, can serve as a system combiner to re-recognize named entities from different systems\u2019 outputs. we experimentally implement 154 systems on 11 datasets, covering three languages, comprehensive results show the effectiveness of span prediction models that both serve as base ner systems and system combiners. we make all codes and datasets available: https://github.com/neulab/spanner, as well as an online system demo: http://spanner.sh. our model also has been deployed into the explainaboard platform, which allows users to flexibly perform a system combination of top-scoring systems in an interactive way: http://explainaboard.nlpedia.ai/leaderboard/task-ner/."], "tagging, chunking, syntax and parsing"], [["conversations are not flat: modeling the dynamic information flow across dialogue utterances", "zekang li | jinchao zhang | zhengcong fei | yang feng | jie zhou", "nowadays, open-domain dialogue models can generate acceptable responses according to the historical context based on the large-scale pre-trained language models. however, they generally concatenate the dialogue history directly as the model input to predict the response, which we named as the flat pattern and ignores the dynamic information flow across dialogue utterances. in this work, we propose the dialoflow model, in which we introduce a dynamic flow mechanism to model the context flow, and design three training objectives to capture the information dynamics across dialogue utterances by addressing the semantic influence brought about by each utterance in large-scale pre-training. experiments on the multi-reference reddit dataset and dailydialog dataset demonstrate that our dialoflow significantly outperforms the dialogpt on the dialogue generation task. besides, we propose the flow score, an effective automatic metric for evaluating interactive human-bot conversation quality based on the pre-trained dialoflow, which presents high chatbot-level correlation (r=0.9) with human ratings among 11 chatbots. code and pre-trained models will be public."], "dialogue and interactive systems"], [["de-biasing distantly supervised named entity recognition via causal intervention", "wenkai zhang | hongyu lin | xianpei han | le sun", "distant supervision tackles the data bottleneck in ner by automatically generating training instances via dictionary matching. unfortunately, the learning of ds-ner is severely dictionary-biased, which suffers from spurious correlations and therefore undermines the effectiveness and the robustness of the learned models. in this paper, we fundamentally explain the dictionary bias via a structural causal model (scm), categorize the bias into intra-dictionary and inter-dictionary biases, and identify their causes. based on the scm, we learn de-biased ds-ner via causal interventions. for intra-dictionary bias, we conduct backdoor adjustment to remove the spurious correlations introduced by the dictionary confounder. for inter-dictionary bias, we propose a causal invariance regularizer which will make ds-ner models more robust to the perturbation of dictionaries. experiments on four datasets and three ds-ner models show that our method can significantly improve the performance of ds-ner."], "information extraction, retrieval and text mining"], [["are missing links predictable? an inferential benchmark for knowledge graph completion", "yixin cao | xiang ji | xin lv | juanzi li | yonggang wen | hanwang zhang", "we present inferwiki, a knowledge graph completion (kgc) dataset that improves upon existing benchmarks in inferential ability, assumptions, and patterns. first, each testing sample is predictable with supportive data in the training set. to ensure it, we propose to utilize rule-guided train/test generation, instead of conventional random split. second, inferwiki initiates the evaluation following the open-world assumption and improves the inferential difficulty of the closed-world assumption, by providing manually annotated negative and unknown triples. third, we include various inference patterns (e.g., reasoning path length and types) for comprehensive evaluation. in experiments, we curate two settings of inferwiki varying in sizes and structures, and apply the construction process on codex as comparative datasets. the results and empirical analyses demonstrate the necessity and high-quality of inferwiki. nevertheless, the performance gap among various inferential assumptions and patterns presents the difficulty and inspires future research direction. our datasets can be found in https://github.com/taominer/inferwiki."], "resources and evaluation"], [["enhancing content preservation in text style transfer using reverse attention and conditional layer normalization", "dongkyu lee | zhiliang tian | lanqing xue | nevin l. zhang", "text style transfer aims to alter the style (e.g., sentiment) of a sentence while preserving its content. a common approach is to map a given sentence to content representation that is free of style, and the content representation is fed to a decoder with a target style. previous methods in filtering style completely remove tokens with style at the token level, which incurs the loss of content information. in this paper, we propose to enhance content preservation by implicitly removing the style information of each token with reverse attention, and thereby retain the content. furthermore, we fuse content information when building the target style representation, making it dynamic with respect to the content. our method creates not only style-independent content representation, but also content-dependent style representation in transferring style. empirical results show that our method outperforms the state-of-the-art baselines by a large margin in terms of content preservation. in addition, it is also competitive in terms of style transfer accuracy and fluency."], "generation"], [["dexperts: decoding-time controlled text generation with experts and anti-experts", "alisa liu | maarten sap | ximing lu | swabha swayamdipta | chandra bhagavatula | noah a. smith | yejin choi", "despite recent advances in natural language generation, it remains challenging to control attributes of generated text. we propose dexperts: decoding-time experts, a decoding-time method for controlled text generation that combines a pretrained language model with \u201cexpert\u201d lms and/or \u201canti-expert\u201d lms in a product of experts. intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. we apply dexperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. moreover, because dexperts operates only on the output of the pretrained lm, it is effective with (anti-)experts of smaller size, including when operating on gpt-3. our work highlights the promise of tuning small lms on text with (un)desirable attributes for efficient decoding-time steering."], "generation"], [["thank you bart! rewarding pre-trained models improves formality style transfer", "huiyuan lai | antonio toral | malvina nissim", "scarcity of parallel data causes formality style transfer models to have scarce success in preserving content. we show that fine-tuning pre-trained language (gpt-2) and sequence-to-sequence (bart) models boosts content preservation, and that this is possible even with limited amounts of parallel data. augmenting these models with rewards that target style and content \u2013the two core aspects of the task\u2013 we achieve a new state-of-the-art."], "sentiment analysis, stylistic analysis, and argument mining"], [["knowledgeable or educated guess? revisiting language models as knowledge bases", "boxi cao | hongyu lin | xianpei han | le sun | lingyong yan | meng liao | tong xue | jin xu", "previous literatures show that pre-trained masked language models (mlms) such as bert can achieve competitive factual knowledge extraction performance on some datasets, indicating that mlms can potentially be a reliable knowledge source. in this paper, we conduct a rigorous study to explore the underlying predicting mechanisms of mlms over different extraction paradigms. by investigating the behaviors of mlms, we find that previous decent performance mainly owes to the biased prompts which overfit dataset artifacts. furthermore, incorporating illustrative cases and external contexts improve knowledge prediction mainly due to entity type guidance and golden answer leakage. our findings shed light on the underlying predicting mechanisms of mlms, and strongly question the previous conclusion that current mlms can potentially serve as reliable factual knowledge bases."], "interpretability and analysis of models for nlp"], [["neuralwoz: learning to collect task-oriented dialogue via model-based simulation", "sungdong kim | minsuk chang | sang-woo lee", "we propose neuralwoz, a novel dialogue collection framework that uses model-based dialogue simulation. neuralwoz has two pipelined models, collector and labeler. collector generates dialogues from (1) user\u2019s goal instructions, which are the user context and task constraints in natural language, and (2) system\u2019s api call results, which is a list of possible query responses for user requests from the given knowledge base. labeler annotates the generated dialogue by formulating the annotation as a multiple-choice problem, in which the candidate labels are extracted from goal instructions and api call results. we demonstrate the effectiveness of the proposed method in the zero-shot domain transfer learning for dialogue state tracking. in the evaluation, the synthetic dialogue corpus generated from neuralwoz achieves a new state-of-the-art with improvements of 4.4% point joint goal accuracy on average across domains, and improvements of 5.7% point of zero-shot coverage against the multiwoz 2.1 dataset."], "dialogue and interactive systems"], [["out-of-scope intent detection with self-supervision and discriminative training", "li-ming zhan | haowen liang | bo liu | lu fan | xiao-ming wu | albert y.s. lam", "out-of-scope intent detection is of practical importance in task-oriented dialogue systems. since the distribution of outlier utterances is arbitrary and unknown in the training stage, existing methods commonly rely on strong assumptions on data distribution such as mixture of gaussians to make inference, resulting in either complex multi-step training procedures or hand-crafted rules such as confidence threshold selection for outlier detection. in this paper, we propose a simple yet effective method to train an out-of-scope intent classifier in a fully end-to-end manner by simulating the test scenario in training, which requires no assumption on data distribution and no additional post-processing or threshold setting. specifically, we construct a set of pseudo outliers in the training stage, by generating synthetic outliers using inliner features via self-supervision and sampling out-of-scope sentences from easily available open-domain datasets. the pseudo outliers are used to train a discriminative classifier that can be directly applied to and generalize well on the test task. we evaluate our method extensively on four benchmark dialogue datasets and observe significant improvements over state-of-the-art approaches. our code has been released at https://github.com/liam0949/dcloos."], "dialogue and interactive systems"], [["which linguist invented the lightbulb? presupposition verification for question-answering", "najoung kim | ellie pavlick | burcu karagol ayan | deepak ramachandran", "many question-answering (qa) datasets contain unanswerable questions, but their treatment in qa systems remains primitive. our analysis of the natural questions (kwiatkowski et al. 2019) dataset reveals that a substantial portion of unanswerable questions (~21%) can be explained based on the presence of unverifiable presuppositions. through a user preference study, we demonstrate that the oracle behavior of our proposed system\u2014which provides responses based on presupposition failure\u2014is preferred over the oracle behavior of existing qa systems. then, we present a novel framework for implementing such a system in three steps: presupposition generation, presupposition verification, and explanation generation, reporting progress on each. finally, we show that a simple modification of adding presuppositions and their verifiability to the input of a competitive end-to-end qa system yields modest gains in qa performance and unanswerability detection, demonstrating the promise of our approach."], "discourse and pragmatics"], [["don\u2019t let discourse confine your model: sequence perturbations for improved event language models", "mahnaz koupaee | greg durrett | nathanael chambers | niranjan balasubramanian", "event language models represent plausible sequences of events. most existing approaches train autoregressive models on text, which successfully capture event co-occurrence but unfortunately constrain the model to follow the discourse order in which events are presented. other domains may employ different discourse orders, and for many applications, we may care about different notions of ordering (e.g., temporal) or not care about ordering at all (e.g., when predicting related events in a schema). we propose a simple yet surprisingly effective strategy for improving event language models by perturbing event sequences so we can relax model dependence on text order. despite generating completely synthetic event orderings, we show that this technique improves the performance of the event language models on both applications and out-of-domain events data."], "machine learning for nlp"], [["multi-stage pre-training over simplified multimodal pre-training models", "tongtong liu | fangxiang feng | xiaojie wang", "multimodal pre-training models, such as lxmert, have achieved excellent results in downstream tasks. however, current pre-trained models require large amounts of training data and have huge model sizes, which make them impossible to apply in low-resource situations. how to obtain similar or even better performance than a larger model under the premise of less pre-training data and smaller model size has become an important problem. in this paper, we propose a new multi-stage pre-training (msp) method, which uses information at different granularities from word, phrase to sentence in both texts and images to pre-train a model in stages. we also design several different pre-training tasks suitable for the information granularity in different stage in order to efficiently capture the diverse knowledge from a limited corpus. we take a simplified lxmert (lxmert-s) which is with 45.9% parameters of the original lxmert model and only 11.44% of the original pre-training data as the testbed of our msp method. experimental results show that our method achieves comparable performance to the original lxmert model in all downstream tasks, and even outperforms the original model in image-text retrieval task."], "speech and multimodality"], [["database reasoning over text", "james thorne | majid yazdani | marzieh saeidi | fabrizio silvestri | sebastian riedel | alon halevy", "neural models have shown impressive performance gains in answering queries from natural language text. however, existing works are unable to support database queries, such as \u201clist/count all female athletes who were born in 20th century\u201d, which require reasoning over sets of relevant facts with operations such as join, filtering and aggregation. we show that while state-of-the-art transformer models perform very well for small databases, they exhibit limitations in processing noisy data, numerical operations, and queries that aggregate facts. we propose a modular architecture to answer these database-style queries over multiple spans from text and aggregating these at scale. we evaluate the architecture using wikinldb, a novel dataset for exploring such queries. our architecture scales to databases containing thousands of facts whereas contemporary models are limited by how many facts can be encoded. in direct comparison on small databases, our approach increases overall answer accuracy from 85% to 90%. on larger databases, our approach retains its accuracy whereas transformer baselines could not encode the context."], "question answering"], [["leebert: learned early exit for bert with cross-level optimization", "wei zhu", "pre-trained language models like bert are performant in a wide range of natural language tasks. however, they are resource exhaustive and computationally expensive for industrial scenarios. thus, early exits are adopted at each layer of bert to perform adaptive computation by predicting easier samples with the first few layers to speed up the inference. in this work, to improve efficiency without performance drop, we propose a novel training scheme called learned early exit for bert (leebert). first, we ask each exit to learn from each other, rather than learning only from the last layer. second, the weights of different loss terms are learned, thus balancing off different objectives. we formulate the optimization of leebert as a bi-level optimization problem, and we propose a novel cross-level optimization (clo) algorithm to improve the optimization results. experiments on the glue benchmark show that our proposed methods improve the performance of the state-of-the-art (sota) early exit methods for pre-trained models."], "machine learning for nlp"], [["intrinsic bias metrics do not correlate with application bias", "seraphina goldfarb-tarrant | rebecca marchant | ricardo mu\u00f1oz s\u00e1nchez | mugdha pandya | adam lopez", "natural language processing (nlp) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations. to guide efforts at debiasing these systems, the nlp community relies on a variety of metrics that quantify bias in models. some of these metrics are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings enable. do these intrinsic and extrinsic metrics correlate with each other? we compare intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions. our results show no reliable correlation between these metrics that holds in all scenarios across tasks and languages. we urge researchers working on debiasing to focus on extrinsic measures of bias, and to make using these measures more feasible via creation of new challenge sets and annotated test data. to aid this effort, we release code, a new intrinsic metric, and an annotated test set focused on gender bias in hate speech."], "ethics in nlp"], [["citationie: leveraging the citation graph for scientific information extraction", "vijay viswanathan | graham neubig | pengfei liu", "automatically extracting key information from scientific documents has the potential to help scientists work more efficiently and accelerate the pace of scientific progress. prior work has considered extracting document-level entity clusters and relations end-to-end from raw scientific text, which can improve literature search and help identify methods and materials for a given problem. despite the importance of this task, most existing works on scientific information extraction (sciie) consider extraction solely based on the content of an individual paper, without considering the paper\u2019s place in the broader literature. in contrast to prior work, we augment our text representations by leveraging a complementary source of document context: the citation graph of referential links between citing and cited papers. on a test set of english-language scientific documents, we show that simple ways of utilizing the structure and content of the citation graph can each lead to significant gains in different scientific information extraction tasks. when these tasks are combined, we observe a sizable improvement in end-to-end information extraction over the state-of-the-art, suggesting the potential for future work along this direction. we release software tools to facilitate citation-aware sciie development."], "information extraction, retrieval and text mining"], [["dialoguecrn: contextual reasoning networks for emotion recognition in conversations", "dou hu | lingwei wei | xiaoyong huai", "emotion recognition in conversations (erc) has gained increasing attention for developing empathetic machines. recently, many approaches have been devoted to perceiving conversational context by deep learning models. however, these approaches are insufficient in understanding the context due to lacking the ability to extract and integrate emotional clues. in this work, we propose novel contextual reasoning networks (dialoguecrn) to fully understand the conversational context from a cognitive perspective. inspired by the cognitive theory of emotion, we design multi-turn reasoning modules to extract and integrate emotional clues. the reasoning module iteratively performs an intuitive retrieving process and a conscious reasoning process, which imitates human unique cognitive thinking. extensive experiments on three public benchmark datasets demonstrate the effectiveness and superiority of the proposed model."], "dialogue and interactive systems"], [["rope: reading order equivariant positional encoding for graph-based document information extraction", "chen-yu lee | chun-liang li | chu wang | renshen wang | yasuhisa fujii | siyang qin | ashok popat | tomas pfister", "natural reading orders of words are crucial for information extraction from form-like documents. despite recent advances in graph convolutional networks (gcns) on modeling spatial layout patterns of documents, they have limited ability to capture reading orders of given word-level node representations in a graph. we propose reading order equivariant positional encoding (rope), a new positional encoding technique designed to apprehend the sequential presentation of words in documents. rope generates unique reading order codes for neighboring words relative to the target word given a word-level graph connectivity. we study two fundamental document entity extraction tasks including word labeling and word grouping on the public funsd dataset and a large-scale payment dataset. we show that rope consistently improves existing gcns with a margin up to 8.4% f1-score."], "information extraction, retrieval and text mining"], [["modeling language usage and listener engagement in podcasts", "sravana reddy | mariya lazarova | yongze yu | rosie jones", "while there is an abundance of advice to podcast creators on how to speak in ways that engage their listeners, there has been little data-driven analysis of podcasts that relates linguistic style with engagement. in this paper, we investigate how various factors \u2013 vocabulary diversity, distinctiveness, emotion, and syntax, among others \u2013 correlate with engagement, based on analysis of the creators\u2019 written descriptions and transcripts of the audio. we build models with different textual representations, and show that the identified features are highly predictive of engagement. our analysis tests popular wisdom about stylistic elements in high-engagement podcasts, corroborating some pieces of advice and adding new perspectives on others."], "computational social science, social media and cultural analytics"], [["from discourse to narrative: knowledge projection for event relation extraction", "jialong tang | hongyu lin | meng liao | yaojie lu | xianpei han | le sun | weijian xie | jin xu", "current event-centric knowledge graphs highly rely on explicit connectives to mine relations between events. unfortunately, due to the sparsity of connectives, these methods severely undermine the coverage of eventkgs. the lack of high-quality labelled corpora further exacerbates that problem. in this paper, we propose a knowledge projection paradigm for event relation extraction: projecting discourse knowledge to narratives by exploiting the commonalities between them. specifically, we propose multi-tier knowledge projection network (mkpnet), which can leverage multi-tier discourse knowledge effectively for event relation extraction. in this way, the labelled data requirement is significantly reduced, and implicit event relations can be effectively extracted. intrinsic experimental results show that mkpnet achieves the new state-of-the-art performance and extrinsic experimental results verify the value of the extracted event relations."], "information extraction, retrieval and text mining"], [["measuring fine-grained domain relevance of terms: a hierarchical core-fringe approach", "jie huang | kevin chang | jinjun xiong | wen-mei hwu", "we propose to measure fine-grained domain relevance\u2013 the degree that a term is relevant to a broad (e.g., computer science) or narrow (e.g., deep learning) domain. such measurement is crucial for many downstream tasks in natural language processing. to handle long-tail terms, we build a core-anchored semantic graph, which uses core terms with rich description information to bridge the vast remaining fringe terms semantically. to support a fine-grained domain without relying on a matching corpus for supervision, we develop hierarchical core-fringe learning, which learns core and fringe terms jointly in a semi-supervised manner contextualized in the hierarchy of the domain. to reduce expensive human efforts, we employ automatic annotation and hierarchical positive-unlabeled learning. our approach applies to big or small domains, covers head or tail terms, and requires little human effort. extensive experiments demonstrate that our methods outperform strong baselines and even surpass professional human performance."], "semantics"], [["adept: an adjective-dependent plausibility task", "ali emami | ian porada | alexandra olteanu | kaheer suleman | adam trischler | jackie chi kit cheung", "a false contract is more likely to be rejected than a contract is, yet a false key is less likely than a key to open doors. while correctly interpreting and assessing the effects of such adjective-noun pairs (e.g., false key) on the plausibility of given events (e.g., opening doors) underpins many natural language understanding tasks, doing so often requires a significant degree of world knowledge and common-sense reasoning. we introduce adept \u2013 a large-scale semantic plausibility task consisting of over 16 thousand sentences that are paired with slightly modified versions obtained by adding an adjective to a noun. overall, we find that while the task appears easier for human judges (85% accuracy), it proves more difficult for transformer-based models like roberta (71% accuracy). our experiments also show that neither the adjective itself nor its taxonomic class suffice in determining the correct plausibility judgement, emphasizing the importance of endowing automatic natural language understanding systems with more context sensitivity and common-sense reasoning."], "semantics"], [["end-to-end training of neural retrievers for open-domain question answering", "devendra sachan | mostofa patwary | mohammad shoeybi | neel kant | wei ping | william l. hamilton | bryan catanzaro", "recent work on training neural retrievers for open-domain question answering (openqa) has employed both supervised and unsupervised approaches. however, it remains unclear how unsupervised and supervised methods can be used most effectively for neural retrievers. in this work, we systematically study retriever pre-training. we first propose an approach of unsupervised pre-training with the inverse cloze task and masked salient spans, followed by supervised finetuning using question-context pairs. this approach leads to absolute gains of 2+ points over the previous best result in the top-20 retrieval accuracy on natural questions and triviaqa datasets. we next explore two approaches for end-to-end training of the reader and retriever components in openqa models, which differ in the manner the reader ingests the retrieved documents. our experiments demonstrate the effectiveness of these approaches as we obtain state-of-the-art results. on the natural questions dataset, we obtain a top-20 retrieval accuracy of 84%, an improvement of 5 points over the recent dpr model. we also achieve good results on answer extraction, outperforming recent models like realm and rag by 3+ points."], "question answering"], [["smurf: semantic and linguistic understanding fusion for caption evaluation via typicality analysis", "joshua feinglass | yezhou yang", "the open-ended nature of visual captioning makes it a challenging area for evaluation. the majority of proposed models rely on specialized training to improve human-correlation, resulting in limited adoption, generalizability, and explainabilty. we introduce \u201ctypicality\u201d, a new formulation of evaluation rooted in information theory, which is uniquely suited for problems lacking a definite ground truth. typicality serves as our framework to develop a novel semantic comparison, sparcs, as well as referenceless fluency evaluation metrics. over the course of our analysis, two separate dimensions of fluency naturally emerge: style, captured by metric spurts, and grammar, captured in the form of grammatical outlier penalties. through extensive experiments and ablation studies on benchmark datasets, we show how these decomposed dimensions of semantics and fluency provide greater system-level insight into captioner differences. our proposed metrics along with their combination, smurf, achieve state-of-the-art correlation with human judgment when compared with other rule-based evaluation metrics."], "resources and evaluation"], [["neural semi-markov crf for monolingual word alignment", "wuwei lan | chao jiang | wei xu", "monolingual word alignment is important for studying fine-grained editing operations (i.e., deletion, addition, and substitution) in text-to-text generation tasks, such as paraphrase generation, text simplification, neutralizing biased language, etc. in this paper, we present a novel neural semi-markov crf alignment model, which unifies word and phrase alignments through variable-length spans. we also create a new benchmark with human annotations that cover four different text genres to evaluate monolingual word alignment models in more realistic settings. experimental results show that our proposed model outperforms all previous approaches for monolingual word alignment as well as a competitive qa-based baseline, which was previously only applied to bilingual data. our model demonstrates good generalizability to three out-of-domain datasets and shows great utility in two downstream applications: automatic text simplification and sentence pair classification tasks."], "resources and evaluation"], [["best of both worlds: making high accuracy non-incremental transformer-based disfluency detection incremental", "morteza rohanian | julian hough", "while transformer-based text classifiers pre-trained on large volumes of text have yielded significant improvements on a wide range of computational linguistics tasks, their implementations have been unsuitable for live incremental processing thus far, operating only on the level of complete sentence inputs. we address the challenge of introducing methods for word-by-word left-to-right incremental processing to transformers such as bert, models without an intrinsic sense of linear order. we modify the training method and live decoding of non-incremental models to detect speech disfluencies with minimum latency and without pre-segmentation of dialogue acts. we experiment with several decoding methods to predict the rightward context of the word currently being processed using a gpt-2 language model and apply a bert-based disfluency detector to sequences, including predicted words. we show our method of incrementalising transformers maintains most of their high non-incremental performance while operating strictly incrementally. we also evaluate our models\u2019 incremental performance to establish the trade-off between incremental performance and final performance, using different prediction strategies. we apply our system to incremental speech recognition results as they arrive into a live system and achieve state-of-the-art results in this setting."], "dialogue and interactive systems"], [["a unified generative framework for aspect-based sentiment analysis", "hang yan | junqi dai | tuo ji | xipeng qiu | zheng zhang", "aspect-based sentiment analysis (absa) aims to identify the aspect terms, their corresponding sentiment polarities, and the opinion terms. there exist seven subtasks in absa. most studies only focus on the subsets of these subtasks, which leads to various complicated absa models while hard to solve these subtasks in a unified framework. in this paper, we redefine every subtask target as a sequence mixed by pointer indexes and sentiment class indexes, which converts all absa subtasks into a unified generative formulation. based on the unified formulation, we exploit the pre-training sequence-to-sequence model bart to solve all absa subtasks in an end-to-end framework. extensive experiments on four absa datasets for seven subtasks demonstrate that our framework achieves substantial performance gain and provides a real unified end-to-end solution for the whole absa subtasks, which could benefit multiple tasks."], "sentiment analysis, stylistic analysis, and argument mining"], [["unsupervised extractive summarization-based representations for accurate and explainable collaborative filtering", "reinald adrian pugoy | hung-yu kao", "we pioneer the first extractive summarization-based collaborative filtering model called escofilt. our proposed model specifically produces extractive summaries for each item and user. unlike other types of explanations, summary-level explanations closely resemble real-life explanations. the strength of escofilt lies in the fact that it unifies representation and explanation. in other words, extractive summaries both represent and explain the items and users. our model uniquely integrates bert, k-means embedding clustering, and multilayer perceptron to learn sentence embeddings, representation-explanations, and user-item interactions, respectively. we argue that our approach enhances both rating prediction accuracy and user/item explainability. our experiments illustrate that escofilt\u2019s prediction accuracy is better than the other state-of-the-art recommender models. furthermore, we propose a comprehensive set of criteria that assesses the real-life explainability of explanations. our explainability study demonstrates the superiority of and preference for summary-level explanations over other explanation types."], "nlp applications"], [["an empirical study on adversarial attack on nmt: languages and positions matter", "zhiyuan zeng | deyi xiong", "in this paper, we empirically investigate adversarial attack on nmt from two aspects: languages (the source vs. the target language) and positions (front vs. rear). for autoregressive nmt models that generate target words from left to right, we observe that adversarial attack on the source language is more effective than on the target language, and that attacking front positions of target sentences or positions of source sentences aligned to the front positions of corresponding target sentences is more effective than attacking other positions. we further exploit the attention distribution of the victim model to attack source sentences at positions that have a strong association with front target words. experiment results demonstrate that our attention-based adversarial attack is more effective than adversarial attacks by sampling positions randomly or according to gradients."], "machine translation and multilinguality"], [["bert is to nlp what alexnet is to cv: can pre-trained language models identify analogies?", "asahi ushio | luis espinosa anke | steven schockaert | jose camacho-collados", "analogies play a central role in human commonsense reasoning. the ability to recognize analogies such as \u201ceye is to seeing what ear is to hearing\u201d, sometimes referred to as analogical proportions, shape how we structure knowledge and understand language. surprisingly, however, the task of identifying such analogies has not yet received much attention in the language model era. in this paper, we analyze the capabilities of transformer-based language models on this unsupervised task, using benchmarks obtained from educational settings, as well as more commonly used datasets. we find that off-the-shelf language models can identify analogies to a certain extent, but struggle with abstract and complex relations, and results are highly sensitive to model architecture and hyperparameters. overall the best results were obtained with gpt-2 and roberta, while configurations using bert were not able to outperform word embedding models. our results raise important questions for future work about how, and to what extent, pre-trained language models capture knowledge about abstract semantic relations."], "semantics"], [["i like fish, especially dolphins: addressing contradictions in dialogue modeling", "yixin nie | mary williamson | mohit bansal | douwe kiela | jason weston", "to quantify how well natural language understanding models can capture consistency in a general conversation, we introduce the dialogue contradiction detection task (decode) and a new conversational dataset containing both human-human and human-bot contradictory dialogues. we show that: (i) our newly collected dataset is notably more effective at providing supervision for the dialogue contradiction detection task than existing nli data including those aimed to cover the dialogue domain; (ii) transformer models that explicitly hinge on utterance structures for dialogue contradiction detection are more robust and generalize well on both analysis and out-of-distribution dialogues than standard (unstructured) transformers. we also show that our best contradiction detection model correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots."], "dialogue and interactive systems"], [["unimo: towards unified-modal understanding and generation via cross-modal contrastive learning", "wei li | can gao | guocheng niu | xinyan xiao | hao liu | jiachen liu | hua wu | haifeng wang", "existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other. they can only utilize single-modal data (i.e., text or image) or limited multi-modal data (i.e., image-text pairs). in this work, we propose a unified-modal pre-training architecture, namely unimo, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks. large scale of free text corpus and image collections are utilized to improve the capability of visual and textual understanding, and cross-modal contrastive learning (cmcl) is leveraged to align the textual and visual information into a unified semantic space, over a corpus of image-text pairs augmented with related images and texts. with the help of rich non-paired single-modal data, our model is able to learn more generalizable representations, by allowing textual knowledge and visual knowledge to enhance each other in the unified semantic space. the experimental results show that unimo greatly improves the performance of several single-modal and multi-modal downstream tasks. our code and pre-trained models are public at https://github.com/paddlepaddle/research/tree/master/nlp/unimo."], "speech and multimodality"], [["multilingual speech translation from efficient finetuning of pretrained models", "xian li | changhan wang | yun tang | chau tran | yuqing tang | juan pino | alexei baevski | alexis conneau | michael auli", "we present a simple yet effective approach to build multilingual speech-to-text (st) translation through efficient transfer learning from a pretrained speech encoder and text decoder. our key finding is that a minimalistic lna (layernorm and attention) finetuning can achieve zero-shot crosslingual and cross-modality transfer ability by only finetuning 10 50% of the pretrained parameters. this effectively leverages large pretrained models at low training cost such as wav2vec 2.0 for acoustic modeling, and mbart for multilingual text generation. this sets a new state-of-the-art for 36 translation directions (and surpassing cascaded st for 26 of them) on the large-scale multilingual st benchmark covost 2 (+6.4 bleu on average for en-x directions and +6.7 bleu for x-en directions). our approach demonstrates strong zero-shot performance in a many-to-many multilingual model (+5.6 bleu on average across 28 non-english directions), making it an appealing approach for attaining high-quality speech translation with improved parameter and data efficiency."], "machine translation and multilinguality"], [["deep context- and relation-aware learning for aspect-based sentiment analysis", "shinhyeok oh | dongyub lee | taesun whang | ilnam park | seo gaeun | eunggyun kim | harksoo kim", "existing works for aspect-based sentiment analysis (absa) have adopted a unified approach, which allows the interactive relations among subtasks. however, we observe that these methods tend to predict polarities based on the literal meaning of aspect and opinion terms and mainly consider relations implicitly among subtasks at the word level. in addition, identifying multiple aspect\u2013opinion pairs with their polarities is much more challenging. therefore, a comprehensive understanding of contextual information w.r.t. the aspect and opinion are further required in absa. in this paper, we propose deep contextualized relation-aware network (dcran), which allows interactive relations among subtasks with deep contextual information based on two modules (i.e., aspect and opinion propagation and explicit self-supervised strategies). especially, we design novel self-supervised strategies for absa, which have strengths in dealing with multiple aspects. experimental results show that dcran significantly outperforms previous state-of-the-art methods by large margins on three widely used benchmarks."], "sentiment analysis, stylistic analysis, and argument mining"], [["adapting high-resource nmt models to translate low-resource related languages without parallel data", "wei-jen ko | ahmed el-kishky | adithya renduchintala | vishrav chaudhary | naman goyal | francisco guzm\u00e1n | pascale fung | philipp koehn | mona diab", "the scarcity of parallel data is a major obstacle for training high-quality machine translation systems for low-resource languages. fortunately, some low-resource languages are linguistically related or similar to high-resource languages; these related languages may share many lexical or syntactic structures. in this work, we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data, in addition to any parallel data in the related high-resource language. our method, nmt-adapt, combines denoising autoencoding, back-translation and adversarial objectives to utilize monolingual data for low-resource adaptation. we experiment on 7 languages from three different language families and show that our technique significantly improves translation into low-resource language compared to other translation baselines."], "machine translation and multilinguality"], [["mednli is not immune: natural language inference artifacts in the clinical domain", "christine herlihy | rachel rudinger", "crowdworker-constructed natural language inference (nli) datasets have been found to contain statistical artifacts associated with the annotation process that allow hypothesis-only classifiers to achieve better-than-random performance (citation). we investigate whether mednli, a physician-annotated dataset with premises extracted from clinical notes, contains such artifacts (citation). we find that entailed hypotheses contain generic versions of specific concepts in the premise, as well as modifiers related to responsiveness, duration, and probability. neutral hypotheses feature conditions and behaviors that co-occur with, or cause, the condition(s) in the premise. contradiction hypotheses feature explicit negation of the premise and implicit negation via assertion of good health. adversarial filtering demonstrates that performance degrades when evaluated on the difficult subset. we provide partition information and recommendations for alternative dataset construction strategies for knowledge-intensive domains."], "nlp applications"], [["pass: perturb-and-select summarizer for product reviews", "nadav oved | ran levy", "the product reviews summarization task aims to automatically produce a short summary for a set of reviews of a given product. such summaries are expected to aggregate a range of different opinions in a concise, coherent and informative manner. this challenging task gives rise to two shortcomings in existing work. first, summarizers tend to favor generic content that appears in reviews for many different products, resulting in template-like, less informative summaries. second, as reviewers often disagree on the pros and cons of a given product, summarizers sometimes yield inconsistent, self-contradicting summaries. we propose the pass system (perturb-and-select summarizer) that employs a large pre-trained transformer-based model (t5 in our case), which follows a few-shot fine-tuning scheme. a key component of the pass system relies on applying systematic perturbations to the model\u2019s input during inference, which allows it to generate multiple different summaries per product. we develop a method for ranking these summaries according to desired criteria, coherence in our case, enabling our system to almost entirely avoid the problem of self-contradiction. we compare our system against strong baselines on publicly available datasets, and show that it produces summaries which are more informative, diverse and coherent."], "summarization"], [["abcd: a graph framework to convert complex sentences to a covering set of simple sentences", "yanjun gao | ting-hao huang | rebecca j. passonneau", "atomic clauses are fundamental text units for understanding complex sentences. identifying the atomic sentences within complex sentences is important for applications such as summarization, argument mining, discourse analysis, discourse parsing, and question answering. previous work mainly relies on rule-based methods dependent on parsing. we propose a new task to decompose each complex sentence into simple sentences derived from the tensed clauses in the source, and a novel problem formulation as a graph edit task. our neural model learns to accept, break, copy or drop elements of a graph that combines word adjacency and grammatical dependencies. the full processing pipeline includes modules for graph construction, graph editing, and sentence generation from the output graph. we introduce desse, a new dataset designed to train and evaluate complex sentence decomposition, and minwiki, a subset of minwikisplit. abcd achieves comparable performance as two parsing baselines on minwiki. on desse, which has a more even balance of complex sentence types, our model achieves higher accuracy on the number of atomic sentences than an encoder-decoder baseline. results include a detailed error analysis."], "discourse and pragmatics"], [["understanding the properties of minimum bayes risk decoding in neural machine translation", "mathias m\u00fcller | rico sennrich", "neural machine translation (nmt) currently exhibits biases such as producing translations that are too short and overgenerating frequent words, and shows poor robustness to copy noise in training data or domain shift. recent work has tied these shortcomings to beam search \u2013 the de facto standard inference algorithm in nmt \u2013 and eikema & aziz (2020) propose to use minimum bayes risk (mbr) decoding on unbiased samples instead. in this paper, we empirically investigate the properties of mbr decoding on a number of previously reported biases and failure cases of beam search. we find that mbr still exhibits a length and token frequency bias, owing to the mt metrics used as utility functions, but that mbr also increases robustness against copy noise in the training data and domain shift."], "machine translation and multilinguality"], [["interpretable and low-resource entity matching via decoupling feature learning from decision making", "zijun yao | chengjiang li | tiansi dong | xin lv | jifan yu | lei hou | juanzi li | yichi zhang | zelin dai", "entity matching (em) aims at recognizing entity records that denote the same real-world object. neural em models learn vector representation of entity descriptions and match entities end-to-end. though robust, these methods require many annotated resources for training, and lack of interpretability. in this paper, we propose a novel em framework that consists of heterogeneous information fusion (hif) and key attribute tree (kat) induction to decouple feature representation from matching decision. using self-supervised learning and mask mechanism in pre-trained language modeling, hif learns the embeddings of noisy attribute values by inter-attribute attention with unlabeled data. using a set of comparison features and a limited amount of annotated data, kat induction learns an efficient decision tree that can be interpreted by generating entity matching rules whose structure is advocated by domain experts. experiments on 6 public datasets and 3 industrial datasets show that our method is highly efficient and outperforms sota em models in most cases. we will release the codes upon acceptance."], "information extraction, retrieval and text mining"], [["simcls: a simple framework for contrastive learning of abstractive summarization", "yixin liu | pengfei liu", "in this paper, we present a conceptually simple while empirically powerful framework for abstractive summarization, simcls, which can bridge the gap between the learning objective and evaluation metrics resulting from the currently dominated sequence-to-sequence learning framework by formulating text generation as a reference-free evaluation problem (i.e., quality estimation) assisted by contrastive learning. experimental results show that, with minor modification over existing top-scoring systems, simcls can improve the performance of existing top-performing models by a large margin. particularly, 2.51 absolute improvement against bart and 2.50 over pegasus w.r.t rouge-1 on the cnn/dailymail dataset, driving the state-of-the-art performance to a new level. we have open-sourced our codes and results: https://github.com/yixinl7/simcls. results of our proposed models have been deployed into explainaboard platform, which allows researchers to understand our systems in a more fine-grained way."], "summarization"], [["tickettalk: toward human-level performance with end-to-end, transaction-based dialog systems", "bill byrne | karthik krishnamoorthi | saravanan ganesh | mihir kale", "we present a data-driven, end-to-end approach to transaction-based dialog systems that performs at near-human levels in terms of verbal response quality and factual grounding accuracy. we show that two essential components of the system produce these results: a sufficiently large and diverse, in-domain labeled dataset, and a neural network-based, pre-trained model that generates both verbal responses and api call predictions. in terms of data, we introduce tickettalk, a movie ticketing dialog dataset with 23,789 annotated conversations. the conversations range from completely open-ended and unrestricted to more structured, both in terms of their knowledge base, discourse features, and number of turns. in qualitative human evaluations, model-generated responses trained on just 10,000 tickettalk dialogs were rated to \u201cmake sense\u201d 86.5% of the time, almost the same as human responses in the same contexts. our simple, api-focused annotation schema results in a much easier labeling task making it faster and more cost effective. it is also the key component for being able to predict api calls accurately. we handle factual grounding by incorporating api calls in the training data, allowing our model to learn which actions to take and when. trained on the same 10,000-dialog set, the model\u2019s api call predictions were rated to be correct 93.9% of the time in our evaluations, surpassing the ratings for the corresponding human labels. we show how api prediction and response generation scores improve as the dataset size incrementally increases from 5000 to 21,000 dialogs. our analysis also clearly illustrates the benefits of pre-training. to facilitate future work on transaction-based dialog systems, we are publicly releasing the tickettalk dataset at https://git.io/jl8an."], "dialogue and interactive systems"], [["contributions of transformer attention heads in multi- and cross-lingual tasks", "weicheng ma | kai zhang | renze lou | lili wang | soroush vosoughi", "this paper studies the relative importance of attention heads in transformer-based models to aid their interpretability in cross-lingual and multi-lingual tasks. prior research has found that only a few attention heads are important in each mono-lingual natural language processing (nlp) task and pruning the remaining heads leads to comparable or improved performance of the model. however, the impact of pruning attention heads is not yet clear in cross-lingual and multi-lingual tasks. through extensive experiments, we show that (1) pruning a number of attention heads in a multi-lingual transformer-based model has, in general, positive effects on its performance in cross-lingual and multi-lingual tasks and (2) the attention heads to be pruned can be ranked using gradients and identified with a few trial experiments. our experiments focus on sequence labeling tasks, with potential applicability on other cross-lingual and multi-lingual tasks. for comprehensiveness, we examine two pre-trained multi-lingual models, namely multi-lingual bert (mbert) and xlm-r, on three tasks across 9 languages each. we also discuss the validity of our findings and their extensibility to truly resource-scarce languages and other task settings."], "machine translation and multilinguality"], [["multimodal sentiment detection based on multi-channel graph neural networks", "xiaocui yang | shi feng | yifei zhang | daling wang", "with the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. we observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. however, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. in this paper, we propose multi-channel graph neural networks with sentiment-awareness (mgnns) for image-text sentiment detection. specifically, we first encode different modalities to capture hidden representations. then, we introduce multi-channel graph neural networks to learn multimodal representations based on the global characteristics of the dataset. finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs. extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection."], "sentiment analysis, stylistic analysis, and argument mining"], [["understanding and countering stereotypes: a computational approach to the stereotype content model", "kathleen c. fraser | isar nejadgholi | svetlana kiritchenko", "stereotypical language expresses widely-held beliefs about different social categories. many stereotypes are overtly negative, while others may appear positive on the surface, but still lead to negative consequences. in this work, we present a computational approach to interpreting stereotypes in text through the stereotype content model (scm), a comprehensive causal theory from social psychology. the scm proposes that stereotypes can be understood along two primary dimensions: warmth and competence. we present a method for defining warmth and competence axes in semantic embedding space, and show that the four quadrants defined by this subspace accurately represent the warmth and competence concepts, according to annotated lexicons. we then apply our computational scm model to textual stereotype data and show that it compares favourably with survey-based studies in the psychological literature. furthermore, we explore various strategies to counter stereotypical beliefs with anti-stereotypes. it is known that countering stereotypes with anti-stereotypical examples is one of the most effective ways to reduce biased thinking, yet the problem of generating anti-stereotypes has not been previously studied. thus, a better understanding of how to generate realistic and effective anti-stereotypes can contribute to addressing pressing societal concerns of stereotyping, prejudice, and discrimination."], "computational social science, social media and cultural analytics"], [["transition-based bubble parsing: improvements on coordination structure prediction", "tianze shi | lillian lee", "we propose a transition-based bubble parser to perform coordination structure identification and dependency-based syntactic analysis simultaneously. bubble representations were proposed in the formal linguistics literature decades ago; they enhance dependency trees by encoding coordination boundaries and internal relationships within coordination structures explicitly. in this paper, we introduce a transition system and neural models for parsing these bubble-enhanced structures. experimental results on the english penn treebank and the english genia corpus show that our parsers beat previous state-of-the-art approaches on the task of coordination structure prediction, especially for the subset of sentences with complex coordination structures."], "tagging, chunking, syntax and parsing"], [["warp: word-level adversarial reprogramming", "karen hambardzumyan | hrant khachatrian | jonathan may", "transfer learning from pretrained language models recently became the dominant approach for solving many nlp tasks. a common approach to transfer learning for multiple tasks that maximize parameter sharing trains one or more task-specific layers on top of the language model. in this paper, we present an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation. adversarial reprogramming attempts to learn task-specific word embeddings that, when concatenated to the input text, instruct the language model to solve the specified task. using up to 25k trainable parameters per task, this approach outperforms all existing methods with up to 25m trainable parameters on the public leaderboard of the glue benchmark. our method, initialized with task-specific human-readable prompts, also works in a few-shot setting, outperforming gpt-3 on two superglue tasks with just 32 training samples."], "machine learning for nlp"], [["timedial: temporal commonsense reasoning in dialog", "lianhui qin | aditya gupta | shyam upadhyay | luheng he | yejin choi | manaal faruqui", "everyday conversations require understanding everyday events, which in turn, requires understanding temporal commonsense concepts interwoven with those events. despite recent progress with massive pre-trained language models (lms) such as t5 and gpt-3, their capability of temporal reasoning in dialogs remains largely under-explored. in this paper, we present the first study to investigate pre-trained lms for their temporal reasoning capabilities in dialogs by introducing a new task and a crowd-sourced english challenge set, timedial. we formulate timedial as a multiple choice cloze task with over 1.1k carefully curated dialogs. empirical results demonstrate that even the best performing models struggle on this task compared to humans, with 23 absolute points of gap in accuracy. furthermore, our analysis reveals that the models fail to reason about dialog context correctly; instead, they rely on shallow cues based on existing temporal patterns in context, motivating future research for modeling temporal concepts in text and robust contextual reasoning about them. the dataset is publicly available at https://github.com/google-research-datasets/timedial."], "resources and evaluation"], [["refining sample embeddings with relation prototypes to enhance continual relation extraction", "li cui | deqing yang | jiaxin yu | chengwei hu | jiayang cheng | jingjie yi | yanghua xiao", "continual learning has gained increasing attention in recent years, thanks to its biological interpretation and efficiency in many real-world applications. as a typical task of continual learning, continual relation extraction (cre) aims to extract relations between entities from texts, where the samples of different relations are delivered into the model continuously. some previous works have proved that storing typical samples of old relations in memory can help the model keep a stable understanding of old relations and avoid forgetting them. however, most methods heavily depend on the memory size in that they simply replay these memorized samples in subsequent tasks. to fully utilize memorized samples, in this paper, we employ relation prototype to extract useful information of each relation. specifically, the prototype embedding for a specific relation is computed based on memorized samples of this relation, which is collected by k-means algorithm. the prototypes of all observed relations at current learning stage are used to re-initialize a memory network to refine subsequent sample embeddings, which ensures the model\u2019s stable understanding on all observed relations when learning a new task. compared with previous cre models, our model utilizes the memory information sufficiently and efficiently, resulting in enhanced cre performance. our experiments show that the proposed model outperforms the state-of-the-art cre models and has great advantage in avoiding catastrophic forgetting. the code and datasets are released on https://github.com/fd2014cl/rp-cre."], "information extraction, retrieval and text mining"], [["engage the public: poll question generation for social media posts", "zexin lu | keyang ding | yuji zhang | jing li | baolin peng | lemao liu", "this paper presents a novel task to generate poll questions for social media posts. it offers an easy way to hear the voice from the public and learn from their feelings to important social topics. while most related work tackles formal languages (e.g., exam papers), we generate poll questions for short and colloquial social media messages exhibiting severe data sparsity. to deal with that, we propose to encode user comments and discover latent topics therein as contexts. they are then incorporated into a sequence-to-sequence (s2s) architecture for question generation and its extension with dual decoders to additionally yield poll choices (answers). for experiments, we collect a large-scale chinese dataset from sina weibo containing over 20k polls. the results show that our model outperforms the popular s2s models without exploiting topics from comments and the dual decoder design can further benefit the prediction of both questions and answers. human evaluations further exhibit our superiority in yielding high-quality polls helpful to draw user engagements."], "computational social science, social media and cultural analytics"], [["dyploc: dynamic planning of content using mixed language models for text generation", "xinyu hua | ashwin sreevatsa | lu wang", "we study the task of long-form opinion text generation, which faces at least two distinct challenges. first, existing neural generation models fall short of coherence, thus requiring efficient content planning. second, diverse types of information are needed to guide the generator to cover both subjective and objective content. to this end, we propose dyploc, a generation framework that conducts dynamic planning of content while generating the output based on a novel design of mixed language models. to enrich the generation with diverse content, we further propose to use large pre-trained models to predict relevant concepts and to generate claims. we experiment with two challenging tasks on newly collected datasets: (1) argument generation with reddit changemyview, and (2) writing articles using new york times\u2019 opinion section. automatic evaluation shows that our model significantly outperforms competitive comparisons. human judges further confirm that our generations are more coherent with richer content."], "generation"], [["quotation recommendation and interpretation based on transformation from queries to quotations", "lingzhi wang | xingshan zeng | kam-fai wong", "to help individuals express themselves better, quotation recommendation is receiving growing attention. nevertheless, most prior efforts focus on modeling quotations and queries separately and ignore the relationship between the quotations and the queries. in this work, we introduce a transformation matrix that directly maps the query representations to quotation representations. to better learn the mapping relationship, we employ a mapping loss that minimizes the distance of two semantic spaces (one for quotation and another for mapped-query). furthermore, we explore using the words in history queries to interpret the figurative language of quotations, where quotation-aware attention is applied on top of history queries to highlight the indicator words. experiments on two datasets in english and chinese show that our model outperforms previous state-of-the-art models."], "nlp applications"], [["controllable open-ended question generation with a new question type ontology", "shuyang cao | lu wang", "we investigate the less-explored task of generating open-ended questions that are typically answered by multiple sentences. we first define a new question type ontology which differentiates the nuanced nature of questions better than widely used question words. a new dataset with 4,959 questions is labeled based on the new ontology. we then propose a novel question type-aware question generation framework, augmented by a semantic graph representation, to jointly predict question focuses and produce the question. based on this framework, we further use both exemplars and automatically generated templates to improve controllability and diversity. experiments on two newly collected large-scale datasets show that our model improves question quality over competitive comparisons based on automatic metrics. human judges also rate our model outputs highly in answerability, coverage of scope, and overall quality. finally, our model variants with templates can produce questions with enhanced controllability and diversity."], "generation"], [["personalized transformer for explainable recommendation", "lei li | yongfeng zhang | li chen", "personalization of natural language generation plays a vital role in a large spectrum of tasks, such as explainable recommendation, review summarization and dialog systems. in these tasks, user and item ids are important identifiers for personalization. transformer, which is demonstrated with strong language modeling capability, however, is not personalized and fails to make use of the user and item ids since the id tokens are not even in the same semantic space as the words. to address this problem, we present a personalized transformer for explainable recommendation (peter), on which we design a simple and effective learning objective that utilizes the ids to predict the words in the target explanation, so as to endow the ids with linguistic meanings and to achieve personalized transformer. besides generating explanations, peter can also make recommendations, which makes it a unified model for the whole recommendation-explanation pipeline. extensive experiments show that our small unpretrained model outperforms fine-tuned bert on the generation task, in terms of both effectiveness and efficiency, which highlights the importance and the nice utility of our design."], "nlp applications"], [["generation-augmented retrieval for open-domain question answering", "yuning mao | pengcheng he | xiaodong liu | yelong shen | jianfeng gao | jiawei han | weizhu chen", "we propose generation-augmented retrieval (gar) for answering open-domain questions, which augments a query through text generation of heuristically discovered relevant contexts without external resources as supervision. we demonstrate that the generated contexts substantially enrich the semantics of the queries and gar with sparse representations (bm25) achieves comparable or better performance than state-of-the-art dense retrieval methods such as dpr. we show that generating diverse contexts for a query is beneficial as fusing their results consistently yields better retrieval accuracy. moreover, as sparse and dense representations are often complementary, gar can be easily combined with dpr to achieve even better performance. gar achieves state-of-the-art performance on natural questions and triviaqa datasets under the extractive qa setup when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used."], "question answering"], [["metaphor generation with conceptual mappings", "kevin stowe | tuhin chakrabarty | nanyun peng | smaranda muresan | iryna gurevych", "generating metaphors is a difficult task as it requires understanding nuanced relationships between abstract concepts. in this paper, we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs. guided by conceptual metaphor theory, we propose to control the generation process by encoding conceptual mappings between cognitive domains to generate meaningful metaphoric expressions. to achieve this, we develop two methods: 1) using framenet-based embeddings to learn mappings between domains and applying them at the lexical level (cm-lex), and 2) deriving source/target pairs to train a controlled seq-to-seq generation model (cm-bart). we assess our methods through automatic and human evaluation for basic metaphoricity and conceptual metaphor presence. we show that the unsupervised cm-lex model is competitive with recent deep learning metaphor generation systems, and cm-bart outperforms all other models both in automatic and human evaluations."], "generation"], [["w-rst: towards a weighted rst-style discourse framework", "patrick huber | wen xiao | giuseppe carenini", "aiming for a better integration of data-driven and linguistically-inspired approaches, we explore whether rst nuclearity, assigning a binary assessment of importance between text segments, can be replaced by automatically generated, real-valued scores, in what we call a weighted-rst framework. in particular, we find that weighted discourse trees from auxiliary tasks can benefit key nlp downstream applications, compared to nuclearity-centered approaches. we further show that real-valued importance distributions partially and interestingly align with the assessment and uncertainty of human annotators."], "discourse and pragmatics"], [["on training instance selection for few-shot neural text generation", "ernie chang | xiaoyu shen | hui-syuan yeh | vera demberg", "large-scale pretrained language models have led to dramatic improvements in text generation. impressive performance can be achieved by finetuning only on a small number of instances (few-shot setting). nonetheless, almost all previous work simply applies random sampling to select the few-shot training instances. little to no attention has been paid to the selection strategies and how they would affect model performance. in this work, we present a study on training instance selection in few-shot neural text generation. the selection decision is made based only on the unlabeled data so as to identify the most worthwhile data points that should be annotated under some budget of labeling cost. based on the intuition that the few-shot training instances should be diverse and representative of the entire data distribution, we propose a simple selection strategy with k-means clustering. we show that even with the naive clustering-based approach, the generation models consistently outperform random sampling on three text generation tasks: data-to-text generation, document summarization and question generation. the code and training data are made available. we hope that this work will call for more attention on this largely unexplored area."], "generation"], [["a mutual information maximization approach for the spurious solution problem in weakly supervised question answering", "zhihong shao | lifeng shang | qun liu | minlie huang", "weakly supervised question answering usually has only the final answers as supervision signals while the correct solutions to derive the answers are not provided. this setting gives rise to the spurious solution problem: there may exist many spurious solutions that coincidentally derive the correct answer, but training on such solutions can hurt model performance (e.g., producing wrong solutions or answers). for example, for discrete reasoning tasks as on drop, there may exist many equations to derive a numeric answer, and typically only one of them is correct. previous learning methods mostly filter out spurious solutions with heuristics or using model confidence, but do not explicitly exploit the semantic correlations between a question and its solution. in this paper, to alleviate the spurious solution problem, we propose to explicitly exploit such semantic correlations by maximizing the mutual information between question-answer pairs and predicted solutions. extensive experiments on four question answering datasets show that our method significantly outperforms previous learning methods in terms of task performance and is more effective in training models to produce correct solutions."], "question answering"], [["locate and label: a two-stage identifier for nested named entity recognition", "yongliang shen | xinyin ma | zeqi tan | shuai zhang | wen wang | weiming lu", "named entity recognition (ner) is a well-studied task in natural language processing. traditional ner research only deals with flat entities and ignores nested entities. the span-based methods treat entity recognition as a span classification task. although these methods have the innate ability to handle nested ner, they suffer from high computational cost, ignorance of boundary information, under-utilization of the spans that partially match with entities, and difficulties in long entity recognition. to tackle these issues, we propose a two-stage entity identifier. first we generate span proposals by filtering and boundary regression on the seed spans to locate the entities, and then label the boundary-adjusted span proposals with the corresponding categories. our method effectively utilizes the boundary information of entities and partially matched spans during training. through boundary regression, entities of any length can be covered theoretically, which improves the ability to recognize long entities. in addition, many low-quality seed spans are filtered out in the first stage, which reduces the time complexity of inference. experiments on nested ner datasets demonstrate that our proposed method outperforms previous state-of-the-art models."], "information extraction, retrieval and text mining"], [["saroco: detecting satire in a novel romanian corpus of news articles", "ana-cristina rogoz | gaman mihaela | radu tudor ionescu", "in this work, we introduce a corpus for satire detection in romanian news. we gathered 55,608 public news articles from multiple real and satirical news sources, composing one of the largest corpora for satire detection regardless of language and the only one for the romanian language. we provide an official split of the text samples, such that training news articles belong to different sources than test news articles, thus ensuring that models do not achieve high performance simply due to overfitting. we conduct experiments with two state-of-the-art deep neural models, resulting in a set of strong baselines for our novel corpus. our results show that the machine-level accuracy for satire detection in romanian is quite low (under 73% on the test set) compared to the human-level accuracy (87%), leaving enough room for improvement in future research."], "resources and evaluation"], [["parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks", "rabeeh karimi mahabadi | sebastian ruder | mostafa dehghani | james henderson", "state-of-the-art parameter-efficient fine-tuning methods rely on introducing adapter modules between the layers of a pretrained language model. however, such modules are trained separately for each task and thus do not enable sharing information across tasks. in this paper, we show that we can learn adapter parameters for all layers and tasks by generating them using shared hypernetworks, which condition on task, adapter position, and layer id in a transformer model. this parameter-efficient multi-task learning framework allows us to achieve the best of both worlds by sharing knowledge across tasks via hypernetworks while enabling the model to adapt to each individual task through task-specific adapters. experiments on the well-known glue benchmark show improved performance in multi-task learning while adding only 0.29% parameters per task. we additionally demonstrate substantial performance improvements in few-shot domain generalization across a variety of tasks. our code is publicly available in https://github.com/rabeehk/hyperformer."], "machine learning for nlp"], [["in factuality: efficient integration of relevant facts for visual question answering", "peter vickers | nikolaos aletras | emilio monti | lo\u00efc barrault", "visual question answering (vqa) methods aim at leveraging visual input to answer questions that may require complex reasoning over entities. current models are trained on labelled data that may be insufficient to learn complex knowledge representations. in this paper, we propose a new method to enhance the reasoning capabilities of a multi-modal pretrained model (vision+language bert) by integrating facts extracted from an external knowledge base. evaluation on the kvqa dataset benchmark demonstrates that our method outperforms competitive baselines by 19%, achieving new state-of-the-art results. we also perform an extensive analysis highlighting the limitations of our best performing model through an ablation study."], "question answering"], [["better than average: paired evaluation of nlp systems", "maxime peyrard | wei zhao | steffen eger | robert west", "evaluation in nlp is usually done by comparing the scores of competing systems independently averaged over a common set of test instances. in this work, we question the use of averages for aggregating evaluation scores into a final number used to decide which system is best, since the average, as well as alternatives such as the median, ignores the pairing arising from the fact that systems are evaluated on the same test instances. we illustrate the importance of taking the instancelevel pairing of evaluation scores into account and demonstrate, both theoretically and empirically, the advantages of aggregation methods based on pairwise comparisons, such as the bradley\u2013terry (bt) model, a mechanism based on the estimated probability that a given system scores better than another on the test set. by re-evaluating 296 real nlp evaluation setups across four tasks and 18 evaluation metrics, we show that the choice of aggregation mechanism matters and yields different conclusions as to which systems are state of the art in about 30% of the setups. to facilitate the adoption of pairwise evaluation, we release a practical tool for performing the full analysis of evaluation scores with the mean, median, bt, and two variants of bt (elo and trueskill), alongside functionality for appropriate statistical testing."], "resources and evaluation"], [["deeprapper: neural rap generation with rhyme and rhythm modeling", "lanqing xue | kaitao song | duocai wu | xu tan | nevin l. zhang | tao qin | wei-qiang zhang | tie-yan liu", "rap generation, which aims to produce lyrics and corresponding singing beats, needs to model both rhymes and rhythms. previous works for rap generation focused on rhyming lyrics, but ignored rhythmic beats, which are important for rap performance. in this paper, we develop deeprapper, a transformer-based rap generation system that can model both rhymes and rhythms. since there is no available rap datasets with rhythmic beats, we develop a data mining pipeline to collect a large-scale rap dataset, which includes a large number of rap songs with aligned lyrics and rhythmic beats. second, we design a transformer-based autoregressive language model which carefully models rhymes and rhythms. specifically, we generate lyrics in the reverse order with rhyme representation and constraint for rhyme enhancement, and insert a beat symbol into lyrics for rhythm/beat modeling. to our knowledge, deeprapper is the first system to generate rap with both rhymes and rhythms. both objective and subjective evaluations demonstrate that deeprapper generates creative and high-quality raps with rhymes and rhythms."], "generation"], [["earlybert: efficient bert training via early-bird lottery tickets", "xiaohan chen | yu cheng | shuohang wang | zhe gan | zhangyang wang | jingjing liu", "heavily overparameterized language models such as bert, xlnet and t5 have achieved impressive success in many nlp tasks. however, their high model complexity requires enormous computation resources and extremely long training time for both pre-training and fine-tuning. many works have studied model compression on large nlp models, but only focusing on reducing inference time while still requiring an expensive training process. other works use extremely large batch sizes to shorten the pre-training time, at the expense of higher computational resource demands. in this paper, inspired by the early-bird lottery tickets recently studied for computer vision tasks, we propose earlybert, a general computationally-efficient training algorithm applicable to both pre-training and fine-tuning of large-scale language models. by slimming the self-attention and fully-connected sub-layers inside a transformer, we are the first to identify structured winning tickets in the early stage of bert training. we apply those tickets towards efficient bert training, and conduct comprehensive pre-training and fine-tuning experiments on glue and squad downstream tasks. our results show that earlybert achieves comparable performance to standard bert, with 35 45% less training time. code is available at https://github.com/vita-group/earlybert."], "machine learning for nlp"], [["increasing faithfulness in knowledge-grounded dialogue with controllable features", "hannah rashkin | david reitter | gaurav singh tomar | dipanjan das", "knowledge-grounded dialogue systems are intended to convey information that is based on evidence provided in a given source text. we discuss the challenges of training a generative neural dialogue model for such systems that is controlled to stay faithful to the evidence. existing datasets contain a mix of conversational responses that are faithful to selected evidence as well as more subjective or chit-chat style responses. we propose different evaluation measures to disentangle these different styles of responses by quantifying the informativeness and objectivity. at training time, additional inputs based on these evaluation measures are given to the dialogue model. at generation time, these additional inputs act as stylistic controls that encourage the model to generate responses that are faithful to the provided evidence. we also investigate the usage of additional controls at decoding time using resampling techniques. in addition to automatic metrics, we perform a human evaluation study where raters judge the output of these controlled generation models to be generally more objective and faithful to the evidence compared to baseline dialogue systems."], "dialogue and interactive systems"], [["weight distillation: transferring the knowledge in neural network parameters", "ye lin | yanyang li | ziyang wang | bei li | quan du | tong xiao | jingbo zhu", "knowledge distillation has been proven to be effective in model acceleration and compression. it transfers knowledge from a large neural network to a small one by using the large neural network predictions as targets of the small neural network. but this way ignores the knowledge inside the large neural networks, e.g., parameters. our preliminary study as well as the recent success in pre-training suggests that transferring parameters are more effective in distilling knowledge. in this paper, we propose weight distillation to transfer the knowledge in parameters of a large neural network to a small neural network through a parameter generator. on the wmt16 en-ro, nist12 zh-en, and wmt14 en-de machine translation tasks, our experiments show that weight distillation learns a small network that is 1.88 2.94x faster than the large network but with competitive bleu performance. when fixing the size of small networks, weight distillation outperforms knowledge distillation by 0.51 1.82 bleu points."], "machine learning for nlp"], [["towards robustness of text-to-sql models against synonym substitution", "yujian gan | xinyun chen | qiuping huang | matthew purver | john r. woodward | jinxia xie | pengsheng huang", "recently, there has been significant progress in studying neural networks to translate text descriptions into sql queries. despite achieving good performance on some public benchmarks, existing text-to-sql models typically rely on the lexical matching between words in natural language (nl) questions and tokens in table schemas, which may render the models vulnerable to attacks that break the schema linking mechanism. in this work, we investigate the robustness of text-to-sql models to synonym substitution. in particular, we introduce spider-syn, a human-curated dataset based on the spider benchmark for text-to-sql translation. nl questions in spider-syn are modified from spider, by replacing their schema-related words with manually selected synonyms that reflect real-world question paraphrases. we observe that the accuracy dramatically drops by eliminating such explicit correspondence between nl questions and table schemas, even if the synonyms are not adversarially selected to conduct worst-case attacks. finally, we present two categories of approaches to improve the model robustness. the first category of approaches utilizes additional synonym annotations for table schemas by modifying the model input, while the second category is based on adversarial training. we demonstrate that both categories of approaches significantly outperform their counterparts without the defense, and the first category of approaches are more effective."], "semantics"], [["enslm: ensemble language model for data diversity by semantic clustering", "zhibin duan | hao zhang | chaojie wang | zhengjue wang | bo chen | mingyuan zhou", "natural language processing (nlp) often faces the problem of data diversity such as different domains, themes, styles, and so on. therefore, a single language model (lm) is insufficient to learn all knowledge from diverse samples. to solve this problem, we firstly propose an autoencoding topic model with a mixture prior (matm) to perform clustering for the data, where the clusters defined in semantic space describes the data diversity. having obtained the clustering assignment for each sample, we develop the ensemble lm (enslm) with the technique of weight modulation. specifically, enslm contains a backbone that is adjusted by a few modulated weights to fit for different sample clusters. as a result, the backbone learns the shared knowledge among all clusters while modulated weights extract the cluster-specific features. enslm can be trained jointly with matm with a flexible lm backbone. we evaluate the effectiveness of both matm and enslm on various tasks."], "machine learning for nlp"], [["do context-aware translation models pay the right attention?", "kayo yin | patrick fernandes | danish pruthi | aditi chaudhary | andr\u00e9 f. t. martins | graham neubig", "context-aware machine translation models are designed to leverage contextual information, but often fail to do so. as a result, they inaccurately disambiguate pronouns and polysemous words that require context for resolution. in this paper, we ask several questions: what contexts do human translators use to resolve ambiguous words? are models paying large amounts of attention to the same context? what if we explicitly train them to do so? to answer these questions, we introduce scat (supporting context for ambiguous translations), a new english-french dataset comprising supporting context words for 14k translations that professional translators found useful for pronoun disambiguation. using scat, we perform an in-depth analysis of the context used to disambiguate, examining positional and lexical characteristics of the supporting words. furthermore, we measure the degree of alignment between the model\u2019s attention scores and the supporting context from scat, and apply a guided attention strategy to encourage agreement between the two."], "machine translation and multilinguality"], [["cdrnn: discovering complex dynamics in human language processing", "cory shain", "the human mind is a dynamical system, yet many analysis techniques used to study it are limited in their ability to capture the complex dynamics that may characterize mental processes. this study proposes the continuous-time deconvolutional regressive neural network (cdrnn), a deep neural extension of continuous-time deconvolutional regression (shain & schuler, 2021) that jointly captures time-varying, non-linear, and delayed influences of predictors (e.g. word surprisal) on the response (e.g. reading time). despite this flexibility, cdrnn is interpretable and able to illuminate patterns in human cognition that are otherwise difficult to study. behavioral and fmri experiments reveal detailed and plausible estimates of human language processing dynamics that generalize better than cdr and other baselines, supporting a potential role for cdrnn in studying human language processing."], "linguistic theories, cognitive modeling and psycholinguistics"], [["evaluating morphological typology in zero-shot cross-lingual transfer", "antonio mart\u00ednez-garc\u00eda | toni badia | jeremy barnes", "cross-lingual transfer has improved greatly through multi-lingual language model pretraining, reducing the need for parallel data and increasing absolute performance. however, this progress has also brought to light the differences in performance across languages. specifically, certain language families and typologies seem to consistently perform worse in these models. in this paper, we address what effects morphological typology has on zero-shot cross-lingual transfer for two tasks: part-of-speech tagging and sentiment analysis. we perform experiments on 19 languages from four language typologies (fusional, isolating, agglutinative, and introflexive) and find that transfer to another morphological type generally implies a higher loss than transfer to another language with the same morphological typology. furthermore, pos tagging is more sensitive to morphological typology than sentiment analysis and, on this task, models perform much better on fusional languages than on the other typologies."], "machine translation and multilinguality"], [["learning language specific sub-network for multilingual machine translation", "zehui lin | liwei wu | mingxuan wang | lei li", "multilingual neural machine translation aims at learning a single translation model for multiple languages. these jointly trained models often suffer from performance degradationon rich-resource language pairs. we attribute this degeneration to parameter interference. in this paper, we propose lass to jointly train a single unified multilingual mt model. lass learns language specific sub-network (lass) for each language pair to counter parameter interference. comprehensive experiments on iwslt and wmt datasets with various transformer architectures show that lass obtains gains on 36 language pairs by up to 1.2 bleu. besides, lass shows its strong generalization performance at easy adaptation to new language pairs and zero-shot translation. lass boosts zero-shot translation with an average of 8.3 bleu on 30 language pairs. codes and trained models are available at https://github.com/nlp-playground/lass."], "machine translation and multilinguality"], [["joint verification and reranking for open fact checking over tables", "michael sejr schlichtkrull | vladimir karpukhin | barlas oguz | mike lewis | wen-tau yih | sebastian riedel", "structured information is an important knowledge source for automatic verification of factual claims. nevertheless, the majority of existing research into this task has focused on textual data, and the few recent inquiries into structured data have been for the closed-domain setting where appropriate evidence for each claim is assumed to have already been retrieved. in this paper, we investigate verification over structured data in the open-domain setting, introducing a joint reranking-and-verification model which fuses evidence documents in the verification component. our open-domain model achieves performance comparable to the closed-domain state-of-the-art on the tabfact dataset, and demonstrates performance gains from the inclusion of multiple tables as well as a significant improvement over a heuristic retrieval baseline."], "nlp applications"], [["from machine translation to code-switching: generating high-quality code-switched text", "ishan tarunesh | syamantak kumar | preethi jyothi", "generating code-switched text is a problem of growing interest, especially given the scarcity of corpora containing large volumes of real code-switched text. in this work, we adapt a state-of-the-art neural machine translation model to generate hindi-english code-switched sentences starting from monolingual hindi sentences. we outline a carefully designed curriculum of pretraining steps, including the use of synthetic code-switched text, that enable the model to generate high-quality code-switched text. using text generated from our model as data augmentation, we show significant reductions in perplexity on a language modeling task, compared to using text from other generative models of cs text. we also show improvements using our text for a downstream code-switched natural language inference task. our generated text is further subjected to a rigorous evaluation using a human evaluation study and a range of objective metrics, where we show performance comparable (and sometimes even superior) to code-switched text obtained via crowd workers who are native hindi speakers."], "machine translation and multilinguality"], [["g-transformer for document-level machine translation", "guangsheng bao | yue zhang | zhiyang teng | boxing chen | weihua luo", "document-level mt models are still far from satisfactory. existing work extend translation unit from single sentence to multiple sentences. however, study shows that when we further enlarge the translation unit to a whole document, supervised training of transformer can fail. in this paper, we find such failure is not caused by overfitting, but by sticking around local minima during training. our analysis shows that the increased complexity of target-to-source attention is a reason for the failure. as a solution, we propose g-transformer, introducing locality assumption as an inductive bias into transformer, reducing the hypothesis space of the attention from target to source. experiments show that g-transformer converges faster and more stably than transformer, achieving new state-of-the-art bleu scores for both nonpretraining and pre-training settings on three benchmark datasets."], "machine translation and multilinguality"], [["learnda: learnable knowledge-guided data augmentation for event causality identification", "xinyu zuo | pengfei cao | yubo chen | kang liu | jun zhao | weihua peng | yuguang chen", "modern models for event causality identification (eci) are mainly based on supervised learning, which are prone to the data lacking problem. unfortunately, the existing nlp-related augmentation methods cannot directly produce available data required for this task. to solve the data lacking problem, we introduce a new approach to augment training data for event causality identification, by iteratively generating new examples and classifying event causality in a dual learning framework. on the one hand, our approach is knowledge guided, which can leverage existing knowledge bases to generate well-formed new sentences. on the other hand, our approach employs a dual mechanism, which is a learnable augmentation framework, and can interactively adjust the generation process to generate task-related sentences. experimental results on two benchmarks eventstoryline and causal-timebank show that 1) our method can augment suitable task-related training data for eci; 2) our method outperforms previous methods on eventstoryline and causal-timebank (+2.5 and +2.1 points on f1 value respectively)."], "information extraction, retrieval and text mining"], [["a joint model for dropped pronoun recovery and conversational discourse parsing in chinese conversational speech", "jingxuan yang | kerui xu | jun xu | si li | sheng gao | jun guo | nianwen xue | ji-rong wen", "in this paper, we present a neural model for joint dropped pronoun recovery (dpr) and conversational discourse parsing (cdp) in chinese conversational speech. we show that dpr and cdp are closely related, and a joint model benefits both tasks. we refer to our model as discproreco, and it first encodes the tokens in each utterance in a conversation with a directed graph convolutional network (gcn). the token states for an utterance are then aggregated to produce a single state for each utterance. the utterance states are then fed into a biaffine classifier to construct a conversational discourse graph. a second (multi-relational) gcn is then applied to the utterance states to produce a discourse relation-augmented representation for the utterances, which are then fused together with token states in each utterance as input to a dropped pronoun recovery layer. the joint model is trained and evaluated on a new structure parsing-enhanced dropped pronoun recovery (spdpr) data set that we annotated with both two types of information. experimental results on the spdpr dataset and other benchmarks show that discproreco significantly outperforms the state-of-the-art baselines of both tasks."], "dialogue and interactive systems"], [["verb knowledge injection for multilingual event processing", "olga majewska | ivan vuli\u0107 | goran glava\u0161 | edoardo maria ponti | anna korhonen", "linguistic probing of pretrained transformer-based language models (lms) revealed that they encode a range of syntactic and semantic properties of a language. however, they are still prone to fall back on superficial cues and simple heuristics to solve downstream tasks, rather than leverage deeper linguistic information. in this paper, we target a specific facet of linguistic knowledge, the interplay between verb meaning and argument structure. we investigate whether injecting explicit information on verbs\u2019 semantic-syntactic behaviour improves the performance of pretrained lms in event extraction tasks, where accurate verb processing is paramount. concretely, we impart the verb knowledge from curated lexical resources into dedicated adapter modules (verb adapters), allowing it to complement, in downstream tasks, the language knowledge obtained during lm-pretraining. we first demonstrate that injecting verb knowledge leads to performance gains in english event extraction. we then explore the utility of verb adapters for event extraction in other languages: we investigate 1) zero-shot language transfer with multilingual transformers and 2) transfer via (noisy automatic) translation of english verb-based lexical knowledge. our results show that the benefits of verb knowledge injection indeed extend to other languages, even when relying on noisily translated lexical knowledge."], "semantics"], [["value-agnostic conversational semantic parsing", "emmanouil antonios platanios | adam pauls | subhro roy | yuchen zhang | alexander kyte | alan guo | sam thomson | jayant krishnamurthy | jason wolfe | jacob andreas | dan klein", "conversational semantic parsers map user utterances to executable programs given dialogue histories composed of previous utterances, programs, and system responses. existing parsers typically condition on rich representations of history that include the complete set of values and computations previously discussed. we propose a model that abstracts over values to focus prediction on type- and function-level context. this approach provides a compact encoding of dialogue histories and predicted programs, improving generalization and computational efficiency. our model incorporates several other components, including an atomic span copy operation and structural enforcement of well-formedness constraints on predicted programs, that are particularly advantageous in the low-data regime. trained on the smcalflow and treedst datasets, our model outperforms prior work by 7.3% and 10.6% respectively in terms of absolute accuracy. trained on only a thousand examples from each dataset, it outperforms strong baselines by 12.4% and 6.4%. these results indicate that simple representations are key to effective generalization in conversational semantic parsing."], "dialogue and interactive systems"], [["excar: event graph knowledge enhanced explainable causal reasoning", "li du | xiao ding | kai xiong | ting liu | bing qin", "prior work infers the causation between events mainly based on the knowledge induced from the annotated causal event pairs. however, additional evidence information intermediate to the cause and effect remains unexploited. by incorporating such information, the logical law behind the causality can be unveiled, and the interpretability and stability of the causal reasoning system can be improved. to facilitate this, we present an event graph knowledge enhanced explainable causal reasoning framework (excar). excar first acquires additional evidence information from a large-scale causal event graph as logical rules for causal reasoning. to learn the conditional probabilistic of logical rules, we propose the conditional markov neural logic network (cmnln) that combines the representation learning and structure learning of logical rules in an end-to-end differentiable manner. experimental results demonstrate that excar outperforms previous state-of-the-art methods. adversarial evaluation shows the improved stability of excar over baseline systems. human evaluation shows that excar can achieve a promising explainable performance."], "semantics"], [["super tickets in pre-trained language models: from model compression to improving generalization", "chen liang | simiao zuo | minshuo chen | haoming jiang | xiaodong liu | pengcheng he | tuo zhao | weizhu chen", "the lottery ticket hypothesis suggests that an over-parametrized network consists of \u201dlottery tickets\u201d, and training a certain collection of them (i.e., a subnetwork) can match the performance of the full model. in this paper, we study such a collection of tickets, which is referred to as \u201dwinning tickets\u201d, in extremely over-parametrized models, e.g., pre-trained language models. we observe that at certain compression ratios, the generalization performance of the winning tickets can not only match but also exceed that of the full model. in particular, we observe a phase transition phenomenon: as the compression ratio increases, generalization performance of the winning tickets first improves then deteriorates after a certain threshold. we refer to the tickets on the threshold as \u201dsuper tickets\u201d. we further show that the phase transition is task and model dependent \u2014 as the model size becomes larger and the training data set becomes smaller, the transition becomes more pronounced. our experiments on the glue benchmark show that the super tickets improve single task fine-tuning by 0.9 points on bert-base and 1.0 points on bert-large, in terms of task-average score. we also demonstrate that adaptively sharing the super tickets across tasks benefits multi-task learning."], "machine learning for nlp"], [["changing the world by changing the data", "anna rogers", "nlp community is currently investing a lot more research and resources into development of deep learning models than training data. while we have made a lot of progress, it is now clear that our models learn all kinds of spurious patterns, social biases, and annotation artifacts. algorithmic solutions have so far had limited success. an alternative that is being actively discussed is more careful design of datasets so as to deliver specific signals. this position paper maps out the arguments for and against data curation, and argues that fundamentally the point is moot: curation already is and will be happening, and it is changing the world. the question is only how much thought we want to invest into that process."], "nlp applications"], [["a span-based model for joint overlapped and discontinuous named entity recognition", "fei li | zhichao lin | meishan zhang | donghong ji", "research on overlapped and discontinuous named entity recognition (ner) has received increasing attention. the majority of previous work focuses on either overlapped or discontinuous entities. in this paper, we propose a novel span-based model that can recognize both overlapped and discontinuous entities jointly. the model includes two major steps. first, entity fragments are recognized by traversing over all possible text spans, thus, overlapped entities can be recognized. second, we perform relation classification to judge whether a given pair of entity fragments to be overlapping or succession. in this way, we can recognize not only discontinuous entities, and meanwhile doubly check the overlapped entities. as a whole, our model can be regarded as a relation extraction paradigm essentially. experimental results on multiple benchmark datasets (i.e., clef, genia and ace05) show that our model is highly competitive for overlapped and discontinuous ner."], "information extraction, retrieval and text mining"], [["lexical semantic change discovery", "sinan kurtyigit | maike park | dominik schlechtweg | jonas kuhn | sabine schulte im walde", "while there is a large amount of research in the field of lexical semantic change detection, only few approaches go beyond a standard benchmark evaluation of existing models. in this paper, we propose a shift of focus from change detection to change discovery, i.e., discovering novel word senses over time from the full corpus vocabulary. by heavily fine-tuning a type-based and a token-based approach on recently published german data, we demonstrate that both models can successfully be applied to discover new words undergoing meaning change. furthermore, we provide an almost fully automated framework for both evaluation and discovery."], "semantics"], [["mention flags (mf): constraining transformer-based text generators", "yufei wang | ian wood | stephen wan | mark dras | mark johnson", "this paper focuses on seq2seq (s2s) constrained text generation where the text generator is constrained to mention specific words which are inputs to the encoder in the generated outputs. pre-trained s2s models or a copy mechanism are trained to copy the surface tokens from encoders to decoders, but they cannot guarantee constraint satisfaction. constrained decoding algorithms always produce hypotheses satisfying all constraints. however, they are computationally expensive and can lower the generated text quality. in this paper, we propose mention flags (mf), which traces whether lexical constraints are satisfied in the generated outputs in an s2s decoder. the mf models can be trained to generate tokens in a hypothesis until all constraints are satisfied, guaranteeing high constraint satisfaction. our experiments on the common sense generation task (commongen) (lin et al., 2020), end2end restaurant dialog task (e2enlg) (du\u02c7sek et al., 2020) and novel object captioning task (nocaps) (agrawal et al., 2019) show that the mf models maintain higher constraint satisfaction and text quality than the baseline models and other constrained decoding algorithms, achieving state-of-the-art performance on all three tasks. these results are achieved with a much lower run-time than constrained decoding algorithms. we also show that the mf models work well in the low-resource setting."], "generation"], [["cogalign: learning to align textual neural representations to cognitive language processing signals", "yuqi ren | deyi xiong", "most previous studies integrate cognitive language processing signals (e.g., eye-tracking or eeg data) into neural models of natural language processing (nlp) just by directly concatenating word embeddings with cognitive features, ignoring the gap between the two modalities (i.e., textual vs. cognitive) and noise in cognitive features. in this paper, we propose a cogalign approach to these issues, which learns to align textual neural representations to cognitive features. in cogalign, we use a shared encoder equipped with a modality discriminator to alternatively encode textual and cognitive inputs to capture their differences and commonalities. additionally, a text-aware attention mechanism is proposed to detect task-related information and to avoid using noise in cognitive features. experimental results on three nlp tasks, namely named entity recognition, sentiment analysis and relation extraction, show that cogalign achieves significant improvements with multiple cognitive features over state-of-the-art models on public datasets. moreover, our model is able to transfer cognitive information to other datasets that do not have any cognitive processing signals."], "linguistic theories, cognitive modeling and psycholinguistics"], [["prevent the language model from being overconfident in neural machine translation", "mengqi miao | fandong meng | yijin liu | xiao-hua zhou | jie zhou", "the neural machine translation (nmt) model is essentially a joint language model conditioned on both the source sentence and partial translation. therefore, the nmt model naturally involves the mechanism of the language model (lm) that predicts the next token only based on partial translation. despite its success, nmt still suffers from the hallucination problem, generating fluent but inadequate translations. the main reason is that nmt pays excessive attention to the partial translation while neglecting the source sentence to some extent, namely overconfidence of the lm. accordingly, we define the margin between the nmt and the lm, calculated by subtracting the predicted probability of the lm from that of the nmt model for each token. the margin is negatively correlated to the overconfidence degree of the lm. based on the property, we propose a margin-based token-level objective (mto) and a margin-based sentence-level objective (mso) to maximize the margin for preventing the lm from being overconfident. experiments on wmt14 english-to-german, wmt19 chinese-to-english, and wmt14 english-to-french translation tasks demonstrate the effectiveness of our approach, with 1.36, 1.50, and 0.63 bleu improvements, respectively, compared to the transformer baseline. the human evaluation further verifies that our approaches improve translation adequacy as well as fluency."], "machine translation and multilinguality"], [["more than text: multi-modal chinese word segmentation", "dong zhang | zheng hu | shoushan li | hanqian wu | qiaoming zhu | guodong zhou", "chinese word segmentation (cws) is undoubtedly an important basic task in natural language processing. previous works only focus on the textual modality, but there are often audio and video utterances (such as news broadcast and face-to-face dialogues), where textual, acoustic and visual modalities normally exist. to this end, we attempt to combine the multi-modality (mainly the converted text and actual voice information) to perform cws. in this paper, we annotate a new dataset for cws containing text and audio. moreover, we propose a time-dependent multi-modal interactive model based on transformer framework to integrate multi-modal information for word sequence labeling. the experimental results on three different training sets show the effectiveness of our approach with fusing text and audio."], "phonology, morphology and word segmentation"], [["measuring and increasing context usage in context-aware machine translation", "patrick fernandes | kayo yin | graham neubig | andr\u00e9 f. t. martins", "recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context, context from sentences other than those currently being translated. however, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. in this paper, we introduce a new metric, conditional cross-mutual information, to quantify usage of context by these models. using this metric, we measure how much document-level machine translation systems use particular varieties of context. we find that target context is referenced more than source context, and that including more context has a diminishing affect on results. we then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. experiments show that our method not only increases context usage, but also improves the translation quality according to metrics such as bleu and comet, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets."], "machine translation and multilinguality"], [["marginal utility diminishes: exploring the minimum knowledge for bert knowledge distillation", "yuanxin liu | fandong meng | zheng lin | weiping wang | jie zhou", "recently, knowledge distillation (kd) has shown great success in bert compression. instead of only learning from the teacher\u2019s soft label as in conventional kd, researchers find that the rich information contained in the hidden layers of bert is conducive to the student\u2019s performance. to better exploit the hidden knowledge, a common practice is to force the student to deeply mimic the teacher\u2019s hidden states of all the tokens in a layer-wise manner. in this paper, however, we observe that although distilling the teacher\u2019s hidden state knowledge (hsk) is helpful, the performance gain (marginal utility) diminishes quickly as more hsk is distilled. to understand this effect, we conduct a series of analysis. specifically, we divide the hsk of bert into three dimensions, namely depth, length and width. we first investigate a variety of strategies to extract crucial knowledge for each single dimension and then jointly compress the three dimensions. in this way, we show that 1) the student\u2019s performance can be improved by extracting and distilling the crucial hsk, and 2) using a tiny fraction of hsk can achieve the same performance as extensive hsk distillation. based on the second finding, we further propose an efficient kd paradigm to compress bert, which does not require loading the teacher during the training of student. for two kinds of student models and computing devices, the proposed kd paradigm gives rise to training speedup of 2.7x 3.4x."], "machine learning for nlp"], [["a sequence-to-sequence approach to dialogue state tracking", "yue feng | yang wang | hang li", "this paper is concerned with dialogue state tracking (dst) in a task-oriented dialogue system. building a dst module that is highly effective is still a challenging issue, although significant progresses have been made recently. this paper proposes a new approach to dialogue state tracking, referred to as seq2seq-du, which formalizes dst as a sequence-to-sequence problem. seq2seq-du employs two bert-based encoders to respectively encode the utterances in the dialogue and the descriptions of schemas, an attender to calculate attentions between the utterance embeddings and the schema embeddings, and a decoder to generate pointers to represent the current state of dialogue. seq2seq-du has the following advantages. it can jointly model intents, slots, and slot values; it can leverage the rich representations of utterances and schemas based on bert; it can effectively deal with categorical and non-categorical slots, and unseen schemas. in addition, seq2seq-du can also be used in the nlu (natural language understanding) module of a dialogue system. experimental results on benchmark datasets in different settings (sgd, multiwoz2.2, multiwoz2.1, woz2.0, dstc2, m2m, snips, and atis) show that seq2seq-du outperforms the existing methods."], "dialogue and interactive systems"], [["video paragraph captioning as a text summarization task", "hui liu | xiaojun wan", "video paragraph captioning aims to generate a set of coherent sentences to describe a video that contains several events. most previous methods simplify this task by using ground-truth event segments. in this work, we propose a novel framework by taking this task as a text summarization task. we first generate lots of sentence-level captions focusing on different video clips and then summarize these captions to obtain the final paragraph caption. our method does not depend on ground-truth event segments. experiments on two popular datasets activitynet captions and youcookii demonstrate the advantages of our new framework. on the activitynet dataset, our method even outperforms some previous methods using ground-truth event segment labels."], "language grounding to vision, robotics and beyond"], [["mid-air hand gestures for post-editing of machine translation", "rashad albo jamara | nico herbig | antonio kr\u00fcger | josef van genabith", "to translate large volumes of text in a globally connected world, more and more translators are integrating machine translation (mt) and post-editing (pe) into their translation workflows to generate publishable quality translations. while this process has been shown to save time and reduce errors, the task of translation is changing from mostly text production from scratch to fixing errors within useful but partly incorrect mt output. this is affecting the interface design of translation tools, where better support for text editing tasks is required. here, we present the first study that investigates the usefulness of mid-air hand gestures in combination with the keyboard (gk) for text editing in pe of mt. guided by a gesture elicitation study with 14 freelance translators, we develop a prototype supporting mid-air hand gestures for cursor placement, text selection, deletion, and reordering. these gestures combined with the keyboard facilitate all editing types required for pe. an evaluation of the prototype shows that the average editing duration of gk is only slightly slower than the standard mouse and keyboard (mk), even though participants are very familiar with the latter, and relative novices to the former. furthermore, the qualitative analysis shows positive attitudes towards hand gestures for pe, especially when manipulating single words."], "nlp applications"], [["deep differential amplifier for extractive summarization", "ruipeng jia | yanan cao | fang fang | yuchen zhou | zheng fang | yanbing liu | shi wang", "for sentence-level extractive summarization, there is a disproportionate ratio of selected and unselected sentences, leading to flatting the summary features when maximizing the accuracy. the imbalanced classification of summarization is inherent, which can\u2019t be addressed by common algorithms easily. in this paper, we conceptualize the single-document extractive summarization as a rebalance problem and present a deep differential amplifier framework. specifically, we first calculate and amplify the semantic difference between each sentence and all other sentences, and then apply the residual unit as the second item of the differential amplifier to deepen the architecture. finally, to compensate for the imbalance, the corresponding objective loss of minority class is boosted by a weighted cross-entropy. in contrast to previous approaches, this model pays more attention to the pivotal information of one sentence, instead of all the informative context modeling by recurrent or transformer architecture. we demonstrate experimentally on two benchmark datasets that our summarizer performs competitively against state-of-the-art methods. our source code will be available on github."], "summarization"], [["accelerating text communication via abbreviated sentence input", "jiban adhikary | jamie berger | keith vertanen", "typing every character in a text message may require more time or effort than strictly necessary. skipping spaces or other characters may be able to speed input and reduce a user\u2019s physical input effort. this can be particularly important for people with motor impairments. in a large crowdsourced study, we found workers frequently abbreviated text by omitting mid-word vowels. we designed a recognizer optimized for expanding noisy abbreviated input where users often omit spaces and mid-word vowels. we show using neural language models for selecting conversational-style training text and for rescoring the recognizer\u2019s n-best sentences improved accuracy. on noisy touchscreen data collected from hundreds of users, we found accurate abbreviated input was possible even if a third of characters was omitted. finally, in a study where users had to dwell for a second on each key, sentence abbreviated input was competitive with a conventional keyboard with word predictions. after practice, users wrote abbreviated sentences at 9.6 words-per-minute versus word input at 9.9 words-per-minute."], "nlp applications"], [["tree-structured topic modeling with nonparametric neural variational inference", "ziye chen | cheng ding | zusheng zhang | yanghui rao | haoran xie", "topic modeling has been widely used for discovering the latent semantic structure of documents, but most existing methods learn topics with a flat structure. although probabilistic models can generate topic hierarchies by introducing nonparametric priors like chinese restaurant process, such methods have data scalability issues. in this study, we develop a tree-structured topic model by leveraging nonparametric neural variational inference. particularly, the latent components of the stick-breaking process are first learned for each document, then the affiliations of latent components are modeled by the dependency matrices between network layers. utilizing this network structure, we can efficiently extract a tree-structured topic hierarchy with reasonable structure, low redundancy, and adaptable widths. experiments on real-world datasets validate the effectiveness of our method."], "semantics"], [["r2d2: recursive transformer based on differentiable tree for interpretable hierarchical language modeling", "xiang hu | haitao mi | zujie wen | yafang wang | yi su | jing zheng | gerard de melo", "human language understanding operates at multiple levels of granularity (e.g., words, phrases, and sentences) with increasing levels of abstraction that can be hierarchically combined. however, existing deep models with stacked layers do not explicitly model any sort of hierarchical process. in this paper, we propose a recursive transformer model based on differentiable cky style binary trees to emulate this composition process, and we extend the bidirectional language model pre-training objective to this architecture, attempting to predict each word given its left and right abstraction nodes. to scale up our approach, we also introduce an efficient pruning and growing algorithm to reduce the time complexity and enable encoding in linear time. experimental results on language modeling and unsupervised parsing show the effectiveness of our approach."], "machine learning for nlp"], [["covid-fact: fact extraction and verification of real-world claims on covid-19 pandemic", "arkadiy saakyan | tuhin chakrabarty | smaranda muresan", "we introduce a fever-like dataset covid-fact of 4,086 claims concerning the covid-19 pandemic. the dataset contains claims, evidence for the claims, and contradictory claims refuted by the evidence. unlike previous approaches, we automatically detect true claims and their source articles and then generate counter-claims using automatic methods rather than employing human annotators. along with our constructed resource, we formally present the task of identifying relevant evidence for the claims and verifying whether the evidence refutes or supports a given claim. in addition to scientific claims, our data contains simplified general claims from media sources, making it better suited for detecting general misinformation regarding covid-19. our experiments indicate that covid-fact will provide a challenging testbed for the development of new systems and our approach will reduce the costs of building domain-specific datasets for detecting misinformation."], "nlp applications"], [["n-ary constituent tree parsing with recursive semi-markov model", "xin xin | jinlong li | zeqi tan", "in this paper, we study the task of graph-based constituent parsing in the setting that binarization is not conducted as a pre-processing step, where a constituent tree may consist of nodes with more than two children. previous graph-based methods on this setting typically generate hidden nodes with the dummy label inside the n-ary nodes, in order to transform the tree into a binary tree for prediction. the limitation is that the hidden nodes break the sibling relations of the n-ary node\u2019s children. consequently, the dependencies of such sibling constituents might not be accurately modeled and is being ignored. to solve this limitation, we propose a novel graph-based framework, which is called \u201crecursive semi-markov model\u201d. the main idea is to utilize 1-order semi-markov model to predict the immediate children sequence of a constituent candidate, which then recursively serves as a child candidate of its parent. in this manner, the dependencies of sibling constituents can be described by 1-order transition features, which solves the above limitation. through experiments, the proposed framework obtains the f1 of 95.92% and 92.50% on the datasets of ptb and ctb 5.1 respectively. specially, the recursive semi-markov model shows advantages in modeling nodes with more than two children, whose average f1 can be improved by 0.3-1.1 points in ptb and 2.3-6.8 points in ctb 5.1."], "tagging, chunking, syntax and parsing"], [["breaking the corpus bottleneck for context-aware neural machine translation with cross-task pre-training", "linqing chen | junhui li | zhengxian gong | boxing chen | weihua luo | min zhang | guodong zhou", "context-aware neural machine translation (nmt) remains challenging due to the lack of large-scale document-level parallel corpora. to break the corpus bottleneck, in this paper we aim to improve context-aware nmt by taking the advantage of the availability of both large-scale sentence-level parallel dataset and source-side monolingual documents. to this end, we propose two pre-training tasks. one learns to translate a sentence from source language to target language on the sentence-level parallel dataset while the other learns to translate a document from deliberately noised to original on the monolingual documents. importantly, the two pre-training tasks are jointly and simultaneously learned via the same model, thereafter fine-tuned on scale-limited parallel documents from both sentence-level and document-level perspectives. experimental results on four translation tasks show that our approach significantly improves translation performance. one nice property of our approach is that the fine-tuned model can be used to translate both sentences and documents."], "machine translation and multilinguality"], [["tat-qa: a question answering benchmark on a hybrid of tabular and textual content in finance", "fengbin zhu | wenqiang lei | youcheng huang | chao wang | shuo zhang | jiancheng lv | fuli feng | tat-seng chua", "hybrid data combining both tabular and textual content (e.g., financial reports) are quite pervasive in the real world. however, question answering (qa) over such hybrid data is largely neglected in existing research. in this work, we extract samples from real financial reports to build a new large-scale qa dataset containing both tabular and textual data, named tat-qa, where numerical reasoning is usually required to infer the answer, such as addition, subtraction, multiplication, division, counting, comparison/sorting, and the compositions. we further propose a novel qa model termed tagop, which is capable of reasoning over both tables and text. it adopts sequence tagging to extract relevant cells from the table along with relevant spans from the text to infer their semantics, and then applies symbolic reasoning over them with a set of aggregation operators to arrive at the final answer. tagop achieves 58.0% inf1, which is an 11.1% absolute increase over the previous best baseline model, according to our experiments on tat-qa. but this result still lags far behind performance of expert human, i.e.90.8% in f1. it is demonstrated that our tat-qa is very challenging and can serve as a benchmark for training and testing powerful qa models that address hybrid form data."], "question answering"], [["examining the inductive bias of neural language models with artificial languages", "jennifer c. white | ryan cotterell", "since language models are used to model a wide variety of languages, it is natural to ask whether the neural architectures used for the task have inductive biases towards modeling particular types of languages. investigation of these biases has proved complicated due to the many variables that appear in the experimental setup. languages vary in many typological dimensions, and it is difficult to single out one or two to investigate without the others acting as confounders. we propose a novel method for investigating the inductive biases of language models using artificial languages. these languages are constructed to allow us to create parallel corpora across languages that differ only in the typological feature being investigated, such as word order. we then use them to train and test language models. this constitutes a fully controlled causal framework, and demonstrates how grammar engineering can serve as a useful tool for analyzing neural models. using this method, we find that commonly used neural architectures exhibit different inductive biases: lstms display little preference with respect to word ordering, while transformers display a clear preference for some orderings over others. further, we find that neither the inductive bias of the lstm nor that of the transformer appear to reflect any tendencies that we see in attested natural languages."], "interpretability and analysis of models for nlp"], [["improving named entity recognition by external context retrieving and cooperative learning", "xinyu wang | yong jiang | nguyen bach | tao wang | zhongqiang huang | fei huang | kewei tu", "recent advances in named entity recognition (ner) show that document-level contexts can significantly improve model performance. in many application scenarios, however, such contexts are not available. in this paper, we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a search engine, with the original sentence as the query. we find empirically that the contextual representations computed on the retrieval-based input view, constructed through the concatenation of a sentence and its external contexts, can achieve significantly improved performance compared to the original input view based only on the sentence. furthermore, we can improve the model performance of both input views by cooperative learning, a training method that encourages the two input views to produce similar contextual representations or output label distributions. experiments show that our approach can achieve new state-of-the-art performance on 8 ner data sets across 5 domains."], "information extraction, retrieval and text mining"], [["are vqa systems rad? measuring robustness to augmented data with focused interventions", "daniel rosenberg | itai gat | amir feder | roi reichart", "deep learning algorithms have shown promising results in visual question answering (vqa) tasks, but a more careful look reveals that they often do not understand the rich signal they are being fed with. to understand and better measure the generalization capabilities of vqa systems, we look at their robustness to counterfactually augmented data. our proposed augmentations are designed to make a focused intervention on a specific property of the question such that the answer changes. using these augmentations, we propose a new robustness measure, robustness to augmented data (rad), which measures the consistency of model predictions between original and augmented examples. through extensive experimentation, we show that rad, unlike classical accuracy measures, can quantify when state-of-the-art systems are not robust to counterfactuals. we find substantial failure cases which reveal that current vqa systems are still brittle. finally, we connect between robustness and generalization, demonstrating the predictive power of rad for performance on unseen augmentations."], "language grounding to vision, robotics and beyond"], [["on the efficacy of adversarial data collection for question answering: results from a large-scale randomized study", "divyansh kaushik | douwe kiela | zachary c. lipton | wen-tau yih", "in adversarial data collection (adc), a human workforce interacts with a model in real time, attempting to produce examples that elicit incorrect predictions. researchers hope that models trained on these more challenging datasets will rely less on superficial patterns, and thus be less brittle. however, despite adc\u2019s intuitive appeal, it remains unclear when training on adversarial datasets produces more robust models. in this paper, we conduct a large-scale controlled study focused on question answering, assigning workers at random to compose questions either (i) adversarially (with a model in the loop); or (ii) in the standard fashion (without a model). across a variety of models and datasets, we find that models trained on adversarial data usually perform better on other adversarial datasets but worse on a diverse collection of out-of-domain evaluation sets. finally, we provide a qualitative analysis of adversarial (vs standard) data, identifying key differences and offering guidance for future research."], "question answering"], [["dialogue response selection with hierarchical curriculum learning", "yixuan su | deng cai | qingyu zhou | zibo lin | simon baker | yunbo cao | shuming shi | nigel collier | yan wang", "we study the learning of a matching model for dialogue response selection. motivated by the recent finding that models trained with random negative samples are not ideal in real-world scenarios, we propose a hierarchical curriculum learning framework that trains the matching model in an \u201ceasy-to-difficult\u201d scheme. our learning framework consists of two complementary curricula: (1) corpus-level curriculum (cc); and (2) instance-level curriculum (ic). in cc, the model gradually increases its ability in finding the matching clues between the dialogue context and a response candidate. as for ic, it progressively strengthens the model\u2019s ability in identifying the mismatching information between the dialogue context and a response candidate. empirical studies on three benchmark datasets with three state-of-the-art matching models demonstrate that the proposed learning framework significantly improves the model performance across various evaluation metrics."], "dialogue and interactive systems"], [["the r-u-a-robot dataset: helping avoid chatbot deception by detecting user questions about human or non-human identity", "david gros | yu li | zhou yu", "humans are increasingly interacting with machines through language, sometimes in contexts where the user may not know they are talking to a machine (like over the phone or a text chatbot). we aim to understand how system designers and researchers might allow their systems to confirm its non-human identity. we collect over 2,500 phrasings related to the intent of \u201care you a robot?\u201d. this is paired with over 2,500 adversarially selected utterances where only confirming the system is non-human would be insufficient or disfluent. we compare classifiers to recognize the intent and discuss the precision/recall and model complexity tradeoffs. such classifiers could be integrated into dialog systems to avoid undesired deception. we then explore how both a generative research model (blender) as well as two deployed systems (amazon alexa, google assistant) handle this intent, finding that systems often fail to confirm their non-human identity. finally, we try to understand what a good response to the intent would be, and conduct a user study to compare the important aspects when responding to this intent."], "dialogue and interactive systems"], [["is sparse attention more interpretable?", "clara meister | stefan lazov | isabelle augenstein | ryan cotterell", "sparse attention has been claimed to increase model interpretability under the assumption that it highlights influential inputs. yet the attention distribution is typically over representations internal to the model rather than the inputs themselves, suggesting this assumption may not have merit. we build on the recent work exploring the interpretability of attention; we design a set of experiments to help us understand how sparsity affects our ability to use attention as an explainability tool. on three text classification tasks, we verify that only a weak relationship between inputs and co-indexed intermediate representations exists\u2014under sparse attention and otherwise. further, we do not find any plausible mappings from sparse attention distributions to a sparse set of influential inputs through other avenues. rather, we observe in this setting that inducing sparsity may make it less plausible that attention can be used as a tool for understanding model behavior."], "interpretability and analysis of models for nlp"], [["neural bi-lexicalized pcfg induction", "songlin yang | yanpeng zhao | kewei tu", "neural lexicalized pcfgs (l-pcfgs) have been shown effective in grammar induction. however, to reduce computational complexity, they make a strong independence assumption on the generation of the child word and thus bilexical dependencies are ignored. in this paper, we propose an approach to parameterize l-pcfgs without making implausible independence assumptions. our approach directly models bilexical dependencies and meanwhile reduces both learning and representation complexities of l-pcfgs. experimental results on the english wsj dataset confirm the effectiveness of our approach in improving both running speed and unsupervised parsing performance."], "tagging, chunking, syntax and parsing"], [["how did this get funded?! automatically identifying quirky scientific achievements", "chen shani | nadav borenstein | dafna shahaf", "humor is an important social phenomenon, serving complex social and psychological functions. however, despite being studied for millennia humor is computationally not well understood, often considered an ai-complete problem. in this work, we introduce a novel setting in humor mining: automatically detecting funny and unusual scientific papers. we are inspired by the ig nobel prize, a satirical prize awarded annually to celebrate funny scientific achievements (example past winner: \u201care cows more likely to lie down the longer they stand?\u201d). this challenging task has unique characteristics that make it particularly suitable for automatic learning. we construct a dataset containing thousands of funny papers and use it to learn classifiers, combining findings from psychology and linguistics with recent advances in nlp. we use our models to identify potentially funny papers in a large dataset of over 630,000 articles. the results demonstrate the potential of our methods, and more broadly the utility of integrating state-of-the-art nlp methods with insights from more traditional disciplines"], "computational social science, social media and cultural analytics"], [["on orthogonality constraints for transformers", "aston zhang | alvin chan | yi tay | jie fu | shuohang wang | shuai zhang | huajie shao | shuochao yao | roy ka-wei lee", "orthogonality constraints encourage matrices to be orthogonal for numerical stability. these plug-and-play constraints, which can be conveniently incorporated into model training, have been studied for popular architectures in natural language processing, such as convolutional neural networks and recurrent neural networks. however, a dedicated study on such constraints for transformers has been absent. to fill this gap, this paper studies orthogonality constraints for transformers, showing the effectiveness with empirical evidence from ten machine translation tasks and two dialogue generation tasks. for example, on the large-scale wmt\u201916 en\u2192de benchmark, simply plugging-and-playing orthogonality constraints on the original transformer model (vaswani et al., 2017) increases the bleu from 28.4 to 29.6, coming close to the 29.7 bleu achieved by the very competitive dynamic convolution (wu et al., 2019)."], "machine learning for nlp"], [["openmeva: a benchmark for evaluating open-ended story generation metrics", "jian guan | zhexin zhang | zhuoer feng | zitao liu | wenbiao ding | xiaoxi mao | changjie fan | minlie huang", "automatic metrics are essential for developing natural language generation (nlg) models, particularly for open-ended language generation tasks such as story generation. however, existing automatic metrics are observed to correlate poorly with human evaluation. the lack of standardized benchmark datasets makes it difficult to fully evaluate the capabilities of a metric and fairly compare different metrics. therefore, we propose openmeva, a benchmark for evaluating open-ended story generation metrics. openmeva provides a comprehensive test suite to assess the capabilities of metrics, including (a) the correlation with human judgments, (b) the generalization to different model outputs and datasets, (c) the ability to judge story coherence, and (d) the robustness to perturbations. to this end, openmeva includes both manually annotated stories and auto-constructed test examples. we evaluate existing metrics on openmeva and observe that they have poor correlation with human judgments, fail to recognize discourse-level incoherence, and lack inferential knowledge (e.g., causal order between events), the generalization ability and robustness. our study presents insights for developing nlg models and metrics in further research."], "generation"], [["hierarchical context-aware network for dense video event captioning", "lei ji | xianglin guo | haoyang huang | xilin chen", "dense video event captioning aims to generate a sequence of descriptive captions for each event in a long untrimmed video. video-level context provides important information and facilities the model to generate consistent and less redundant captions between events. in this paper, we introduce a novel hierarchical context-aware network for dense video event captioning (hcn) to capture context from various aspects. in detail, the model leverages local and global context with different mechanisms to jointly learn to generate coherent captions. the local context module performs full interaction between neighbor frames and the global context module selectively attends to previous or future events. according to our extensive experiment on both youcook2 and activitynet captioning datasets, the video-level hcn model outperforms the event-level context-agnostic model by a large margin. the code is available at https://github.com/kirkguo/hcn."], "language grounding to vision, robotics and beyond"], [["hatecheck: functional tests for hate speech detection models", "paul r\u00f6ttger | bertie vidgen | dong nguyen | zeerak waseem | helen margetts | janet pierrehumbert", "detecting online hate is a difficult task that even state-of-the-art models struggle with. typically, hate speech detection models are evaluated by measuring their performance on held-out test data using metrics such as accuracy and f1 score. however, this approach makes it difficult to identify specific model weak points. it also risks overestimating generalisable model performance due to increasingly well-evidenced systematic gaps and biases in hate speech datasets. to enable more targeted diagnostic insights, we introduce hatecheck, a suite of functional tests for hate speech detection models. we specify 29 model functionalities motivated by a review of previous research and a series of interviews with civil society stakeholders. we craft test cases for each functionality and validate their quality through a structured annotation process. to illustrate hatecheck\u2019s utility, we test near-state-of-the-art transformer models as well as two popular commercial models, revealing critical model weaknesses."], "computational social science, social media and cultural analytics"], [["hidden killer: invisible textual backdoor attacks with syntactic trigger", "fanchao qi | mukai li | yangyi chen | zhengyan zhang | zhiyuan liu | yasheng wang | maosong sun", "backdoor attacks are a kind of insidious security threat against machine learning models. after being injected with a backdoor in training, the victim model will produce adversary-specified outputs on the inputs embedded with predesigned triggers but behave properly on normal inputs during inference. as a sort of emergent attack, backdoor attacks in natural language processing (nlp) are investigated insufficiently. as far as we know, almost all existing textual backdoor attack methods insert additional contents into normal samples as triggers, which causes the trigger-embedded samples to be detected and the backdoor attacks to be blocked without much effort. in this paper, we propose to use the syntactic structure as the trigger in textual backdoor attacks. we conduct extensive experiments to demonstrate that the syntactic trigger-based attack method can achieve comparable attack performance (almost 100% success rate) to the insertion-based methods but possesses much higher invisibility and stronger resistance to defenses. these results also reveal the significant insidiousness and harmfulness of textual backdoor attacks. all the code and data of this paper can be obtained at https://github.com/thunlp/hiddenkiller."], "interpretability and analysis of models for nlp"], [["adversarial learning for discourse rhetorical structure parsing", "longyin zhang | fang kong | guodong zhou", "text-level discourse rhetorical structure (drs) parsing is known to be challenging due to the notorious lack of training data. although recent top-down drs parsers can better leverage global document context and have achieved certain success, the performance is still far from perfect. to our knowledge, all previous drs parsers make local decisions for either bottom-up node composition or top-down split point ranking at each time step, and largely ignore drs parsing from the global view point. obviously, it is not sufficient to build an entire drs tree only through these local decisions. in this work, we present our insight on evaluating the pros and cons of the entire drs tree for global optimization. specifically, based on recent well-performing top-down frameworks, we introduce a novel method to transform both gold standard and predicted constituency trees into tree diagrams with two color channels. after that, we learn an adversarial bot between gold and fake tree diagrams to estimate the generated drs trees from a global perspective. we perform experiments on both rst-dt and cdtb corpora and use the original parseval for performance evaluation. the experimental results show that our parser can substantially improve the performance when compared with previous state-of-the-art parsers."], "discourse and pragmatics"], [["infosurgeon: cross-media fine-grained information consistency checking for fake news detection", "yi fung | christopher thomas | revanth gangi reddy | sandeep polisetty | heng ji | shih-fu chang | kathleen mckeown | mohit bansal | avi sil", "to defend against machine-generated fake news, an effective mechanism is urgently needed. we contribute a novel benchmark for fake news detection at the knowledge element level, as well as a solution for this task which incorporates cross-media consistency checking to detect the fine-grained knowledge elements making news articles misinformative. due to training data scarcity, we also formulate a novel data synthesis method by manipulating knowledge elements within the knowledge graph to generate noisy training data with specific, hard to detect, known inconsistencies. our detection approach outperforms the state-of-the-art (up to 16.8% accuracy gain), and more critically, yields fine-grained explanations."], "computational social science, social media and cultural analytics"], [["cline: contrastive learning with semantic negative examples for natural language understanding", "dong wang | ning ding | piji li | haitao zheng", "despite pre-trained language models have proven useful for learning high-quality semantic representations, these models are still vulnerable to simple perturbations. recent works aimed to improve the robustness of pre-trained models mainly focus on adversarial training from perturbed examples with similar semantics, neglecting the utilization of different or even opposite semantics. different from the image processing field, the text is discrete and few word substitutions can cause significant semantic changes. to study the impact of semantics caused by small perturbations, we conduct a series of pilot experiments and surprisingly find that adversarial training is useless or even harmful for the model to detect these semantic changes. to address this problem, we propose contrastive learning with semantic negative examples (cline), which constructs semantic negative examples unsupervised to improve the robustness under semantically adversarial attacking. by comparing with similar and opposite semantic examples, the model can effectively perceive the semantic changes caused by small perturbations. empirical results show that our approach yields substantial improvements on a range of sentiment analysis, reasoning, and reading comprehension tasks. and cline also ensures the compactness within the same semantics and separability across different semantics in sentence-level."], "semantics"], [["early detection of sexual predators in chats", "matthias vogt | ulf leser | alan akbik", "an important risk that children face today is online grooming, where a so-called sexual predator establishes an emotional connection with a minor online with the objective of sexual abuse. prior work has sought to automatically identify grooming chats, but only after an incidence has already happened in the context of legal prosecution. in this work, we instead investigate this problem from the point of view of prevention. we define and study the task of early sexual predator detection (espd) in chats, where the goal is to analyze a running chat from its beginning and predict grooming attempts as early and as accurately as possible. we survey existing datasets and their limitations regarding espd, and create a new dataset called panc for more realistic evaluations. we present strong baselines built on bert that also reach state-of-the-art results for conventional spd. finally, we consider coping with limited computational resources, as real-life applications require espd on mobile devices."], "nlp applications"], [["cascade versus direct speech translation: do the differences still make a difference?", "luisa bentivogli | mauro cettolo | marco gaido | alina karakanta | alberto martinelli | matteo negri | marco turchi", "five years after the first published proofs of concept, direct approaches to speech translation (st) are now competing with traditional cascade solutions. in light of this steady progress, can we claim that the performance gap between the two is closed? starting from this question, we present a systematic comparison between state-of-the-art systems representative of the two paradigms. focusing on three language directions (english-german/italian/spanish), we conduct automatic and manual evaluations, exploiting high-quality professional post-edits and annotations. our multi-faceted analysis on one of the few publicly available st benchmarks attests for the first time that: i) the gap between the two paradigms is now closed, and ii) the subtle differences observed in their behavior are not sufficient for humans neither to distinguish them nor to prefer one over the other."], "machine translation and multilinguality"], [["cascaded head-colliding attention", "lin zheng | zhiyong wu | lingpeng kong", "transformers have advanced the field of natural language processing (nlp) on a variety of important tasks. at the cornerstone of the transformer architecture is the multi-head attention (mha) mechanism which models pairwise interactions between the elements of the sequence. despite its massive success, the current framework ignores interactions among different heads, leading to the problem that many of the heads are redundant in practice, which greatly wastes the capacity of the model. to improve parameter efficiency, we re-formulate the mha as a latent variable model from a probabilistic perspective. we present cascaded head-colliding attention (coda) which explicitly models the interactions between attention heads through a hierarchical variational distribution. we conduct extensive experiments and demonstrate that coda outperforms the transformer baseline, by 0.6 perplexity on wikitext-103 in language modeling, and by 0.6 bleu on wmt14 en-de in machine translation, due to its improvements on the parameter efficiency."], "machine learning for nlp"], [["counterfactuals to control latent disentangled text representations for style transfer", "sharmila reddy nangi | niyati chhaya | sopan khosla | nikhil kaushik | harshit nyati", "disentanglement of latent representations into content and style spaces has been a commonly employed method for unsupervised text style transfer. these techniques aim to learn the disentangled representations and tweak them to modify the style of a sentence. in this paper, we propose a counterfactual-based method to modify the latent representation, by posing a \u2018what-if\u2019 scenario. this simple and disciplined approach also enables a fine-grained control on the transfer strength. we conduct experiments with the proposed methodology on multiple attribute transfer tasks like sentiment, formality and excitement to support our hypothesis."], "sentiment analysis, stylistic analysis, and argument mining"], [["dynasent: a dynamic benchmark for sentiment analysis", "christopher potts | zhengxuan wu | atticus geiger | douwe kiela", "we introduce dynasent (\u2018dynamic sentiment\u2019), a new english-language benchmark task for ternary (positive/negative/neutral) sentiment analysis. dynasent combines naturally occurring sentences with sentences created using the open-source dynabench platform, which facilities human-and-model-in-the-loop dataset creation. dynasent has a total of 121,634 sentences, each validated by five crowdworkers, and its development and test splits are designed to produce chance performance for even the best models we have been able to develop; when future models solve this task, we will use them to create dynasent version 2, continuing the dynamic evolution of this benchmark. here, we report on the dataset creation effort, focusing on the steps we took to increase quality and reduce artifacts. we also present evidence that dynasent\u2019s neutral category is more coherent than the comparable category in other benchmarks, and we motivate training models from scratch for each round over successive fine-tuning."], "sentiment analysis, stylistic analysis, and argument mining"], [["learning prototypical functions for physical artifacts", "tianyu jiang | ellen riloff", "humans create things for a reason. ancient people created spears for hunting, knives for cutting meat, pots for preparing food, etc. the prototypical function of a physical artifact is a kind of commonsense knowledge that we rely on to understand natural language. for example, if someone says \u201cshe borrowed the book\u201d then you would assume that she intends to read the book, or if someone asks \u201ccan i use your knife?\u201d then you would assume that they need to cut something. in this paper, we introduce a new nlp task of learning the prototypical uses for human-made physical objects. we use frames from framenet to represent a set of common functions for objects, and describe a manually annotated data set of physical objects labeled with their prototypical function. we also present experimental results for this task, including bert-based models that use predictions from masked patterns as well as artifact sense definitions from wordnet and frame definitions from framenet."], "semantics"], [["timers: document-level temporal relation extraction", "puneet mathur | rajiv jain | franck dernoncourt | vlad morariu | quan hung tran | dinesh manocha", "we present timers - a time, rhetorical and syntactic-aware model for document-level temporal relation classification in the english language. our proposed method leverages rhetorical discourse features and temporal arguments from semantic role labels, in addition to traditional local syntactic features, trained through a gated relational-gcn. extensive experiments show that the proposed model outperforms previous methods by 5-18% on the tddiscourse, timebank-dense, and matres datasets due to our discourse-level modeling."], "information extraction, retrieval and text mining"], [["joint models for answer verification in question answering systems", "zeyu zhang | thuy vu | alessandro moschitti", "this paper studies joint models for selecting correct answer sentences among the top k provided by answer sentence selection (as2) modules, which are core components of retrieval-based question answering (qa) systems. our work shows that a critical step to effectively exploiting an answer set regards modeling the interrelated information between pair of answers. for this purpose, we build a three-way multi-classifier, which decides if an answer supports, refutes, or is neutral with respect to another one. more specifically, our neural architecture integrates a state-of-the-art as2 module with the multi-classifier, and a joint layer connecting all components. we tested our models on wikiqa, trec-qa, and a real-world dataset. the results show that our models obtain the new state of the art in as2."], "question answering"], [["attention flows are shapley value explanations", "kawin ethayarajh | dan jurafsky", "shapley values, a solution to the credit assignment problem in cooperative game theory, are a popular type of explanation in machine learning, having been used to explain the importance of features, embeddings, and even neurons. in nlp, however, leave-one-out and attention-based explanations still predominate. can we draw a connection between these different methods? we formally prove that \u2014 save for the degenerate case \u2014 attention weights and leave-one-out values cannot be shapley values. attention flow is a post-processed variant of attention weights obtained by running the max-flow algorithm on the attention graph. perhaps surprisingly, we prove that attention flows are indeed shapley values, at least at the layerwise level. given the many desirable theoretical qualities of shapley values \u2014 which has driven their adoption among the ml community \u2014 we argue that nlp practitioners should, when possible, adopt attention flow explanations alongside more traditional ones."], "interpretability and analysis of models for nlp"], [["length-adaptive transformer: train once with length drop, use anytime with search", "gyuwan kim | kyunghyun cho", "despite transformers\u2019 impressive accuracy, their computational cost is often prohibitive to use with limited computational resources. most previous approaches to improve inference efficiency require a separate model for each possible computational budget. in this paper, we extend power-bert (goyal et al., 2020) and propose length-adaptive transformer that can be used for various inference scenarios after one-shot training. we train a transformer with lengthdrop, a structural variant of dropout, which stochastically determines a sequence length at each layer. we then conduct a multi-objective evolutionary search to find a length configuration that maximizes the accuracy and minimizes the efficiency metric under any given computational budget. additionally, we significantly extend the applicability of power-bert beyond sequence-level classification into token-level classification with drop-and-restore process that drops word-vectors temporarily in intermediate layers and restores at the last layer if necessary. we empirically verify the utility of the proposed approach by demonstrating the superior accuracy-efficiency trade-off under various setups, including span-based question answering and text classification. code is available at https://github.com/clovaai/lengthadaptive-transformer."], "machine learning for nlp"], [["bertac: enhancing transformer-based language models with adversarially pretrained convolutional neural networks", "jong-hoon oh | ryu iida | julien kloetzer | kentaro torisawa", "transformer-based language models (tlms), such as bert, albert and gpt-3, have shown strong performance in a wide range of nlp tasks and currently dominate the field of nlp. however, many researchers wonder whether these models can maintain their dominance forever. of course, we do not have answers now, but, as an attempt to find better neural architectures and training schemes, we pretrain a simple cnn using a gan-style learning scheme and wikipedia data, and then integrate it with standard tlms. we show that on the glue tasks, the combination of our pretrained cnn with albert outperforms the original albert and achieves a similar performance to that of sota. furthermore, on open-domain qa (quasar-t and searchqa), the combination of the cnn with albert or roberta achieved stronger performance than sota and the original tlms. we hope that this work provides a hint for developing a novel strong network architecture along with its training scheme. our source code and models are available at https://github.com/nict-wisdom/bertac."], "machine learning for nlp"], [["ultra-fine entity typing with weak supervision from a masked language model", "hongliang dai | yangqiu song | haixun wang", "recently, there is an effort to extend fine-grained entity typing by using a richer and ultra-fine set of types, and labeling noun phrases including pronouns and nominal nouns instead of just named entity mentions. a key challenge for this ultra-fine entity typing task is that human annotated data are extremely scarce, and the annotation ability of existing distant or weak supervision approaches is very limited. to remedy this problem, in this paper, we propose to obtain training data for ultra-fine entity typing by using a bert masked language model (mlm). given a mention in a sentence, our approach constructs an input for the bert mlm so that it predicts context dependent hypernyms of the mention, which can be used as type labels. experimental results demonstrate that, with the help of these automatically generated labels, the performance of an ultra-fine entity typing model can be improved substantially. we also show that our approach can be applied to improve traditional fine-grained entity typing after performing simple type mapping."], "information extraction, retrieval and text mining"], [["superbizarre is not superb: derivational morphology improves bert\u2019s interpretation of complex words", "valentin hofmann | janet pierrehumbert | hinrich sch\u00fctze", "how does the input segmentation of pretrained language models (plms) affect their interpretations of complex words? we present the first study investigating this question, taking bert as the example plm and focusing on its semantic representations of english derivatives. we show that plms can be interpreted as serial dual-route models, i.e., the meanings of complex words are either stored or else need to be computed from the subwords, which implies that maximally meaningful input tokens should allow for the best generalization on new words. this hypothesis is confirmed by a series of semantic probing tasks on which delbert (derivation leveraging bert), a model with derivational input segmentation, substantially outperforms bert with wordpiece segmentation. our results suggest that the generalization capabilities of plms could be further improved if a morphologically-informed vocabulary of input tokens were used."], "phonology, morphology and word segmentation"], [["the limitations of limited context for constituency parsing", "yuchen li | andrej risteski", "incorporating syntax into neural approaches in nlp has a multitude of practical and scientific benefits. for instance, a language model that is syntax-aware is likely to be able to produce better samples; even a discriminative model like bert with a syntax module could be used for core nlp tasks like unsupervised syntactic parsing. rapid progress in recent years was arguably spurred on by the empirical success of the parsing-reading-predict architecture of (shen et al., 2018a), later simplified by the order neuron lstm of (shen et al., 2019). most notably, this is the first time neural approaches were able to successfully perform unsupervised syntactic parsing (evaluated by various metrics like f-1 score). however, even heuristic (much less fully mathematical) understanding of why and when these architectures work is lagging severely behind. in this work, we answer representational questions raised by the architectures in (shen et al., 2018a, 2019), as well as some transition-based syntax-aware language models (dyer et al., 2016): what kind of syntactic structure can current neural approaches to syntax represent? concretely, we ground this question in the sandbox of probabilistic context-free-grammars (pcfgs), and identify a key aspect of the representational power of these approaches: the amount and directionality of context that the predictor has access to when forced to make parsing decision. we show that with limited context (either bounded, or unidirectional), there are pcfgs, for which these approaches cannot represent the max-likelihood parse; conversely, if the context is unlimited, they can represent the max-likelihood parse of any pcfg."], "tagging, chunking, syntax and parsing"], [["a cluster-based approach for improving isotropy in contextual embedding space", "sara rajaee | mohammad taher pilehvar", "the representation degeneration problem in contextual word representations (cwrs) hurts the expressiveness of the embedding space by forming an anisotropic cone where even unrelated words have excessively positive correlations. existing techniques for tackling this issue require a learning process to re-train models with additional objectives and mostly employ a global assessment to study isotropy. our quantitative analysis over isotropy shows that a local assessment could be more accurate due to the clustered structure of cwrs. based on this observation, we propose a local cluster-based method to address the degeneration issue in contextual embedding spaces. we show that in clusters including punctuations and stop words, local dominant directions encode structural information, removing which can improve cwrs performance on semantic tasks. moreover, we find that tense information in verb representations dominates sense semantics. we show that removing dominant directions of verb representations can transform the space to better suit semantic applications. our experiments demonstrate that the proposed cluster-based method can mitigate the degeneration problem on multiple tasks."], "semantics"], [["advpicker: effectively leveraging unlabeled data via adversarial discriminator for cross-lingual ner", "weile chen | huiqiang jiang | qianhui wu | b\u00f6rje karlsson | yi guan", "neural methods have been shown to achieve high performance in named entity recognition (ner), but rely on costly high-quality labeled data for training, which is not always available across languages. while previous works have shown that unlabeled data in a target language can be used to improve cross-lingual model performance, we propose a novel adversarial approach (advpicker) to better leverage such data and further improve results. we design an adversarial learning framework in which an encoder learns entity domain knowledge from labeled source-language data and better shared features are captured via adversarial training - where a discriminator selects less language-dependent target-language data via similarity to the source language. experimental results on standard benchmark datasets well demonstrate that the proposed method benefits strongly from this data selection process and outperforms existing state-of-the-art methods; without requiring any additional external resources (e.g., gazetteers or via machine translation)."], "information extraction, retrieval and text mining"], [["improving paraphrase detection with the adversarial paraphrasing task", "animesh nighojkar | john licato", "if two sentences have the same meaning, it should follow that they are equivalent in their inferential properties, i.e., each sentence should textually entail the other. however, many paraphrase datasets currently in widespread use rely on a sense of paraphrase based on word overlap and syntax. can we teach them instead to identify paraphrases in a way that draws on the inferential properties of the sentences, and is not over-reliant on lexical and syntactic similarities of a sentence pair? we apply the adversarial paradigm to this question, and introduce a new adversarial method of dataset creation for paraphrase identification: the adversarial paraphrasing task (apt), which asks participants to generate semantically equivalent (in the sense of mutually implicative) but lexically and syntactically disparate paraphrases. these sentence pairs can then be used both to test paraphrase identification models (which get barely random accuracy) and then improve their performance. to accelerate dataset generation, we explore automation of apt using t5, and show that the resulting dataset also improves accuracy. we discuss implications for paraphrase detection and release our dataset in the hope of making paraphrase detection models better able to detect sentence-level meaning equivalence."], "semantics"], [["inter-gps: interpretable geometry problem solving with formal language and symbolic reasoning", "pan lu | ran gong | shibiao jiang | liang qiu | siyuan huang | xiaodan liang | song-chun zhu", "geometry problem solving has attracted much attention in the nlp community recently. the task is challenging as it requires abstract problem understanding and symbolic reasoning with axiomatic knowledge. however, current datasets are either small in scale or not publicly available. thus, we construct a new large-scale benchmark, geometry3k, consisting of 3,002 geometry problems with dense annotation in formal language. we further propose a novel geometry solving approach with formal language and symbolic reasoning, called interpretable geometry problem solver (inter-gps). inter-gps first parses the problem text and diagram into formal language automatically via rule-based text parsing and neural object detecting, respectively. unlike implicit learning in existing methods, inter-gps incorporates theorem knowledge as conditional rules and performs symbolic reasoning step by step. also, a theorem predictor is designed to infer the theorem application sequence fed to the symbolic solver for the more efficient and reasonable searching path. extensive experiments on the geometry3k and geos datasets demonstrate that inter-gps achieves significant improvements over existing methods. the project with code and data is available at https://lupantech.github.io/inter-gps."], "nlp applications"], [["beyond offline mapping: learning cross-lingual word embeddings through context anchoring", "aitor ormazabal | mikel artetxe | aitor soroa | gorka labaka | eneko agirre", "recent research on cross-lingual word embeddings has been dominated by unsupervised mapping approaches that align monolingual embeddings. such methods critically rely on those embeddings having a similar structure, but it was recently shown that the separate training in different languages causes departures from this assumption. in this paper, we propose an alternative approach that does not have this limitation, while requiring a weak seed dictionary (e.g., a list of identical words) as the only form of supervision. rather than aligning two fixed embedding spaces, our method works by fixing the target language embeddings, and learning a new set of embeddings for the source language that are aligned with them. to that end, we use an extension of skip-gram that leverages translated context words as anchor points, and incorporates self-learning and iterative restarts to reduce the dependency on the initial dictionary. our approach outperforms conventional mapping methods on bilingual lexicon induction, and obtains competitive results in the downstream xnli task."], "machine translation and multilinguality"], [["supporting cognitive and emotional empathic writing of students", "thiemo wambsganss | christina niklaus | matthias s\u00f6llner | siegfried handschuh | jan marco leimeister", "we present an annotation approach to capturing emotional and cognitive empathy in student-written peer reviews on business models in german. we propose an annotation scheme that allows us to model emotional and cognitive empathy scores based on three types of review components. also, we conducted an annotation study with three annotators based on 92 student essays to evaluate our annotation scheme. the obtained inter-rater agreement of \u03b1=0.79 for the components and the multi-\u03c0=0.41 for the empathy scores indicate that the proposed annotation scheme successfully guides annotators to a substantial to moderate agreement. moreover, we trained predictive models to detect the annotated empathy structures and embedded them in an adaptive writing support system for students to receive individual empathy feedback independent of an instructor, time, and location. we evaluated our tool in a peer learning exercise with 58 students and found promising results for perceived empathy skill learning, perceived feedback accuracy, and intention to use. finally, we present our freely available corpus of 500 empathy-annotated, student-written peer reviews on business models and our annotation guidelines to encourage future research on the design and development of empathy support systems."], "resources and evaluation"], [["generalising multilingual concept-to-text nlg with language agnostic delexicalisation", "giulio zhou | gerasimos lampouras", "concept-to-text natural language generation is the task of expressing an input meaning representation in natural language. previous approaches in this task have been able to generalise to rare or unseen instances by relying on a delexicalisation of the input. however, this often requires that the input appears verbatim in the output text. this poses challenges in multilingual settings, where the task expands to generate the output text in multiple languages given the same input. in this paper, we explore the application of multilingual models in concept-to-text and propose language agnostic delexicalisation, a novel delexicalisation method that uses multilingual pretrained embeddings, and employs a character-level post-editing model to inflect words in their correct form during relexicalisation. our experiments across five datasets and five languages show that multilingual models outperform monolingual models in concept-to-text and that our framework outperforms previous approaches, especially in low resource conditions."], "generation"], [["learning domain-specialised representations for cross-lingual biomedical entity linking", "fangyu liu | ivan vuli\u0107 | anna korhonen | nigel collier", "injecting external domain-specific knowledge (e.g., umls) into pretrained language models (lms) advances their capability to handle specialised in-domain tasks such as biomedical entity linking (bel). however, such abundant expert knowledge is available only for a handful of languages (e.g., english). in this work, by proposing a novel cross-lingual biomedical entity linking task (xl-bel) and establishing a new xl-bel benchmark spanning 10 typologically diverse languages, we first investigate the ability of standard knowledge-agnostic as well as knowledge-enhanced monolingual and multilingual lms beyond the standard monolingual english bel task. the scores indicate large gaps to english performance. we then address the challenge of transferring domain-specific knowledge in resource-rich languages to resource-poor ones. to this end, we propose and evaluate a series of cross-lingual transfer methods for the xl-bel task, and demonstrate that general-domain bitext helps propagate the available english knowledge to languages with little to no in-domain data. remarkably, we show that our proposed domain-specific transfer methods yield consistent gains across all target languages, sometimes up to 20 precision@1 points, without any in-domain knowledge in the target language, and without any in-domain parallel data."], "semantics"], [["a training-free and reference-free summarization evaluation metric via centrality-weighted relevance and self-referenced redundancy", "wang chen | piji li | irwin king", "in recent years, reference-based and supervised summarization evaluation metrics have been widely explored. however, collecting human-annotated references and ratings are costly and time-consuming. to avoid these limitations, we propose a training-free and reference-free summarization evaluation metric. our metric consists of a centrality-weighted relevance score and a self-referenced redundancy score. the relevance score is computed between the pseudo reference built from the source document and the given summary, where the pseudo reference content is weighted by the sentence centrality to provide importance guidance. besides an f1-based relevance score, we also design an f\ud835\udefd-based variant that pays more attention to the recall score. as for the redundancy score of the summary, we compute a self-masked similarity score with the summary itself to evaluate the redundant information in the summary. finally, we combine the relevance and redundancy scores to produce the final evaluation score of the given summary. extensive experiments show that our methods can significantly outperform existing methods on both multi-document and single-document summarization evaluation. the source code is released at https://github.com/chen-wang-cuhk/training-free-and-ref-free-summ-evaluation."], "summarization"], [["towards quantifiable dialogue coherence evaluation", "zheng ye | liucun lu | lishan huang | liang lin | xiaodan liang", "automatic dialogue coherence evaluation has attracted increasing attention and is crucial for developing promising dialogue systems. however, existing metrics have two major limitations: (a) they are mostly trained in a simplified two-level setting (coherent vs. incoherent), while humans give likert-type multi-level coherence scores, dubbed as \u201cquantifiable\u201d; (b) their predicted coherence scores cannot align with the actual human rating standards due to the absence of human guidance during training. to address these limitations, we propose quantifiable dialogue coherence evaluation (quantidce), a novel framework aiming to train a quantifiable dialogue coherence metric that can reflect the actual human rating standards. specifically, quantidce includes two training stages, multi-level ranking (mlr) pre-training and knowledge distillation (kd) fine-tuning. during mlr pre-training, a new mlr loss is proposed for enabling the model to learn the coarse judgement of coherence degrees. then, during kd fine-tuning, the pretrained model is further finetuned to learn the actual human rating standards with only very few human-annotated data. to advocate the generalizability even with limited fine-tuning data, a novel kd regularization is introduced to retain the knowledge learned at the pre-training stage. experimental results show that the model trained by quantidce presents stronger correlations with human judgements than the other state-of-the-art metrics."], "resources and evaluation"]]
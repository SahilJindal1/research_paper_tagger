[[["meta-learning to compositionally generalize", "henry conklin | bailin wang | kenny smith | ivan titov", "natural language is compositional; the meaning of a sentence is a function of the meaning of its parts. this property allows humans to create and interpret novel sentences, generalizing robustly outside their prior experience. neural networks have been shown to struggle with this kind of generalization, in particular performing poorly on tasks designed to assess compositional generalization (i.e. where training and testing distributions differ in ways that would be trivial for a compositional strategy to resolve). their poor performance on these tasks may in part be due to the nature of supervised learning which assumes training and testing data to be drawn from the same distribution. we implement a meta-learning augmented version of supervised learning whose objective directly optimizes for out-of-distribution generalization. we construct pairs of tasks for meta-learning by sub-sampling existing training data. each pair of tasks is constructed to contain relevant examples, as determined by a similarity metric, in an effort to inhibit models from memorizing their input. experimental results on the cogs and scan datasets show that our similarity-driven meta-learning can improve generalization performance."], "semantics"], [["a neural transition-based joint model for disease named entity recognition and normalization", "zongcheng ji | tian xia | mei han | jing xiao", "disease is one of the fundamental entities in biomedical research. recognizing such entities from biomedical text and then normalizing them to a standardized disease vocabulary offer a tremendous opportunity for many downstream applications. previous studies have demonstrated that joint modeling of the two sub-tasks has superior performance than the pipelined counterpart. although the neural joint model based on multi-task learning framework has achieved state-of-the-art performance, it suffers from the boundary inconsistency problem due to the separate decoding procedures. moreover, it ignores the rich information (e.g., the text surface form) of each candidate concept in the vocabulary, which is quite essential for entity normalization. in this work, we propose a neural transition-based joint model to alleviate these two issues. we transform the end-to-end disease recognition and normalization task as an action sequence prediction task, which not only jointly learns the model with shared representations of the input, but also jointly searches the output by state transitions in one search space. moreover, we introduce attention mechanisms to take advantage of the text surface form of each candidate concept for better normalization performance. experimental results conducted on two publicly available datasets show the effectiveness of the proposed method."], "information extraction, retrieval and text mining"], [["polyjuice: generating counterfactuals for explaining, evaluating, and improving models", "tongshuang wu | marco tulio ribeiro | jeffrey heer | daniel weld", "while counterfactual examples are useful for analysis and training of nlp models, current generation methods either rely on manual labor to create very few counterfactuals, or only instantiate limited types of perturbations such as paraphrases or word substitutions. we present polyjuice, a general-purpose counterfactual generator that allows for control over perturbation types and locations, trained by finetuning gpt-2 on multiple datasets of paired sentences. we show that polyjuice produces diverse sets of realistic counterfactuals, which in turn are useful in various distinct applications: improving training and evaluation on three different tasks (with around 70% less annotation effort than manual generation), augmenting state-of-the-art explanation techniques, and supporting systematic counterfactual error analysis by revealing behaviors easily missed by human experts."], "generation"], [["beyond sentence-level end-to-end speech translation: context helps", "biao zhang | ivan titov | barry haddow | rico sennrich", "document-level contextual information has shown benefits to text-based machine translation, but whether and how context helps end-to-end (e2e) speech translation (st) is still under-studied. we fill this gap through extensive experiments using a simple concatenation-based context-aware st model, paired with adaptive feature selection on speech encodings for computational efficiency. we investigate several decoding approaches, and introduce in-model ensemble decoding which jointly performs document- and sentence-level translation using the same model. our results on the must-c benchmark with transformer demonstrate the effectiveness of context to e2e st. compared to sentence-level st, context-aware st obtains better translation quality (+0.18-2.61 bleu), improves pronoun and homophone translation, shows better robustness to (artificial) audio segmentation errors, and reduces latency and flicker to deliver higher quality for simultaneous translation."], "speech and multimodality"], [["ghostbert: generate more features with cheap operations for bert", "zhiqi huang | lu hou | lifeng shang | xin jiang | xiao chen | qun liu", "transformer-based pre-trained language models like bert, though powerful in many tasks, are expensive in both memory and computation, due to their large number of parameters. previous works show that some parameters in these models can be pruned away without severe accuracy drop. however, these redundant features contribute to a comprehensive understanding of the training data and removing them weakens the model\u2019s representation ability. in this paper, we propose ghostbert, which generates more features with very cheap operations from the remaining features. in this way, ghostbert has similar memory and computational cost as the pruned model, but enjoys much larger representation power. the proposed ghost module can also be applied to unpruned bert models to enhance their performance with negligible additional parameters and computation. empirical results on the glue benchmark on three backbone models (i.e., bert, roberta and electra) verify the efficacy of our proposed method."], "machine learning for nlp"], [["ernie-doc: a retrospective long-document modeling transformer", "siyu ding | junyuan shang | shuohuan wang | yu sun | hao tian | hua wu | haifeng wang", "transformers are not suited for processing long documents, due to their quadratically increasing memory and time consumption. simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. in this paper, we propose ernie-doc, a document-level language pretraining model based on recurrence transformers. two well-designed techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ernie-doc, which has a much longer effective context length, to capture the contextual information of a complete document. we pretrain ernie-doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective. various experiments were conducted on both english and chinese document-level tasks. ernie-doc improved the state-of-the-art language modeling result of perplexity to 16.8 on wikitext-103. moreover, it outperformed competitive pretraining models by a large margin on most language understanding tasks, such as text classification and question answering."], "machine learning for nlp"], [["pens: a dataset and generic framework for personalized news headline generation", "xiang ao | xiting wang | ling luo | ying qiao | qing he | xing xie", "in this paper, we formulate the personalized news headline generation problem whose goal is to output a user-specific title based on both a user\u2019s reading interests and a candidate news body to be exposed to her. to build up a benchmark for this problem, we publicize a large-scale dataset named pens (personalized news headlines). the training set is collected from user impressions logs of microsoft news, and the test set is manually created by hundreds of native speakers to enable a fair testbed for evaluating models in an offline mode. we propose a generic framework as a preparatory solution to our problem. at its heart, user preference is learned by leveraging the user behavioral data, and three kinds of user preference injections are proposed to personalize a text generator and establish personalized headlines. we investigate our dataset by implementing several state-of-the-art user modeling methods in our framework to demonstrate a benchmark score for the proposed dataset. the dataset is available at https://msnews.github.io/pens.html."], "generation"], [["irene: interpretable energy prediction for transformers", "qingqing cao | yash kumar lal | harsh trivedi | aruna balasubramanian | niranjan balasubramanian", "existing software-based energy measurements of nlp models are not accurate because they do not consider the complex interactions between energy consumption and model execution. we present irene, an interpretable and extensible energy prediction system that accurately predicts the inference energy consumption of a wide range of transformer-based nlp models. irene constructs a model tree graph that breaks down the nlp model into modules that are further broken down into low-level machine learning (ml) primitives. irene predicts the inference energy consumption of the ml primitives as a function of generalizable features and fine-grained runtime resource usage. irene then aggregates these low-level predictions recursively to predict the energy of each module and finally of the entire model. experiments across multiple transformer models show irene predicts inference energy consumption of transformer models with an error of under 7% compared to the ground truth. in contrast, existing energy models see an error of over 50%. we also show how irene can be used to conduct energy bottleneck analysis and to easily evaluate the energy impact of different architectural choices. we release the code and data at https://github.com/stonybrooknlp/irene."], "nlp applications"], [["recollection versus imagination: exploring human memory and cognition via neural language models", "maarten sap | eric horvitz | yejin choi | noah a. smith | james pennebaker", "we investigate the use of nlp as a measure of the cognitive processes involved in storytelling, contrasting imagination and recollection of events. to facilitate this, we collect and release hippocorpus, a dataset of 7,000 stories about imagined and recalled events. we introduce a measure of narrative flow and use this to examine the narratives for imagined and recalled events. additionally, we measure the differential recruitment of knowledge attributed to semantic memory versus episodic memory (tulving, 1972) for imagined and recalled storytelling by comparing the frequency of descriptions of general commonsense events with more specific realis events. our analyses show that imagined stories have a substantially more linear narrative flow, compared to recalled stories in which adjacent sentences are more disconnected. in addition, while recalled stories rely more on autobiographical events based on episodic memory, imagined stories express more commonsense knowledge based on semantic memory. finally, our measures reveal the effect of narrativization of memories in stories (e.g., stories about frequently recalled memories flow more linearly; bartlett, 1932). our findings highlight the potential of using nlp tools to study the traces of human cognition in language."], "linguistic theories, cognitive modeling and psycholinguistics"], [["chase: a large-scale and pragmatic chinese dataset for cross-database context-dependent text-to-sql", "jiaqi guo | ziliang si | yu wang | qian liu | ming fan | jian-guang lou | zijiang yang | ting liu", "the cross-database context-dependent text-to-sql (xdts) problem has attracted considerable attention in recent years due to its wide range of potential applications. however, we identify two biases in existing datasets for xdts: (1) a high proportion of context-independent questions and (2) a high proportion of easy sql queries. these biases conceal the major challenges in xdts to some extent. in this work, we present chase, a large-scale and pragmatic chinese dataset for xdts. it consists of 5,459 coherent question sequences (17,940 questions with their sql queries annotated) over 280 databases, in which only 35% of questions are context-independent, and 28% of sql queries are easy. we experiment on chase with three state-of-the-art xdts approaches. the best approach only achieves an exact match accuracy of 40% over all questions and 16% over all question sequences, indicating that chase highlights the challenging problems of xdts. we believe that xdts can provide fertile soil for addressing the problems."], "semantics"], [["structural information preserving for graph-to-text generation", "linfeng song | ante wang | jinsong su | yue zhang | kun xu | yubin ge | dong yu", "the task of graph-to-text generation aims at producing sentences that preserve the meaning of input graphs. as a crucial defect, the current state-of-the-art models may mess up or even drop the core structural information of input graphs when generating outputs. we propose to tackle this problem by leveraging richer training signals that can guide our model for preserving input information. in particular, we introduce two types of autoencoding losses, each individually focusing on different aspects (a.k.a. views) of input graphs. the losses are then back-propagated to better calibrate our model via multi-task training. experiments on two benchmarks for graph-to-text generation show the effectiveness of our approach over a state-of-the-art baseline."], "generation"], [["glancing transformer for non-autoregressive neural machine translation", "lihua qian | hao zhou | yu bao | mingxuan wang | lin qiu | weinan zhang | yong yu | lei li", "recent work on non-autoregressive neural machine translation (nat) aims at improving the efficiency by parallel decoding without sacrificing the quality. however, existing nat methods are either inferior to transformer or require multiple decoding passes, leading to reduced speedup. we propose the glancing language model (glm) for single-pass parallel generation models. with glm, we develop glancing transformer (glat) for machine translation. with only single-pass parallel decoding, glat is able to generate high-quality translation with 8\u00d7-15\u00d7 speedup. note that glat does not modify the network architecture, which is a training method to learn word interdependency. experiments on multiple wmt language directions show that glat outperforms all previous single pass non-autoregressive methods, and is nearly comparable to transformer, reducing the gap to 0.25-0.9 bleu points."], "machine translation and multilinguality"], [["bertgen: multi-task generation through bert", "faidon mitzalis | ozan caglayan | pranava madhyastha | lucia specia", "we present bertgen, a novel, generative, decoder-only model which extends bert by fusing multimodal and multilingual pre-trained models vl-bert and m-bert, respectively. bertgen is auto-regressively trained for language generation tasks, namely image captioning, machine translation and multimodal machine translation, under a multi-task setting. with a comprehensive set of evaluations, we show that bertgen outperforms many strong baselines across the tasks explored. we also show bertgen\u2019s ability for zero-shot language generation, where it exhibits competitive performance to supervised counterparts. finally, we conduct ablation studies which demonstrate that bertgen substantially benefits from multi-tasking and effectively transfers relevant inductive biases from the pre-trained models."], "generation"], [["oommix: out-of-manifold regularization in contextual embedding space for text classification", "seonghyeon lee | dongha lee | hwanjo yu", "recent studies on neural networks with pre-trained weights (i.e., bert) have mainly focused on a low-dimensional subspace, where the embedding vectors computed from input words (or their contexts) are located. in this work, we propose a new approach, called oommix, to finding and regularizing the remainder of the space, referred to as out-of-manifold, which cannot be accessed through the words. specifically, we synthesize the out-of-manifold embeddings based on two embeddings obtained from actually-observed words, to utilize them for fine-tuning the network. a discriminator is trained to detect whether an input embedding is located inside the manifold or not, and simultaneously, a generator is optimized to produce new embeddings that can be easily identified as out-of-manifold by the discriminator. these two modules successfully collaborate in a unified and end-to-end manner for regularizing the out-of-manifold. our extensive evaluation on various text classification benchmarks demonstrates the effectiveness of our approach, as well as its good compatibility with existing data augmentation techniques which aim to enhance the manifold."], "machine learning for nlp"], [["otters: one-turn topic transitions for open-domain dialogue", "karin sevegnani | david m. howcroft | ioannis konstas | verena rieser", "mixed initiative in open-domain dialogue requires a system to pro-actively introduce new topics. the one-turn topic transition task explores how a system connects two topics in a cooperative and coherent manner. the goal of the task is to generate a \u201cbridging\u201d utterance connecting the new topic to the topic of the previous conversation turn. we are especially interested in commonsense explanations of how a new topic relates to what has been mentioned before. we first collect a new dataset of human one-turn topic transitions, which we callotters. we then explore different strategies used by humans when asked to complete such a task, and notice that the use of a bridging utterance to connect the two topics is the approach used the most. we finally show how existing state-of-the-art text generation models can be adapted to this task and examine the performance of these baselines on different splits of the otters data."], "dialogue and interactive systems"], [["improving pretrained cross-lingual language models via self-labeled word alignment", "zewen chi | li dong | bo zheng | shaohan huang | xian-ling mao | heyan huang | furu wei", "the cross-lingual language models are typically pretrained with masked language modeling on multilingual text or parallel sentences. in this paper, we introduce denoising word alignment as a new cross-lingual pre-training task. specifically, the model first self-label word alignments for parallel sentences. then we randomly mask tokens in a bitext pair. given a masked token, the model uses a pointer network to predict the aligned token in the other language. we alternately perform the above two steps in an expectation-maximization manner. experimental results show that our method improves cross-lingual transferability on various datasets, especially on the token-level tasks, such as question answering, and structured prediction. moreover, the model can serve as a pretrained word aligner, which achieves reasonably low error rate on the alignment benchmarks. the code and pretrained parameters are available at github.com/czwin32768/xlm-align."], "machine translation and multilinguality"], [["ildc for cjpe: indian legal documents corpus for court judgment prediction and explanation", "vijit malik | rishabh sanjay | shubham kumar nigam | kripabandhu ghosh | shouvik kumar guha | arnab bhattacharya | ashutosh modi", "an automated system that could assist a judge in predicting the outcome of a case would help expedite the judicial process. for such a system to be practically useful, predictions by the system should be explainable. to promote research in developing such a system, we introduce ildc (indian legal documents corpus). ildc is a large corpus of 35k indian supreme court cases annotated with original court decisions. a portion of the corpus (a separate test set) is annotated with gold standard explanations by legal experts. based on ildc, we propose the task of court judgment prediction and explanation (cjpe). the task requires an automated system to predict an explainable outcome of a case. we experiment with a battery of baseline models for case predictions and propose a hierarchical occlusion based model for explainability. our best prediction model has an accuracy of 78% versus 94% for human legal experts, pointing towards the complexity of the prediction task. the analysis of explanations by the proposed algorithm reveals a significant difference in the point of view of the algorithm and legal experts for explaining the judgments, pointing towards scope for future research."], "resources and evaluation"], [["raw-c: relatedness of ambiguous words in context (a new lexical resource for english)", "sean trott | benjamin bergen", "most words are ambiguous\u2014-i.e., they convey distinct meanings in different contexts\u2014-and even the meanings of unambiguous words are context-dependent. both phenomena present a challenge for nlp. recently, the advent of contextualized word embeddings has led to success on tasks involving lexical ambiguity, such as word sense disambiguation. however, there are few tasks that directly evaluate how well these contextualized embeddings accommodate the more continuous, dynamic nature of word meaning\u2014-particularly in a way that matches human intuitions. we introduce raw-c, a dataset of graded, human relatedness judgments for 112 ambiguous words in context (with 672 sentence pairs total), as well as human estimates of sense dominance. the average inter-annotator agreement (assessed using a leave-one-annotator-out method) was 0.79. we then show that a measure of cosine distance, computed using contextualized embeddings from bert and elmo, correlates with human judgments, but that cosine distance also systematically underestimates how similar humans find uses of the same sense of a word to be, and systematically overestimates how similar humans find uses of different-sense homonyms. finally, we propose a synthesis between psycholinguistic theories of the mental lexicon and computational models of lexical semantics."], "resources and evaluation"], [["a tale of two perplexities: sensitivity of neural language models to lexical retrieval deficits in dementia of the alzheimer\u2019s type", "trevor cohen | serguei pakhomov", "in recent years there has been a burgeoning interest in the use of computational methods to distinguish between elicited speech samples produced by patients with dementia, and those from healthy controls. the difference between perplexity estimates from two neural language models (lms) - one trained on transcripts of speech produced by healthy participants and one trained on those with dementia - as a single feature for diagnostic classification of unseen transcripts has been shown to produce state-of-the-art performance. however, little is known about why this approach is effective, and on account of the lack of case/control matching in the most widely-used evaluation set of transcripts (dementiabank), it is unclear if these approaches are truly diagnostic, or are sensitive to other variables. in this paper, we interrogate neural lms trained on participants with and without dementia by using synthetic narratives previously developed to simulate progressive semantic dementia by manipulating lexical frequency. we find that perplexity of neural lms is strongly and differentially associated with lexical frequency, and that using a mixture model resulting from interpolating control and dementia lms improves upon the current state-of-the-art for models trained on transcript text exclusively."], "linguistic theories, cognitive modeling and psycholinguistics"], [["lgesql: line graph enhanced text-to-sql model with mixed local and non-local relations", "ruisheng cao | lu chen | zhi chen | yanbin zhao | su zhu | kai yu", "this work aims to tackle the challenging heterogeneous graph encoding problem in the text-to-sql task. previous methods are typically node-centric and merely utilize different weight matrices to parameterize edge types, which 1) ignore the rich semantics embedded in the topological structure of edges, and 2) fail to distinguish local and non-local relations for each node. to this end, we propose a line graph enhanced text-to-sql (lgesql) model to mine the underlying relational features without constructing meta-paths. by virtue of the line graph, messages propagate more efficiently through not only connections between nodes, but also the topology of directed edges. furthermore, both local and non-local relations are integrated distinctively during the graph iteration. we also design an auxiliary task called graph pruning to improve the discriminative capability of the encoder. our framework achieves state-of-the-art results (62.8% with glove, 72.0% with electra) on the cross-domain text-to-sql benchmark spider at the time of writing."], "semantics"], [["don\u2019t rule out monolingual speakers: a method for crowdsourcing machine translation data", "rajat bhatnagar | ananya ganesh | katharina kann", "high-performing machine translation (mt) systems can help overcome language barriers while making it possible for everyone to communicate and use language technologies in the language of their choice. however, such systems require large amounts of parallel sentences for training, and translators can be difficult to find and expensive. here, we present a data collection strategy for mt which, in contrast, is cheap and simple, as it does not require bilingual speakers. based on the insight that humans pay specific attention to movements, we use graphics interchange formats (gifs) as a pivot to collect parallel sentences from monolingual annotators. we use our strategy to collect data in hindi, tamil and english. as a baseline, we also collect data using images as a pivot. we perform an intrinsic evaluation by manually evaluating a subset of the sentence pairs and an extrinsic evaluation by finetuning mbart (liu et al., 2020) on the collected data. we find that sentences collected via gifs are indeed of higher quality."], "machine translation and multilinguality"], [["consistency regularization for cross-lingual fine-tuning", "bo zheng | li dong | shaohan huang | wenhui wang | zewen chi | saksham singhal | wanxiang che | ting liu | xia song | furu wei", "fine-tuning pre-trained cross-lingual language models can transfer task-specific supervision from one language to the others. in this work, we propose to improve cross-lingual fine-tuning with consistency regularization. specifically, we use example consistency regularization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, gaussian noise, code-switch substitution, and machine translation. in addition, we employ model consistency to regularize the models trained with two augmented versions of the same training set. experimental results on the xtreme benchmark show that our method significantly improves cross-lingual fine-tuning across various tasks, including text classification, question answering, and sequence labeling."], "machine translation and multilinguality"], [["modeling fine-grained entity types with box embeddings", "yasumasa onoe | michael boratko | andrew mccallum | greg durrett", "neural entity typing models typically represent fine-grained entity types as vectors in a high-dimensional space, but such spaces are not well-suited to modeling these types\u2019 complex interdependencies. we study the ability of box embeddings, which embed concepts as d-dimensional hyperrectangles, to capture hierarchies of types even when these relationships are not defined explicitly in the ontology. our model represents both types and entity mentions as boxes. each mention and its context are fed into a bert-based model to embed that mention in our box space; essentially, this model leverages typological clues present in the surface text to hypothesize a type representation for the mention. box containment can then be used to derive both the posterior probability of a mention exhibiting a given type and the conditional probability relations between types themselves. we compare our approach with a vector-based typing model and observe state-of-the-art performance on several entity typing benchmarks. in addition to competitive typing performance, our box-based model shows better performance in prediction consistency (predicting a supertype and a subtype together) and confidence (i.e., calibration), demonstrating that the box-based model captures the latent type hierarchies better than the vector-based model does."], "machine learning for nlp"], [["rejuvenating low-frequency words: making the most of parallel data in non-autoregressive translation", "liang ding | longyue wang | xuebo liu | derek f. wong | dacheng tao | zhaopeng tu", "knowledge distillation (kd) is commonly used to construct synthetic data for training non-autoregressive translation (nat) models. however, there exists a discrepancy on low-frequency words between the distilled and the original data, leading to more errors on predicting low-frequency words. to alleviate the problem, we directly expose the raw data into nat by leveraging pretraining. by analyzing directed alignments, we found that kd makes low-frequency source words aligned with targets more deterministically but fails to align sufficient low-frequency words from target to source. accordingly, we propose reverse kd to rejuvenate more alignments for low-frequency target words. to make the most of authentic and synthetic data, we combine these complementary approaches as a new training strategy for further boosting nat performance. we conduct experiments on five translation benchmarks over two advanced architectures. results demonstrate that the proposed approach can significantly and universally improve translation quality by reducing translation errors on low-frequency words. encouragingly, our approach achieves 28.2 and 33.9 bleu points on the wmt14 english-german and wmt16 romanian-english datasets, respectively. our code, data, and trained models are available at https://github.com/longyuewangdcu/rlfw-nat."], "machine translation and multilinguality"], [["nested named entity recognition via explicitly excluding the influence of the best path", "yiran wang | hiroyuki shindo | yuji matsumoto | taro watanabe", "this paper presents a novel method for nested named entity recognition. as a layered method, our method extends the prior second-best path recognition method by explicitly excluding the influence of the best path. our method maintains a set of hidden states at each time step and selectively leverages them to build a different potential function for recognition at each level. in addition, we demonstrate that recognizing innermost entities first results in better performance than the conventional outermost entities first scheme. we provide extensive experimental results on ace2004, ace2005, and genia datasets to show the effectiveness and efficiency of our proposed method."], "information extraction, retrieval and text mining"], [["exploring listwise evidence reasoning with t5 for fact verification", "kelvin jiang | ronak pradeep | jimmy lin", "this work explores a framework for fact verification that leverages pretrained sequence-to-sequence transformer models for sentence selection and label prediction, two key sub-tasks in fact verification. most notably, improving on previous pointwise aggregation approaches for label prediction, we take advantage of t5 using a listwise approach coupled with data augmentation. with this enhancement, we observe that our label prediction stage is more robust to noise and capable of verifying complex claims by jointly reasoning over multiple pieces of evidence. experimental results on the fever task show that our system attains a fever score of 75.87% on the blind test set. this puts our approach atop the competitive fever leaderboard at the time of our work, scoring higher than the second place submission by almost two points in label accuracy and over one point in fever score."], "semantics"], [["bird\u2019s eye: probing for linguistic graph structures with a simple information-theoretic approach", "yifan hou | mrinmaya sachan", "nlp has a rich history of representing our prior understanding of language in the form of graphs. recent work on analyzing contextualized text representations has focused on hand-designed probe models to understand how and to what extent do these representations encode a particular linguistic phenomenon. however, due to the inter-dependence of various phenomena and randomness of training probe models, detecting how these representations encode the rich information in these linguistic graphs remains a challenging problem. in this paper, we propose a new information-theoretic probe, bird\u2019s eye, which is a fairly simple probe method for detecting if and how these representations encode the information in these linguistic graphs. instead of using model performance, our probe takes an information-theoretic view of probing and estimates the mutual information between the linguistic graph embedded in a continuous space and the contextualized word representations. furthermore, we also propose an approach to use our probe to investigate localized linguistic information in the linguistic graphs using perturbation analysis. we call this probing setup worm\u2019s eye. using these probes, we analyze the bert models on its ability to encode a syntactic and a semantic graph structure, and find that these models encode to some degree both syntactic as well as semantic information; albeit syntactic information to a greater extent."], "interpretability and analysis of models for nlp"], [["probabilistic, structure-aware algorithms for improved variety, accuracy, and coverage of amr alignments", "austin blodgett | nathan schneider", "we present algorithms for aligning components of abstract meaning representation (amr) graphs to spans in english sentences. we leverage unsupervised learning in combination with heuristics, taking the best of both worlds from previous amr aligners. our unsupervised models, however, are more sensitive to graph substructures, without requiring a separate syntactic parse. our approach covers a wider variety of amr substructures than previously considered, achieves higher coverage of nodes and edges, and does so with higher accuracy. we will release our leamr datasets and aligner for use in research on amr parsing, generation, and evaluation."], "semantics"], [["learning from the worst: dynamically generated datasets to improve online hate detection", "bertie vidgen | tristan thrush | zeerak waseem | douwe kiela", "we present a human-and-model-in-the-loop process for dynamically generating datasets and training better performing and more robust hate detection models. we provide a new dataset of 40,000 entries, generated and labelled by trained annotators over four rounds of dynamic data creation. it includes 15,000 challenging perturbations and each hateful entry has fine-grained labels for the type and target of hate. hateful entries make up 54% of the dataset, which is substantially higher than comparable datasets. we show that model performance is substantially improved using this approach. models trained on later rounds of data collection perform better on test sets and are harder for annotators to trick. they also have better performance on hatecheck, a suite of functional tests for online hate detection. we provide the code, dataset and annotation guidelines for other researchers to use."], "computational social science, social media and cultural analytics"], [["robustness testing of language understanding in task-oriented dialog", "jiexi liu | ryuichi takanobu | jiaxin wen | dazhen wan | hongguang li | weiran nie | cheng li | wei peng | minlie huang", "most language understanding models in task-oriented dialog systems are trained on a small amount of annotated training data, and evaluated in a small set from the same distribution. however, these models can lead to system failure or undesirable output when being exposed to natural language perturbation or variation in practice. in this paper, we conduct comprehensive evaluation and analysis with respect to the robustness of natural language understanding models, and introduce three important aspects related to language understanding in real-world dialog systems, namely, language variety, speech characteristics, and noise perturbation. we propose a model-agnostic toolkit laug to approximate natural language perturbations for testing the robustness issues in task-oriented dialog. four data augmentation approaches covering the three aspects are assembled in laug, which reveals critical robustness issues in state-of-the-art models. the augmented dataset through laug can be used to facilitate future research on the robustness testing of language understanding in task-oriented dialog."], "dialogue and interactive systems"], [["guiding teacher forcing with seer forcing for neural machine translation", "yang feng | shuhao gu | dengji guo | zhengxin yang | chenze shao", "although teacher forcing has become the main training paradigm for neural machine translation, it usually makes predictions only conditioned on past information, and hence lacks global planning for the future. to address this problem, we introduce another decoder, called seer decoder, into the encoder-decoder framework during training, which involves future information in target predictions. meanwhile, we force the conventional decoder to simulate the behaviors of the seer decoder via knowledge distillation. in this way, at test the conventional decoder can perform like the seer decoder without the attendance of it. experiment results on the chinese-english, english-german and english-romanian translation tasks show our method can outperform competitive baselines significantly and achieves greater improvements on the bigger data sets. besides, the experiments also prove knowledge distillation the best way to transfer knowledge from the seer decoder to the conventional decoder compared to adversarial learning and l2 regularization."], "machine translation and multilinguality"], [["multi-timeline summarization (mtls): improving timeline summarization by generating multiple summaries", "yi yu | adam jatowt | antoine doucet | kazunari sugiyama | masatoshi yoshikawa", "in this paper, we address a novel task, multiple timeline summarization (mtls), which extends the flexibility and versatility of time-line summarization (tls). given any collection of time-stamped news articles, mtls automatically discovers important yet different stories and generates a corresponding time-line for each story.to achieve this, we propose a novel unsupervised summarization framework based on two-stage affinity propagation. we also introduce a quantitative evaluation measure for mtls based on previoustls evaluation methods. experimental results show that our mtls framework demonstrates high effectiveness and mtls task can give bet-ter results than tls."], "summarization"], [["turn the combination lock: learnable textual backdoor attacks via word substitution", "fanchao qi | yuan yao | sophia xu | zhiyuan liu | maosong sun", "recent studies show that neural natural language processing (nlp) models are vulnerable to backdoor attacks. injected with backdoors, models perform normally on benign examples but produce attacker-specified predictions when the backdoor is activated, presenting serious security threats to real-world applications. since existing textual backdoor attacks pay little attention to the invisibility of backdoors, they can be easily detected and blocked. in this work, we present invisible backdoors that are activated by a learnable combination of word substitution. we show that nlp models can be injected with backdoors that lead to a nearly 100% attack success rate, whereas being highly invisible to existing defense strategies and even human inspections. the results raise a serious alarm to the security of nlp models, which requires further research to be resolved. all the data and code of this paper are released at https://github.com/thunlp/bkdatk-lws."], "machine learning for nlp"], [["revisiting the negative data of distantly supervised relation extraction", "chenhao xie | jiaqing liang | jingping liu | chengsong huang | wenhao huang | yanghua xiao", "distantly supervision automatically generates plenty of training samples for relation extraction. however, it also incurs two major problems: noisy labels and imbalanced training data. previous works focus more on reducing wrongly labeled relations (false positives) while few explore the missing relations that are caused by incompleteness of knowledge base (false negatives). furthermore, the quantity of negative labels overwhelmingly surpasses the positive ones in previous problem formulations. in this paper, we first provide a thorough analysis of the above challenges caused by negative data. next, we formulate the problem of relation extraction into as a positive unlabeled learning task to alleviate false negative problem. thirdly, we propose a pipeline approach, dubbed rere, that first performs sentence classification with relational labels and then extracts the subjects/objects. experimental results show that the proposed method consistently outperforms existing approaches and remains excellent performance even learned with a large quantity of false positive samples. source code is available online at https://github.com/redreamality/rere-relation-extraction."], "information extraction, retrieval and text mining"], [["space efficient context encoding for non-task-oriented dialogue generation with graph attention transformer", "fabian galetzka | jewgeni rose | david schlangen | jens lehmann", "to improve the coherence and knowledge retrieval capabilities of non-task-oriented dialogue systems, recent transformer-based models aim to integrate fixed background context. this often comes in the form of knowledge graphs, and the integration is done by creating pseudo utterances through paraphrasing knowledge triples, added into the accumulated dialogue context. however, the context length is fixed in these architectures, which restricts how much background or dialogue context can be kept. in this work, we propose a more concise encoding for background context structured in the form of knowledge graphs, by expressing the graph connections through restrictions on the attention weights. the results of our human evaluation show that this encoding reduces space requirements without negative effects on the precision of reproduction of knowledge and perceived consistency. further, models trained with our proposed context encoding generate dialogues that are judged to be more comprehensive and interesting."], "dialogue and interactive systems"], [["happy dance, slow clap: using reaction gifs to predict induced affect on twitter", "boaz shmueli | soumya ray | lun-wei ku", "datasets with induced emotion labels are scarce but of utmost importance for many nlp tasks. we present a new, automated method for collecting texts along with their induced reaction labels. the method exploits the online use of reaction gifs, which capture complex affective states. we show how to augment the data with induced emotion and induced sentiment labels. we use our method to create and publish reactiongif, a first-of-its-kind affective dataset of 30k tweets. we provide baselines for three new tasks, including induced sentiment prediction and multilabel classification of induced emotions. our method and dataset open new research opportunities in emotion detection and affective computing."], "resources and evaluation"], [["annotating online misogyny", "philine zeinert | nanna inie | leon derczynski", "online misogyny, a category of online abusive language, has serious and harmful social consequences. automatic detection of misogynistic language online, while imperative, poses complicated challenges to both data gathering, data annotation, and bias mitigation, as this type of data is linguistically complex and diverse. this paper makes three contributions in this area: firstly, we describe the detailed design of our iterative annotation process and codebook. secondly, we present a comprehensive taxonomy of labels for annotating misogyny in natural written language, and finally, we introduce a high-quality dataset of annotated posts sampled from social media posts."], "resources and evaluation"], [["multimodal multi-speaker merger & acquisition financial modeling: a new task, dataset, and neural baselines", "ramit sawhney | mihir goyal | prakhar goel | puneet mathur | rajiv ratn shah", "risk prediction is an essential task in financial markets. merger and acquisition (m&a) calls provide key insights into the claims made by company executives about the restructuring of the financial firms. extracting vocal and textual cues from m&a calls can help model the risk associated with such financial activities. to aid the analysis of m&a calls, we curate a dataset of conference call transcripts and their corresponding audio recordings for the time period ranging from 2016 to 2020. we introduce m3anet, a baseline architecture that takes advantage of the multimodal multi-speaker input to forecast the financial risk associated with the m&a calls. empirical results prove that the task is challenging, with the pro-posed architecture performing marginally better than strong bert-based baselines. we release the m3a dataset and benchmark models to motivate future research on this challenging problem domain."], "nlp applications"], [["towards more equitable question answering systems: how much more data do you need?", "arnab debnath | navid rajabi | fardina fathmiul alam | antonios anastasopoulos", "question answering (qa) in english has been widely explored, but multilingual datasets are relatively new, with several methods attempting to bridge the gap between high- and low-resourced languages using data augmentation through translation and cross-lingual transfer. in this project we take a step back and study which approaches allow us to take the most advantage of existing resources in order to produce qa systems in many languages. specifically, we perform extensive analysis to measure the efficacy of few-shot approaches augmented with automatic translations and permutations of context-question-answer pairs. in addition, we make suggestions for future dataset development efforts that make better use of a fixed annotation budget, with a goal of increasing the language coverage of qa datasets and systems."], "question answering"], [["structural knowledge distillation: tractably distilling information for structured predictor", "xinyu wang | yong jiang | zhaohui yan | zixia jia | nguyen bach | tao wang | zhongqiang huang | fei huang | kewei tu", "knowledge distillation is a critical technique to transfer knowledge between models, typically from a large model (the teacher) to a more fine-grained one (the student). the objective function of knowledge distillation is typically the cross-entropy between the teacher and the student\u2019s output distributions. however, for structured prediction problems, the output space is exponential in size; therefore, the cross-entropy objective becomes intractable to compute and optimize directly. in this paper, we derive a factorized form of the knowledge distillation objective for structured prediction, which is tractable for many typical choices of the teacher and student models. in particular, we show the tractability and empirical effectiveness of structural knowledge distillation between sequence labeling and dependency parsing models under four different scenarios: 1) the teacher and student share the same factorization form of the output structure scoring function; 2) the student factorization produces more fine-grained substructures than the teacher factorization; 3) the teacher factorization produces more fine-grained substructures than the student factorization; 4) the factorization forms from the teacher and the student are incompatible."], "machine learning for nlp"], [["end-to-end lexically constrained machine translation for morphologically rich languages", "josef jon | jo\u00e3o paulo aires | dusan varis | ond\u0159ej bojar", "lexically constrained machine translation allows the user to manipulate the output sentence by enforcing the presence or absence of certain words and phrases. although current approaches can enforce terms to appear in the translation, they often struggle to make the constraint word form agree with the rest of the generated output. our manual analysis shows that 46% of the errors in the output of a baseline constrained model for english to czech translation are related to agreement. we investigate mechanisms to allow neural machine translation to infer the correct word inflection given lemmatized constraints. in particular, we focus on methods based on training the model with constraints provided as part of the input sequence. our experiments on english-czech language pair show that this approach improves translation of constrained terms in both automatic and manual evaluation by reducing errors in agreement. our approach thus eliminates inflection errors, without introducing new errors or decreasing overall quality of the translation."], "machine translation and multilinguality"], [["an exploratory analysis of multilingual word-level quality estimation with cross-lingual transformers", "tharindu ranasinghe | constantin orasan | ruslan mitkov", "most studies on word-level quality estimation (qe) of machine translation focus on language-specific models. the obvious disadvantages of these approaches are the need for labelled data for each language pair and the high cost required to maintain several language-specific models. to overcome these problems, we explore different approaches to multilingual, word-level qe. we show that multilingual qe models perform on par with the current language-specific models. in the cases of zero-shot and few-shot qe, we demonstrate that it is possible to accurately predict word-level quality for any given new language pair from models trained on other language pairs. our findings suggest that the word-level qe models based on powerful pre-trained transformers that we propose in this paper generalise well across languages, making them more useful in real-world scenarios."], "machine translation and multilinguality"], [["using adversarial attacks to reveal the statistical bias in machine reading comprehension models", "jieyu lin | jiajie zou | nai ding", "pre-trained language models have achieved human-level performance on many machine reading comprehension (mrc) tasks, but it remains unclear whether these models truly understand language or answer questions by exploiting statistical biases in datasets. here, we demonstrate a simple yet effective method to attack mrc models and reveal the statistical biases in these models. we apply the method to the race dataset, for which the answer to each mrc question is selected from 4 options. it is found that several pre-trained language models, including bert, albert, and roberta, show consistent preference to some options, even when these options are irrelevant to the question. when interfered by these irrelevant options, the performance of mrc models can be reduced from human-level performance to the chance-level performance. human readers, however, are not clearly affected by these irrelevant options. finally, we propose an augmented training method that can greatly reduce models\u2019 statistical biases."], "interpretability and analysis of models for nlp"], [["cross-lingual text classification with heterogeneous graph neural network", "ziyun wang | xuan liu | peiji yang | shixing liu | zhisheng wang", "cross-lingual text classification aims at training a classifier on the source language and transferring the knowledge to target languages, which is very useful for low-resource languages. recent multilingual pretrained language models (mplm) achieve impressive results in cross-lingual classification tasks, but rarely consider factors beyond semantic similarity, causing performance degradation between some language pairs. in this paper we propose a simple yet effective method to incorporate heterogeneous information within and across languages for cross-lingual text classification using graph convolutional networks (gcn). in particular, we construct a heterogeneous graph by treating documents and words as nodes, and linking nodes with different relations, which include part-of-speech roles, semantic similarity, and document translations. extensive experiments show that our graph-based method significantly outperforms state-of-the-art models on all tasks, and also achieves consistent performance gain over baselines in low-resource settings where external tools like translators are unavailable."], "machine translation and multilinguality"], [["descgen: a distantly supervised datasetfor generating entity descriptions", "weijia shi | mandar joshi | luke zettlemoyer", "short textual descriptions of entities provide summaries of their key attributes and have been shown to be useful sources of background knowledge for tasks such as entity linking and question answering. however, generating entity descriptions, especially for new and long-tail entities, can be challenging since relevant information is often scattered across multiple sources with varied content and style. we introduce descgen: given mentions spread over multiple documents, the goal is to generate an entity summary description. descgen consists of 37k entity descriptions from wikipedia and fandom, each paired with nine evidence documents on average. the documents were collected using a combination of entity linking and hyperlinks into the entity pages, which together provide high-quality distant supervision. compared to other multi-document summarization tasks, our task is entity-centric, more abstractive, and covers a wide range of domains. we also propose a two-stage extract-then-generate baseline and show that there exists a large gap (19.9% in rouge-l) between state-of-art models and human performance, suggesting that the data will support significant future work."], "summarization"], [["towards generative aspect-based sentiment analysis", "wenxuan zhang | xin li | yang deng | lidong bing | wai lam", "aspect-based sentiment analysis (absa) has received increasing attention recently. most existing work tackles absa in a discriminative manner, designing various task-specific classification networks for the prediction. despite their effectiveness, these methods ignore the rich label semantics in absa problems and require extensive task-specific designs. in this paper, we propose to tackle various absa tasks in a unified generative framework. two types of paradigms, namely annotation-style and extraction-style modeling, are designed to enable the training process by formulating each absa task as a text generation problem. we conduct experiments on four absa tasks across multiple benchmark datasets where our proposed generative approach achieves new state-of-the-art results in almost all cases. this also validates the strong generality of the proposed framework which can be easily adapted to arbitrary absa task without additional task-specific model design."], "sentiment analysis, stylistic analysis, and argument mining"], [["answering ambiguous questions through generative evidence fusion and round-trip prediction", "yifan gao | henghui zhu | patrick ng | cicero nogueira dos santos | zhiguo wang | feng nan | dejiao zhang | ramesh nallapati | andrew o. arnold | bing xiang", "in open-domain question answering, questions are highly likely to be ambiguous because users may not know the scope of relevant topics when formulating them. therefore, a system needs to find possible interpretations of the question, and predict one or multiple plausible answers. when multiple plausible answers are found, the system should rewrite the question for each answer to resolve the ambiguity. in this paper, we present a model that aggregates and combines evidence from multiple passages to adaptively predict a single answer or a set of question-answer pairs for ambiguous questions. in addition, we propose a novel round-trip prediction approach to iteratively generate additional interpretations that our model fails to find in the first pass, and then verify and filter out the incorrect question-answer pairs to arrive at the final disambiguated output. our model, named refuel, achieves a new state-of-the-art performance on the ambigqa dataset, and shows competitive performance on nq-open and triviaqa. the proposed round-trip prediction is a model-agnostic general approach for answering ambiguous open-domain questions, which improves our refuel as well as several baseline models. we release source code for our models and experiments at https://github.com/amzn/refuel-open-domain-qa."], "question answering"], [["self-guided contrastive learning for bert sentence representations", "taeuk kim | kang min yoo | sang-goo lee", "although bert and its variants have reshaped the nlp landscape, it still remains unclear how best to derive sentence embeddings from such pre-trained transformers. in this work, we propose a contrastive learning method that utilizes self-guidance for improving the quality of bert sentence representations. our method fine-tunes bert in a self-supervised fashion, does not rely on data augmentation, and enables the usual [cls] token embeddings to function as sentence vectors. moreover, we redesign the contrastive learning objective (nt-xent) and apply it to sentence representation learning. we demonstrate with extensive experiments that our approach is more effective than competitive baselines on diverse sentence-related tasks. we also show it is efficient at inference and robust to domain shifts."], "semantics"], [["predicting degrees of technicality in automatic terminology extraction", "anna h\u00e4tty | dominik schlechtweg | michael dorna | sabine schulte im walde", "while automatic term extraction is a well-researched area, computational approaches to distinguish between degrees of technicality are still understudied. we semi-automatically create a german gold standard of technicality across four domains, and illustrate the impact of a web-crawled general-language corpus on technicality prediction. when defining a classification approach that combines general-language and domain-specific word embeddings, we go beyond previous work and align vector spaces to gain comparative embeddings. we suggest two novel models to exploit general- vs. domain-specific comparisons: a simple neural network model with pre-computed comparative-embedding information as input, and a multi-channel model computing the comparison internally. both models outperform previous approaches, with the multi-channel model performing best."], "semantics"], [["learning latent structures for cross action phrase relations in wet lab protocols", "chaitanya kulkarni | jany chan | eric fosler-lussier | raghu machiraju", "wet laboratory protocols (wlps) are critical for conveying reproducible procedures in biological research. they are composed of instructions written in natural language describing the step-wise processing of materials by specific actions. this process flow description for reagents and materials synthesis in wlps can be captured by material state transfer graphs (mstgs), which encode global temporal and causal relationships between actions. here, we propose methods to automatically generate a mstg for a given protocol by extracting all action relationships across multiple sentences. we also note that previous corpora and methods focused primarily on local intra-sentence relationships between actions and entities and did not address two critical issues: (i) resolution of implicit arguments and (ii) establishing long-range dependencies across sentences. we propose a new model that incrementally learns latent structures and is better suited to resolving inter-sentence relations and implicit arguments. this model draws upon a new corpus wlp-mstg which was created by extending annotations in the wlp corpora for inter-sentence relations and implicit arguments. our model achieves an f1 score of 54.53% for temporal and causal relations in protocols from our corpus, which is a significant improvement over previous models - dygie++:28.17%; spert:27.81%. we make our annotated wlp-mstg corpus available to the research community."], "nlp applications"], [["gtm: a generative triple-wise model for conversational question generation", "lei shen | fandong meng | jinchao zhang | yang feng | jie zhou", "generating some appealing questions in open-domain conversations is an effective way to improve human-machine interactions and lead the topic to a broader or deeper direction. to avoid dull or deviated questions, some researchers tried to utilize answer, the \u201cfuture\u201d information, to guide question generation. however, they separate a post-question-answer (pqa) triple into two parts: post-question (pq) and question-answer (qa) pairs, which may hurt the overall coherence. besides, the qa relationship is modeled as a one-to-one mapping that is not reasonable in open-domain conversations. to tackle these problems, we propose a generative triple-wise model with hierarchical variations for open-domain conversational question generation (cqg). latent variables in three hierarchies are used to represent the shared background of a triple and one-to-many semantic mappings in both pq and qa pairs. experimental results on a large-scale cqg dataset show that our method significantly improves the quality of questions in terms of fluency, coherence and diversity over competitive baselines."], "dialogue and interactive systems"], [["investigating word-class distributions in word vector spaces", "ryohei sasano | anna korhonen", "this paper presents an investigation on the distribution of word vectors belonging to a certain word class in a pre-trained word vector space. to this end, we made several assumptions about the distribution, modeled the distribution accordingly, and validated each assumption by comparing the goodness of each model. specifically, we considered two types of word classes \u2013 the semantic class of direct objects of a verb and the semantic class in a thesaurus \u2013 and tried to build models that properly estimate how likely it is that a word in the vector space is a member of a given word class. our results on selectional preference and wordnet datasets show that the centroid-based model will fail to achieve good enough performance, the geometry of the distribution and the existence of subgroups will have limited impact, and also the negative instances need to be considered for adequate modeling of the distribution. we further investigated the relationship between the scores calculated by each model and the degree of membership and found that discriminative learning-based models are best in finding the boundaries of a class, while models based on the offset between positive and negative instances perform best in determining the degree of membership."], "semantics"], [["transferable dialogue systems and user simulators", "bo-hsiang tseng | yinpei dai | florian kreyssig | bill byrne", "one of the difficulties in training dialogue systems is the lack of training data. we explore the possibility of creating dialogue data through the interaction between a dialogue system and a user simulator. our goal is to develop a modelling framework that can incorporate new dialogue scenarios through self-play between the two agents. in this framework, we first pre-train the two agents on a collection of source domain dialogues, which equips the agents to converse with each other via natural language. with further fine-tuning on a small amount of target domain data, the agents continue to interact with the aim of improving their behaviors using reinforcement learning with structured reward functions. in experiments on the multiwoz dataset, two practical transfer learning problems are investigated: 1) domain adaptation and 2) single-to-multiple domain transfer. we demonstrate that the proposed framework is highly effective in bootstrapping the performance of the two agents in transfer learning. we also show that our method leads to improvements in dialogue system performance on complete datasets."], "dialogue and interactive systems"], [["zero-shot fact verification by claim generation", "liangming pan | wenhu chen | wenhan xiong | min-yen kan | william yang wang", "neural models for automated fact verification have achieved promising results thanks to the availability of large, human-annotated datasets. however, for each new domain that requires fact verification, creating a dataset by manually writing claims and linking them to their supporting evidence is expensive. we develop qacg, a framework for training a robust fact verification model by using automatically generated claims that can be supported, refuted, or unverifiable from evidence from wikipedia. qacg generates question-answer pairs from the evidence and then converts them into different types of claims. experiments on the fever dataset show that our qacg framework significantly reduces the demand for human-annotated training data. in a zero-shot scenario, qacg improves a roberta model\u2019s f1 from 50% to 77%, equivalent in performance to 2k+ manually-curated examples. our qacg code is publicly available."], "semantics"], [["unsupervised cross-domain prerequisite chain learning using variational graph autoencoders", "irene li | vanessa yan | tianxiao li | rihao qu | dragomir radev", "learning prerequisite chains is an important task for one to pick up knowledge efficiently in both known and unknown domains. for example, one may be an expert in the natural language processing (nlp) domain, but want to determine the best order in which to learn new concepts in an unfamiliar computer vision domain (cv). both domains share some common concepts, such as machine learning basics and deep learning models. in this paper, we solve the task of unsupervised cross-domain concept prerequisite chain learning, using an optimized variational graph autoencoder. our model learns to transfer concept prerequisite relations from an information-rich domain (source domain) to an information-poor domain (target domain), substantially surpassing other baseline models. in addition, we expand an existing dataset by introducing two new domains\u2014-cv and bioinformatics (bio). the annotated data and resources as well as the code will be made publicly available."], "nlp applications"], [["exploring discourse structures for argument impact classification", "xin liu | jiefu ou | yangqiu song | xin jiang", "discourse relations among arguments reveal logical structures of a debate conversation. however, no prior work has explicitly studied how the sequence of discourse relations influence a claim\u2019s impact. this paper empirically shows that the discourse relations between two arguments along the context path are essential factors for identifying the persuasive power of an argument. we further propose discoc to inject and fuse the sentence-level structural discourse information with contextualized features derived from large-scale language models. experimental results and extensive analysis show that the attention and gate mechanisms that explicitly model contexts and texts can indeed help the argument impact classification task defined by durmus et al. (2019), and discourse structures among the context path of the claim to be classified can further boost the performance."], "discourse and pragmatics"], [["comprehensive study: how the context information of different granularity affects dialogue state tracking?", "puhai yang | heyan huang | xian-ling mao", "dialogue state tracking (dst) plays a key role in task-oriented dialogue systems to monitor the user\u2019s goal. in general, there are two strategies to track a dialogue state: predicting it from scratch and updating it from previous state. the scratch-based strategy obtains each slot value by inquiring all the dialogue history, and the previous-based strategy relies on the current turn dialogue to update the previous dialogue state. however, it is hard for the scratch-based strategy to correctly track short-dependency dialogue state because of noise; meanwhile, the previous-based strategy is not very useful for long-dependency dialogue state tracking. obviously, it plays different roles for the context information of different granularity to track different kinds of dialogue states. thus, in this paper, we will study and discuss how the context information of different granularity affects dialogue state tracking. first, we explore how greatly different granularities affect dialogue state tracking. then, we further discuss how to combine multiple granularities for dialogue state tracking. finally, we apply the findings about context granularity to few-shot learning scenario. besides, we have publicly released all codes."], "dialogue and interactive systems"], [["automatic fake news detection: are models learning to reason?", "casper hansen | christian hansen | lucas chaves lima", "most fact checking models for automatic fake news detection are based on reasoning: given a claim with associated evidence, the models aim to estimate the claim veracity based on the supporting or refuting content within the evidence. when these models perform well, it is generally assumed to be due to the models having learned to reason over the evidence with regards to the claim. in this paper, we investigate this assumption of reasoning, by exploring the relationship and importance of both claim and evidence. surprisingly, we find on political fact checking datasets that most often the highest effectiveness is obtained by utilizing only the evidence, as the impact of including the claim is either negligible or harmful to the effectiveness. this highlights an important problem in what constitutes evidence in existing approaches for automatic fake news detection."], "computational social science, social media and cultural analytics"], [["question answering over temporal knowledge graphs", "apoorv saxena | soumen chakrabarti | partha talukdar", "temporal knowledge graphs (temporal kgs) extend regular knowledge graphs by providing temporal scopes (start and end times) on each edge in the kg. while question answering over kg (kgqa) has received some attention from the research community, qa over temporal kgs (temporal kgqa) is a relatively unexplored area. lack of broad coverage datasets has been another factor limiting progress in this area. we address this challenge by presenting cronquestions, the largest known temporal kgqa dataset, clearly stratified into buckets of structural complexity. cronquestions expands the only known previous dataset by a factor of 340x. we find that various state-of-the-art kgqa methods fall far short of the desired performance on this new dataset. in response, we also propose cronkgqa, a transformer-based solution that exploits recent advances in temporal kg embeddings, and achieves performance superior to all baselines, with an increase of 120% in accuracy over the next best performing method. through extensive experiments, we give detailed insights into the workings of cronkgqa, as well as situations where significant further improvements appear possible. in addition to the dataset, we have released our code as well."], "question answering"], [["enhancing entity boundary detection for better chinese named entity recognition", "chun chen | fang kong", "in comparison with english, due to the lack of explicit word boundary and tenses information, chinese named entity recognition (ner) is much more challenging. in this paper, we propose a boundary enhanced approach for better chinese ner. in particular, our approach enhances the boundary information from two perspectives. on one hand, we enhance the representation of the internal dependency of phrases by an additional graph attention network(gat) layer. on the other hand, taking the entity head-tail prediction (i.e., boundaries) as an auxiliary task, we propose an unified framework to learn the boundary information and recognize the ne jointly. experiments on both the ontonotes and the weibo corpora show the effectiveness of our approach."], "information extraction, retrieval and text mining"], [["readonce transformers: reusable representations of text for transformers", "shih-ting lin | ashish sabharwal | tushar khot", "we present readonce transformers, an approach to convert a transformer-based model into one that can build an information-capturing, task-independent, and compressed representation of text. the resulting representation is reusable across different examples and tasks, thereby requiring a document shared across many examples or tasks to only be read once. this leads to faster training and evaluation of models. additionally, we extend standard text-to-text transformer models to representation+text-to-text models, and evaluate on multiple downstream tasks: multi-hop qa, abstractive qa, and long-document summarization. our one-time computed representation results in a 2x-5x speedup compared to standard text-to-text models, while the compression also allows existing language models to handle longer documents without the need for designing new pre-trained models."], "semantics"], [["a unified approach to sentence segmentation of punctuated text in many languages", "rachel wicks | matt post", "the sentence is a fundamental unit of text processing. yet sentences in the wild are commonly encountered not in isolation, but unsegmented within larger paragraphs and documents. therefore, the first step in many nlp pipelines is sentence segmentation. despite its importance, this step is the subject of relatively little research. there are no standard test sets or even methods for evaluation, leaving researchers and engineers without a clear footing for evaluating and selecting models for the task. existing tools have relatively small language coverage, and efforts to extend them to other languages are often ad hoc. we introduce a modern context-based modeling approach that provides a solution to the problem of segmenting punctuated text in many languages, and show how it can be trained on noisily-annotated data. we also establish a new 23-language multilingual evaluation set. our approach exceeds high baselines set by existing methods on prior english corpora (wsj and brown corpora), and also performs well on average on our new evaluation set. we release our tool, ersatz, as open source."], "machine translation and multilinguality"], [["zero-shot event extraction via transfer learning: challenges and insights", "qing lyu | hongming zhang | elior sulem | dan roth", "event extraction has long been a challenging task, addressed mostly with supervised methods that require expensive annotation and are not extensible to new event ontologies. in this work, we explore the possibility of zero-shot event extraction by formulating it as a set of textual entailment (te) and/or question answering (qa) queries (e.g. \u201ca city was attacked\u201d entails \u201cthere is an attack\u201d), exploiting pretrained te/qa models for direct transfer. on ace-2005 and ere, our system achieves acceptable results, yet there is still a large gap from supervised approaches, showing that current qa and te technologies fail in transferring to a different domain. to investigate the reasons behind the gap, we analyze the remaining key challenges, their respective impact, and possible improvement directions."], "information extraction, retrieval and text mining"], [["hate speech detection based on sentiment knowledge sharing", "xianbing zhou | yang yong | xiaochao fan | ge ren | yunfeng song | yufeng diao | liang yang | hongfei lin", "the wanton spread of hate speech on the internet brings great harm to society and families. it is urgent to establish and improve automatic detection and active avoidance mechanisms for hate speech. while there exist methods for hate speech detection, they stereotype words and hence suffer from inherently biased training. in other words, getting more affective features from other affective resources will significantly affect the performance of hate speech detection. in this paper, we propose a hate speech detection framework based on sentiment knowledge sharing. while extracting the affective features of the target sentence itself, we make better use of the sentiment features from external resources, and finally fuse features from different feature extraction units to detect hate speech. experimental results on two public datasets demonstrate the effectiveness of our model."], "semantics"], [["lnn-el: a neuro-symbolic approach to short-text entity linking", "hang jiang | sairam gurajada | qiuhao lu | sumit neelam | lucian popa | prithviraj sen | yunyao li | alexander gray", "entity linking (el) is the task of disambiguating mentions appearing in text by linking them to entities in a knowledge graph, a crucial task for text understanding, question answering or conversational systems. in the special case of short-text el, which poses additional challenges due to limited context, prior approaches have reached good performance by employing heuristics-based methods or purely neural approaches. here, we take a different, neuro-symbolic approach that combines the advantages of using interpretable rules based on first-order logic with the performance of neural learning. even though constrained to use rules, we show that we reach competitive or better performance with sota black-box neural approaches. furthermore, our framework has the benefits of extensibility and transferability. we show that we can easily blend existing rule templates given by a human expert, with multiple types of features (priors, bert encodings, box embeddings, etc), and even with scores resulting from previous el methods, thus improving on such methods. as an example of improvement, on the lc-quad-1.0 dataset, we show more than 3% increase in f1 score relative to previous sota. finally, we show that the inductive bias offered by using logic results in a set of learned rules that transfers from one dataset to another, sometimes without finetuning, while still having high accuracy."], "information extraction, retrieval and text mining"], [["the summary loop: learning to write abstractive summaries without examples", "philippe laban | andrew hsi | john canny | marti a. hearst", "this work presents a new approach to unsupervised abstractive summarization based on maximizing a combination of coverage and fluency for a given length constraint. it introduces a novel method that encourages the inclusion of key terms from the original document into the summary: key terms are masked out of the original document and must be filled in by a coverage model using the current generated summary. a novel unsupervised training procedure leverages this coverage model along with a fluency model to generate and score summaries. when tested on popular news summarization datasets, the method outperforms previous unsupervised methods by more than 2 r-1 points, and approaches results of competitive supervised methods. our model attains higher levels of abstraction with copied passages roughly two times shorter than prior work, and learns to compress and merge sentences without supervision."], "summarization"], [["h-transformer-1d: fast one-dimensional hierarchical attention for sequences", "zhenhai zhu | radu soricut", "we describe an efficient hierarchical method to compute attention in the transformer architecture. the proposed attention mechanism exploits a matrix structure similar to the hierarchical matrix (h-matrix) developed by the numerical analysis community, and has linear run time and memory complexity. we perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. our method is superior to alternative sub-quadratic proposals by over +6 points on average on the long range arena benchmark. it also sets a new sota test perplexity on one-billion word dataset with 5x fewer model parameters than that of the previous-best transformer-based models."], "machine learning for nlp"], [["lexicon learning for few shot sequence modeling", "ekin akyurek | jacob andreas", "sequence-to-sequence transduction is the core problem in language processing applications as diverse as semantic parsing, machine translation, and instruction following. the neural network models that provide the dominant solution to these problems are brittle, especially in low-resource settings: they fail to generalize correctly or systematically from small datasets. past work has shown that many failures of systematic generalization arise from neural models\u2019 inability to disentangle lexical phenomena from syntactic ones. to address this, we augment neural decoders with a lexical translation mechanism that generalizes existing copy mechanisms to incorporate learned, decontextualized, token-level translation rules. we describe how to initialize this mechanism using a variety of lexicon learning algorithms, and show that it improves systematic generalization on a diverse set of sequence modeling tasks drawn from cognitive science, formal semantics, and machine translation."], "machine learning for nlp"], [["determinantal beam search", "clara meister | martina forster | ryan cotterell", "beam search is a go-to strategy for decoding neural sequence models. the algorithm can naturally be viewed as a subset optimization problem, albeit one where the corresponding set function does not reflect interactions between candidates. empirically, this leads to sets often exhibiting high overlap, e.g., strings may differ by only a single word. yet in use-cases that call for multiple solutions, a diverse or representative set is often desired. to address this issue, we propose a reformulation of beam search, which we call determinantal beam search. determinantal beam search has a natural relationship to determinantal point processes (dpps), models over sets that inherently encode intra-set interactions. by posing iterations in beam search as a series of subdeterminant maximization problems, we can turn the algorithm into a diverse subset selection process. in a case study, we use the string subsequence kernel to explicitly encourage n-gram coverage in text generated from a sequence model. we observe that our algorithm offers competitive performance against other diverse set generation strategies in the context of language generation, while providing a more general approach to optimizing for diversity."], "machine learning for nlp"], [["gwlan: general word-level autocompletion for computer-aided translation", "huayang li | lemao liu | guoping huang | shuming shi", "computer-aided translation (cat), the use of software to assist a human translator in the translation process, has been proven to be useful in enhancing the productivity of human translators. autocompletion, which suggests translation results according to the text pieces provided by human translators, is a core function of cat. there are two limitations in previous research in this line. first, most research works on this topic focus on sentence-level autocompletion (i.e., generating the whole translation as a sentence based on human input), but word-level autocompletion is under-explored so far. second, almost no public benchmarks are available for the autocompletion task of cat. this might be among the reasons why research progress in cat is much slower compared to automatic mt. in this paper, we propose the task of general word-level autocompletion (gwlan) from a real-world cat scenario, and construct the first public benchmark to facilitate research in this topic. in addition, we propose an effective method for gwlan and compare it with several strong baselines. experiments demonstrate that our proposed method can give significantly more accurate predictions than the baseline methods on our benchmark datasets."], "machine translation and multilinguality"], [["uncertainty and surprisal jointly deliver the punchline: exploiting incongruity-based features for humor recognition", "yubo xie | junze li | pearl pu", "humor recognition has been widely studied as a text classification problem using data-driven approaches. however, most existing work does not examine the actual joke mechanism to understand humor. we break down any joke into two distinct components: the set-up and the punchline, and further explore the special relationship between them. inspired by the incongruity theory of humor, we model the set-up as the part developing semantic uncertainty, and the punchline disrupting audience expectations. with increasingly powerful language models, we were able to feed the set-up along with the punchline into the gpt-2 language model, and calculate the uncertainty and surprisal values of the jokes. by conducting experiments on the semeval 2021 task 7 dataset, we found that these two features have better capabilities of telling jokes from non-jokes, compared with existing baselines."], "sentiment analysis, stylistic analysis, and argument mining"], [["an empirical comparison of unsupervised constituency parsing methods", "jun li | yifan cao | jiong cai | yong jiang | kewei tu", "unsupervised constituency parsing aims to learn a constituency parser from a training corpus without parse tree annotations. while many methods have been proposed to tackle the problem, including statistical and neural methods, their experimental results are often not directly comparable due to discrepancies in datasets, data preprocessing, lexicalization, and evaluation metrics. in this paper, we first examine experimental settings used in previous work and propose to standardize the settings for better comparability between methods. we then empirically compare several existing methods, including decade-old and newly proposed ones, under the standardized settings on english and japanese, two languages with different branching tendencies. we find that recent models do not show a clear advantage over decade-old models in our experiments. we hope our work can provide new insights into existing methods and facilitate future empirical evaluation of unsupervised constituency parsing."], "tagging, chunking, syntax and parsing"], [["meta-kd: a meta knowledge distillation framework for language model compression across domains", "haojie pan | chengyu wang | minghui qiu | yichang zhang | yaliang li | jun huang", "pre-trained language models have been applied to various nlp tasks with considerable performance gains. however, the large model sizes, together with the long inference time, limit the deployment of such models in real-time applications. one line of model compression approaches considers knowledge distillation to distill large teacher models into small student models. most of these studies focus on single-domain only, which ignores the transferable knowledge from other domains. we notice that training a teacher with transferable knowledge digested across domains can achieve better generalization capability to help knowledge distillation. hence we propose a meta-knowledge distillation (meta-kd) framework to build a meta-teacher model that captures transferable knowledge across domains and passes such knowledge to students. specifically, we explicitly force the meta-teacher to capture transferable knowledge at both instance-level and feature-level from multiple domains, and then propose a meta-distillation algorithm to learn single-domain student models with guidance from the meta-teacher. experiments on public multi-domain nlp tasks show the effectiveness and superiority of the proposed meta-kd framework. further, we also demonstrate the capability of meta-kd in the settings where the training data is scarce."], "nlp applications"], [["exact yet efficient graph parsing, bi-directional locality and the constructivist hypothesis", "yajie ye | weiwei sun", "a key problem in processing graph-based meaning representations is graph parsing, i.e. computing all possible derivations of a given graph according to a (competence) grammar. we demonstrate, for the first time, that exact graph parsing can be efficient for large graphs and with large hyperedge replacement grammars (hrgs). the advance is achieved by exploiting locality as terminal edge-adjacency in hrg rules. in particular, we highlight the importance of 1) a terminal edge-first parsing strategy, 2) a categorization of a subclass of hrg, i.e. what we call weakly regular graph grammar, and 3) distributing argument-structures to both lexical and phrasal rules."], "tagging, chunking, syntax and parsing"], [["distributed representations of emotion categories in emotion space", "xiangyu wang | chengqing zong", "emotion category is usually divided into different ones by human beings, but it is indeed difficult to clearly distinguish and define the boundaries between different emotion categories. the existing studies working on emotion detection usually focus on how to improve the performance of model prediction, in which emotions are represented with one-hot vectors. however, emotion relations are ignored in one-hot representations. in this article, we first propose a general framework to learn the distributed representations for emotion categories in emotion space from a given emotion classification dataset. furthermore, based on the soft labels predicted by the pre-trained neural network model, we derive a simple and effective algorithm. experiments have validated that the proposed representations in emotion space can express emotion relations much better than word vectors in semantic space."], "sentiment analysis, stylistic analysis, and argument mining"], [["language model augmented relevance score", "ruibo liu | jason wei | soroush vosoughi", "although automated metrics are commonly used to evaluate nlg systems, they often correlate poorly with human judgements. newer metrics such as bertscore have addressed many weaknesses in prior metrics such as bleu and rouge, which rely on n-gram matching. these newer methods, however, are still limited in that they do not consider the generation context, so they cannot properly reward generated text that is correct but deviates from the given reference. in this paper, we propose language model augmented relevance score (mars), a new context-aware metric for nlg evaluation. mars leverages off-the-shelf language models, guided by reinforcement learning, to create augmented references that consider both the generation context and available human references, which are then used as additional references to score generated text. compared with seven existing metrics in three common nlg tasks, mars not only achieves higher correlation with human reference judgements, but also differentiates well-formed candidates from adversarial samples to a larger degree."], "generation"], [["saying no is an art: contextualized fallback responses for unanswerable dialogue queries", "ashish shrivastava | kaustubh dhole | abhinav bhatt | sharvani raghunath", "despite end-to-end neural systems making significant progress in the last decade for task-oriented as well as chit-chat based dialogue systems, most dialogue systems rely on hybrid approaches which use a combination of rule-based, retrieval and generative approaches for generating a set of ranked responses. such dialogue systems need to rely on a fallback mechanism to respond to out-of-domain or novel user queries which are not answerable within the scope of the dialogue system. while, dialogue systems today rely on static and unnatural responses like \u201ci don\u2019t know the answer to that question\u201d or \u201ci\u2019m not sure about that\u201d, we design a neural approach which generates responses which are contextually aware with the user query as well as say no to the user. such customized responses provide paraphrasing ability and contextualization as well as improve the interaction with the user and reduce dialogue monotonicity. our simple approach makes use of rules over dependency parses and a text-to-text transformer fine-tuned on synthetic data of question-response pairs generating highly relevant, grammatical as well as diverse questions. we perform automatic and manual evaluations to demonstrate the efficacy of the system."], "dialogue and interactive systems"], [["redditbias: a real-world resource for bias evaluation and debiasing of conversational language models", "soumya barikeri | anne lauscher | ivan vuli\u0107 | goran glava\u0161", "text representation models are prone to exhibit a range of societal biases, reflecting the non-controlled and biased nature of the underlying pretraining data, which consequently leads to severe ethical issues and even bias amplification. recent work has predominantly focused on measuring and mitigating bias in pretrained language models. surprisingly, the landscape of bias measurements and mitigation resources and methods for conversational language models is still very scarce: it is limited to only a few types of bias, artificially constructed resources, and completely ignores the impact that debiasing methods may have on the final perfor mance in dialog tasks, e.g., conversational response generation. in this work, we present redditbias, the first conversational data set grounded in the actual human conversations from reddit, allowing for bias measurement and mitigation across four important bias dimensions: gender,race,religion, and queerness. further, we develop an evaluation framework which simultaneously 1)measures bias on the developed redditbias resource, and 2)evaluates model capability in dialog tasks after model debiasing. we use the evaluation framework to benchmark the widely used conversational dialogpt model along with the adaptations of four debiasing methods. our results indicate that dialogpt is biased with respect to religious groups and that some debiasing techniques can remove this bias while preserving downstream task performance."], "ethics in nlp"], [["on compositional generalization of neural machine translation", "yafu li | yongjing yin | yulong chen | yue zhang", "modern neural machine translation (nmt) models have achieved competitive performance in standard benchmarks such as wmt. however, there still exist significant issues such as robustness, domain generalization, etc. in this paper, we study nmt models from the perspective of compositional generalization by building a benchmark dataset, cognition, consisting of 216k clean and consistent sentence pairs. we quantitatively analyze effects of various factors using compound translation error rate, then demonstrate that the nmt model fails badly on compositional generalization, although it performs remarkably well under traditional metrics."], "machine translation and multilinguality"], [["a novel estimator of mutual information for learning to disentangle textual representations", "pierre colombo | pablo piantanida | chlo\u00e9 clavel", "learning disentangled representations of textual data is essential for many natural language tasks such as fair classification, style transfer and sentence generation, among others. the existent dominant approaches in the context of text data either rely on training an adversary (discriminator) that aims at making attribute values difficult to be inferred from the latent code or rely on minimising variational bounds of the mutual information between latent code and the value attribute. however, the available methods suffer of the impossibility to provide a fine-grained control of the degree (or force) of disentanglement. in contrast to adversarial methods, which are remarkably simple, although the adversary seems to be performing perfectly well during the training phase, after it is completed a fair amount of information about the undesired attribute still remains. this paper introduces a novel variational upper bound to the mutual information between an attribute and the latent code of an encoder. our bound aims at controlling the approximation error via the renyi\u2019s divergence, leading to both better disentangled representations and in particular, a precise control of the desirable degree of disentanglement than state-of-the-art methods proposed for textual data. furthermore, it does not suffer from the degeneracy of other losses in multi-class scenarios. we show the superiority of this method on fair classification and on textual style transfer tasks. additionally, we provide new insights illustrating various trade-offs in style transfer when attempting to learn disentangled representations and quality of the generated sentence."], "machine learning for nlp"], [["dense-caption matching and frame-selection gating for temporal localization in videoqa", "hyounghun kim | zineng tang | mohit bansal", "videos convey rich information. dynamic spatio-temporal relationships between people/objects, and diverse multimodal events are present in a video clip. hence, it is important to develop automated models that can accurately extract such information from videos. answering questions on videos is one of the tasks which can evaluate such ai abilities. in this paper, we propose a video question answering model which effectively integrates multi-modal input sources and finds the temporally relevant information to answer questions. specifically, we first employ dense image captions to help identify objects and their detailed salient regions and actions, and hence give the model useful extra information (in explicit textual format to allow easier matching) for answering questions. moreover, our model is also comprised of dual-level attention (word/object and frame level), multi-head self/cross-integration for different sources (video and dense captions), and gates which pass more relevant information to the classifier. finally, we also cast the frame selection problem as a multi-label classification task and introduce two loss functions, in-andout frame score margin (iofsm) and balanced binary cross-entropy (bbce), to better supervise the model with human importance annotations. we evaluate our model on the challenging tvqa dataset, where each of our model components provides significant gains, and our overall model outperforms the state-of-the-art by a large margin (74.09% versus 70.52%). we also present several word, object, and frame level visualization studies."], "language grounding to vision, robotics and beyond"], [["line graph enhanced amr-to-text generation with mix-order graph attention networks", "yanbin zhao | lu chen | zhi chen | ruisheng cao | su zhu | kai yu", "efficient structure encoding for graphs with labeled edges is an important yet challenging point in many graph-based models. this work focuses on amr-to-text generation \u2013 a graph-to-sequence task aiming to recover natural language from abstract meaning representations (amr). existing graph-to-sequence approaches generally utilize graph neural networks as their encoders, which have two limitations: 1) the message propagation process in amr graphs is only guided by the first-order adjacency information. 2) the relationships between labeled edges are not fully considered. in this work, we propose a novel graph encoding framework which can effectively explore the edge relations. we also adopt graph attention networks with higher-order neighborhood information to encode the rich structure in amr graphs. experiment results show that our approach obtains new state-of-the-art performance on english amr benchmark datasets. the ablation analyses also demonstrate that both edge relations and higher-order information are beneficial to graph-to-sequence modeling."], "generation"], [["mlbinet: a cross-sentence collective event detection network", "dongfang lou | zhilin liao | shumin deng | ningyu zhang | huajun chen", "we consider the problem of collectively detecting multiple events, particularly in cross-sentence settings. the key to dealing with the problem is to encode semantic information and model event inter-dependency at a document-level. in this paper, we reformulate it as a seq2seq task and propose a multi-layer bidirectional network (mlbinet) to capture the document-level association of events and semantic information simultaneously. specifically, a bidirectional decoder is firstly devised to model event inter-dependency within a sentence when decoding the event tag vector sequence. secondly, an information aggregation module is employed to aggregate sentence-level semantic and event tag information. finally, we stack multiple bidirectional decoders and feed cross-sentence information, forming a multi-layer bidirectional tagging architecture to iteratively propagate information across sentences. we show that our approach provides significant improvement in performance compared to the current state-of-the-art results."], "information extraction, retrieval and text mining"], [["discontinuous named entity recognition as maximal clique discovery", "yucheng wang | bowen yu | hongsong zhu | tingwen liu | nan yu | limin sun", "named entity recognition (ner) remains challenging when entity mentions can be discontinuous. existing methods break the recognition process into several sequential steps. in training, they predict conditioned on the golden intermediate results, while at inference relying on the model output of the previous steps, which introduces exposure bias. to solve this problem, we first construct a segment graph for each sentence, in which each node denotes a segment (a continuous entity on its own, or a part of discontinuous entities), and an edge links two nodes that belong to the same entity. the nodes and edges can be generated respectively in one stage with a grid tagging scheme and learned jointly using a novel architecture named mac. then discontinuous ner can be reformulated as a non-parametric process of discovering maximal cliques in the graph and concatenating the spans in each clique. experiments on three benchmarks show that our method outperforms the state-of-the-art (sota) results, with up to 3.5 percentage points improvement on f1, and achieves 5x speedup over the sota model."], "information extraction, retrieval and text mining"], [["structural guidance for transformer language models", "peng qian | tahira naseem | roger levy | ram\u00f3n fernandez astudillo", "transformer-based language models pre-trained on large amounts of text data have proven remarkably successful in learning generic transferable linguistic representations. here we study whether structural guidance leads to more human-like systematic linguistic generalization in transformer language models without resorting to pre-training on very large amounts of data. we explore two general ideas. the \u201cgenerative parsing\u201d idea jointly models the incremental parse and word sequence as part of the same sequence modeling task. the \u201cstructural scaffold\u201d idea guides the language model\u2019s representation via additional structure loss that separately predicts the incremental constituency parse. we train the proposed models along with a vanilla transformer language model baseline on a 14 million-token and a 46 million-token subset of the bllip dataset, and evaluate models\u2019 syntactic generalization performances on sg test suites and sized blimp. experiment results across two benchmarks suggest converging evidence that generative structural supervisions can induce more robust and humanlike linguistic generalization in transformer language models without the need for data intensive pre-training."], "linguistic theories, cognitive modeling and psycholinguistics"], [["machine translation into low-resource language varieties", "sachin kumar | antonios anastasopoulos | shuly wintner | yulia tsvetkov", "state-of-the-art machine translation (mt) systems are typically trained to generate \u201cstandard\u201d target language; however, many languages have multiple varieties (regional varieties, dialects, sociolects, non-native varieties) that are different from the standard language. such varieties are often low-resource, and hence do not benefit from contemporary nlp solutions, mt included. we propose a general framework to rapidly adapt mt systems to generate language varieties that are close to, but different from, the standard target language, using no parallel (source\u2013variety) data. this also includes adaptation of mt systems to low-resource typologically-related target languages. we experiment with adapting an english\u2013russian mt system to generate ukrainian and belarusian, an english\u2013norwegian bokm\u00e5l system to generate nynorsk, and an english\u2013arabic system to generate four arabic dialects, obtaining significant improvements over competitive baselines."], "machine translation and multilinguality"], [["position bias mitigation: a knowledge-aware graph model for emotion cause extraction", "hanqi yan | lin gui | gabriele pergola | yulan he", "the emotion cause extraction (ece) task aims to identify clauses which contain emotion-evoking information for a particular emotion expressed in text. we observe that a widely-used ece dataset exhibits a bias that the majority of annotated cause clauses are either directly before their associated emotion clauses or are the emotion clauses themselves. existing models for ece tend to explore such relative position information and suffer from the dataset bias. to investigate the degree of reliance of existing ece models on clause relative positions, we propose a novel strategy to generate adversarial examples in which the relative position information is no longer the indicative feature of cause clauses. we test the performance of existing models on such adversarial examples and observe a significant performance drop. to address the dataset bias, we propose a novel graph-based method to explicitly model the emotion triggering paths by leveraging the commonsense knowledge to enhance the semantic dependencies between a candidate clause and an emotion clause. experimental results show that our proposed approach performs on par with the existing state-of-the-art methods on the original ece dataset, and is more robust against adversarial attacks compared to existing models."], "sentiment analysis, stylistic analysis, and argument mining"], [["knowing the no-match: entity alignment with dangling cases", "zequn sun | muhao chen | wei hu", "this paper studies a new problem setting of entity alignment for knowledge graphs (kgs). since kgs possess different sets of entities, there could be entities that cannot find alignment across them, leading to the problem of dangling entities. as the first attempt to this problem, we construct a new dataset and design a multi-task learning framework for both entity alignment and dangling entity detection. the framework can opt to abstain from predicting alignment for the detected dangling entities. we propose three techniques for dangling entity detection that are based on the distribution of nearest-neighbor distances, i.e., nearest neighbor classification, marginal ranking and background ranking. after detecting and removing dangling entities, an incorporated entity alignment model in our framework can provide more robust alignment for remaining entities. comprehensive experiments and analyses demonstrate the effectiveness of our framework. we further discover that the dangling entity detection module can, in turn, improve alignment learning and the final performance. the contributed resource is publicly available to foster further research."], "information extraction, retrieval and text mining"], [["compare to the knowledge: graph neural fake news detection with external knowledge", "linmei hu | tianchi yang | luhao zhang | wanjun zhong | duyu tang | chuan shi | nan duan | ming zhou", "nowadays, fake news detection, which aims to verify whether a news document is trusted or fake, has become urgent and important. most existing methods rely heavily on linguistic and semantic features from the news content, and fail to effectively exploit external knowledge which could help determine whether the news document is trusted. in this paper, we propose a novel end-to-end graph neural model called comparenet, which compares the news to the knowledge base (kb) through entities for fake news detection. considering that fake news detection is correlated with topics, we also incorporate topics to enrich the news representation. specifically, we first construct a directed heterogeneous document graph for each news incorporating topics and entities. based on the graph, we develop a heterogeneous graph attention network for learning the topic-enriched news representation as well as the contextual entity representations that encode the semantics of the news content. the contextual entity representations are then compared to the corresponding kb-based entity representations through a carefully designed entity comparison network, to capture the consistency between the news content and kb. finally, the topic-enriched news representation combining the entity comparison features is fed into a fake news classifier. experimental results on two benchmark datasets demonstrate that comparenet significantly outperforms state-of-the-art methods."], "information extraction, retrieval and text mining"], [["defsent: sentence embeddings using definition sentences", "hayato tsukagoshi | ryohei sasano | koichi takeda", "sentence embedding methods using natural language inference (nli) datasets have been successfully applied to various tasks. however, these methods are only available for limited languages due to relying heavily on the large nli datasets. in this paper, we propose defsent, a sentence embedding method that uses definition sentences from a word dictionary, which performs comparably on unsupervised semantics textual similarity (sts) tasks and slightly better on senteval tasks than conventional methods. since dictionaries are available for many languages, defsent is more broadly applicable than methods using nli datasets without constructing additional datasets. we demonstrate that defsent performs comparably on unsupervised semantics textual similarity (sts) tasks and slightly better on senteval tasks to the methods using large nli datasets. our code is publicly available at https://github.com/hpprc/defsent."], "semantics"], [["poisoning knowledge graph embeddings via relation inference patterns", "peru bhardwaj | john kelleher | luca costabello | declan o\u2019sullivan", "we study the problem of generating data poisoning attacks against knowledge graph embedding (kge) models for the task of link prediction in knowledge graphs. to poison kge models, we propose to exploit their inductive abilities which are captured through the relationship patterns like symmetry, inversion and composition in the knowledge graph. specifically, to degrade the model\u2019s prediction confidence on target facts, we propose to improve the model\u2019s prediction confidence on a set of decoy facts. thus, we craft adversarial additions that can improve the model\u2019s prediction confidence on decoy facts through different inference patterns. our experiments demonstrate that the proposed poisoning attacks outperform state-of-art baselines on four kge models for two publicly available datasets. we also find that the symmetry pattern based attacks generalize across all model-dataset combinations which indicates the sensitivity of kge models to this pattern."], "interpretability and analysis of models for nlp"], [["multi-view cross-lingual structured prediction with minimum supervision", "zechuan hu | yong jiang | nguyen bach | tao wang | zhongqiang huang | fei huang | kewei tu", "in structured prediction problems, cross-lingual transfer learning is an efficient way to train quality models for low-resource languages, and further improvement can be obtained by learning from multiple source languages. however, not all source models are created equal and some may hurt performance on the target language. previous work has explored the similarity between source and target sentences as an approximate measure of strength for different source models. in this paper, we propose a multi-view framework, by leveraging a small number of labeled target sentences, to effectively combine multiple source models into an aggregated source view at different granularity levels (language, sentence, or sub-structure), and transfer it to a target view based on a task-specific model. by encouraging the two views to interact with each other, our framework can dynamically adjust the confidence level of each source model and improve the performance of both views during training. experiments for three structured prediction tasks on sixteen data sets show that our framework achieves significant improvement over all existing approaches, including these with access to additional source language data."], "tagging, chunking, syntax and parsing"], [["a large-scale chinese multimodal ner dataset with speech clues", "dianbo sui | zhengkun tian | yubo chen | kang liu | jun zhao", "in this paper, we aim to explore an uncharted territory, which is chinese multimodal named entity recognition (ner) with both textual and acoustic contents. to achieve this, we construct a large-scale human-annotated chinese multimodal ner dataset, named cnerta. our corpus totally contains 42,987 annotated sentences accompanying by 71 hours of speech data. based on this dataset, we propose a family of strong and representative baseline models, which can leverage textual features or multimodal features. upon these baselines, to capture the natural monotonic alignment between the textual modality and the acoustic modality, we further propose a simple multimodal multitask model by introducing a speech-to-text alignment auxiliary task. through extensive experiments, we observe that: (1) progressive performance boosts as we move from unimodal to multimodal, verifying the necessity of integrating speech clues into chinese ner. (2) our proposed model yields state-of-the-art (sota) results on cnerta, demonstrating its effectiveness. for further research, the annotated dataset is publicly available at http://github.com/dianbowork/cnerta."], "information extraction, retrieval and text mining"], [["kace: generating knowledge aware contrastive explanations for natural language inference", "qianglong chen | feng ji | xiangji zeng | feng-lin li | ji zhang | haiqing chen | yin zhang", "in order to better understand the reason behind model behaviors (i.e., making predictions), most recent works have exploited generative models to provide complementary explanations. however, existing approaches in nlp mainly focus on \u201cwhy a\u201d rather than contrastive \u201cwhy a not b\u201d, which is shown to be able to better distinguish confusing candidates and improve data efficiency in other research fields.in this paper, we focus on generating contrastive explanations with counterfactual examples in nli and propose a novel knowledge-aware contrastive explanation generation framework (kace).specifically, we first identify rationales (i.e., key phrases) from input sentences, and use them as key perturbations for generating counterfactual examples. after obtaining qualified counterfactual examples, we take them along with original examples and external knowledge as input, and employ a knowledge-aware generative pre-trained language model to generate contrastive explanations. experimental results show that contrastive explanations are beneficial to fit the scenarios by clarifying the difference between the predicted answer and other possible wrong ones. moreover, we train an nli model enhanced with contrastive explanations and achieves an accuracy of 91.9% on snli, gaining improvements of 5.7% against etpa (\u201cexplain-then-predict-attention\u201d) and 0.6% against nile (\u201cwhy a\u201d)."], "semantics"], [["multi-head highly parallelized lstm decoder for neural machine translation", "hongfei xu | qiuhui liu | josef van genabith | deyi xiong | meng zhang", "one of the reasons transformer translation models are popular is that self-attention networks for context modelling can be easily parallelized at sequence level. however, the computational complexity of a self-attention network is o(n2), increasing quadratically with sequence length. by contrast, the complexity of lstm-based approaches is only o(n). in practice, however, lstms are much slower to train than self-attention networks as they cannot be parallelized at sequence level: to model context, the current lstm state relies on the full lstm computation of the preceding state. this has to be computed n times for a sequence of length n. the linear transformations involved in the lstm gate and state computations are the major cost factors in this. to enable sequence-level parallelization of lstms, we approximate full lstm context modelling by computing hidden states and gates with the current input and a simple bag-of-words representation of the preceding tokens context. this allows us to compute each input step efficiently in parallel, avoiding the formerly costly sequential linear transformations. we then connect the outputs of each parallel step with computationally cheap element-wise computations. we call this the highly parallelized lstm. to further constrain the number of lstm parameters, we compute several small hplstms in parallel like multi-head attention in the transformer. the experiments show that our mhplstm decoder achieves significant bleu improvements, while being even slightly faster than the self-attention network in training, and much faster than the standard lstm."], "machine translation and multilinguality"], [["integrating semantics and neighborhood information with graph-driven generative models for document retrieval", "zijing ou | qinliang su | jianxing yu | bang liu | jingwen wang | ruihui zhao | changyou chen | yefeng zheng", "with the need of fast retrieval speed and small memory footprint, document hashing has been playing a crucial role in large-scale information retrieval. to generate high-quality hashing code, both semantics and neighborhood information are crucial. however, most existing methods leverage only one of them or simply combine them via some intuitive criteria, lacking a theoretical principle to guide the integration process. in this paper, we encode the neighborhood information with a graph-induced gaussian distribution, and propose to integrate the two types of information with a graph-driven generative model. to deal with the complicated correlations among documents, we further propose a tree-structured approximation method for learning. under the approximation, we prove that the training objective can be decomposed into terms involving only singleton or pairwise documents, enabling the model to be trained as efficiently as uncorrelated ones. extensive experimental results on three benchmark datasets show that our method achieves superior performance over state-of-the-art methods, demonstrating the effectiveness of the proposed model for simultaneously preserving semantic and neighborhood information."], "machine learning for nlp"], [["generating soap notes from doctor-patient conversations using modular summarization techniques", "kundan krishna | sopan khosla | jeffrey bigham | zachary c. lipton", "following each patient visit, physicians draft long semi-structured clinical summaries called soap notes. while invaluable to clinicians and researchers, creating digital soap notes is burdensome, contributing to physician burnout. in this paper, we introduce the first complete pipelines to leverage deep summarization models to generate these notes based on transcripts of conversations between physicians and patients. after exploring a spectrum of methods across the extractive-abstractive spectrum, we propose cluster2sent, an algorithm that (i) extracts important utterances relevant to each summary section; (ii) clusters together related utterances; and then (iii) generates one summary sentence per cluster. cluster2sent outperforms its purely abstractive counterpart by 8 rouge-1 points, and produces significantly more factual and coherent sentences as assessed by expert human evaluators. for reproducibility, we demonstrate similar benefits on the publicly available ami dataset. our results speak to the benefits of structuring summaries into sections and annotating supporting evidence when constructing summarization corpora."], "nlp applications"], [["self-attention networks can process bounded hierarchical languages", "shunyu yao | binghui peng | christos papadimitriou | karthik narasimhan", "despite their impressive performance in nlp, self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure, such as dyck-k, the language consisting of well-nested parentheses of k types. this suggested that natural language can be approximated well with models that are too weak for formal languages, or that the role of hierarchy and recursion in natural language might be limited. we qualify this implication by proving that self-attention networks can process dyck-(k, d), the subset of dyck-k with depth bounded by d, which arguably better captures the bounded hierarchical structure of natural language. specifically, we construct a hard-attention network with d+1 layers and o(log k) memory size (per token per layer) that recognizes dyck-(k, d), and a soft-attention network with two layers and o(log k) memory size that generates dyck-(k, d). experiments show that self-attention networks trained on dyck-(k, d) generalize to longer inputs with near-perfect accuracy, and also verify the theoretical memory advantage of self-attention networks over recurrent networks."], "machine learning for nlp"], [["improving arabic diacritization with regularized decoding and adversarial training", "han qin | guimin chen | yuanhe tian | yan song", "arabic diacritization is a fundamental task for arabic language processing. previous studies have demonstrated that automatically generated knowledge can be helpful to this task. however, these studies regard the auto-generated knowledge instances as gold references, which limits their effectiveness since such knowledge is not always accurate and inferior instances can lead to incorrect predictions. in this paper, we propose to use regularized decoding and adversarial training to appropriately learn from such noisy knowledge for diacritization. experimental results on two benchmark datasets show that, even with quite flawed auto-generated knowledge, our model can still learn adequate diacritics and outperform all previous studies, on both datasets."], "phonology, morphology and word segmentation"], [["neural syntactic preordering for controlled paraphrase generation", "tanya goyal | greg durrett", "paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly \u201creorder\u201d the source sentence and guide our neural paraphrasing model. first, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. this model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases."], "generation"], [["explanations for commonsenseqa: new dataset and models", "shourya aggarwal | divyanshu mandowara | vishwajeet agrawal | dinesh khandelwal | parag singla | dinesh garg", "commonsenseqa (cqa) (talmor et al., 2019) dataset was recently released to advance the research on common-sense question answering (qa) task. whereas the prior work has mostly focused on proposing qa models for this dataset, our aim is to retrieve as well as generate explanation for a given (question, correct answer choice, incorrect answer choices) tuple from this dataset. our explanation definition is based on certain desiderata, and translates an explanation into a set of positive and negative common-sense properties (aka facts) which not only explain the correct answer choice but also refute the incorrect ones. we human-annotate a first-of-its-kind dataset (called ecqa) of positive and negative properties, as well as free-flow explanations, for 11k qa pairs taken from the cqa dataset. we propose a latent representation based property retrieval model as well as a gpt-2 based property generation model with a novel two step fine-tuning procedure. we also propose a free-flow explanation generation model. extensive experiments show that our retrieval model beats bm25 baseline by a relative gain of 100% in f1 score, property generation model achieves a respectable f1 score of 36.4, and free-flow generation model achieves a similarity score of 61.9, where last two scores are based on a human correlated semantic similarity metric."], "question answering"], [["enhancing the generalization for intent classification and out-of-domain detection in slu", "yilin shen | yen-chang hsu | avik ray | hongxia jin", "intent classification is a major task in spoken language understanding (slu). since most models are built with pre-collected in-domain (ind) training utterances, their ability to detect unsupported out-of-domain (ood) utterances has a critical effect in practical use. recent works have shown that using extra data and labels can improve the ood detection performance, yet it could be costly to collect such data. this paper proposes to train a model with only ind data while supporting both ind intent classification and ood detection. our method designs a novel domain-regularized module (drm) to reduce the overconfident phenomenon of a vanilla classifier, achieving a better generalization in both cases. besides, drm can be used as a drop-in replacement for the last layer in any neural network-based intent classifier, providing a low-cost strategy for a significant improvement. the evaluation on four datasets shows that our method built on bert and roberta models achieves state-of-the-art performance against existing approaches and the strong baselines we created for the comparisons."], "dialogue and interactive systems"], [["edited media understanding frames: reasoning about the intent and implications of visual misinformation", "jeff da | maxwell forbes | rowan zellers | anthony zheng | jena d. hwang | antoine bosselut | yejin choi", "understanding manipulated media, from automatically generated \u2018deepfakes\u2019 to manually edited ones, raises novel research challenges. because the vast majority of edited or manipulated images are benign, such as photoshopped images for visual enhancements, the key challenge is to understand the complex layers of underlying intents of media edits and their implications with respect to disinformation. in this paper, we study edited media frames, a new formalism to understand visual media manipulation as structured annotations with respect to the intents, emotional reactions, attacks on individuals, and the overall implications of disinformation. we introduce a dataset for our task, emu, with 56k question-answer pairs written in rich natural language. we evaluate a wide variety of vision-and-language models for our task, and introduce a new model pelican, which builds upon recent progress in pretrained multimodal representations. our model obtains promising results on our dataset, with humans rating its answers as accurate 48.2% of the time. at the same time, there is still much work to be done \u2013 and we provide analysis that highlights areas for further progress."], "language grounding to vision, robotics and beyond"], [["weakly-supervised methods for suicide risk assessment: role of related domains", "chenghao yang | yudong zhang | smaranda muresan", "social media has become a valuable resource for the study of suicidal ideation and the assessment of suicide risk. among social media platforms, reddit has emerged as the most promising one due to its anonymity and its focus on topic-based communities (subreddits) that can be indicative of someone\u2019s state of mind or interest regarding mental health disorders such as r/suicidewatch, r/anxiety, r/depression. a challenge for previous work on suicide risk assessment has been the small amount of labeled data. we propose an empirical investigation into several classes of weakly-supervised approaches, and show that using pseudo-labeling based on related issues around mental health (e.g., anxiety, depression) helps improve model performance for suicide risk assessment."], "nlp applications"], [["missing modality imagination network for emotion recognition with uncertain missing modalities", "jinming zhao | ruichen li | qin jin", "multimodal fusion has been proved to improve emotion recognition performance in previous works. however, in real-world applications, we often encounter the problem of missing modality, and which modalities will be missing is uncertain. it makes the fixed multimodal fusion fail in such cases. in this work, we propose a unified model, missing modality imagination network (mmin), to deal with the uncertain missing modality problem. mmin learns robust joint multimodal representations, which can predict the representation of any missing modality given available modalities under different missing modality conditions.comprehensive experiments on two benchmark datasets demonstrate that the unified mmin model significantly improves emotion recognition performance under both uncertain missing-modality testing conditions and full-modality ideal testing condition. the code will be available at https://github.com/aim3-ruc/mmin."], "speech and multimodality"], [["multimet: a multimodal dataset for metaphor understanding", "dongyu zhang | minghao zhang | heting zhang | liang yang | hongfei lin", "metaphor involves not only a linguistic phenomenon, but also a cognitive phenomenon structuring human thought, which makes understanding it challenging. as a means of cognition, metaphor is rendered by more than texts alone, and multimodal information in which vision/audio content is integrated with the text can play an important role in expressing and understanding metaphor. however, previous metaphor processing and understanding has focused on texts, partly due to the unavailability of large-scale datasets with ground truth labels of multimodal metaphor. in this paper, we introduce multimet, a novel multimodal metaphor dataset to facilitate understanding metaphorical information from multimodal text and image. it contains 10,437 text-image pairs from a range of sources with multimodal annotations of the occurrence of metaphors, domain relations, sentiments metaphors convey, and author intents. multimet opens the door to automatic metaphor understanding by investigating multimodal cues and their interplay. moreover, we propose a range of strong baselines and show the importance of combining multimodal cues for metaphor understanding. multimet will be released publicly for research."], "resources and evaluation"], [["multi-hop graph convolutional network with high-order chebyshev approximation for text reasoning", "shuoran jiang | qingcai chen | xin liu | baotian hu | lisai zhang", "graph convolutional network (gcn) has become popular in various natural language processing (nlp) tasks with its superiority in long-term and non-consecutive word interactions. however, existing single-hop graph reasoning in gcn may miss some important non-consecutive dependencies. in this study, we define the spectral graph convolutional network with the high-order dynamic chebyshev approximation (hdgcn), which augments the multi-hop graph reasoning by fusing messages aggregated from direct and long-term dependencies into one convolutional layer. to alleviate the over-smoothing in high-order chebyshev approximation, a multi-vote-based cross-attention (mvcattn) with linear computation complexity is also proposed. the empirical results on four transductive and inductive nlp tasks and the ablation study verify the efficacy of the proposed model."], "machine learning for nlp"], [["knowledge supports visual language grounding: a case study on colour terms", "simeon sch\u00fcz | sina zarrie\u00df", "in human cognition, world knowledge supports the perception of object colours: knowing that trees are typically green helps to perceive their colour in certain contexts. we go beyond previous studies on colour terms using isolated colour swatches and study visual grounding of colour terms in realistic objects. our models integrate processing of visual information and object-specific knowledge via hard-coded (late) or learned (early) fusion. we find that both models consistently outperform a bottom-up baseline that predicts colour terms solely from visual inputs, but show interesting differences when predicting atypical colours of so-called colour diagnostic objects. our models also achieve promising results when tested on new object categories not seen during training."], "language grounding to vision, robotics and beyond"], [["making pre-trained language models better few-shot learners", "tianyu gao | adam fisch | danqi chen", "the recent gpt-3 model (brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. we present lm-bff\u2014better few-shot fine-tuning of language models\u2014a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. finally, we present a systematic evaluation for analyzing few-shot performance on a range of nlp tasks, including classification and regression. our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning."], "machine learning for nlp"], [["control image captioning spatially and temporally", "kun yan | lei ji | huaishao luo | ming zhou | nan duan | shuai ma", "generating image captions with user intention is an emerging need. the recently published localized narratives dataset takes mouse traces as another input to the image captioning task, which is an intuitive and efficient way for a user to control what to describe in the image. however, how to effectively employ traces to improve generation quality and controllability is still under exploration. this paper aims to solve this problem by proposing a novel model called loopcag, which connects contrastive constraints and attention guidance in a loop manner, engaged explicit spatial and temporal constraints to the generating process. precisely, each generated sentence is temporally aligned to the corresponding trace sequence through a contrastive learning strategy. besides, each generated text token is supervised to attend to the correct visual objects under heuristic spatial attention guidance. comprehensive experimental results demonstrate that our loopcag model learns better correspondence among the three modalities (vision, language, and traces) and achieves sota performance on trace-controlled image captioning task. moreover, the controllability and explainability of loopcag are validated by analyzing spatial and temporal sensitivity during the generation process."], "language grounding to vision, robotics and beyond"], [["unire: a unified label space for entity relation extraction", "yijun wang | changzhi sun | yuanbin wu | hao zhou | lei li | junchi yan", "many joint entity relation extraction models setup two separated label spaces for the two sub-tasks (i.e., entity detection and relation classification). we argue that this setting may hinder the information interaction between entities and relations. in this work, we propose to eliminate the different treatment on the two sub-tasks\u2019 label spaces. the input of our model is a table containing all word pairs from a sentence. entities and relations are represented by squares and rectangles in the table. we apply a unified classifier to predict each cell\u2019s label, which unifies the learning of two sub-tasks. for testing, an effective (yet fast) approximate decoder is proposed for finding squares and rectangles from tables. experiments on three benchmarks (ace04, ace05, scierc) show that, using only half the number of parameters, our model achieves competitive accuracy with the best extractor, and is faster."], "information extraction, retrieval and text mining"], [["bad seeds: evaluating lexical methods for bias measurement", "maria antoniak | david mimno", "a common factor in bias measurement methods is the use of hand-curated seed lexicons, but there remains little guidance for their selection. we gather seeds used in prior work, documenting their common sources and rationales, and in case studies of three english-language corpora, we enumerate the different types of social biases and linguistic features that, once encoded in the seeds, can affect subsequent bias measurements. seeds developed in one context are often re-used in other contexts, but documentation and evaluation remain necessary precursors to relying on seeds for sensitive measurements."], "ethics in nlp"], [["alignarr: aligning narratives on movies", "paramita mirza | mostafa abouhamra | gerhard weikum", "high-quality alignment between movie scripts and plot summaries is an asset for learning to summarize stories and to generate dialogues. the alignment task is challenging as scripts and summaries substantially differ in details and abstraction levels as well as in linguistic register. this paper addresses the alignment problem by devising a fully unsupervised approach based on a global optimization model. experimental results on ten movies show the viability of our method with 76% f1-score and its superiority over a previous baseline. we publish alignments for 914 movies to foster research in this new topic."], "resources and evaluation"], [["structurizing misinformation stories via rationalizing fact-checks", "shan jiang | christo wilson", "misinformation has recently become a well-documented matter of public concern. existing studies on this topic have hitherto adopted a coarse concept of misinformation, which incorporates a broad spectrum of story types ranging from political conspiracies to misinterpreted pranks. this paper aims to structurize these misinformation stories by leveraging fact-check articles. our intuition is that key phrases in a fact-check article that identify the misinformation type(s) (e.g., doctored images, urban legends) also act as rationales that determine the verdict of the fact-check (e.g., false). we experiment on rationalized models with domain knowledge as weak supervision to extract these phrases as rationales, and then cluster semantically similar rationales to summarize prevalent misinformation types. using archived fact-checks from snopes.com, we identify ten types of misinformation stories. we discuss how these types have evolved over the last ten years and compare their prevalence between the 2016/2020 us presidential elections and the h1n1/covid-19 pandemics."], "computational social science, social media and cultural analytics"], [["novel slot detection: a benchmark for discovering unknown slot types in the task-oriented dialogue system", "yanan wu | zhiyuan zeng | keqing he | hong xu | yuanmeng yan | huixing jiang | weiran xu", "existing slot filling models can only recognize pre-defined in-domain slot types from a limited slot set. in the practical application, a reliable dialogue system should know what it does not know. in this paper, we introduce a new task, novel slot detection (nsd), in the task-oriented dialogue system. nsd aims to discover unknown or out-of-domain slot types to strengthen the capability of a dialogue system based on in-domain training data. besides, we construct two public nsd datasets, propose several strong nsd baselines, and establish a benchmark for future work. finally, we conduct exhaustive experiments and qualitative analysis to comprehend key challenges and provide new guidance for future directions."], "dialogue and interactive systems"], [["explaining contextualization in language models using visual analytics", "rita sevastjanova | aikaterini-lida kalouli | christin beck | hanna sch\u00e4fer | mennatallah el-assady", "despite the success of contextualized language models on various nlp tasks, it is still unclear what these models really learn. in this paper, we contribute to the current efforts of explaining such models by exploring the continuum between function and content words with respect to contextualization in bert, based on linguistically-informed insights. in particular, we utilize scoring and visual analytics techniques: we use an existing similarity-based score to measure contextualization and integrate it into a novel visual analytics technique, presenting the model\u2019s layers simultaneously and highlighting intra-layer properties and inter-layer differences. we show that contextualization is neither driven by polysemy nor by pure context variation. we also provide insights on why bert fails to model words in the middle of the functionality continuum."], "interpretability and analysis of models for nlp"], [["dissecting generation modes for abstractive summarization models via ablation and attribution", "jiacheng xu | greg durrett", "despite the prominence of neural abstractive summarization models, we know little about how they actually form summaries and how to understand where their decisions come from. we propose a two-step method to interpret summarization model decisions. we first analyze the model\u2019s behavior by ablating the full model to categorize each decoder decision into one of several generation modes: roughly, is the model behaving like a language model, is it relying heavily on the input, or is it somewhere in between? after isolating decisions that do depend on the input, we explore interpreting these decisions using several different attribution methods. we compare these techniques based on their ability to select content and reconstruct the model\u2019s predicted token from perturbations of the input, thus revealing whether highlighted attributions are truly important for the generation of the next token. while this machinery can be broadly useful even beyond summarization, we specifically demonstrate its capability to identify phrases the summarization model has memorized and determine where in the training pipeline this memorization happened, as well as study complex generation phenomena like sentence fusion on a per-instance basis."], "summarization"], [["plotcoder: hierarchical decoding for synthesizing visualization code in programmatic context", "xinyun chen | linyuan gong | alvin cheung | dawn song", "creating effective visualization is an important part of data analytics. while there are many libraries for creating visualization, writing such code remains difficult given the myriad of parameters that users need to provide. in this paper, we propose the new task of synthesizing visualization programs from a combination of natural language utterances and code context. to tackle the learning problem, we introduce plotcoder, a new hierarchical encoder-decoder architecture that models both the code context and the input utterance. we use plotcoder to first determine the template of the visualization code, followed by predicting the data to be plotted. we use jupyter notebooks containing visualization programs crawled from github to train plotcoder. on a comprehensive set of test samples from those notebooks, we show that plotcoder correctly predicts the plot type of about 70% samples, and synthesizes the correct programs for 35% samples, performing 3-4.5% better than the baselines."], "nlp applications"], [["on the effectiveness of adapter-based tuning for pretrained language model adaptation", "ruidan he | linlin liu | hai ye | qingyu tan | bosheng ding | liying cheng | jiawei low | lidong bing | luo si", "adapter-based tuning has recently arisen as an alternative to fine-tuning. it works by adding light-weight adapter modules to a pretrained language model (prlm) and only updating the parameters of adapter modules when learning on a downstream task. as such, it adds only a few trainable parameters per new task, allowing a high degree of parameter sharing. prior studies have shown that adapter-based tuning often achieves comparable results to fine-tuning. however, existing work only focuses on the parameter-efficient aspect of adapter-based tuning while lacking further investigation on its effectiveness. in this paper, we study the latter. we first show that adapter-based tuning better mitigates forgetting issues than fine-tuning since it yields representations with less deviation from those generated by the initial prlm. we then empirically compare the two tuning methods on several downstream nlp tasks and settings. we demonstrate that 1) adapter-based tuning outperforms fine-tuning on low-resource and cross-lingual tasks; 2) it is more robust to overfitting and less sensitive to changes in learning rates."], "machine learning for nlp"], [["bridge-based active domain adaptation for aspect term extraction", "zhuang chen | tieyun qian", "as a fine-grained task, the annotation cost of aspect term extraction is extremely high. recent attempts alleviate this issue using domain adaptation that transfers common knowledge across domains. since most aspect terms are domain-specific, they cannot be transferred directly. existing methods solve this problem by associating aspect terms with pivot words (we call this passive domain adaptation because the transfer of aspect terms relies on the links to pivots). however, all these methods need either manually labeled pivot words or expensive computing resources to build associations. in this paper, we propose a novel active domain adaptation method. our goal is to transfer aspect terms by actively supplementing transferable knowledge. to this end, we construct syntactic bridges by recognizing syntactic roles as pivots instead of as links to pivots. we also build semantic bridges by retrieving transferable semantic prototypes. extensive experiments show that our method significantly outperforms previous approaches."], "sentiment analysis, stylistic analysis, and argument mining"], [["learning dense representations of phrases at scale", "jinhyuk lee | mujeen sung | jaewoo kang | danqi chen", "open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (seo et al., 2019). however, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. in this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain qa. we present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. we also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. on five popular open-domain qa datasets, our model densephrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on cpus. finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing densephrases as a dense knowledge base for downstream tasks."], "question answering"], [["check it again:progressive visual question answering via visual entailment", "qingyi si | zheng lin | ming yu zheng | peng fu | weiping wang", "while sophisticated neural-based models have achieved remarkable success in visual question answering (vqa), these models tend to answer questions only according to superficial correlations between question and answer. several recent approaches have been developed to address this language priors problem. however, most of them predict the correct answer according to one best output without checking the authenticity of answers. besides, they only explore the interaction between image and question, ignoring the semantics of candidate answers. in this paper, we propose a select-and-rerank (sar) progressive framework based on visual entailment. specifically, we first select the candidate answers relevant to the question or the image, then we rerank the candidate answers by a visual entailment task, which verifies whether the image semantically entails the synthetic statement of the question and each candidate answer. experimental results show the effectiveness of our proposed framework, which establishes a new state-of-the-art accuracy on vqa-cp v2 with a 7.55% improvement."], "question answering"], [["a hierarchical vae for calibrating attributes while generating text using normalizing flow", "bidisha samanta | mohit agrawal | niloy ganguly", "in this digital age, online users expect personalized content. to cater to diverse group of audiences across online platforms it is necessary to generate multiple variants of same content with differing degree of characteristics (sentiment, style, formality, etc.). though text-style transfer is a well explored related area, it focuses on flipping the style attribute polarity instead of regulating a fine-grained attribute transfer. in this paper we propose a hierarchical architecture for finer control over the at- tribute, preserving content using attribute dis- entanglement. we demonstrate the effective- ness of the generative process for two different attributes with varied complexity, namely sentiment and formality. with extensive experiments and human evaluation on five real-world datasets, we show that the framework can generate natural looking sentences with finer degree of control of intensity of a given attribute."], "sentiment analysis, stylistic analysis, and argument mining"], [["avoiding overlap in data augmentation for amr-to-text generation", "wenchao du | jeffrey flanigan", "leveraging additional unlabeled data to boost model performance is common practice in machine learning and natural language processing. for generation tasks, if there is overlap between the additional data and the target text evaluation data, then training on the additional data is training on answers of the test set. this leads to overly-inflated scores with the additional data compared to real-world testing scenarios and problems when comparing models. we study the amr dataset and gigaword, which is popularly used for improving amr-to-text generators, and find significant overlap between gigaword and a subset of the amr dataset. we propose methods for excluding parts of gigaword to remove this overlap, and show that our approach leads to a more realistic evaluation of the task of amr-to-text generation. going forward, we give simple best-practice recommendations for leveraging additional data in amr-to-text generation."], "generation"], [["learning to understand child-directed and adult-directed speech", "lieke gelderloos | grzegorz chrupa\u0142a | afra alishahi", "speech directed to children differs from adult-directed speech in linguistic aspects such as repetition, word choice, and sentence length, as well as in aspects of the speech signal itself, such as prosodic and phonemic variation. human language acquisition research indicates that child-directed speech helps language learners. this study explores the effect of child-directed speech when learning to extract semantic information from speech directly. we compare the task performance of models trained on adult-directed speech (ads) and child-directed speech (cds). we find indications that cds helps in the initial stages of learning, but eventually, models trained on ads reach comparable task performance, and generalize better. the results suggest that this is at least partially due to linguistic rather than acoustic properties of the two registers, as we see the same pattern when looking at models trained on acoustically comparable synthetic speech."], "linguistic theories, cognitive modeling and psycholinguistics"], [["text2event: controllable sequence-to-structure generation for end-to-end event extraction", "yaojie lu | hongyu lin | jin xu | xianpei han | jialong tang | annan li | le sun | meng liao | shaoyi chen", "event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. in this paper, we propose text2event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings."], "information extraction, retrieval and text mining"], [["zpr2: joint zero pronoun recovery and resolution using multi-task learning and bert", "linfeng song | kun xu | yue zhang | jianshu chen | dong yu", "zero pronoun recovery and resolution aim at recovering the dropped pronoun and pointing out its anaphoric mentions, respectively. we propose to better explore their interaction by solving both tasks together, while the previous work treats them separately. for zero pronoun resolution, we study this task in a more realistic setting, where no parsing trees or only automatic trees are available, while most previous work assumes gold trees. experiments on two benchmarks show that joint modeling significantly outperforms our baseline that already beats the previous state of the arts."], "discourse and pragmatics"], [["capturing event argument interaction via a bi-directional entity-level recurrent decoder", "xi xiangyu | wei ye | shikun zhang | quanxiu wang | huixing jiang | wei wu", "capturing interactions among event arguments is an essential step towards robust event argument extraction (eae). however, existing efforts in this direction suffer from two limitations: 1) the argument role type information of contextual entities is mainly utilized as training signals, ignoring the potential merits of directly adopting it as semantically rich input features; 2) the argument-level sequential semantics, which implies the overall distribution pattern of argument roles over an event mention, is not well characterized. to tackle the above two bottlenecks, we formalize eae as a seq2seq-like learning problem for the first time, where a sentence with a specific event trigger is mapped to a sequence of event argument roles. a neural architecture with a novel bi-directional entity-level recurrent decoder (berd) is proposed to generate argument roles by incorporating contextual entities\u2019 argument role predictions, like a word-by-word text generation process, thereby distinguishing implicit argument distribution patterns within an event more accurately."], "information extraction, retrieval and text mining"], [["automated concatenation of embeddings for structured prediction", "xinyu wang | yong jiang | nguyen bach | tao wang | zhongqiang huang | fei huang | kewei tu", "pretrained contextualized embeddings are powerful word representations for structured prediction tasks. recent work found that better word representations can be obtained by concatenating different types of embeddings. however, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. in this paper, we propose automated concatenation of embeddings (ace) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. we follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations."], "tagging, chunking, syntax and parsing"], [["investigating label suggestions for opinion mining in german covid-19 social media", "tilman beck | ji-ung lee | christina viehmann | marcus maurer | oliver quiring | iryna gurevych", "this work investigates the use of interactively updated label suggestions to improve upon the efficiency of gathering annotations on the task of opinion mining in german covid-19 social media data. we develop guidelines to conduct a controlled annotation study with social science students and find that suggestions from a model trained on a small, expert-annotated dataset already lead to a substantial improvement \u2013 in terms of inter-annotator agreement (+.14 fleiss\u2019 \u03ba) and annotation quality \u2013 compared to students that do not receive any label suggestions. we further find that label suggestions from interactively trained models do not lead to an improvement over suggestions from a static model. nonetheless, our analysis of suggestion bias shows that annotators remain capable of reflecting upon the suggested label in general. finally, we confirm the quality of the annotated data in transfer learning experiments between different annotator groups. to facilitate further research in opinion mining on social media data, we release our collected data consisting of 200 expert and 2,785 student annotations."], "computational social science, social media and cultural analytics"], [["point, disambiguate and copy: incorporating bilingual dictionaries for neural machine translation", "tong zhang | long zhang | wei ye | bo li | jinan sun | xiaoyu zhu | wen zhao | shikun zhang", "this paper proposes a sophisticated neural architecture to incorporate bilingual dictionaries into neural machine translation (nmt) models. by introducing three novel components: pointer, disambiguator, and copier, our method pdc achieves the following merits inherently compared with previous efforts: (1) pointer leverages the semantic information from bilingual dictionaries, for the first time, to better locate source words whose translation in dictionaries can potentially be used; (2) disambiguator synthesizes contextual information from the source view and the target view, both of which contribute to distinguishing the proper translation of a specific source word from multiple candidates in dictionaries; (3) copier systematically connects pointer and disambiguator based on a hierarchical copy mechanism seamlessly integrated with transformer, thereby building an end-to-end architecture that could avoid error propagation problems in alternative pipe-line methods. the experimental results on chinese-english and english-japanese benchmarks demonstrate the pdc\u2019s overall superiority and effectiveness of each component."], "machine translation and multilinguality"], [["unsupervised enrichment of persona-grounded dialog with background stories", "bodhisattwa prasad majumder | taylor berg-kirkpatrick | julian mcauley | harsh jhamtani", "humans often refer to personal narratives, life experiences, and events to make a conversation more engaging and rich. while persona-grounded dialog models are able to generate responses that follow a given persona, they often miss out on stating detailed experiences or events related to a persona, often leaving conversations shallow and dull. in this work, we equip dialog models with \u2018background stories\u2019 related to a persona by leveraging fictional narratives from existing story datasets (e.g. rocstories). since current dialog datasets do not contain such narratives as responses, we perform an unsupervised adaptation of a retrieved story for generating a dialog response using a gradient-based rewriting technique. our proposed method encourages the generated response to be fluent (i.e., highly likely) with the dialog history, minimally different from the retrieved story to preserve event ordering and consistent with the original persona. we demonstrate that our method can generate responses that are more diverse, and are rated more engaging and human-like by human evaluators, compared to outputs from existing dialog models."], "dialogue and interactive systems"], [["taming pre-trained language models with n-gram representations for low-resource domain adaptation", "shizhe diao | ruijia xu | hongjin su | yilei jiang | yan song | tong zhang", "large pre-trained models such as bert are known to improve different downstream nlp tasks, even when such a model is trained on a generic domain. moreover, recent studies have shown that when large domain-specific corpora are available, continued pre-training on domain-specific data can further improve the performance of in-domain tasks. however, this practice requires significant domain-specific data and computational resources which may not always be available. in this paper, we aim to adapt a generic pretrained model with a relatively small amount of domain-specific data. we demonstrate that by explicitly incorporating multi-granularity information of unseen and domain-specific words via the adaptation of (word based) n-grams, the performance of a generic pretrained model can be greatly improved. specifically, we introduce a transformer-based domain-aware n-gram adaptor, t-dna, to effectively learn and incorporate the semantic representation of different combinations of words in the new domain. experimental results illustrate the effectiveness of t-dna on eight low-resource downstream tasks from four domains. we show that t-dna is able to achieve significant improvements compared to existing methods on most tasks using limited data with lower computational costs. moreover, further analyses demonstrate the importance and effectiveness of both unseen words and the information of different granularities. our code is available at https://github.com/shizhediao/t-dna."], "semantics"], [["long text generation by modeling sentence-level and discourse-level coherence", "jian guan | xiaoxi mao | changjie fan | zitao liu | wenbiao ding | minlie huang", "generating long and coherent text is an important but challenging task, particularly for open-ended language generation tasks such as story generation. despite the success in modeling intra-sentence coherence, existing generation models (e.g., bart) still struggle to maintain a coherent event sequence throughout the generated text. we conjecture that this is because of the difficulty for the decoder to capture the high-level semantics and discourse structures in the context beyond token-level co-occurrence. in this paper, we propose a long text generation model, which can represent the prefix sentences at sentence level and discourse level in the decoding process. to this end, we propose two pretraining objectives to learn the representations by predicting inter-sentence semantic similarity and distinguishing between normal and shuffled sentence orders. extensive experiments show that our model can generate more coherent texts than state-of-the-art baselines."], "generation"], [["named entity recognition with small strongly labeled and large weakly labeled data", "haoming jiang | danqing zhang | tianyu cao | bing yin | tuo zhao", "weak supervision has shown promising results in many natural language processing tasks, such as named entity recognition (ner). existing work mainly focuses on learning deep ner models only with weak supervision, i.e., without any human annotation, and shows that by merely using weakly labeled data, one can achieve good performance, though still underperforms fully supervised ner with manually/strongly labeled data. in this paper, we consider a more practical scenario, where we have both a small amount of strongly labeled data and a large amount of weakly labeled data. unfortunately, we observe that weakly labeled data does not necessarily improve, or even deteriorate the model performance (due to the extensive noise in the weak labels) when we train deep ner models over a simple or weighted combination of the strongly labeled and weakly labeled data. to address this issue, we propose a new multi-stage computational framework \u2013 needle with three essential ingredients: (1) weak label completion, (2) noise-aware loss function, and (3) final fine-tuning over the strongly labeled data. through experiments on e-commerce query ner and biomedical ner, we demonstrate that needle can effectively suppress the noise of the weak labels and outperforms existing methods. in particular, we achieve new sota f1-scores on 3 biomedical ner datasets: bc5cdr-chem 93.74, bc5cdr-disease 90.69, ncbi-disease 92.28."], "information extraction, retrieval and text mining"], [["erica: improving entity and relation understanding for pre-trained language models via contrastive learning", "yujia qin | yankai lin | ryuichi takanobu | zhiyuan liu | peng li | heng ji | minlie huang | maosong sun | jie zhou", "pre-trained language models (plms) have shown superior performance on various downstream natural language processing (nlp) tasks. however, conventional pre-training objectives do not explicitly model relational facts in text, which are crucial for textual understanding. to address this issue, we propose a novel contrastive learning framework erica to obtain a deep understanding of the entities and their relations in text. specifically, we define two novel pre-training tasks to better understand entities and relations: (1) the entity discrimination task to distinguish which tail entity can be inferred by the given head entity and relation; (2) the relation discrimination task to distinguish whether two relations are close or not semantically, which involves complex relational reasoning. experimental results demonstrate that erica can improve typical plms (bert and roberta) on several language understanding tasks, including relation extraction, entity typing and question answering, especially under low-resource settings."], "semantics"], [["training adaptive computation for open-domain question answering with computational constraints", "yuxiang wu | pasquale minervini | pontus stenetorp | sebastian riedel", "adaptive computation (ac) has been shown to be effective in improving the efficiency of open-domain question answering (odqa) systems. however, the current ac approaches require tuning of all model parameters, and training state-of-the-art odqa models requires significant computational resources that may not be available for most researchers. we propose adaptive passage encoder, an ac method that can be applied to an existing odqa model and can be trained efficiently on a single gpu. it keeps the parameters of the base odqa model fixed, but it overrides the default layer-by-layer computation of the encoder with an ac policy that is trained to optimise the computational efficiency of the model. our experimental results show that our method improves upon a state-of-the-art model on two datasets, and is also more accurate than previous ac methods due to the stronger base odqa model. all source code and datasets are available at https://github.com/uclnlp/ape."], "question answering"], [["replicating and extending \u201cbecause their treebanks leak\u201d: graph isomorphism, covariants, and parser performance", "mark anderson | anders s\u00f8gaard | carlos g\u00f3mez-rodr\u00edguez", "s\u00f8gaard (2020) obtained results suggesting the fraction of trees occurring in the test data isomorphic to trees in the training set accounts for a non-trivial variation in parser performance. similar to other statistical analyses in nlp, the results were based on evaluating linear regressions. however, the study had methodological issues and was undertaken using a small sample size leading to unreliable results. we present a replication study in which we also bin sentences by length and find that only a small subset of sentences vary in performance with respect to graph isomorphism. further, the correlation observed between parser performance and graph isomorphism in the wild disappears when controlling for covariants. however, in a controlled experiment, where covariants are kept fixed, we do observe a correlation. we suggest that conclusions drawn from statistical analyses like this need to be tempered and that controlled experiments can complement them by more readily teasing factors apart."], "tagging, chunking, syntax and parsing"], [["few-nerd: a few-shot named entity recognition dataset", "ning ding | guangwei xu | yulin chen | xiaobin wang | xu han | pengjun xie | haitao zheng | zhiyuan liu", "recently, considerable literature has grown up around the theme of few-shot named entity recognition (ner), but little published benchmark data specifically focused on the practical and challenging task. current approaches collect existing supervised ner datasets and re-organize them to the few-shot setting for empirical study. these strategies conventionally aim to recognize coarse-grained entity types with few examples, while in practice, most unseen entity types are fine-grained. in this paper, we present few-nerd, a large-scale human-annotated few-shot ner dataset with a hierarchy of 8 coarse-grained and 66 fine-grained entity types. few-nerd consists of 188,238 sentences from wikipedia, 4,601,160 words are included and each is annotated as context or a part of the two-level entity type. to the best of our knowledge, this is the first few-shot ner dataset and the largest human-crafted ner dataset. we construct benchmark tasks with different emphases to comprehensively assess the generalization capability of models. extensive empirical results and analysis show that few-nerd is challenging and the problem requires further research. the few-nerd dataset and the baselines will be publicly available to facilitate the research on this problem."], "resources and evaluation"], [["learning relation alignment for calibrated cross-modal retrieval", "shuhuai ren | junyang lin | guangxiang zhao | rui men | an yang | jingren zhou | xu sun | hongxia yang", "despite the achievements of large-scale multimodal pre-training approaches, cross-modal retrieval, e.g., image-text retrieval, remains a challenging task. to bridge the semantic gap between the two modalities, previous studies mainly focus on word-region alignment at the object level, lacking the matching between the linguistic relation among the words and the visual relation among the regions. the neglect of such relation consistency impairs the contextualized representation of image-text pairs and hinders the model performance and the interpretability. in this paper, we first propose a novel metric, intra-modal self-attention distance (isd), to quantify the relation consistency by measuring the semantic distance between linguistic and visual relations. in response, we present inter-modal alignment on intra-modal self-attentions (iais), a regularized training method to optimize the isd and calibrate intra-modal self-attentions from the two modalities mutually via inter-modal alignment. the iais regularizer boosts the performance of prevailing models on flickr30k and ms coco datasets by a considerable margin, which demonstrates the superiority of our approach."], "language grounding to vision, robotics and beyond"], [["dual reader-parser on hybrid textual and tabular evidence for open domain question answering", "alexander hanbo li | patrick ng | peng xu | henghui zhu | zhiguo wang | bing xiang", "the current state-of-the-art generative models for open-domain question answering (odqa) have focused on generating direct answers from unstructured textual information. however, a large amount of world\u2019s knowledge is stored in structured databases, and need to be accessed using query languages such as sql. furthermore, query languages can answer questions that require complex reasoning, as well as offering full explainability. in this paper, we propose a hybrid framework that takes both textual and tabular evidences as input and generates either direct answers or sql queries depending on which form could better answer the question. the generated sql queries can then be executed on the associated databases to obtain the final answers. to the best of our knowledge, this is the first paper that applies text2sql to odqa tasks. empirically, we demonstrate that on several odqa datasets, the hybrid methods consistently outperforms the baseline models that only takes homogeneous input by a large margin. specifically we achieve the state-of-the-art performance on opensquad dataset using a t5-base model. in a detailed analysis, we demonstrate that the being able to generate structural sql queries can always bring gains, especially for those questions that requires complex reasoning."], "question answering"], [["herald: an annotation efficient method to detect user disengagement in social conversations", "weixin liang | kai-hui liang | zhou yu", "open-domain dialog systems have a user-centric goal: to provide humans with an engaging conversation experience. user engagement is one of the most important metrics for evaluating open-domain dialog systems, and could also be used as real-time feedback to benefit dialog policy learning. existing work on detecting user disengagement typically requires hand-labeling many dialog samples. we propose herald, an efficient annotation framework that reframes the training data annotation process as a denoising problem. specifically, instead of manually labeling training samples, we first use a set of labeling heuristics to label training samples automatically. we then denoise the weakly labeled data using the shapley algorithm. finally, we use the denoised data to train a user engagement detector. our experiments show that herald improves annotation efficiency significantly and achieves 86% user disengagement detection accuracy in two dialog corpora."], "dialogue and interactive systems"], [["self-training sampling with monolingual data uncertainty for neural machine translation", "wenxiang jiao | xing wang | zhaopeng tu | shuming shi | michael lyu | irwin king", "self-training has proven effective for improving nmt performance by augmenting model training with synthetic parallel data. the common practice is to construct synthetic data based on a randomly sampled subset of large-scale monolingual data, which we empirically show is sub-optimal. in this work, we propose to improve the sampling procedure by selecting the most informative monolingual sentences to complement the parallel data. to this end, we compute the uncertainty of monolingual sentences using the bilingual dictionary extracted from the parallel data. intuitively, monolingual sentences with lower uncertainty generally correspond to easy-to-translate patterns which may not provide additional gains. accordingly, we design an uncertainty-based sampling strategy to efficiently exploit the monolingual data for self-training, in which monolingual sentences with higher uncertainty would be sampled with higher probability. experimental results on large-scale wmt english\u21d2german and english\u21d2chinese datasets demonstrate the effectiveness of the proposed approach. extensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side."], "machine translation and multilinguality"], [["attentive multiview text representation for differential diagnosis", "hadi amiri | mitra mohtarami | isaac kohane", "we present a text representation approach that can combine different views (representations) of the same input through effective data fusion and attention strategies for ranking purposes. we apply our model to the problem of differential diagnosis, which aims to find the most probable diseases that match with clinical descriptions of patients, using data from the undiagnosed diseases network. our model outperforms several ranking approaches (including a commercially-supported system) by effectively prioritizing and combining representations obtained from traditional and recent text representation techniques. we elaborate on several aspects of our model and shed light on its improved performance."], "nlp applications"], [["a bidirectional transformer based alignment model for unsupervised word alignment", "jingyi zhang | josef van genabith", "word alignment and machine translation are two closely related tasks. neural translation models, such as rnn-based and transformer models, employ a target-to-source attention mechanism which can provide rough word alignments, but with a rather low accuracy. high-quality word alignment can help neural machine translation in many different ways, such as missing word detection, annotation transfer and lexicon injection. existing methods for learning word alignment include statistical word aligners (e.g. giza++) and recently neural word alignment models. this paper presents a bidirectional transformer based alignment (btba) model for unsupervised learning of the word alignment task. our btba model predicts the current target word by attending the source context and both left-side and right-side target context to produce accurate target-to-source attention (alignment). we further fine-tune the target-to-source attention in the btba model to obtain better alignments using a full context based optimization method and self-supervised training. we test our method on three word alignment tasks and show that our method outperforms both previous neural word alignment approaches and the popular statistical word aligner giza++."], "machine translation and multilinguality"], [["predicting the growth of morphological families from social and linguistic factors", "valentin hofmann | janet pierrehumbert | hinrich sch\u00fctze", "we present the first study that examines the evolution of morphological families, i.e., sets of morphologically related words such as \u201ctrump\u201d, \u201cantitrumpism\u201d, and \u201cdetrumpify\u201d, in social media. we introduce the novel task of morphological family expansion prediction (mfep) as predicting the increase in the size of a morphological family. we create a ten-year reddit corpus as a benchmark for mfep and evaluate a number of baselines on this benchmark. our experiments demonstrate very good performance on mfep."], "phonology, morphology and word segmentation"], [["contrastive learning for many-to-many multilingual neural machine translation", "xiao pan | mingxuan wang | liwei wu | lei li", "existing multilingual machine translation approaches mainly focus on english-centric directions, while the non-english directions still lag behind. in this work, we aim to build a many-to-many translation system with an emphasis on the quality of non-english language directions. our intuition is based on the hypothesis that a universal cross-language representation leads to better multilingual translation performance. to this end, we propose mrasp2, a training method to obtain a single unified multilingual translation model. mrasp2 is empowered by two techniques: a) a contrastive learning scheme to close the gap among representations of different languages, and b) data augmentation on both multiple parallel and monolingual data to further align token representations. for english-centric directions, mrasp2 achieves competitive or even better performance than a strong pre-trained model mbart on tens of wmt benchmarks. for non-english directions, mrasp2 achieves an improvement of average 10+ bleu compared with the multilingual baseline"], "machine translation and multilinguality"], [["unitedqa: a hybrid approach for open domain question answering", "hao cheng | yelong shen | xiaodong liu | pengcheng he | weizhu chen | jianfeng gao", "to date, most of recent work under the retrieval-reader framework for open-domain qa focuses on either extractive or generative reader exclusively. in this paper, we study a hybrid approach for leveraging the strengths of both models. we apply novel techniques to enhance both extractive and generative readers built upon recent pretrained neural language models, and find that proper training methods can provide large improvement over previous state-of-the-art models. we demonstrate that a simple hybrid approach by combining answers from both readers can efficiently take advantages of extractive and generative answer inference strategies and outperforms single models as well as homogeneous ensembles. our approach outperforms previous state-of-the-art models by 3.3 and 2.7 points in exact match on naturalquestions and triviaqa respectively."], "question answering"], [["discrete cosine transform as universal sentence encoder", "nada almarwani | mona diab", "modern sentence encoders are used to generate dense vector representations that capture the underlying linguistic characteristics for a sequence of words, including phrases, sentences, or paragraphs. these kinds of representations are ideal for training a classifier for an end task such as sentiment analysis, question answering and text classification. different models have been proposed to efficiently generate general purpose sentence representations to be used in pretraining protocols. while averaging is the most commonly used efficient sentence encoder, discrete cosine transform (dct) was recently proposed as an alternative that captures the underlying syntactic characteristics of a given text without compromising practical efficiency compared to averaging. however, as with most other sentence encoders, the dct sentence encoder was only evaluated in english. to this end, we utilize dct encoder to generate universal sentence representation for different languages such as german, french, spanish and russian. the experimental results clearly show the superior effectiveness of dct encoding in which consistent performance improvements are achieved over strong baselines on multiple standardized datasets"], "semantics"], [["catchphrase: automatic detection of cultural references", "nir sweed | dafna shahaf", "a snowclone is a customizable phrasal template that can be realized in multiple, instantly recognized variants. for example, \u201c* is the new *\" (orange is the new black, 40 is the new 30). snowclones are extensively used in social media. in this paper, we study snowclones originating from pop-culture quotes; our goal is to automatically detect cultural references in text. we introduce a new, publicly available data set of pop-culture quotes and their corresponding snowclone usages and train models on them. we publish code for catchphrase, an internet browser plugin to automatically detect and mark references in real-time, and examine its performance via a user study. aside from assisting people to better comprehend cultural references, we hope that detecting snowclones can complement work on paraphrasing and help tackling long-standing questions in social science about the dynamics of information propagation."], "computational social science, social media and cultural analytics"], [["conditional generation of temporally-ordered event sequences", "shih-ting lin | nathanael chambers | greg durrett", "models of narrative schema knowledge have proven useful for a range of event-related tasks, but they typically do not capture the temporal relationships between events. we propose a single model that addresses both temporal ordering, sorting given events into the order they occurred, and event infilling, predicting new events which fit into an existing temporally-ordered sequence. we use a bart-based conditional generation model that can capture both temporality and common event co-occurrence, meaning it can be flexibly applied to different tasks in this space. our model is trained as a denoising autoencoder: we take temporally-ordered event sequences, shuffle them, delete some events, and then attempt to recover the original event sequence. this task teaches the model to make inferences given incomplete knowledge about the events in an underlying scenario. on the temporal ordering task, we show that our model is able to unscramble event sequences from existing datasets without access to explicitly labeled temporal training data, outperforming both a bert-based pairwise model and a bert-based pointer network. on event infilling, human evaluation shows that our model is able to generate events that fit better temporally into the input events when compared to gpt-2 story completion models."], "semantics"], [["transs-driven joint learning architecture for implicit discourse relation recognition", "ruifang he | jian wang | fengyu guo | yugui han", "implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues. previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task, which have not fully exploited the annotated relation signal. therefore, we propose a novel transs-driven joint learning architecture to address the issues. specifically, based on the multi-level encoder, we 1) translate discourse relations in low-dimensional embedding space (called transs), which could mine the latent geometric structure information of argument-relation instances; 2) further exploit the semantic features of arguments to assist discourse understanding; 3) jointly learn 1) and 2) to mutually reinforce each other to obtain the better argument representations, so as to improve the performance of the task. extensive experimental results on the penn discourse treebank (pdtb) show that our model achieves competitive results against several state-of-the-art systems."], "discourse and pragmatics"], [["a survey of code-switching: linguistic and social perspectives for language technologies", "a. seza do\u011fru\u00f6z | sunayana sitaram | barbara e. bullock | almeida jacqueline toribio", "the analysis of data in which multiple languages are represented has gained popularity among computational linguists in recent years. so far, much of this research focuses mainly on the improvement of computational methods and largely ignores linguistic and social aspects of c-s discussed across a wide range of languages within the long-established literature in linguistics. to fill this gap, we offer a survey of code-switching (c-s) covering the literature in linguistics with a reflection on the key issues in language technologies. from the linguistic perspective, we provide an overview of structural and functional patterns of c-s focusing on the literature from european and indian contexts as highly multilingual areas. from the language technologies perspective, we discuss how massive language models fail to represent diverse c-s types due to lack of appropriate training data, lack of robust evaluation benchmarks for c-s (across multilingual situations and types of c-s) and lack of end-to- end systems that cover sociolinguistic aspects of c-s as well. our survey will be a step to- wards an outcome of mutual benefit for computational scientists and linguists with a shared interest in multilingualism and c-s."], "computational social science, social media and cultural analytics"], [["improved natural language generation via loss truncation", "daniel kang | tatsunori b. hashimoto", "neural language models are usually trained to match the distributional properties of large-scale corpora by minimizing the log loss. while straightforward to optimize, this approach forces the model to reproduce all variations in the dataset, including noisy and invalid references (e.g., misannotations and hallucinated facts). even a small fraction of noisy data can degrade the performance of log loss. as an alternative, prior work has shown that minimizing the distinguishability of generated samples is a principled and robust loss that can handle invalid references. however, distinguishability has not been used in practice due to challenges in optimization and estimation. we propose loss truncation: a simple and scalable procedure which adaptively removes high log loss examples as a way to optimize for distinguishability. empirically, we demonstrate that loss truncation outperforms existing baselines on distinguishability on a summarization task. furthermore, we show that samples generated by the loss truncation model have factual accuracy ratings that exceed those of baselines and match human references."], "generation"], [["continual learning for task-oriented dialogue system with iterative network pruning, expanding and masking", "binzong geng | fajie yuan | qiancheng xu | ying shen | ruifeng xu | min yang", "this ability to learn consecutive tasks without forgetting how to perform previously trained problems is essential for developing an online dialogue system. this paper proposes an effective continual learning method for the task-oriented dialogue system with iterative network pruning, expanding, and masking (tpem), which preserves performance on previously encountered tasks while accelerating learning progress on subsequent tasks. specifically, tpem (i) leverages network pruning to keep the knowledge for old tasks, (ii) adopts network expanding to create free weights for new tasks, and (iii) introduces task-specific network masking to alleviate the negative impact of fixed weights of old tasks on new tasks. we conduct extensive experiments on seven different tasks from three benchmark datasets and show empirically that tpem leads to significantly improved results over the strong competitors."], "dialogue and interactive systems"], [["knowledge-enriched event causality identification via latent structure induction networks", "pengfei cao | xinyu zuo | yubo chen | kang liu | jun zhao | yuguang chen | weihua peng", "identifying causal relations of events is an important task in natural language processing area. however, the task is very challenging, because event causality is usually expressed in diverse forms that often lack explicit causal clues. existing methods cannot handle well the problem, especially in the condition of lacking training data. nonetheless, humans can make a correct judgement based on their background knowledge, including descriptive knowledge and relational knowledge. inspired by it, we propose a novel latent structure induction network (lsin) to incorporate the external structural knowledge into this task. specifically, to make use of the descriptive knowledge, we devise a descriptive graph induction module to obtain and encode the graph-structured descriptive knowledge. to leverage the relational knowledge, we propose a relational graph induction module which is able to automatically learn a reasoning structure for event causality reasoning. experimental results on two widely used datasets indicate that our approach significantly outperforms previous state-of-the-art methods."], "information extraction, retrieval and text mining"], [["emailsum: abstractive email thread summarization", "shiyue zhang | asli celikyilmaz | jianfeng gao | mohit bansal", "recent years have brought about an interest in the challenging task of summarizing conversation threads (meetings, online discussions, etc.). such summaries help analysis of the long text to quickly catch up with the decisions made and thus improve our work or communication efficiency. to spur research in thread summarization, we have developed an abstractive email thread summarization (emailsum) dataset, which contains human-annotated short (<30 words) and long (<100 words) summaries of 2,549 email threads (each containing 3 to 10 emails) over a wide variety of topics. we perform a comprehensive empirical study to explore different summarization techniques (including extractive and abstractive methods, single-document and hierarchical models, as well as transfer and semisupervised learning) and conduct human evaluations on both short and long summary generation tasks. our results reveal the key challenges of current abstractive summarization models in this task, such as understanding the sender\u2019s intent and identifying the roles of sender and receiver. furthermore, we find that widely used automatic evaluation metrics (rouge, bertscore) are weakly correlated with human judgments on this email thread summarization task. hence, we emphasize the importance of human evaluation and the development of better metrics by the community."], "summarization"], [["using meta-knowledge mined from identifiers to improve intent recognition in conversational systems", "claudio pinhanez | paulo cavalin | victor henrique alves ribeiro | ana appel | heloisa candello | julio nogima | mauro pichiliani | melina guerra | maira de bayser | gabriel malfatti | henrique ferreira", "in this paper we explore the improvement of intent recognition in conversational systems by the use of meta-knowledge embedded in intent identifiers. developers often include such knowledge, structure as taxonomies, in the documentation of chatbots. by using neuro-symbolic algorithms to incorporate those taxonomies into embeddings of the output space, we were able to improve accuracy in intent recognition. in datasets with intents and example utterances from 200 professional chatbots, we saw decreases in the equal error rate (eer) in more than 40% of the chatbots in comparison to the baseline of the same algorithm without the meta-knowledge. the meta-knowledge proved also to be effective in detecting out-of-scope utterances, improving the false acceptance rate (far) in two thirds of the chatbots, with decreases of 0.05 or more in far in almost 40% of the chatbots. when considering only the well-developed workspaces with a high level use of taxonomies, far decreased more than 0.05 in 77% of them, and more than 0.1 in 39% of the chatbots."], "dialogue and interactive systems"], [["discriminative reranking for neural machine translation", "ann lee | michael auli | marc\u2019aurelio ranzato", "reranking models enable the integration of rich features to select a better output hypothesis within an n-best list or lattice. these models have a long history in nlp, and we revisit discriminative reranking for modern neural machine translation models by training a large transformer architecture. this takes as input both the source sentence as well as a list of hypotheses to output a ranked list. the reranker is trained to predict the observed distribution of a desired metric, e.g. bleu, over the n-best list. since such a discriminator contains hundreds of millions of parameters, we improve its generalization using pre-training and data augmentation techniques. experiments on four wmt directions show that our discriminative reranking approach is effective and complementary to existing generative reranking approaches, yielding improvements of up to 4 bleu over the beam search output."], "machine translation and multilinguality"], [["piglet: language grounding through neuro-symbolic interaction in a 3d world", "rowan zellers | ari holtzman | matthew peters | roozbeh mottaghi | aniruddha kembhavi | ali farhadi | yejin choi", "we propose piglet: a model that learns physical commonsense knowledge through interaction, and then uses this knowledge to ground language. we factorize piglet into a physical dynamics model, and a separate language model. our dynamics model learns not just what objects are but also what they do: glass cups break when thrown, plastic ones don\u2019t. we then use it as the interface to our language model, giving us a unified model of linguistic form and grounded meaning. piglet can read a sentence, simulate neurally what might happen next, and then communicate that result through a literal symbolic representation, or natural language. experimental results show that our model effectively learns world dynamics, along with how to communicate them. it is able to correctly forecast what happens next given an english sentence over 80% of the time, outperforming a 100x larger, text-to-text approach by over 10%. likewise, its natural language summaries of physical interactions are also judged by humans as more accurate than lm alternatives. we present comprehensive analysis showing room for future work."], "language grounding to vision, robotics and beyond"], [["bart: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension", "mike lewis | yinhan liu | naman goyal | marjan ghazvininejad | abdelrahman mohamed | omer levy | veselin stoyanov | luke zettlemoyer", "we present bart, a denoising autoencoder for pretraining sequence-to-sequence models. bart is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. it uses a standard tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing bert (due to the bidirectional encoder), gpt (with the left-to-right decoder), and other recent pretraining schemes. we evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. bart is particularly effective when fine tuned for text generation but also works well for comprehension tasks. it matches the performance of roberta on glue and squad, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 rouge. bart also provides a 1.1 bleu increase over a back-translation system for machine translation, with only target language pretraining. we also replicate other pretraining schemes within the bart framework, to understand their effect on end-task performance."], "generation"], [["pral: a tailored pre-training model for task-oriented dialog generation", "jing gu | qingyang wu | chongruo wu | weiyan shi | zhou yu", "large pre-trained language generation models such as gpt-2 have demonstrated their effectiveness as language priors by reaching state-of-the-art results in various language generation tasks. however, the performance of pre-trained models on task-oriented dialog tasks is still under-explored. we propose a pre-trainedrole alternating language model (pral), explicitly designed for task-oriented conversational systems. we design several techniques: start position randomization, knowledge distillation, and history discount to improve pre-training performance. in addition, we introduce a high-quality large-scale task-oriented dialog pre-training dataset by post-prossessing13 dialog datasets. we effectively adapt pralon three downstream tasks. the results show that pral outperforms or is on par with state-of-the-art models."], "dialogue and interactive systems"], [["discovering dialog structure graph for coherent dialog generation", "jun xu | zeyang lei | haifeng wang | zheng-yu niu | hua wu | wanxiang che", "learning discrete dialog structure graph from human-human dialogs yields basic insights into the structure of conversation, and also provides background knowledge to facilitate dialog generation. however, this problem is less studied in open-domain dialogue. in this paper, we conduct unsupervised discovery of discrete dialog structure from chitchat corpora, and then leverage it to facilitate coherent dialog generation in downstream systems. to this end, we present an unsupervised model, discrete variational auto-encoder with graph neural network (dvae-gnn), to discover discrete hierarchical latent dialog states (at the level of both session and utterance) and their transitions from corpus as a dialog structure graph. then we leverage it as background knowledge to facilitate dialog management in a rl based dialog system. experimental results on two benchmark corpora confirm that dvae-gnn can discover meaningful dialog structure graph, and the use of dialog structure as background knowledge can significantly improve multi-turn coherence."], "dialogue and interactive systems"], [["improving the faithfulness of attention-based explanations with task-specific information for text classification", "george chrysostomou | nikolaos aletras", "neural network architectures in natural language processing often use attention mechanisms to produce probability distributions over input token representations. attention has empirically been demonstrated to improve performance in various tasks, while its weights have been extensively used as explanations for model predictions. recent studies (jain and wallace, 2019; serrano and smith, 2019; wiegreffe and pinter, 2019) have showed that it cannot generally be considered as a faithful explanation (jacovi and goldberg, 2020) across encoders and tasks. in this paper, we seek to improve the faithfulness of attention-based explanations for text classification. we achieve this by proposing a new family of task-scaling (tasc) mechanisms that learn task-specific non-contextualised information to scale the original attention weights. evaluation tests for explanation faithfulness, show that the three proposed variants of tasc improve attention-based explanations across two attention mechanisms, five encoders and five text classification datasets without sacrificing predictive performance. finally, we demonstrate that tasc consistently provides more faithful attention-based explanations compared to three widely-used interpretability techniques."], "interpretability and analysis of models for nlp"], [["few-shot question answering by pretraining span selection", "ori ram | yuval kirstain | jonathan berant | amir globerson | omer levy", "in several question answering benchmarks, pretrained models have reached human parity through fine-tuning on an order of 100,000 annotated questions and answers. we explore the more realistic few-shot setting, where only a few hundred training examples are available, and observe that standard models perform poorly, highlighting the discrepancy between current pretraining objectives and question answering. we propose a new pretraining scheme tailored for question answering: recurring span selection. given a passage with multiple sets of recurring spans, we mask in each set all recurring spans but one, and ask the model to select the correct span in the passage for each masked span. masked spans are replaced with a special token, viewed as a question representation, that is later used during fine-tuning to select the answer span. the resulting model obtains surprisingly good results on multiple benchmarks (e.g., 72.7 f1 on squad with only 128 training examples), while maintaining competitive performance in the high-resource setting."], "question answering"], [["lightweight cross-lingual sentence representation learning", "zhuoyuan mao | prakhar gupta | chenhui chu | martin jaggi | sadao kurohashi", "large-scale models for learning fixed-dimensional cross-lingual sentence representations like laser (artetxe and schwenk, 2019b) lead to significant improvement in performance on downstream tasks. however, further increases and modifications based on such large-scale models are usually impractical due to memory limitations. in this work, we introduce a lightweight dual-transformer architecture with just 2 layers for generating memory-efficient cross-lingual sentence representations. we explore different training tasks and observe that current cross-lingual training tasks leave a lot to be desired for this shallow architecture. to ameliorate this, we propose a novel cross-lingual language model, which combines the existing single-word masked language model with the newly proposed cross-lingual token-level reconstruction task. we further augment the training task by the introduction of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model."], "machine learning for nlp"], [["modularized interaction network for named entity recognition", "fei li | zheng wang | siu cheung hui | lejian liao | dandan song | jing xu | guoxiu he | meihuizi jia", "although the existing named entity recognition (ner) models have achieved promising performance, they suffer from certain drawbacks. the sequence labeling-based ner models do not perform well in recognizing long entities as they focus only on word-level information, while the segment-based ner models which focus on processing segment instead of single word are unable to capture the word-level dependencies within the segment. moreover, as boundary detection and type prediction may cooperate with each other for the ner task, it is also important for the two sub-tasks to mutually reinforce each other by sharing their information. in this paper, we propose a novel modularized interaction network (min) model which utilizes both segment-level information and word-level dependencies, and incorporates an interaction mechanism to support information sharing between boundary detection and type prediction to enhance the performance for the ner task. we have conducted extensive experiments based on three ner benchmark datasets. the performance results have shown that the proposed min model has outperformed the current state-of-the-art models."], "information extraction, retrieval and text mining"], [["handling extreme class imbalance in technical logbook datasets", "farhad akhbardeh | cecilia ovesdotter alm | marcos zampieri | travis desell", "technical logbooks are a challenging and under-explored text type in automated event identification. these texts are typically short and written in non-standard yet technical language, posing challenges to off-the-shelf nlp pipelines. the granularity of issue types described in these datasets additionally leads to class imbalance, making it challenging for models to accurately predict which issue each logbook entry describes. in this paper we focus on the problem of technical issue classification by considering logbook datasets from the automotive, aviation, and facilities maintenance domains. we adapt a feedback strategy from computer vision for handling extreme class imbalance, which resamples the training data based on its error in the prediction process. our experiments show that with statistical significance this feedback strategy provides the best results for four different neural network models trained across a suite of seven different technical logbook datasets from distinct technical domains. the feedback strategy is also generic and could be applied to any learning problem with substantial class imbalances."], "resources and evaluation"], [["rational lamol: a rationale-based lifelong learning framework", "kasidis kanwatchara | thanapapas horsuwan | piyawat lertvittayakumjorn | boonserm kijsirikul | peerapon vateekul", "lifelong learning (ll) aims to train a neural network on a stream of tasks while retaining knowledge from previous tasks. however, many prior attempts in nlp still suffer from the catastrophic forgetting issue, where the model completely forgets what it just learned in the previous tasks. in this paper, we introduce rational lamol, a novel end-to-end ll framework for language models. in order to alleviate catastrophic forgetting, rational lamol enhances lamol, a recent ll model, by applying critical freezing guided by human rationales. when the human rationales are not available, we propose exploiting unsupervised generated rationales as substitutions. in the experiment, we tested rational lamol on permutations of three datasets from the eraser benchmark. the results show that our proposed framework outperformed vanilla lamol on most permutations. furthermore, unsupervised rationale generation was able to consistently improve the overall ll performance from the baseline without relying on human-annotated rationales."], "machine learning for nlp"], [["every bite is an experience: key point analysis of business reviews", "roy bar-haim | lilach eden | yoav kantor | roni friedman | noam slonim", "previous work on review summarization focused on measuring the sentiment toward the main aspects of the reviewed product or business, or on creating a textual summary. these approaches provide only a partial view of the data: aspect-based sentiment summaries lack sufficient explanation or justification for the aspect rating, while textual summaries do not quantify the significance of each element, and are not well-suited for representing conflicting views. recently, key point analysis (kpa) has been proposed as a summarization framework that provides both textual and quantitative summary of the main points in the data. we adapt kpa to review data by introducing collective key point mining for better key point extraction; integrating sentiment analysis into kpa; identifying good key point candidates for review summaries; and leveraging the massive amount of available reviews and their metadata. we show empirically that these novel extensions of kpa substantially improve its performance. we demonstrate that promising results can be achieved without any domain-specific annotation, while human supervision can lead to further improvement."], "sentiment analysis, stylistic analysis, and argument mining"], [["regression bugs are in your model! measuring, reducing and analyzing regressions in nlp model updates", "yuqing xie | yi-an lai | yuanjun xiong | yi zhang | stefano soatto", "behavior of deep neural networks can be inconsistent between different versions. regressions during model update are a common cause of concern that often over-weigh the benefits in accuracy or efficiency gain. this work focuses on quantifying, reducing and analyzing regression errors in the nlp model updates. using negative flip rate as regression measure, we show that regression has a prevalent presence across tasks in the glue benchmark. we formulate the regression-free model updates into a constrained optimization problem, and further reduce it into a relaxed form which can be approximately optimized through knowledge distillation training method. we empirically analyze how model ensemble reduces regression. finally, we conduct checklist behavioral testing to understand the distribution of regressions across linguistic phenomena, and the efficacy of ensemble and distillation methods."], "nlp applications"], [["improving factual consistency of abstractive summarization via question answering", "feng nan | cicero nogueira dos santos | henghui zhu | patrick ng | kathleen mckeown | ramesh nallapati | dejiao zhang | zhiguo wang | andrew o. arnold | bing xiang", "a commonly observed problem with the state-of-the art abstractive summarization models is that the generated summaries can be factually inconsistent with the input documents. the fact that automatic summarization may produce plausible-sounding yet inaccurate summaries is a major concern that limits its wide application. in this paper we present an approach to address factual consistency in summarization. we first propose an efficient automatic evaluation metric to measure factual consistency; next, we propose a novel learning algorithm that maximizes the proposed metric during model training. through extensive experiments, we confirm that our method is effective in improving factual consistency and even overall quality of the summaries, as judged by both automatic metrics and human evaluation."], "summarization"], [["learning syntactic dense embedding with correlation graph for automatic readability assessment", "xinying qiu | yuan chen | hanwu chen | jian-yun nie | yuming shen | dawei lu", "deep learning models for automatic readability assessment generally discard linguistic features traditionally used in machine learning models for the task. we propose to incorporate linguistic features into neural network models by learning syntactic dense embeddings based on linguistic features. to cope with the relationships between the features, we form a correlation graph among features and use it to learn their embeddings so that similar features will be represented by similar embeddings. experiments with six data sets of two proficiency levels demonstrate that our proposed methodology can complement bert-only model to achieve significantly better performances for automatic readability assessment."], "nlp applications"], [["can transformer models measure coherence in text: re-thinking the shuffle test", "philippe laban | luke dai | lucas bandarkar | marti a. hearst", "the shuffle test is the most common task to evaluate whether nlp models can measure coherence in text. most recent work uses direct supervision on the task; we show that by simply finetuning a roberta model, we can achieve a near perfect accuracy of 97.8%, a state-of-the-art. we argue that this outstanding performance is unlikely to lead to a good model of text coherence, and suggest that the shuffle test should be approached in a zero-shot setting: models should be evaluated without being trained on the task itself. we evaluate common models in this setting, such as generative and bi-directional transformers, and find that larger architectures achieve high-performance out-of-the-box. finally, we suggest the k-block shuffle test, a modification of the original by increasing the size of blocks shuffled. even though human reader performance remains high (around 95% accuracy), model performance drops from 94% to 78% as block size increases, creating a conceptually simple challenge to benchmark nlp models."], "resources and evaluation"], [["aspect-category-opinion-sentiment quadruple extraction with implicit aspects and opinions", "hongjie cai | rui xia | jianfei yu", "product reviews contain a large number of implicit aspects and implicit opinions. however, most of the existing studies in aspect-based sentiment analysis ignored this problem. in this work, we introduce a new task, named aspect-category-opinion-sentiment (acos) quadruple extraction, with the goal to extract all aspect-category-opinion-sentiment quadruples in a review sentence and provide full support for aspect-based sentiment analysis with implicit aspects and opinions. we furthermore construct two new datasets, restaurant-acos and laptop-acos, for this new task, both of which contain the annotations of not only aspect-category-opinion-sentiment quadruples but also implicit aspects and opinions. the former is an extension of the semeval restaurant dataset; the latter is a newly collected and annotated laptop dataset, twice the size of the semeval laptop dataset. we finally benchmark the task with four baseline systems. experiments demonstrate the feasibility of the new task and its effectiveness in extracting and describing implicit aspects and implicit opinions. the two datasets and source code of four systems are publicly released at https://github.com/nustm/acos."], "sentiment analysis, stylistic analysis, and argument mining"]]